id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/691#issuecomment-502557529:657,Availability,avail,available,657,"Hi @LuckyMD,. Thank you for your rapid reply!; In mnnCorrect's paper, the authors claimed that ComBat cannot be applied to some single-cell RNA sequencing data, since there are always multiple different cell types in each dataset, how do you think about that? ; Maybe, ComBat cannot handle well with the cases where different cell types are influenced by the batch effect in different ways or levels.; I am afraid that batch effects are not accurately corrected, and I am still puzzled about which method may give better results, i.e., calculating marker genes basing on batch-corrected data or including batch as a covariate in the raw data. (Is there any available paper discussing this problem?). In addition, I will check `adata.var` soon. Thanks,; BP",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-502557529
https://github.com/scverse/scanpy/issues/691#issuecomment-502582404:1535,Availability,avail,available,1535,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:; 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods.; 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-502582404
https://github.com/scverse/scanpy/issues/691#issuecomment-502582404:991,Safety,detect,detection,991,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:; 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods.; 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-502582404
https://github.com/scverse/scanpy/issues/691#issuecomment-502582404:1139,Safety,detect,detect,1139,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:; 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods.; 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-502582404
https://github.com/scverse/scanpy/issues/691#issuecomment-502582404:1234,Safety,detect,detection,1234,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:; 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods.; 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-502582404
https://github.com/scverse/scanpy/issues/691#issuecomment-502582404:1412,Testability,test,testing,1412,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:; 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods.; 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-502582404
https://github.com/scverse/scanpy/issues/691#issuecomment-502582404:699,Usability,simpl,simple,699,"Hi @genecell,. We have a review paper on current best-practices in scRNA-seq analysis which is coming out soon in Molecular Systems Biology that discusses this a bit. The issue with batch correction in scRNA-seq data isn't that batch affects different cell types differently, but rather that if cell type compositions change between batches, then transcriptional differences between the cell types that differ between the batches confound the technical batch effect estimation. So you end up correcting for more than just the technical effect. This means that you can use Combat if the cell type compositions are expected to be similar between batches. Indeed, ComBat is shown to outperform MNN for simple batch correction scenarios ([kBet paper](http://www.nature.com/articles/s41592-018-0254-1)). Inspite of the above argument, the better way to do things is definitely to include batch as a covariate. That way you don't underestimate your background variance. In the case of marker gene detection, this is not quite so problematic as:; 1. It is an easy problem, as cell-type differences tend to be very pronounced so you should always detect a signal even with non-optimal methods.; 2. The p-values you calculate from marker gene detection are inflated anyway and therefore not meaningful. We discuss the above points in our manuscript. I'm not aware whether using corrected data for differential expression testing is discussed anywhere else though. If you email me, I could forward you a copy of the manuscript, but it should be available in MSB in the next weeks. The issue with inflated p-values is also discussed is a few other places like [here](https://www.biorxiv.org/content/early/2018/11/05/463265).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-502582404
https://github.com/scverse/scanpy/issues/691#issuecomment-503083216:164,Performance,perform,performance,164,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward.; In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:; ```; tmp_cluster=adata.obs['leiden'].astype(int); ```; ```; %%R -i tmp_cluster -i adata -o tmp_allMarkers; tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""); tmp_allMarkers<-as.list(tmp_allMarkers); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-503083216
https://github.com/scverse/scanpy/issues/691#issuecomment-503083216:189,Usability,simpl,simple,189,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward.; In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:; ```; tmp_cluster=adata.obs['leiden'].astype(int); ```; ```; %%R -i tmp_cluster -i adata -o tmp_allMarkers; tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""); tmp_allMarkers<-as.list(tmp_allMarkers); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-503083216
https://github.com/scverse/scanpy/issues/692#issuecomment-504362945:160,Usability,simpl,simply,160,Actually the gray is hardcoded:. https://github.com/theislab/scanpy/blob/b3dc34a57ccaa6ac9a4ac8718fe9f128c967e3dc/scanpy/plotting/_anndata.py#L303. But you can simply use a color-like string to specify your favourite color (Unless you happend to have an `.obs` column named `#fe57a1` or so :grin:):. https://github.com/theislab/scanpy/blob/b3dc34a57ccaa6ac9a4ac8718fe9f128c967e3dc/scanpy/plotting/_anndata.py#L384-L386,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692#issuecomment-504362945
https://github.com/scverse/scanpy/issues/692#issuecomment-505559808:0,Usability,Simpl,Simply,0,Simply adding the matplotlib `is_color_like` seems to do the trick; ```; and (color is None or color in adata.obs.keys() or color in adata.var.index or is_color_like(color))):; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/692#issuecomment-505559808
https://github.com/scverse/scanpy/issues/695#issuecomment-503465544:52,Availability,down,downgrade,52,"Hi,. This might have to do with scipy 1.3.0. If you downgrade to 1.2.1 this should work for the moment. Scipy 1.3.0 compatibility is being fixed atm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/695#issuecomment-503465544
https://github.com/scverse/scanpy/issues/696#issuecomment-503465631:52,Availability,down,downgrade,52,"Hi,. This might have to do with scipy 1.3.0. If you downgrade to 1.2.1 this should work for the moment. Scipy 1.3.0 compatibility is being fixed atm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/696#issuecomment-503465631
https://github.com/scverse/scanpy/issues/698#issuecomment-513445670:169,Testability,test,testing,169,"Great, just wanted to make sure that we had that out of the way first. About the tie correction, I'm not the most knowledgeable person about our differential expression testing. Maybe @falexwolf or @a-munoz-rojas would be able to comment on this?. @idavydov, what do you think our results should be? Is there a gold standard in scipy.stats which we should be returning the same results as?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698#issuecomment-513445670
https://github.com/scverse/scanpy/issues/698#issuecomment-528512005:336,Performance,perform,performance,336,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698#issuecomment-528512005
https://github.com/scverse/scanpy/issues/698#issuecomment-528788211:338,Modifiability,rewrite,rewrite,338,"Hi @a-munoz-rojas,. I don't think there is a way to speed-up `scipy.stats.mannwhitney`, as it expects 1d vectors; not a matrix. Regarding ties, this is a simple multiplier. So should be easy to implement or use from `scipy.stats`. I have a matrix version of `scipy.stats.mannwhitney` and `scipy.stats.tiecorrect` which is almost a 1-to-1 rewrite. I can share it in case you are interested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698#issuecomment-528788211
https://github.com/scverse/scanpy/issues/698#issuecomment-528788211:154,Usability,simpl,simple,154,"Hi @a-munoz-rojas,. I don't think there is a way to speed-up `scipy.stats.mannwhitney`, as it expects 1d vectors; not a matrix. Regarding ties, this is a simple multiplier. So should be easy to implement or use from `scipy.stats`. I have a matrix version of `scipy.stats.mannwhitney` and `scipy.stats.tiecorrect` which is almost a 1-to-1 rewrite. I can share it in case you are interested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698#issuecomment-528788211
https://github.com/scverse/scanpy/issues/699#issuecomment-504639751:395,Performance,perform,performed,395,"It looks like it's a bug in how we handle views of objects. If you copy the view you get from subsetting, that should give you the result you want, i.e.:. ```python; pbmc = sc.datasets.pbmc68k_reduced(); pbmc = pbmc[pbmc.obs['louvain'] == '0', :].copy(); sc.pp.scale(pbmc); ```. On our side, I think we should change the behavior of `ArrayViews` so they return `np.ndarrays` when operations are performed on them. Maybe we need to override `__array_ufunc__`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/699#issuecomment-504639751
https://github.com/scverse/scanpy/issues/701#issuecomment-1662515521:1024,Availability,down,down,1024,"When I run sc.rank_genes_groups() on my gene expression data, I do get separate matrices for names, scores, pvals, and pvals_adj. However, the scores don't match the pvals. In other words, with decreasing pvals, we don't always have increasing score. I did check out the description on the main documentations page, and they say they're calculating the zscores underlying the distribution, however if that's the case shouldn't it always be higher with decreasing pval? Also, I went through the code: it looks like they're calculating the scores on the absolute values instead of the real values--why is this? Are the scores basically U1 values corresponding to the pvalues, in whcih case once again lower pvalues should always have higher scores right?; ares calculated from the p-values? What'e the relation between the two. I have ran sc.rank_genes_groups() on my gene expression data, and I have generated the matrix for cluster-1 versus the rest, for reference. You can see that one, the pavlues don't increase as we go down the rows; and two, the scores seem kinda arbitrary to the p-values. What am I missing here? Thanks a lot, and sorry for the wordy question. [ClusterOneVsRest.csv](https://github.com/scverse/scanpy/files/12242378/ClusterOneVsRest.csv)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/701#issuecomment-1662515521
https://github.com/scverse/scanpy/issues/702#issuecomment-527330223:54,Deployability,integrat,integrate,54,"> Great, thank you!. hi, did you find the ""merge"" or ""integrate"" commond in scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527330223
https://github.com/scverse/scanpy/issues/702#issuecomment-527330223:54,Integrability,integrat,integrate,54,"> Great, thank you!. hi, did you find the ""merge"" or ""integrate"" commond in scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527330223
https://github.com/scverse/scanpy/issues/702#issuecomment-527337268:23,Deployability,integrat,integration,23,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268
https://github.com/scverse/scanpy/issues/702#issuecomment-527337268:23,Integrability,integrat,integration,23,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268
https://github.com/scverse/scanpy/issues/702#issuecomment-527337268:253,Usability,simpl,simpler,253,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268
https://github.com/scverse/scanpy/issues/702#issuecomment-527391920:31,Deployability,integrat,integration,31,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527391920
https://github.com/scverse/scanpy/issues/702#issuecomment-527391920:31,Integrability,integrat,integration,31,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527391920
https://github.com/scverse/scanpy/issues/702#issuecomment-527391920:261,Usability,simpl,simpler,261,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527391920
https://github.com/scverse/scanpy/issues/702#issuecomment-527731457:31,Deployability,integrat,integration,31,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. ; I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: ; adata001$Sample <- ""001""; adata002$Sample <- ""002""; adata002$Sample <- ""003""; adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11); adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457
https://github.com/scverse/scanpy/issues/702#issuecomment-527731457:817,Deployability,Integrat,IntegrateData,817,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. ; I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: ; adata001$Sample <- ""001""; adata002$Sample <- ""002""; adata002$Sample <- ""003""; adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11); adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457
https://github.com/scverse/scanpy/issues/702#issuecomment-527731457:31,Integrability,integrat,integration,31,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. ; I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: ; adata001$Sample <- ""001""; adata002$Sample <- ""002""; adata002$Sample <- ""003""; adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11); adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457
https://github.com/scverse/scanpy/issues/702#issuecomment-527731457:817,Integrability,Integrat,IntegrateData,817,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. ; I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: ; adata001$Sample <- ""001""; adata002$Sample <- ""002""; adata002$Sample <- ""003""; adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11); adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457
https://github.com/scverse/scanpy/issues/702#issuecomment-527731457:261,Usability,simpl,simpler,261,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. ; I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: ; adata001$Sample <- ""001""; adata002$Sample <- ""002""; adata002$Sample <- ""003""; adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11); adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457
https://github.com/scverse/scanpy/issues/702#issuecomment-527750807:350,Performance,perform,performs,350,"Just concatenate the datasets first and then use Combat. Something like:; ```; adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'); sc.pp.combat(adata_merge, batch='sample'); ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527750807
https://github.com/scverse/scanpy/issues/702#issuecomment-527750807:314,Usability,simpl,simple,314,"Just concatenate the datasets first and then use Combat. Something like:; ```; adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'); sc.pp.combat(adata_merge, batch='sample'); ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527750807
https://github.com/scverse/scanpy/issues/702#issuecomment-527750807:619,Usability,usab,usable,619,"Just concatenate the datasets first and then use Combat. Something like:; ```; adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'); sc.pp.combat(adata_merge, batch='sample'); ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527750807
https://github.com/scverse/scanpy/issues/702#issuecomment-527754924:370,Performance,perform,performs,370,"> Just concatenate the datasets first and then use Combat. Something like:; > ; > ```; > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'); > sc.pp.combat(adata_merge, batch='sample'); > ```; > ; > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527754924
https://github.com/scverse/scanpy/issues/702#issuecomment-527754924:334,Usability,simpl,simple,334,"> Just concatenate the datasets first and then use Combat. Something like:; > ; > ```; > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'); > sc.pp.combat(adata_merge, batch='sample'); > ```; > ; > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527754924
https://github.com/scverse/scanpy/issues/702#issuecomment-527754924:639,Usability,usab,usable,639,"> Just concatenate the datasets first and then use Combat. Something like:; > ; > ```; > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'); > sc.pp.combat(adata_merge, batch='sample'); > ```; > ; > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527754924
https://github.com/scverse/scanpy/issues/702#issuecomment-1431464728:57,Availability,error,error,57,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`.; I have multiple h5ad files with varying n_obs × n_vars. Here is my code:; ```adatas = [an.read_h5ad(filename) for filename in filenames]; batch_names = []; for i in range(len(adatas)):; adatas[i].var_names_make_unique(); batch_names.append(filenames[i].split('.')[0]); print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],; batch_key = 'ID',; uns_merge=""unique"",; index_unique=None,; batch_categories=batch_names); ``` . and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-1431464728
https://github.com/scverse/scanpy/issues/702#issuecomment-1431464728:588,Availability,error,error,588,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`.; I have multiple h5ad files with varying n_obs × n_vars. Here is my code:; ```adatas = [an.read_h5ad(filename) for filename in filenames]; batch_names = []; for i in range(len(adatas)):; adatas[i].var_names_make_unique(); batch_names.append(filenames[i].split('.')[0]); print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],; batch_key = 'ID',; uns_merge=""unique"",; index_unique=None,; batch_categories=batch_names); ``` . and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-1431464728
https://github.com/scverse/scanpy/pull/703#issuecomment-504923377:159,Performance,load,loaded,159,Nice! This was the original inspiration for having `scanpy.api` btw: `from scanpy.preprocessing import x` was instantaneous because all the other stuff wasn’t loaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/703#issuecomment-504923377
https://github.com/scverse/scanpy/pull/704#issuecomment-505415647:13,Testability,test,test,13,"I started to test also with 3.8, but sadly there’s a bug in scipy: scipy/scipy#10354",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-505415647
https://github.com/scverse/scanpy/pull/704#issuecomment-505432318:190,Integrability,depend,dependencies,190,"I think the `<` should work, it's part of [PEP-508](https://www.python.org/dev/peps/pep-0508/#motivation). At least that's what [this blog post](https://hynek.me/articles/conditional-python-dependencies/) says. Didn't those builds just fail before they got to that requirement?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-505432318
https://github.com/scverse/scanpy/pull/704#issuecomment-505780384:230,Availability,error,error,230,"You’re right! I wasn’t aware of PEP 508. The build appears to fail [here](https://travis-ci.org/theislab/scanpy/jobs/550248884#L285), but actually fails much later, due to numpy/numpy#13790. I filed pypa/pip#6651 to deal with the error being unclear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-505780384
https://github.com/scverse/scanpy/pull/704#issuecomment-506145082:57,Deployability,release,release,57,I don't think we need to test against 3.8 until a stable release is out. I'm thinking we can just drop that from travis and merge?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-506145082
https://github.com/scverse/scanpy/pull/704#issuecomment-506145082:25,Testability,test,test,25,I don't think we need to test against 3.8 until a stable release is out. I'm thinking we can just drop that from travis and merge?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-506145082
https://github.com/scverse/scanpy/pull/704#issuecomment-511887782:15,Availability,error,error,15,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-511887782
https://github.com/scverse/scanpy/pull/704#issuecomment-511887782:21,Performance,load,loading,21,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-511887782
https://github.com/scverse/scanpy/pull/704#issuecomment-511887782:97,Performance,load,loading,97,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-511887782
https://github.com/scverse/scanpy/pull/704#issuecomment-511887782:113,Usability,learn,learn,113,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-511887782
https://github.com/scverse/scanpy/pull/705#issuecomment-506889693:81,Modifiability,refactor,refactored,81,"@ivirshup this looks fine to me, but i just took the cell ranger version for the refactored `highly_variable_genes ` from the last version, so also not an expert. But again, this looks right.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705#issuecomment-506889693
https://github.com/scverse/scanpy/pull/705#issuecomment-507008710:450,Deployability,release,release,450,"@Koncopd, ah, this was as far back as I could trace the code initially: https://github.com/theislab/scanpy/commit/af4a82a2540eee65c732cb5e401d2145846e6d97. Now I've found an earlier commit from @falexwolf at https://github.com/theislab/scanpy/commit/43e71fe8577a8b3a51dc2117bd431911001d9869. @falexwolf, does this change look right to you?. @LuckyMD, I'm not sure if this would count as backward breaking if it's a bug. Should definitely go into the release notes, maybe as warning.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705#issuecomment-507008710
https://github.com/scverse/scanpy/pull/705#issuecomment-520516186:41,Usability,clear,clear,41,"@falexwolf @ivirshup it would be good to clear this up soon. If this is correct, then we choose HVGs incorrectly every time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705#issuecomment-520516186
https://github.com/scverse/scanpy/issues/706#issuecomment-505335006:702,Testability,test,test,702,"Hi,. I've tried to reproduce this with scanpy 1.4.3+80.g740c557 on the pbmc68k_reduced dataset and it works for me. I did the following:. ```; adata = sc.datasets.pbmc68k_reduced() ; sc.pp.filter_cells(adata, min_counts=10) ; sc.pp.filter_genes(adata, min_cells=5) ; sc.pp.normalize_per_cell(adata) ; sc.pp.log1p(adata); sc.tl.rank_genes_groups(adata, groupby='bulk_labels', groups=['CD56+ NK', 'Dendritic', 'CD34+'], reference='rest', method='wilcoxon', corr_method='benjamini-hochberg', log_transformed=True); ```. The `sc.tl.rank_genes_groups()` call is taken more or less from your example. Could you check that this works for you, and otherwise provide a minimal reproducible example that I could test?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/706#issuecomment-505335006
https://github.com/scverse/scanpy/issues/707#issuecomment-505387662:75,Modifiability,variab,variable,75,"Hi @Khalid-Usman,. Regressing out should indeed be performed before highly variable gene selection. This was not in the original scRNA-seq tutorials from Seurat and Scanpy though. If you're interested in a current best-practices tutorial (based on scanpy, but also including R tools), you can find it [here](https://www.github.com/theislab/single-cell-tutorial). The reason it might not have been done on all genes initially is for speed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707#issuecomment-505387662
https://github.com/scverse/scanpy/issues/707#issuecomment-505387662:51,Performance,perform,performed,51,"Hi @Khalid-Usman,. Regressing out should indeed be performed before highly variable gene selection. This was not in the original scRNA-seq tutorials from Seurat and Scanpy though. If you're interested in a current best-practices tutorial (based on scanpy, but also including R tools), you can find it [here](https://www.github.com/theislab/single-cell-tutorial). The reason it might not have been done on all genes initially is for speed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707#issuecomment-505387662
https://github.com/scverse/scanpy/issues/707#issuecomment-841394437:409,Modifiability,variab,variable,409,"Is HVG function designed to be done after regress out?; ```; # Regress out process of interest from expression data; sc.pp.regress_out(adata_b_rn_sub2, keys='LogReg_decision') ; # Find HVGs (across samples, not per sample as samples are very different in terms of beta bio); sc.pp.highly_variable_genes(adata_b_rn_sub2, flavor='cell_ranger', ; batch_key=None, n_top_genes =2000); print('\n','Number of highly variable genes: {:d}'.format(; np.sum(adata_b_rn_sub2.var['highly_variable']))); rcParams['figure.figsize']=(10,5); sc.pl.highly_variable_genes(adata_b_rn_sub2); ```; ![image](https://user-images.githubusercontent.com/47607471/118307048-739dcb00-b4ea-11eb-86fa-904c6b2bf37f.png); If I do scale before HVG; ![image](https://user-images.githubusercontent.com/47607471/118307211-ab0c7780-b4ea-11eb-8c86-44ab1457cb40.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707#issuecomment-841394437
https://github.com/scverse/scanpy/issues/710#issuecomment-507120679:78,Usability,learn,learn,78,"Have you tried passing your PCA directly to a UMAP transformer from the [umap-learn](https://umap-learn.readthedocs.io/en/latest/) library? I'm wondering if the memory usage is due to the underlying layout code, or if it's how we handle it here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710#issuecomment-507120679
https://github.com/scverse/scanpy/issues/710#issuecomment-507120679:98,Usability,learn,learn,98,"Have you tried passing your PCA directly to a UMAP transformer from the [umap-learn](https://umap-learn.readthedocs.io/en/latest/) library? I'm wondering if the memory usage is due to the underlying layout code, or if it's how we handle it here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710#issuecomment-507120679
https://github.com/scverse/scanpy/issues/712#issuecomment-505890739:101,Testability,test,test,101,"Just checked on this example; ```; adata = sc.AnnData(np.array([[1, 2],[-1, 2]])); adata.write_loom('test.loom'); adata = sc.read('test.loom', sparse=True); ```; and it looks fine, with retaining the negative value, for both sparse and non-sparse read-in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/712#issuecomment-505890739
https://github.com/scverse/scanpy/issues/712#issuecomment-505890739:131,Testability,test,test,131,"Just checked on this example; ```; adata = sc.AnnData(np.array([[1, 2],[-1, 2]])); adata.write_loom('test.loom'); adata = sc.read('test.loom', sparse=True); ```; and it looks fine, with retaining the negative value, for both sparse and non-sparse read-in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/712#issuecomment-505890739
https://github.com/scverse/scanpy/pull/713#issuecomment-509109864:104,Testability,test,tests,104,@ivirshup I didn't notice until now that you requested the review. The code looks ok but I will do some tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/713#issuecomment-509109864
https://github.com/scverse/scanpy/issues/718#issuecomment-507264814:159,Availability,echo,echo,159,"Based on my experience setting a single cutoff for all datasets will not work, as I've used a lot of different cutoffs depending on the distributions. I would echo @ivirshup's suggestion of looking at distributions. Joint distributions being a lot more important than individual histograms. There's a small discussion about it in our [best practices paper](https://www.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718#issuecomment-507264814
https://github.com/scverse/scanpy/issues/718#issuecomment-507264814:119,Integrability,depend,depending,119,"Based on my experience setting a single cutoff for all datasets will not work, as I've used a lot of different cutoffs depending on the distributions. I would echo @ivirshup's suggestion of looking at distributions. Joint distributions being a lot more important than individual histograms. There's a small discussion about it in our [best practices paper](https://www.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718#issuecomment-507264814
https://github.com/scverse/scanpy/issues/718#issuecomment-507267593:418,Availability,echo,echo,418,"Thanks everyone! I wonder how this affects one-pipeline-for-everything; portals, like the EBI single cell expression atlas... and standarized; pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>; wrote:. > Based on my experience setting a single cutoff for all datasets will not; > work, as I've used a lot of different cutoffs depending on the; > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s; > suggestion of looking at distributions. Joint distributions being a lot; > more important than individual histograms. There's a small discussion about; > it in our best practices paper; > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718#issuecomment-507267593
https://github.com/scverse/scanpy/issues/718#issuecomment-507267593:47,Deployability,pipeline,pipeline-for-everything,47,"Thanks everyone! I wonder how this affects one-pipeline-for-everything; portals, like the EBI single cell expression atlas... and standarized; pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>; wrote:. > Based on my experience setting a single cutoff for all datasets will not; > work, as I've used a lot of different cutoffs depending on the; > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s; > suggestion of looking at distributions. Joint distributions being a lot; > more important than individual histograms. There's a small discussion about; > it in our best practices paper; > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718#issuecomment-507267593
https://github.com/scverse/scanpy/issues/718#issuecomment-507267593:143,Deployability,pipeline,pipelines,143,"Thanks everyone! I wonder how this affects one-pipeline-for-everything; portals, like the EBI single cell expression atlas... and standarized; pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>; wrote:. > Based on my experience setting a single cutoff for all datasets will not; > work, as I've used a lot of different cutoffs depending on the; > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s; > suggestion of looking at distributions. Joint distributions being a lot; > more important than individual histograms. There's a small discussion about; > it in our best practices paper; > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718#issuecomment-507267593
https://github.com/scverse/scanpy/issues/718#issuecomment-507267593:375,Integrability,depend,depending,375,"Thanks everyone! I wonder how this affects one-pipeline-for-everything; portals, like the EBI single cell expression atlas... and standarized; pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>; wrote:. > Based on my experience setting a single cutoff for all datasets will not; > work, as I've used a lot of different cutoffs depending on the; > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s; > suggestion of looking at distributions. Joint distributions being a lot; > more important than individual histograms. There's a small discussion about; > it in our best practices paper; > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718#issuecomment-507267593
https://github.com/scverse/scanpy/issues/718#issuecomment-507326463:80,Safety,safe,safe,80,Google can't find 4000 and UMI on their website:. https://www.google.com/search?safe=off&ei=PC4aXYaLBcWAad_4vdAG&q=%224000%22++inurl%3A10xgenomics+umi&oq=%224000%22++inurl%3A10xgenomics+umi&gs_l=psy-ab.3...3637.3826..3984...0.0..0.76.218.3......0....1..gws-wiz.......0i71.oYAb2OqUy7I. >,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718#issuecomment-507326463
https://github.com/scverse/scanpy/issues/719#issuecomment-507260999:221,Integrability,depend,depend,221,"Just for reasons of practicality. I figured it would create a mess to visualize more than 10 plots with different groups. Also, you should have sufficient cells per group to make the kde calculation meaningful. That will depend on the number of cells as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719#issuecomment-507260999
https://github.com/scverse/scanpy/issues/719#issuecomment-560101154:258,Availability,reliab,reliable,258,"This looks really cool. I don't see how this is a solution to the many groups issue though. Especially as you'd likely have the densities of many groups overlapping in the same area. Otherwise, I don't really know of a heuristic to assess whether the kde is reliable. I am okay with removing the 10 groups threshold and just letting the user deal with the mess... but maybe that's not the kindest thing to do. This is essentially a discussion of flexibility vs ease-of-use. Might merit a more principled discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719#issuecomment-560101154
https://github.com/scverse/scanpy/issues/719#issuecomment-875362308:122,Integrability,wrap,wrapper,122,"Having run into the limitations of constraining to 10 categories for quite some time, and now seeing @sunnysun515 write a wrapper that brings the functionality and produces meaningful plots: I'd strongly advocate for letting the user decide!. To address concerns raised above, I added a warning to the docstring that enough cells per category are needed for a meaningful estimate: https://github.com/theislab/scanpy/pull/1936/commits/760a967b57b93b0da3296d98430509627f3a80d7. [Imagining a warning showing up whenever the function is called seems more disruptive.]. Otherwise, it'd be great if @LuckyMD and @sunnysun515 could take a look in addition to Isaac: https://github.com/theislab/scanpy/pull/1936",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719#issuecomment-875362308
https://github.com/scverse/scanpy/issues/722#issuecomment-509119409:190,Modifiability,variab,variable,190,"I think that makes sense to allow keeping a measure of magnitude, potentially implemented as an option, like with `sc.pp.scale`s `zero_center`. I'd be interested to see how different highly variable gene selection was on data transformed this way, vs the batched approach we have now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722#issuecomment-509119409
https://github.com/scverse/scanpy/issues/722#issuecomment-509687449:414,Deployability,continuous,continuous,414,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then?. As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722#issuecomment-509687449
https://github.com/scverse/scanpy/issues/722#issuecomment-509687449:627,Deployability,integrat,integration,627,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then?. As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722#issuecomment-509687449
https://github.com/scverse/scanpy/issues/722#issuecomment-509687449:627,Integrability,integrat,integration,627,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then?. As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722#issuecomment-509687449
https://github.com/scverse/scanpy/issues/722#issuecomment-1519822911:19,Deployability,update,updates,19,"Hi @LuckyMD ,; Any updates regarding this issue? I am fairly new to scanpy and I am working on implementing regress_out() and finding HVG in the best way possible. I keep wondering whether or not I should regress out and scale before or after finding HVG. Any tips/updates? Everything is welcome :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722#issuecomment-1519822911
https://github.com/scverse/scanpy/issues/722#issuecomment-1519822911:265,Deployability,update,updates,265,"Hi @LuckyMD ,; Any updates regarding this issue? I am fairly new to scanpy and I am working on implementing regress_out() and finding HVG in the best way possible. I keep wondering whether or not I should regress out and scale before or after finding HVG. Any tips/updates? Everything is welcome :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722#issuecomment-1519822911
https://github.com/scverse/scanpy/issues/723#issuecomment-526079225:306,Deployability,update,update,306,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased; 2. replace lists `rankings_gene_...` by DataFrame; 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test; 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723#issuecomment-526079225
https://github.com/scverse/scanpy/issues/723#issuecomment-526079225:317,Testability,test,test,317,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased; 2. replace lists `rankings_gene_...` by DataFrame; 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test; 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723#issuecomment-526079225
https://github.com/scverse/scanpy/issues/723#issuecomment-526079225:346,Testability,log,logreg,346,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased; 2. replace lists `rankings_gene_...` by DataFrame; 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test; 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723#issuecomment-526079225
https://github.com/scverse/scanpy/issues/723#issuecomment-526079225:209,Usability,simpl,simplifying,209,"Some notes from a brief discussion with Sergei. 1. make helper functions for each method so that level of indentation and length is decreased; 2. replace lists `rankings_gene_...` by DataFrame; 3. think about simplifying the wilcoxon implementation, compare with scipy stats implementation and potentially update the test; 4. investigate how the logreg implementation behaves for different choices of reference groups",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723#issuecomment-526079225
https://github.com/scverse/scanpy/issues/723#issuecomment-526515294:61,Availability,avail,available,61,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON; def rank_genes_groups_df(adata, key='rank_genes_groups'):; # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, ; # logfoldchanges, pvals). ; # Ideally, the list of columns should be consistent between methods; # but 'logreg' does not return logfoldchanges for example. dd = []; groupby = adata.uns['rank_genes_groups']['params']['groupby']; for group in adata.obs[groupby].cat.categories:; cols = []; # inner loop to make data frame by concatenating the columns per group; for col in adata.uns[key].keys():; if col != 'params':; cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])); ; df = pd.concat(cols,axis=1); df['group'] = group; dd.append(df). # concatenate the individual group data frames into one long data frame; rgg = pd.concat(dd); rgg['group'] = rgg['group'].astype('category'); return rgg.set_index('group'); ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723#issuecomment-526515294
https://github.com/scverse/scanpy/issues/723#issuecomment-526515294:344,Testability,log,logfoldchanges,344,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON; def rank_genes_groups_df(adata, key='rank_genes_groups'):; # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, ; # logfoldchanges, pvals). ; # Ideally, the list of columns should be consistent between methods; # but 'logreg' does not return logfoldchanges for example. dd = []; groupby = adata.uns['rank_genes_groups']['params']['groupby']; for group in adata.obs[groupby].cat.categories:; cols = []; # inner loop to make data frame by concatenating the columns per group; for col in adata.uns[key].keys():; if col != 'params':; cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])); ; df = pd.concat(cols,axis=1); df['group'] = group; dd.append(df). # concatenate the individual group data frames into one long data frame; rgg = pd.concat(dd); rgg['group'] = rgg['group'].astype('category'); return rgg.set_index('group'); ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723#issuecomment-526515294
https://github.com/scverse/scanpy/issues/723#issuecomment-526515294:446,Testability,log,logreg,446,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON; def rank_genes_groups_df(adata, key='rank_genes_groups'):; # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, ; # logfoldchanges, pvals). ; # Ideally, the list of columns should be consistent between methods; # but 'logreg' does not return logfoldchanges for example. dd = []; groupby = adata.uns['rank_genes_groups']['params']['groupby']; for group in adata.obs[groupby].cat.categories:; cols = []; # inner loop to make data frame by concatenating the columns per group; for col in adata.uns[key].keys():; if col != 'params':; cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])); ; df = pd.concat(cols,axis=1); df['group'] = group; dd.append(df). # concatenate the individual group data frames into one long data frame; rgg = pd.concat(dd); rgg['group'] = rgg['group'].astype('category'); return rgg.set_index('group'); ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723#issuecomment-526515294
https://github.com/scverse/scanpy/issues/723#issuecomment-526515294:470,Testability,log,logfoldchanges,470,"Can rank_genes_groups be linked to use diffxpy on top of the available methods? . I am using the following code to convert the output of rank_genes_groups to a data frame, in case is useful:. ```PYTHON; def rank_genes_groups_df(adata, key='rank_genes_groups'):; # create a data frame with columns from .uns['rank_genes_groups'] (eg. names, ; # logfoldchanges, pvals). ; # Ideally, the list of columns should be consistent between methods; # but 'logreg' does not return logfoldchanges for example. dd = []; groupby = adata.uns['rank_genes_groups']['params']['groupby']; for group in adata.obs[groupby].cat.categories:; cols = []; # inner loop to make data frame by concatenating the columns per group; for col in adata.uns[key].keys():; if col != 'params':; cols.append(pd.DataFrame(adata.uns[key][col][group], columns=[col])); ; df = pd.concat(cols,axis=1); df['group'] = group; dd.append(df). # concatenate the individual group data frames into one long data frame; rgg = pd.concat(dd); rgg['group'] = rgg['group'].astype('category'); return rgg.set_index('group'); ```. This results on a table like this:. ![image](https://user-images.githubusercontent.com/4964309/64006299-5789a880-cb12-11e9-9196-305a318b9395.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723#issuecomment-526515294
https://github.com/scverse/scanpy/pull/724#issuecomment-508335411:4,Testability,test,test,4,"The test takes a while to run. I'm not sure how much of that is this being the first to import UMAP, but I'll look into it before merging.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724#issuecomment-508335411
https://github.com/scverse/scanpy/pull/724#issuecomment-510394228:59,Energy Efficiency,reduce,reduced,59,"Reducing the size of the dataset used from 700 to 100 only reduced total time from ~7.5 seconds to 6 seconds, so I think it's mostly the UMAP import. I've done that anyways and fixed a warning in the embedding plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724#issuecomment-510394228
https://github.com/scverse/scanpy/pull/724#issuecomment-510402103:54,Testability,test,test,54,"if we import everything slow in conftest.py, then the test times won’t be misleading. Thanks for slaying the “issue of the beast” :metal::stuck_out_tongue_closed_eyes:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724#issuecomment-510402103
https://github.com/scverse/scanpy/pull/724#issuecomment-510418413:195,Testability,test,tests,195,"Ha, yeah, the issue number definitely encouraged me to fix it. That's a good idea! I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724#issuecomment-510418413
https://github.com/scverse/scanpy/pull/724#issuecomment-513132199:145,Modifiability,plugin,plugin,145,"> I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests?. Apparently [it needs a plugin](https://pypi.org/project/pytest-parallel/) for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724#issuecomment-513132199
https://github.com/scverse/scanpy/pull/724#issuecomment-513132199:114,Testability,test,tests,114,"> I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests?. Apparently [it needs a plugin](https://pypi.org/project/pytest-parallel/) for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724#issuecomment-513132199
https://github.com/scverse/scanpy/issues/727#issuecomment-508614967:84,Usability,guid,guides,84,Thanks for the bug report. Could you make an example we can reproduce? There's some guides to this in the [contributing doc](https://github.com/theislab/scanpy/blob/master/CONTRIBUTING.md). It's hard to tell what's going on when we don't know what's in your file. Also version info would be helpful.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727#issuecomment-508614967
https://github.com/scverse/scanpy/issues/727#issuecomment-508621001:263,Availability,error,error,263,"Thank you~ Firstly, it's an AssertionError in sc.pp.normalize_per_cell step ; secondly , toy example csv data is presented as below:. ```; Group,Group1,Group1,Group3,Group6,Group5; Gene1,11,0,0,14,0; Gene2,12,17,9,34,11; Gene3,0,0,0,0,2; ```. so, u can test this error locally by:. ```python; df = pd.read_csv('data/dropout/dropout1/counts.csv', index_col=0); genes = df.index.values; barcodes = df.columns; adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)); adata.var_names_make_unique(); sc.pp.filter_genes(adata, min_cells=1); adata.raw = adata; sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); ```; lastly, the version is 1.4.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727#issuecomment-508621001
https://github.com/scverse/scanpy/issues/727#issuecomment-508621001:28,Testability,Assert,AssertionError,28,"Thank you~ Firstly, it's an AssertionError in sc.pp.normalize_per_cell step ; secondly , toy example csv data is presented as below:. ```; Group,Group1,Group1,Group3,Group6,Group5; Gene1,11,0,0,14,0; Gene2,12,17,9,34,11; Gene3,0,0,0,0,2; ```. so, u can test this error locally by:. ```python; df = pd.read_csv('data/dropout/dropout1/counts.csv', index_col=0); genes = df.index.values; barcodes = df.columns; adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)); adata.var_names_make_unique(); sc.pp.filter_genes(adata, min_cells=1); adata.raw = adata; sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); ```; lastly, the version is 1.4.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727#issuecomment-508621001
https://github.com/scverse/scanpy/issues/727#issuecomment-508621001:253,Testability,test,test,253,"Thank you~ Firstly, it's an AssertionError in sc.pp.normalize_per_cell step ; secondly , toy example csv data is presented as below:. ```; Group,Group1,Group1,Group3,Group6,Group5; Gene1,11,0,0,14,0; Gene2,12,17,9,34,11; Gene3,0,0,0,0,2; ```. so, u can test this error locally by:. ```python; df = pd.read_csv('data/dropout/dropout1/counts.csv', index_col=0); genes = df.index.values; barcodes = df.columns; adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)); adata.var_names_make_unique(); sc.pp.filter_genes(adata, min_cells=1); adata.raw = adata; sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); ```; lastly, the version is 1.4.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727#issuecomment-508621001
https://github.com/scverse/scanpy/issues/727#issuecomment-508640504:704,Availability,error,error,704,"@jefferyUstc I can't reproduce this with `scanpy 1.4.3` and `anndata v0.6.21 `. Here's what I ran on my machine:. ```python; from io import StringIO; import pandas as pd; import numpy as np; import scanpy as sc. csv = """"""; Group,Group1,Group1,Group3,Group6,Group5 ; Gene1,11,0,0,14,0 ; Gene2,12,17,9,34,11 ; Gene3,0,0,0,0,2; """""". df = pd.read_csv(StringIO(csv), index_col=0); genes = df.index.values; barcodes = df.columns; adata = sc.AnnData(np.transpose(df.values), var=pd.DataFrame(genes), obs=pd.DataFrame(barcodes)); adata.var_names_make_unique(); sc.pp.filter_genes(adata, min_cells=1); adata.raw = adata; sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); ```. Could you try that? If the error still occurs, could you post the traceback?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727#issuecomment-508640504
https://github.com/scverse/scanpy/issues/727#issuecomment-508692089:21,Deployability,update,update,21,"problem solved, just update `anndata` from `v0.6.19` to `v0.6.21`, Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727#issuecomment-508692089
https://github.com/scverse/scanpy/issues/727#issuecomment-1159774050:20,Availability,error,error,20,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ?. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727#issuecomment-1159774050
https://github.com/scverse/scanpy/issues/727#issuecomment-1159774050:154,Availability,error,error,154,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ?. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727#issuecomment-1159774050
https://github.com/scverse/scanpy/issues/727#issuecomment-1159774050:217,Availability,error,error,217,"Hi,. I get the same error that OP posted. I have the latest versions of both scanpy and anndata. When I run it locally on my Jupyter notebook there is no error and it works but when I run it on Jupyter lab I get that error. How should I go about fixing this ?. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/727#issuecomment-1159774050
https://github.com/scverse/scanpy/issues/728#issuecomment-508524376:103,Performance,load,load,103,"The weirdest thing is that if I write this adata object to an h5ad file with adata.write(""temp.h5ad""), load it from there and run the same command, it works. . I wonder if this indicates some issue with the .obs object or some version issue...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508524376
https://github.com/scverse/scanpy/issues/728#issuecomment-508526138:480,Testability,log,logFname,480,"Even something simple doesn't work anymore, without going through h5ad:. ```; adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]; Traceback (most recent call last):; File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>; cellbrowser.cbScanpyCli(); File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli; adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname); File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy; adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__; return self._getitem_view(index); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__; self._init_as_view(X, oidx, vidx); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view; self._raw = adata_ref.raw[oidx]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__; oidx, vidx = self._normalize_indices(index); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices; obs = _normalize_index(obs, self._adata.obs_names); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index; positions = positions[index]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__; return self._get_with(key); File ""/cluster/home/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508526138
https://github.com/scverse/scanpy/issues/728#issuecomment-508526138:15,Usability,simpl,simple,15,"Even something simple doesn't work anymore, without going through h5ad:. ```; adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]; Traceback (most recent call last):; File ""/cluster/home/max/projects/czi/cellBrowser/src/cbScanpy"", line 11, in <module>; cellbrowser.cbScanpyCli(); File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4655, in cbScanpyCli; adata, params = cbScanpy(matrixFname, metaFname, inCluster, confFname, figDir, logFname); File ""/cluster/home/max/projects/czi/cellBrowser/src/cbPyLib/cellbrowser/cellbrowser.py"", line 4353, in cbScanpy; adata = adata[adata.obs['n_genes'] < up_thrsh_genes, :]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1224, in __getitem__; return self._getitem_view(index); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1228, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 557, in __init__; self._init_as_view(X, oidx, vidx); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 629, in _init_as_view; self._raw = adata_ref.raw[oidx]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 333, in __getitem__; oidx, vidx = self._normalize_indices(index); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 361, in _normalize_indices; obs = _normalize_index(obs, self._adata.obs_names); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 160, in _normalize_index; positions = positions[index]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 911, in __getitem__; return self._get_with(key); File ""/cluster/home/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508526138
https://github.com/scverse/scanpy/issues/728#issuecomment-508769252:251,Availability,down,downgraded,251,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script; doesn't work. I'm getting the well-known ""TypeError: Categorical is not; ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one"". So; I downgraded anndata, which lead to another new error. I guess I'd also; have to downgrade pandas now. This makes me wonder if there is some testing; with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252
https://github.com/scverse/scanpy/issues/728#issuecomment-508769252:297,Availability,error,error,297,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script; doesn't work. I'm getting the well-known ""TypeError: Categorical is not; ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one"". So; I downgraded anndata, which lead to another new error. I guess I'd also; have to downgrade pandas now. This makes me wonder if there is some testing; with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252
https://github.com/scverse/scanpy/issues/728#issuecomment-508769252:330,Availability,down,downgrade,330,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script; doesn't work. I'm getting the well-known ""TypeError: Categorical is not; ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one"". So; I downgraded anndata, which lead to another new error. I guess I'd also; have to downgrade pandas now. This makes me wonder if there is some testing; with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252
https://github.com/scverse/scanpy/issues/728#issuecomment-508769252:415,Deployability,pipeline,pipeline,415,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script; doesn't work. I'm getting the well-known ""TypeError: Categorical is not; ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one"". So; I downgraded anndata, which lead to another new error. I guess I'd also; have to downgrade pandas now. This makes me wonder if there is some testing; with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252
https://github.com/scverse/scanpy/issues/728#issuecomment-508769252:438,Deployability,release,release,438,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script; doesn't work. I'm getting the well-known ""TypeError: Categorical is not; ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one"". So; I downgraded anndata, which lead to another new error. I guess I'd also; have to downgrade pandas now. This makes me wonder if there is some testing; with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252
https://github.com/scverse/scanpy/issues/728#issuecomment-508769252:390,Testability,test,testing,390,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script; doesn't work. I'm getting the well-known ""TypeError: Categorical is not; ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one"". So; I downgraded anndata, which lead to another new error. I guess I'd also; have to downgrade pandas now. This makes me wonder if there is some testing; with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252
https://github.com/scverse/scanpy/issues/728#issuecomment-508769252:31,Usability,simpl,simple,31,"even with scanpy 1.4.1 my very simple (copied from the tutorial) script; doesn't work. I'm getting the well-known ""TypeError: Categorical is not; ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one"". So; I downgraded anndata, which lead to another new error. I guess I'd also; have to downgrade pandas now. This makes me wonder if there is some testing; with a standard pipeline done before a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508769252
https://github.com/scverse/scanpy/issues/728#issuecomment-508770744:20,Availability,error,error,20,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508770744
https://github.com/scverse/scanpy/issues/728#issuecomment-508770744:238,Testability,test,tests,238,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508770744
https://github.com/scverse/scanpy/issues/728#issuecomment-508770744:251,Usability,clear,clearly,251,The max categorical error was one that I thought was addressed by anndata 0.6.18. I assume this is still on 0.6.22rc1? There was previously a switch from defaulting to ordered categoricals to unordered instead. There are quite a few unit tests... but clearly not perfect coverage. Others will be able to say more about the coverage than me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508770744
https://github.com/scverse/scanpy/issues/728#issuecomment-508906927:156,Deployability,release,releases,156,The original bug you hit was with the `sc.pl.scatter` which has few tests. I'd recommend trying out the master branches of `AnnData` and `scanpy` until new releases can be made in cases like these.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508906927
https://github.com/scverse/scanpy/issues/728#issuecomment-508906927:68,Testability,test,tests,68,The original bug you hit was with the `sc.pl.scatter` which has few tests. I'd recommend trying out the master branches of `AnnData` and `scanpy` until new releases can be made in cases like these.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508906927
https://github.com/scverse/scanpy/issues/728#issuecomment-512184351:20,Availability,error,error,20,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:; `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`; Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:; `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-512184351
https://github.com/scverse/scanpy/issues/728#issuecomment-512184351:329,Availability,error,error,329,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:; `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`; Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:; `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-512184351
https://github.com/scverse/scanpy/issues/728#issuecomment-512184351:144,Deployability,update,update,144,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:; `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`; Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:; `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-512184351
https://github.com/scverse/scanpy/issues/728#issuecomment-512184351:307,Modifiability,variab,variable,307,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:; `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`; Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:; `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-512184351
https://github.com/scverse/scanpy/issues/728#issuecomment-516194235:15,Deployability,update,updated,15,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```; import scanpy as sc; sc.logging.print_versions(); #adata = sc.datasets.pbmc3k(); adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""); print(adata); adata = adata.T; print(adata); adata.raw = adata; print(adata); sc.pp.filter_cells(adata, min_genes=200); print(adata); adata = adata[adata.obs['n_genes'] < 5000, :]; print(adata); adata = adata[adata.obs['n_genes'] > 100, :]; print(adata); ```. output is:; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 ; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; AnnData object with n_obs × n_vars = 60498 × 466 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; obs: 'n_genes'; View of AnnData object with n_obs × n_vars = 311 × 60498 ; obs: 'n_genes'; Traceback (most recent call last):; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values; return self._constructor(self._data.get_slice(indexer),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice; return self.__class__(self._block._slice(slobj),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice; return self.values[slicer]; IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean di",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235
https://github.com/scverse/scanpy/issues/728#issuecomment-516194235:783,Modifiability,Variab,Variable,783,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```; import scanpy as sc; sc.logging.print_versions(); #adata = sc.datasets.pbmc3k(); adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""); print(adata); adata = adata.T; print(adata); adata.raw = adata; print(adata); sc.pp.filter_cells(adata, min_genes=200); print(adata); adata = adata[adata.obs['n_genes'] < 5000, :]; print(adata); adata = adata[adata.obs['n_genes'] > 100, :]; print(adata); ```. output is:; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 ; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; AnnData object with n_obs × n_vars = 60498 × 466 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; obs: 'n_genes'; View of AnnData object with n_obs × n_vars = 311 × 60498 ; obs: 'n_genes'; Traceback (most recent call last):; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values; return self._constructor(self._data.get_slice(indexer),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice; return self.__class__(self._block._slice(slobj),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice; return self.values[slicer]; IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean di",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235
https://github.com/scverse/scanpy/issues/728#issuecomment-516194235:867,Modifiability,Variab,Variable,867,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```; import scanpy as sc; sc.logging.print_versions(); #adata = sc.datasets.pbmc3k(); adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""); print(adata); adata = adata.T; print(adata); adata.raw = adata; print(adata); sc.pp.filter_cells(adata, min_genes=200); print(adata); adata = adata[adata.obs['n_genes'] < 5000, :]; print(adata); adata = adata[adata.obs['n_genes'] > 100, :]; print(adata); ```. output is:; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 ; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; AnnData object with n_obs × n_vars = 60498 × 466 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; obs: 'n_genes'; View of AnnData object with n_obs × n_vars = 311 × 60498 ; obs: 'n_genes'; Traceback (most recent call last):; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values; return self._constructor(self._data.get_slice(indexer),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice; return self.__class__(self._block._slice(slobj),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice; return self.values[slicer]; IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean di",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235
https://github.com/scverse/scanpy/issues/728#issuecomment-516194235:951,Modifiability,Variab,Variable,951,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```; import scanpy as sc; sc.logging.print_versions(); #adata = sc.datasets.pbmc3k(); adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""); print(adata); adata = adata.T; print(adata); adata.raw = adata; print(adata); sc.pp.filter_cells(adata, min_genes=200); print(adata); adata = adata[adata.obs['n_genes'] < 5000, :]; print(adata); adata = adata[adata.obs['n_genes'] > 100, :]; print(adata); ```. output is:; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 ; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; AnnData object with n_obs × n_vars = 60498 × 466 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; obs: 'n_genes'; View of AnnData object with n_obs × n_vars = 311 × 60498 ; obs: 'n_genes'; Traceback (most recent call last):; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values; return self._constructor(self._data.get_slice(indexer),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice; return self.__class__(self._block._slice(slobj),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice; return self.values[slicer]; IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean di",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235
https://github.com/scverse/scanpy/issues/728#issuecomment-516194235:101,Testability,test,test,101,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```; import scanpy as sc; sc.logging.print_versions(); #adata = sc.datasets.pbmc3k(); adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""); print(adata); adata = adata.T; print(adata); adata.raw = adata; print(adata); sc.pp.filter_cells(adata, min_genes=200); print(adata); adata = adata[adata.obs['n_genes'] < 5000, :]; print(adata); adata = adata[adata.obs['n_genes'] > 100, :]; print(adata); ```. output is:; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 ; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; AnnData object with n_obs × n_vars = 60498 × 466 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; obs: 'n_genes'; View of AnnData object with n_obs × n_vars = 311 × 60498 ; obs: 'n_genes'; Traceback (most recent call last):; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values; return self._constructor(self._data.get_slice(indexer),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice; return self.__class__(self._block._slice(slobj),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice; return self.values[slicer]; IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean di",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235
https://github.com/scverse/scanpy/issues/728#issuecomment-516194235:142,Testability,log,logging,142,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```; import scanpy as sc; sc.logging.print_versions(); #adata = sc.datasets.pbmc3k(); adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""); print(adata); adata = adata.T; print(adata); adata.raw = adata; print(adata); sc.pp.filter_cells(adata, min_genes=200); print(adata); adata = adata[adata.obs['n_genes'] < 5000, :]; print(adata); adata = adata[adata.obs['n_genes'] > 100, :]; print(adata); ```. output is:; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 ; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; AnnData object with n_obs × n_vars = 60498 × 466 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; obs: 'n_genes'; View of AnnData object with n_obs × n_vars = 311 × 60498 ; obs: 'n_genes'; Traceback (most recent call last):; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values; return self._constructor(self._data.get_slice(indexer),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice; return self.__class__(self._block._slice(slobj),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice; return self.values[slicer]; IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean di",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235
https://github.com/scverse/scanpy/issues/728#issuecomment-516194235:2128,Testability,test,test,2128,"bs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; obs: 'n_genes'; View of AnnData object with n_obs × n_vars = 311 × 60498 ; obs: 'n_genes'; Traceback (most recent call last):; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values; return self._constructor(self._data.get_slice(indexer),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice; return self.__class__(self._block._slice(slobj),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice; return self.values[slicer]; IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean dimension is 311. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""test.py"", line 14, in <module>; adata = adata[adata.obs['n_genes'] > 100, :]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__; return self._getitem_view(index); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 561, in __init__; self._init_as_view(X, oidx, vidx); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view; self._raw = adata_ref.raw[oidx]; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py"", line 335, in __getitem__; oidx, vidx = self._normalize_indices(index); File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/annd",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235
https://github.com/scverse/scanpy/issues/728#issuecomment-516194235:639,Usability,learn,learn,639,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```; import scanpy as sc; sc.logging.print_versions(); #adata = sc.datasets.pbmc3k(); adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""); print(adata); adata = adata.T; print(adata); adata.raw = adata; print(adata); sc.pp.filter_cells(adata, min_genes=200); print(adata); adata = adata[adata.obs['n_genes'] < 5000, :]; print(adata); adata = adata[adata.obs['n_genes'] > 100, :]; print(adata); ```. output is:; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 ; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; AnnData object with n_obs × n_vars = 60498 × 466 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; obs: 'n_genes'; View of AnnData object with n_obs × n_vars = 311 × 60498 ; obs: 'n_genes'; Traceback (most recent call last):; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values; return self._constructor(self._data.get_slice(indexer),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice; return self.__class__(self._block._slice(slobj),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice; return self.values[slicer]; IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean di",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235
https://github.com/scverse/scanpy/issues/728#issuecomment-516291062:163,Availability,down,down,163,"Hmm...I must admit I don't understand why a ""view"" exists. Views are often tricky to get right, especially in a complex datastructure like anndata. They also slow down processing, especially if users may not be aware that the object they have is a view of something else. I don't see a good use case for views in my pipeline at least. Is there a way to switch off all views in anndata and just return a copy when slicing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516291062
https://github.com/scverse/scanpy/issues/728#issuecomment-516291062:316,Deployability,pipeline,pipeline,316,"Hmm...I must admit I don't understand why a ""view"" exists. Views are often tricky to get right, especially in a complex datastructure like anndata. They also slow down processing, especially if users may not be aware that the object they have is a view of something else. I don't see a good use case for views in my pipeline at least. Is there a way to switch off all views in anndata and just return a copy when slicing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516291062
https://github.com/scverse/scanpy/issues/728#issuecomment-516689711:251,Deployability,release,release,251,"I've just spent a while trying to replicate, before realizing I've seen this issue before over on AnnData (https://github.com/theislab/anndata/issues/182). I've got some good and bad news about this. It's fixed on master, but that fix is slated to be release in `v0.7`, which has intentionally breaking changes. I find views very useful when dealing with large datasets interactively. They're also important for file backed data, since copies are extremely expensive in that case. Unlike numpy, AnnData objects should always return a view when subset. If you'd like to get copies, you could add a `.copy()` to the end of your subsetting statement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516689711
https://github.com/scverse/scanpy/issues/728#issuecomment-516740578:125,Availability,error,error,125,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was; opened after I opened this one. I did search for the error message before I; opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a; pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a; use case for file backed mode either. Any useful operations on file backed; data will be too slow anyways for practical use, and anyone can get a; high-RAM machine these days on Amazon for a few hours, so I've always; wondered file backed mode exists. (sidenote: File backed data is again a; feature that sounds rather complicated to implement. As a user I love; libraries that are small, stable and don't change a lot, especially for; very foundational things like anndata. I guess it's a matter of development; philosophy here). Also, yes, it's because I don't use scanpy interactively; that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy!. On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>; wrote:. > I've just spent a while trying to replicate, before realizing I've seen; > this issue before over on AnnData (theislab/anndata#182; > <https://github.com/theislab/anndata/issues/182>). I've got some good and; > bad news about this. It's fixed on master, but that fix is slated to be; > release in v0.7, which has intentionally breaking changes.; >; > I find views very useful when dealing with large datasets interactively.; > They're also important for file backed data, since copies are extremely; > expensive in that case.; >; > Unlike numpy, AnnData objects should always return a view when subset. If; > you'd like to get copies, you could add a .copy() to the end of your; > subsetting statement.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578
https://github.com/scverse/scanpy/issues/728#issuecomment-516740578:293,Deployability,pipeline,pipeline,293,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was; opened after I opened this one. I did search for the error message before I; opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a; pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a; use case for file backed mode either. Any useful operations on file backed; data will be too slow anyways for practical use, and anyone can get a; high-RAM machine these days on Amazon for a few hours, so I've always; wondered file backed mode exists. (sidenote: File backed data is again a; feature that sounds rather complicated to implement. As a user I love; libraries that are small, stable and don't change a lot, especially for; very foundational things like anndata. I guess it's a matter of development; philosophy here). Also, yes, it's because I don't use scanpy interactively; that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy!. On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>; wrote:. > I've just spent a while trying to replicate, before realizing I've seen; > this issue before over on AnnData (theislab/anndata#182; > <https://github.com/theislab/anndata/issues/182>). I've got some good and; > bad news about this. It's fixed on master, but that fix is slated to be; > release in v0.7, which has intentionally breaking changes.; >; > I find views very useful when dealing with large datasets interactively.; > They're also important for file backed data, since copies are extremely; > expensive in that case.; >; > Unlike numpy, AnnData objects should always return a view when subset. If; > you'd like to get copies, you could add a .copy() to the end of your; > subsetting statement.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578
https://github.com/scverse/scanpy/issues/728#issuecomment-516740578:1469,Deployability,release,release,1469,"for file backed data, I just cannot see a; use case for file backed mode either. Any useful operations on file backed; data will be too slow anyways for practical use, and anyone can get a; high-RAM machine these days on Amazon for a few hours, so I've always; wondered file backed mode exists. (sidenote: File backed data is again a; feature that sounds rather complicated to implement. As a user I love; libraries that are small, stable and don't change a lot, especially for; very foundational things like anndata. I guess it's a matter of development; philosophy here). Also, yes, it's because I don't use scanpy interactively; that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy!. On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>; wrote:. > I've just spent a while trying to replicate, before realizing I've seen; > this issue before over on AnnData (theislab/anndata#182; > <https://github.com/theislab/anndata/issues/182>). I've got some good and; > bad news about this. It's fixed on master, but that fix is slated to be; > release in v0.7, which has intentionally breaking changes.; >; > I find views very useful when dealing with large datasets interactively.; > They're also important for file backed data, since copies are extremely; > expensive in that case.; >; > Unlike numpy, AnnData objects should always return a view when subset. If; > you'd like to get copies, you could add a .copy() to the end of your; > subsetting statement.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/728?email_source=notifications&email_token=AACL4TOSRH3R4VHIARSVCILQCEIBZA5CNFSM4H54LI62YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GA6LY#issuecomment-516689711>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TIAGHQRLMYYAPGI4JTQCEIBZANCNFSM4H54LI6Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578
https://github.com/scverse/scanpy/issues/728#issuecomment-516740578:131,Integrability,message,message,131,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was; opened after I opened this one. I did search for the error message before I; opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a; pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a; use case for file backed mode either. Any useful operations on file backed; data will be too slow anyways for practical use, and anyone can get a; high-RAM machine these days on Amazon for a few hours, so I've always; wondered file backed mode exists. (sidenote: File backed data is again a; feature that sounds rather complicated to implement. As a user I love; libraries that are small, stable and don't change a lot, especially for; very foundational things like anndata. I guess it's a matter of development; philosophy here). Also, yes, it's because I don't use scanpy interactively; that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy!. On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>; wrote:. > I've just spent a while trying to replicate, before realizing I've seen; > this issue before over on AnnData (theislab/anndata#182; > <https://github.com/theislab/anndata/issues/182>). I've got some good and; > bad news about this. It's fixed on master, but that fix is slated to be; > release in v0.7, which has intentionally breaking changes.; >; > I find views very useful when dealing with large datasets interactively.; > They're also important for file backed data, since copies are extremely; > expensive in that case.; >; > Unlike numpy, AnnData objects should always return a view when subset. If; > you'd like to get copies, you could add a .copy() to the end of your; > subsetting statement.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578
https://github.com/scverse/scanpy/issues/728#issuecomment-516740578:254,Usability,simpl,simply,254,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was; opened after I opened this one. I did search for the error message before I; opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a; pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a; use case for file backed mode either. Any useful operations on file backed; data will be too slow anyways for practical use, and anyone can get a; high-RAM machine these days on Amazon for a few hours, so I've always; wondered file backed mode exists. (sidenote: File backed data is again a; feature that sounds rather complicated to implement. As a user I love; libraries that are small, stable and don't change a lot, especially for; very foundational things like anndata. I guess it's a matter of development; philosophy here). Also, yes, it's because I don't use scanpy interactively; that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy!. On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>; wrote:. > I've just spent a while trying to replicate, before realizing I've seen; > this issue before over on AnnData (theislab/anndata#182; > <https://github.com/theislab/anndata/issues/182>). I've got some good and; > bad news about this. It's fixed on master, but that fix is slated to be; > release in v0.7, which has intentionally breaking changes.; >; > I find views very useful when dealing with large datasets interactively.; > They're also important for file backed data, since copies are extremely; > expensive in that case.; >; > Unlike numpy, AnnData objects should always return a view when subset. If; > you'd like to get copies, you could add a .copy() to the end of your; > subsetting statement.; >; > —; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578
https://github.com/scverse/scanpy/pull/730#issuecomment-509060284:96,Deployability,release,release,96,"Sure! I wansn't sure if there were other bugs to fix or PRs to merge before we wanted to make a release. I'd also like to get @fidelram to give this a look over. I think I didn't break anything, but he'd be in a better position to tell if that were the case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-509060284
https://github.com/scverse/scanpy/pull/730#issuecomment-510456004:225,Testability,test,test,225,"@ivirshup I checked and now I don't get warnings :). However, I could not plot the layer. In the following example, I make a new layer that is the negative of the default layer. As you see, bot the default and the negative ('test') layer are identical. ![image](https://user-images.githubusercontent.com/4964309/61049083-df411980-a3e3-11e9-8508-978a78d7f3b4.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510456004
https://github.com/scverse/scanpy/pull/730#issuecomment-510467988:60,Modifiability,layers,layers,60,"Ah, it looks like `use_raw` was being set to True, even if `layers` was passed. . https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/plotting/_tools/scatterplots.py#L74-L76. Any idea why this didn't trigger this test?. https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/tests/test_plotting.py#L294-L298. Edit: I'm guessing vmin.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510467988
https://github.com/scverse/scanpy/pull/730#issuecomment-510467988:249,Testability,test,test,249,"Ah, it looks like `use_raw` was being set to True, even if `layers` was passed. . https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/plotting/_tools/scatterplots.py#L74-L76. Any idea why this didn't trigger this test?. https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/tests/test_plotting.py#L294-L298. Edit: I'm guessing vmin.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510467988
https://github.com/scverse/scanpy/pull/730#issuecomment-510467988:344,Testability,test,tests,344,"Ah, it looks like `use_raw` was being set to True, even if `layers` was passed. . https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/plotting/_tools/scatterplots.py#L74-L76. Any idea why this didn't trigger this test?. https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/tests/test_plotting.py#L294-L298. Edit: I'm guessing vmin.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510467988
https://github.com/scverse/scanpy/pull/730#issuecomment-510771212:27,Testability,test,tests,27,"Fixed. Also added a couple tests. I didn't change the test mentioned above though, which might be a good thing to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510771212
https://github.com/scverse/scanpy/pull/730#issuecomment-510771212:54,Testability,test,test,54,"Fixed. Also added a couple tests. I didn't change the test mentioned above though, which might be a good thing to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510771212
https://github.com/scverse/scanpy/pull/730#issuecomment-510785080:143,Availability,error,error,143,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```; layer : typing.Union[str, NoneType], optional (default: None); Name of the AnnData object layer that wants to be plotted. By default; adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted.; If `layer` is set to a valid layer name, then the layer is plotted. `layer`; takes precedence over `use_raw`.; ``` ; The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510785080
https://github.com/scverse/scanpy/pull/730#issuecomment-510785080:162,Availability,toler,tolerance,162,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```; layer : typing.Union[str, NoneType], optional (default: None); Name of the AnnData object layer that wants to be plotted. By default; adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted.; If `layer` is set to a valid layer name, then the layer is plotted. `layer`; takes precedence over `use_raw`.; ``` ; The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510785080
https://github.com/scverse/scanpy/pull/730#issuecomment-510785080:1083,Integrability,message,message,1083,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```; layer : typing.Union[str, NoneType], optional (default: None); Name of the AnnData object layer that wants to be plotted. By default; adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted.; If `layer` is set to a valid layer name, then the layer is plotted. `layer`; takes precedence over `use_raw`.; ``` ; The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510785080
https://github.com/scverse/scanpy/pull/730#issuecomment-510785080:258,Safety,avoid,avoid,258,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```; layer : typing.Union[str, NoneType], optional (default: None); Name of the AnnData object layer that wants to be plotted. By default; adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted.; If `layer` is set to a valid layer name, then the layer is plotted. `layer`; takes precedence over `use_raw`.; ``` ; The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510785080
https://github.com/scverse/scanpy/pull/730#issuecomment-510785080:66,Testability,test,test,66,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```; layer : typing.Union[str, NoneType], optional (default: None); Name of the AnnData object layer that wants to be plotted. By default; adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted.; If `layer` is set to a valid layer name, then the layer is plotted. `layer`; takes precedence over `use_raw`.; ``` ; The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510785080
https://github.com/scverse/scanpy/pull/730#issuecomment-510785080:232,Testability,test,test,232,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```; layer : typing.Union[str, NoneType], optional (default: None); Name of the AnnData object layer that wants to be plotted. By default; adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted.; If `layer` is set to a valid layer name, then the layer is plotted. `layer`; takes precedence over `use_raw`.; ``` ; The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510785080
https://github.com/scverse/scanpy/pull/730#issuecomment-510785080:786,Testability,log,logic,786,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```; layer : typing.Union[str, NoneType], optional (default: None); Name of the AnnData object layer that wants to be plotted. By default; adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted.; If `layer` is set to a valid layer name, then the layer is plotted. `layer`; takes precedence over `use_raw`.; ``` ; The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510785080
https://github.com/scverse/scanpy/pull/730#issuecomment-510800095:12,Testability,test,tested,12,@ivirshup I tested and now is working as expected. Thanks for adding the new tests. From my side is ready to go.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510800095
https://github.com/scverse/scanpy/pull/730#issuecomment-510800095:77,Testability,test,tests,77,@ivirshup I tested and now is working as expected. Thanks for adding the new tests. From my side is ready to go.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510800095
https://github.com/scverse/scanpy/pull/730#issuecomment-511257594:528,Deployability,release,release,528,"@fidelram I've made sure that only `use_raw` or `layer` has been specified, though the default of `use_raw` being True is still used if `layer` isn't set. I think it would be good if this was covered in the docs for `use_raw` as well. I also get tripped up by `use_raw` pretty frequently. I think a warning for this behavior would be good, but I don't like the idea of the default setting issuing a warning. What if we changed the default to false? This would need a deprecation warning for a bit and waiting until the next big release though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-511257594
https://github.com/scverse/scanpy/pull/730#issuecomment-511274129:5,Deployability,update,updated,5,I've updated the docs a little and am going to go ahead and merge this. Thanks for the feedback @fidelram!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-511274129
https://github.com/scverse/scanpy/pull/730#issuecomment-511274129:87,Usability,feedback,feedback,87,I've updated the docs a little and am going to go ahead and merge this. Thanks for the feedback @fidelram!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-511274129
https://github.com/scverse/scanpy/pull/730#issuecomment-511276816:49,Integrability,depend,dependency,49,"@falexwolf, realized I didn't change the AnnData dependency. I'm not totally sure what to do with that, since we've already got a requirement on 0.6.22 due to scipy and statsmodels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-511276816
https://github.com/scverse/scanpy/issues/731#issuecomment-509464033:208,Availability,error,error,208,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs?. Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731#issuecomment-509464033
https://github.com/scverse/scanpy/issues/731#issuecomment-509464033:273,Availability,error,error,273,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs?. Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731#issuecomment-509464033
https://github.com/scverse/scanpy/issues/731#issuecomment-509464033:162,Deployability,release,release,162,"Thanks for the bug report! I think we've just fixed the first and third issue in #729, but I'm not to sure about the second. Could you try updating to the newest release of AnnData and letting us know if the error still occurs?. Would you mind also letting us know if this error occurs when you use one of the built in datasets, like `sc.datasets.pbmc3k()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731#issuecomment-509464033
https://github.com/scverse/scanpy/issues/731#issuecomment-512933575:31,Deployability,upgrade,upgraded,31,"Sorry for the delay on this! I upgraded to ""scanpy==1.4.3+115.g1aecabf anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0"" and the issue is gone. . The pre-built dataset also works with the upgraded version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731#issuecomment-512933575
https://github.com/scverse/scanpy/issues/731#issuecomment-512933575:256,Deployability,upgrade,upgraded,256,"Sorry for the delay on this! I upgraded to ""scanpy==1.4.3+115.g1aecabf anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0"" and the issue is gone. . The pre-built dataset also works with the upgraded version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731#issuecomment-512933575
https://github.com/scverse/scanpy/issues/731#issuecomment-512933575:154,Usability,learn,learn,154,"Sorry for the delay on this! I upgraded to ""scanpy==1.4.3+115.g1aecabf anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0"" and the issue is gone. . The pre-built dataset also works with the upgraded version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731#issuecomment-512933575
https://github.com/scverse/scanpy/issues/731#issuecomment-513444766:28,Deployability,release,released,28,"Glad to hear it! We've just released `v1.4.4` which has this fix in it, so you can use that if you don't want to be on the development branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731#issuecomment-513444766
https://github.com/scverse/scanpy/issues/732#issuecomment-510146221:77,Usability,clear,clear,77,"Hi @fidelram,; Thanks for your prompt reply! Sorry I didn't make my question clear. ; I would like to show the sample name of each row instead of cluster name. For example:. ![Screen Shot 2019-07-10 at 11 53 30 AM](https://user-images.githubusercontent.com/15947971/60989163-ce70a500-a30a-11e9-9559-9a56d9efb88b.png). PS. I have add `standard_scale='var'` in my heatmap. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/732#issuecomment-510146221
https://github.com/scverse/scanpy/pull/733#issuecomment-512213981:306,Integrability,protocol,protocol,306,"Thanks for opening this @ivirshup. The principle should be that Anndata doesn't change array types to numpy arrays. It may not have done this in the past (and this is one example), but we should fix that. Hopefully with improvements in numpy itself (like https://www.numpy.org/neps/nep-0018-array-function-protocol.html, and `__array_ufunc__`) it should be achievable. (I don't know how much work is needed in Anndata to do that though.). > Also, are `ZappyArray`s always read-only like the `DirectZappyArrays` are?. Yes, the idea is that they are immutable, and they are changed by transformation (e.g. applying a ufunc) to create a new array.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733#issuecomment-512213981
https://github.com/scverse/scanpy/pull/733#issuecomment-515320589:322,Energy Efficiency,reduce,reduced,322,"> The principle should be that Anndata doesn't change array types to numpy arrays. I mostly agree with this, but think there would be a fair amount of work needed in AnnData. Most array types have special treatment in at least one place. A lot of this is due to our need to support sparse matrices. I'm hoping this can be reduced with some new stuff I'm adding though. This is definitely a good use case to test with. There's also the issue of when converting implicitly makes sense. We will do transformations from sparse to dense if the values becomes dense. We will also convert between sparse matrix formats if it will make calculations more efficient. On the topic of Zappy arrays. Can you take a `view` of a zappy array? This would be useful for some of the expectations around subsetting AnnData objects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733#issuecomment-515320589
https://github.com/scverse/scanpy/pull/733#issuecomment-515320589:646,Energy Efficiency,efficient,efficient,646,"> The principle should be that Anndata doesn't change array types to numpy arrays. I mostly agree with this, but think there would be a fair amount of work needed in AnnData. Most array types have special treatment in at least one place. A lot of this is due to our need to support sparse matrices. I'm hoping this can be reduced with some new stuff I'm adding though. This is definitely a good use case to test with. There's also the issue of when converting implicitly makes sense. We will do transformations from sparse to dense if the values becomes dense. We will also convert between sparse matrix formats if it will make calculations more efficient. On the topic of Zappy arrays. Can you take a `view` of a zappy array? This would be useful for some of the expectations around subsetting AnnData objects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733#issuecomment-515320589
https://github.com/scverse/scanpy/pull/733#issuecomment-515320589:407,Testability,test,test,407,"> The principle should be that Anndata doesn't change array types to numpy arrays. I mostly agree with this, but think there would be a fair amount of work needed in AnnData. Most array types have special treatment in at least one place. A lot of this is due to our need to support sparse matrices. I'm hoping this can be reduced with some new stuff I'm adding though. This is definitely a good use case to test with. There's also the issue of when converting implicitly makes sense. We will do transformations from sparse to dense if the values becomes dense. We will also convert between sparse matrix formats if it will make calculations more efficient. On the topic of Zappy arrays. Can you take a `view` of a zappy array? This would be useful for some of the expectations around subsetting AnnData objects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733#issuecomment-515320589
https://github.com/scverse/scanpy/pull/733#issuecomment-517226549:182,Testability,test,test,182,"I've managed to mostly fix this with only a slightly hacky change to AnnData (no longer requiring X to have a view type defined when making a copy). However the `normalize_per_cell` test still fails, as it seems like the matrix isn't actually being modified. I think it's because the reference to the dask array on [this line](https://github.com/theislab/scanpy/blob/610a955f025f5f17328865926a9341a55553e081/scanpy/preprocessing/_simple.py#L667) becomes a copy when assigned to. I've asked about this behaviour, and if it might change or throw a warning in https://github.com/dask/dask/issues/5199",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733#issuecomment-517226549
https://github.com/scverse/scanpy/pull/733#issuecomment-517982303:215,Deployability,update,updated,215,"~~@tomwhite, I'm getting a little lost with dask at the moment. Right now this works on my machine, but doesn't work on travis. Any idea what's going on?~~. Nope, I was dumb. It's obviously that anndata hasn't been updated in this build.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733#issuecomment-517982303
https://github.com/scverse/scanpy/issues/734#issuecomment-509605817:110,Testability,test,tests,110,"Thanks for the bug report! Thats definitely strange, since we should almost certainly be catching that in the tests... What's the full version string of python you're runnning?. Could you also try running this snippet with the same interpreter?. ```python; from typing import Tuple, NamedTuple. class ViewArgs(NamedTuple):; parent: ""AnnData""; attrname: str; keys: Tuple[str, ...] = (). print(ViewArgs(None, ""a"")); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734#issuecomment-509605817
https://github.com/scverse/scanpy/issues/734#issuecomment-509615807:99,Testability,test,test,99,"I am running Python 3.6.0. The following is the output of running just the snippet above:. > File ""test.py"", line 13, in <module>; print(ViewArgs(None, ""a"")); TypeError: __new__() missing 1 required positional argument: 'keys'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734#issuecomment-509615807
https://github.com/scverse/scanpy/issues/734#issuecomment-509619333:207,Deployability,upgrade,upgrade,207,"I think the issue might be with the version of python, since that snippet works fine for me with a fresh python 3.6 conda environment (v3.6.8) and seems to be working in our builds (v3.6.7). Are you able to upgrade to one of those?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734#issuecomment-509619333
https://github.com/scverse/scanpy/issues/734#issuecomment-509622325:51,Availability,error,error,51,I updated it to python 3.6.8 and it gets past that error point. ; Thank you very much for all your help.; Cheers.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734#issuecomment-509622325
https://github.com/scverse/scanpy/issues/734#issuecomment-509622325:2,Deployability,update,updated,2,I updated it to python 3.6.8 and it gets past that error point. ; Thank you very much for all your help.; Cheers.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734#issuecomment-509622325
https://github.com/scverse/scanpy/issues/737#issuecomment-510419262:0,Deployability,Update,Update,0,"Update, the correct docs also show up on master for my local build. Not sure if this is a cacheing issue or a difference between my build and readthedocs'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/737#issuecomment-510419262
https://github.com/scverse/scanpy/issues/737#issuecomment-510419262:90,Performance,cache,cacheing,90,"Update, the correct docs also show up on master for my local build. Not sure if this is a cacheing issue or a difference between my build and readthedocs'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/737#issuecomment-510419262
https://github.com/scverse/scanpy/issues/738#issuecomment-511205979:451,Availability,error,error,451,"Hey!; Using the latest verisons of scanpy and anndata, I have tried reproducing this via:; ```; adata = sc.datasets.pbmc3k(); sc.pp.filter_genes(adata, min_counts = 10); sc.pp.filter_cells(adata, min_counts = 10); sc.pp.normalize_per_cell(adata); sc.pp.log1p(adata); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True); adata = adata[:, adata.var[""highly_variable""]]; sc.pp.scale(adata); ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on?. Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738#issuecomment-511205979
https://github.com/scverse/scanpy/issues/738#issuecomment-511205979:483,Availability,error,error,483,"Hey!; Using the latest verisons of scanpy and anndata, I have tried reproducing this via:; ```; adata = sc.datasets.pbmc3k(); sc.pp.filter_genes(adata, min_counts = 10); sc.pp.filter_cells(adata, min_counts = 10); sc.pp.normalize_per_cell(adata); sc.pp.log1p(adata); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True); adata = adata[:, adata.var[""highly_variable""]]; sc.pp.scale(adata); ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on?. Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738#issuecomment-511205979
https://github.com/scverse/scanpy/issues/738#issuecomment-511205979:571,Availability,error,error,571,"Hey!; Using the latest verisons of scanpy and anndata, I have tried reproducing this via:; ```; adata = sc.datasets.pbmc3k(); sc.pp.filter_genes(adata, min_counts = 10); sc.pp.filter_cells(adata, min_counts = 10); sc.pp.normalize_per_cell(adata); sc.pp.log1p(adata); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=True); adata = adata[:, adata.var[""highly_variable""]]; sc.pp.scale(adata); ```. and I don't get an error. Could you reproduce this error with one of the datasets in `sc.datasets`? That way I could try to reproduce your error. Also, which version of anndata and scanpy are you on?. Other than that, you don't need the line `adata = adata[:, adata.var[""highly_variable""]]` if you use `subset=True` in the `sc.pp.highly_variable_genes()` call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/738#issuecomment-511205979
https://github.com/scverse/scanpy/issues/739#issuecomment-511980962:117,Availability,error,error,117,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console; $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.0; ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package.; 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py; from importlib_metadata import Distribution; def version(name):; for resolver in Distribution._discover_resolvers():; for d in resolver(name):; return d.metadata['Version']; raise PackageNotFoundError(name); ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962
https://github.com/scverse/scanpy/issues/739#issuecomment-511980962:491,Deployability,install,installed,491,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console; $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.0; ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package.; 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py; from importlib_metadata import Distribution; def version(name):; for resolver in Distribution._discover_resolvers():; for d in resolver(name):; return d.metadata['Version']; raise PackageNotFoundError(name); ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962
https://github.com/scverse/scanpy/issues/739#issuecomment-511980962:98,Usability,learn,learn,98,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console; $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.0; ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package.; 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py; from importlib_metadata import Distribution; def version(name):; for resolver in Distribution._discover_resolvers():; for d in resolver(name):; return d.metadata['Version']; raise PackageNotFoundError(name); ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962
https://github.com/scverse/scanpy/issues/739#issuecomment-511980962:247,Usability,learn,learn,247,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console; $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.0; ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package.; 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py; from importlib_metadata import Distribution; def version(name):; for resolver in Distribution._discover_resolvers():; for d in resolver(name):; return d.metadata['Version']; raise PackageNotFoundError(name); ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962
https://github.com/scverse/scanpy/issues/739#issuecomment-511980962:476,Usability,learn,learn,476,"OK, so you’re using Python < 3.8 and `importlib_metadata`. The line `umap_version = version(""umap-learn"")` throws an error. It works for me with the same setup:. ```console; $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.0; ```. You said in #704 that it works “with a commit a few before” that one. You could use `git bisect` to figure out which commit exactly make a difference, but I think the issue might be either. 1. the way umap-learn 0.3.9 is installed on your system. maybe it doesn’t have proper metadata or so. you should have a directory called `umap_learn-0.3.9-py3.7.egg-info` right next to the `umap` package.; 2. You have an older version of `importlib_metadata` with a bug or so. The code basically does this:. ```py; from importlib_metadata import Distribution; def version(name):; for resolver in Distribution._discover_resolvers():; for d in resolver(name):; return d.metadata['Version']; raise PackageNotFoundError(name); ```. I don’t see how importing or not importing umap should change this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-511980962
https://github.com/scverse/scanpy/issues/739#issuecomment-512160342:323,Usability,learn,learn,323,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:; ```; $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version; return distribution(package).version; File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution; return Distribution.from_name(package); File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name; raise PackageNotFoundError(name); importlib_metadata.api.PackageNotFoundError: umap-learn; ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-512160342
https://github.com/scverse/scanpy/issues/739#issuecomment-512160342:980,Usability,learn,learn,980,"Yes, I am using python 3.6.7. And I do have the `umap_learn-0.3.9.dist-info` folder in my conda env. Reverting to the commit before the last one in #704 works. I hope I don't have to `git bisect` for the moment... . A couple more observations:; ```; $ python -c 'from importlib_metadata import version; print(version(""umap-learn""))'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 105, in version; return distribution(package).version; File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 86, in distribution; return Distribution.from_name(package); File ""/home/icb/malte.luecken/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py"", line 52, in from_name; raise PackageNotFoundError(name); importlib_metadata.api.PackageNotFoundError: umap-learn; ```. But, the last couple lines you posted (`from importlib_metadata import Distribution`.... `raise`...) work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-512160342
https://github.com/scverse/scanpy/issues/739#issuecomment-512166181:338,Deployability,install,installed,338,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do; ```; In [3]: import umap ; In [4]: umap.__version__ ; Out[4]: '0.3.9'; ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-512166181
https://github.com/scverse/scanpy/issues/739#issuecomment-512166181:152,Usability,learn,learn,152,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do; ```; In [3]: import umap ; In [4]: umap.__version__ ; Out[4]: '0.3.9'; ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-512166181
https://github.com/scverse/scanpy/issues/739#issuecomment-512166181:295,Usability,learn,learn,295,"I've played around a bit with `version()` from `importlib_metadata`, and it tells me the version of scanpy and anndata, but never finds `umap` or `umap-learn`. I can however do; ```; In [3]: import umap ; In [4]: umap.__version__ ; Out[4]: '0.3.9'; ```. But, `version(""umap"")` or `version(""umap-learn"")` doesn't work. Any ideas? I've not installed anything manually, but everything via conda's pip. Does this have to do with package names and import names being different? It also doesn't work with `gprofiler-official`, which is imported as `import gprofiler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-512166181
https://github.com/scverse/scanpy/issues/739#issuecomment-512171197:82,Usability,learn,learn,82,"Figured it out: it works with `version(""umap_learn"")` but not with `version(""umap-learn"")`. Maybe this should be changed in the commit? I wonder if this is a python version issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-512171197
https://github.com/scverse/scanpy/issues/739#issuecomment-513178294:144,Deployability,release,released,144,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console; $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'; 0.18; $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*; ~/.local/lib/python3.6/site-packages/umap; ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info; $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.9; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-513178294
https://github.com/scverse/scanpy/issues/739#issuecomment-513178294:564,Usability,learn,learn,564,"Seems like you still have [importlib_metadata#21](https://gitlab.com/python-devs/importlib_metadata/issues/21), fixed in version 0.7, which was released 7 months ago. With 0.7 or a newer version, it should work:. ```console; $ python3 -c 'from importlib_metadata import version; print(version(""importlib_metadata""))'; 0.18; $ ls -d1 ~/.local/lib/python3.6/site-packages/umap*; ~/.local/lib/python3.6/site-packages/umap; ~/.local/lib/python3.6/site-packages/umap_learn-0.3.9-py3.6.egg-info; $ python3 -c 'from importlib_metadata import version; print(version(""umap-learn""))'; 0.3.9; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-513178294
https://github.com/scverse/scanpy/issues/739#issuecomment-513181427:62,Deployability,update,update,62,haha... you were a bit quicker than me... I had to rebase and update first ;),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-513181427
https://github.com/scverse/scanpy/issues/739#issuecomment-513183516:92,Deployability,update,update,92,"ah, but then we would not have found the version where this is an issue ;). But yes... will update all relevant packages next time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739#issuecomment-513183516
https://github.com/scverse/scanpy/issues/740#issuecomment-513174083:335,Safety,safe,safe,335,"> best palette out there. That’s quite the generalization. | Pro | Con |; | --- | --- |; | Many colors | Ugly colors, no rhyme or reason |; | Acceptably distinguishable | Dark colors hard to distinguish on white bg, and light ones hard to distinguish on black bg |; | | There’s always colors that are hard to see on any kind of bg (no safe bg color) |; | | Not colorblind friendly |. So I’d recommend against it whenever you can avoid it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740#issuecomment-513174083
https://github.com/scverse/scanpy/issues/740#issuecomment-513174083:429,Safety,avoid,avoid,429,"> best palette out there. That’s quite the generalization. | Pro | Con |; | --- | --- |; | Many colors | Ugly colors, no rhyme or reason |; | Acceptably distinguishable | Dark colors hard to distinguish on white bg, and light ones hard to distinguish on black bg |; | | There’s always colors that are hard to see on any kind of bg (no safe bg color) |; | | Not colorblind friendly |. So I’d recommend against it whenever you can avoid it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740#issuecomment-513174083
https://github.com/scverse/scanpy/issues/740#issuecomment-513822396:48,Energy Efficiency,monitor,monitor,48,"I have a hard time seeing the first color on my monitor too… the yellow is too neon. I think if the colors of the godsnot palette were e.g. clustered by saturation, it would be less ugly in my eyes. It’s of course fixable if we swap that 18th color with a later one that’s similar but less white. #742 was a great idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740#issuecomment-513822396
https://github.com/scverse/scanpy/issues/740#issuecomment-514006827:50,Energy Efficiency,monitor,monitor,50,"> I have a hard time seeing the first color on my monitor too… the yellow is too neon. I wonder if we could add a black border around cells, but one that would always be behind other dots, so we get an outline of the cells against background. This might not work well when you have diffuse clouds of points. The `default_20` palette was the great idea!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740#issuecomment-514006827
https://github.com/scverse/scanpy/issues/740#issuecomment-519249007:47,Integrability,interface,interface,47,"Oh damn, I was meant to send a PR from the web interface but didn't notice the default is to commit directly. Sorry about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740#issuecomment-519249007
https://github.com/scverse/scanpy/issues/746#issuecomment-514115289:33,Testability,log,logging,33,"Just took a look at pbmc3k, your logging still has fractions of a second in there. This logging does not capture times with that accuracy anymore. I tried updating `datetime` in case that's secretly responsible, as you seem to use it internally for time tracking. It was not secretly responsible, the timing discrepancy and lack of deep persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746#issuecomment-514115289
https://github.com/scverse/scanpy/issues/746#issuecomment-514115289:88,Testability,log,logging,88,"Just took a look at pbmc3k, your logging still has fractions of a second in there. This logging does not capture times with that accuracy anymore. I tried updating `datetime` in case that's secretly responsible, as you seem to use it internally for time tracking. It was not secretly responsible, the timing discrepancy and lack of deep persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746#issuecomment-514115289
https://github.com/scverse/scanpy/issues/746#issuecomment-514121664:490,Deployability,update,update,490,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746#issuecomment-514121664
https://github.com/scverse/scanpy/issues/746#issuecomment-514121664:7,Testability,log,logging,7,"> your logging still has fractions of a second in there. Shouldn’t be possible, in 709bafb8ed600daf5f9ee995a0dc845ac1e7e605 I set the microseconds to 0, and in `timedelta.__str__`, microseconds [only get added](https://github.com/python/cpython/blob/83cec020ba177fa27727330ba4ccf60eebc22a54/Lib/datetime.py#L596-L597) if they’re >0. > I tried updating datetime in case that's secretly responsible, as you seem to use it internally for time tracking. How so? It’s a stdlib module, you can’t update it without updating Python itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746#issuecomment-514121664
https://github.com/scverse/scanpy/issues/746#issuecomment-515971919:67,Deployability,release,released,67,"Hi, sorry! deep was nonfunctional, I fixed it in 1.4.4.post1 (just released)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/746#issuecomment-515971919
https://github.com/scverse/scanpy/issues/747#issuecomment-515844864:65,Availability,error,error,65,"Hi. I just tried running that, and wasn't able to reproduce that error. Here's what I ran:. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); sc.pp.filter_genes(adata, min_counts=1); sc.pp.log1p(adata); sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5); sc.pl.highly_variable_genes(adata); adata = adata[:, adata.var['highly_variable']]; ```. Could you update to the latest releases (scanpy `1.4.4`, anndata `0.6.22`) and try that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-515844864
https://github.com/scverse/scanpy/issues/747#issuecomment-515844864:393,Deployability,update,update,393,"Hi. I just tried running that, and wasn't able to reproduce that error. Here's what I ran:. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); sc.pp.filter_genes(adata, min_counts=1); sc.pp.log1p(adata); sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5); sc.pl.highly_variable_genes(adata); adata = adata[:, adata.var['highly_variable']]; ```. Could you update to the latest releases (scanpy `1.4.4`, anndata `0.6.22`) and try that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-515844864
https://github.com/scverse/scanpy/issues/747#issuecomment-515844864:414,Deployability,release,releases,414,"Hi. I just tried running that, and wasn't able to reproduce that error. Here's what I ran:. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); sc.pp.filter_genes(adata, min_counts=1); sc.pp.log1p(adata); sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5); sc.pl.highly_variable_genes(adata); adata = adata[:, adata.var['highly_variable']]; ```. Could you update to the latest releases (scanpy `1.4.4`, anndata `0.6.22`) and try that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-515844864
https://github.com/scverse/scanpy/issues/747#issuecomment-516115061:217,Availability,error,error,217,"Hi,; I converted the obs and values in string and it worked.; Thanks. On Mon, 29 Jul 2019 at 06:59, Isaac Virshup <notifications@github.com>; wrote:. > Hi. I just tried running that, and wasn't able to reproduce that error.; > Here's what I ran:; >; > import scanpy as sc; >; > adata = sc.datasets.pbmc3k(); > sc.pp.filter_genes(adata, min_counts=1); > sc.pp.log1p(adata); > sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5); > sc.pl.highly_variable_genes(adata); > adata = adata[:, adata.var['highly_variable']]; >; > Could you update to the latest releases (scanpy 1.4.4, anndata 0.6.22); > and try that?; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4U77PLSKFM4ZNQRBYLQBZ2LBA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD27SWAA#issuecomment-515844864>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACPDY4VLMX7TXWMWLRDBTPLQBZ2LBANCNFSM4IG2HWJQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-516115061
https://github.com/scverse/scanpy/issues/747#issuecomment-516115061:557,Deployability,update,update,557,"Hi,; I converted the obs and values in string and it worked.; Thanks. On Mon, 29 Jul 2019 at 06:59, Isaac Virshup <notifications@github.com>; wrote:. > Hi. I just tried running that, and wasn't able to reproduce that error.; > Here's what I ran:; >; > import scanpy as sc; >; > adata = sc.datasets.pbmc3k(); > sc.pp.filter_genes(adata, min_counts=1); > sc.pp.log1p(adata); > sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5); > sc.pl.highly_variable_genes(adata); > adata = adata[:, adata.var['highly_variable']]; >; > Could you update to the latest releases (scanpy 1.4.4, anndata 0.6.22); > and try that?; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4U77PLSKFM4ZNQRBYLQBZ2LBA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD27SWAA#issuecomment-515844864>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACPDY4VLMX7TXWMWLRDBTPLQBZ2LBANCNFSM4IG2HWJQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-516115061
https://github.com/scverse/scanpy/issues/747#issuecomment-516115061:578,Deployability,release,releases,578,"Hi,; I converted the obs and values in string and it worked.; Thanks. On Mon, 29 Jul 2019 at 06:59, Isaac Virshup <notifications@github.com>; wrote:. > Hi. I just tried running that, and wasn't able to reproduce that error.; > Here's what I ran:; >; > import scanpy as sc; >; > adata = sc.datasets.pbmc3k(); > sc.pp.filter_genes(adata, min_counts=1); > sc.pp.log1p(adata); > sc.pp.highly_variable_genes(adata, min_mean=0.0001, max_mean=3, min_disp=0.5); > sc.pl.highly_variable_genes(adata); > adata = adata[:, adata.var['highly_variable']]; >; > Could you update to the latest releases (scanpy 1.4.4, anndata 0.6.22); > and try that?; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4U77PLSKFM4ZNQRBYLQBZ2LBA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD27SWAA#issuecomment-515844864>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACPDY4VLMX7TXWMWLRDBTPLQBZ2LBANCNFSM4IG2HWJQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-516115061
https://github.com/scverse/scanpy/issues/747#issuecomment-516271609:85,Performance,load,load,85,"I created an adata without using the functions provided by scanpy that; allow you to load single cell data. This kind of conversion is done is done; in that functions, right?. On Tue, Jul 30, 2019, 06:17 Isaac Virshup <notifications@github.com> wrote:. > That'll do it 😄; >; > Do you know how you ended up with non-string indices? Ideally, we would be; > able to prevent that from happening or at least warn the user about it.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4T3AAWXADJLTLCIMW3QB66FRA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CWRPY#issuecomment-516253887>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACPDY4TBMBQHRYTRFHJCWJTQB66FRANCNFSM4IG2HWJQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-516271609
https://github.com/scverse/scanpy/issues/747#issuecomment-516289839:30,Performance,load,loaded,30,I had the same problem when I loaded sample data from a csv as a data frame; and assigned it to adata.obs = df. >,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-516289839
https://github.com/scverse/scanpy/issues/747#issuecomment-1019247157:32,Performance,load,loaded,32,> I had the same problem when I loaded sample data from a csv as a data frame and assigned it to adata.obs = df; > […](#). I meet the same problem when I try replace the adata.obs with annother pandas dataframe,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-1019247157
https://github.com/scverse/scanpy/issues/747#issuecomment-1242183366:102,Testability,assert,assertionerror,102,Solution here is `adata.obs.index = adata.obs.index.astype(str)`. Should be called by default if this assertionerror is raised.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-1242183366
https://github.com/scverse/scanpy/issues/748#issuecomment-515061065:68,Modifiability,variab,variable,68,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:; ```; CLUST_ID = 0; gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']; gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]; results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]; ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065
https://github.com/scverse/scanpy/issues/748#issuecomment-515061065:911,Performance,perform,perform,911,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:; ```; CLUST_ID = 0; gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']; gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]; results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]; ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065
https://github.com/scverse/scanpy/issues/748#issuecomment-515061065:84,Testability,test,test,84,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:; ```; CLUST_ID = 0; gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']; gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]; results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]; ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065
https://github.com/scverse/scanpy/issues/748#issuecomment-515061065:175,Testability,test,testing,175,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:; ```; CLUST_ID = 0; gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']; gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]; results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]; ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065
https://github.com/scverse/scanpy/issues/748#issuecomment-515061065:339,Testability,test,testing,339,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:; ```; CLUST_ID = 0; gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']; gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]; results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]; ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065
https://github.com/scverse/scanpy/issues/748#issuecomment-515061065:431,Testability,test,test,431,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:; ```; CLUST_ID = 0; gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']; gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]; results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]; ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065
https://github.com/scverse/scanpy/issues/748#issuecomment-515061065:540,Testability,test,test,540,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:; ```; CLUST_ID = 0; gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']; gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]; results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]; ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065
https://github.com/scverse/scanpy/issues/748#issuecomment-515061065:928,Testability,test,testing,928,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:; ```; CLUST_ID = 0; gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']; gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]; results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]; ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065
https://github.com/scverse/scanpy/issues/748#issuecomment-515061065:1048,Testability,test,testing,1048,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:; ```; CLUST_ID = 0; gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']; gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]; results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]; ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065
https://github.com/scverse/scanpy/issues/748#issuecomment-515114575:366,Deployability,integrat,integrated,366,"Hi, ; Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515114575
https://github.com/scverse/scanpy/issues/748#issuecomment-515114575:366,Integrability,integrat,integrated,366,"Hi, ; Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515114575
https://github.com/scverse/scanpy/issues/748#issuecomment-515114575:495,Safety,predict,predict,495,"Hi, ; Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515114575
https://github.com/scverse/scanpy/issues/748#issuecomment-515114575:143,Testability,Log,Logistic,143,"Hi, ; Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515114575
https://github.com/scverse/scanpy/issues/748#issuecomment-515114575:182,Testability,test,testing,182,"Hi, ; Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515114575
https://github.com/scverse/scanpy/issues/748#issuecomment-515114575:484,Testability,test,testing,484,"Hi, ; Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515114575
https://github.com/scverse/scanpy/issues/748#issuecomment-515168347:119,Modifiability,variab,variables,119,"Hey!. Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515168347
https://github.com/scverse/scanpy/issues/748#issuecomment-515168347:783,Safety,predict,predict,783,"Hey!. Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515168347
https://github.com/scverse/scanpy/issues/748#issuecomment-515168347:6,Testability,Log,Logistic,6,"Hey!. Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515168347
https://github.com/scverse/scanpy/issues/748#issuecomment-515168347:989,Usability,clear,clearer,989,"Hey!. Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515168347
https://github.com/scverse/scanpy/issues/749#issuecomment-515127872:82,Availability,error,error,82,"Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515127872
https://github.com/scverse/scanpy/issues/749#issuecomment-515127872:24,Integrability,message,message,24,"Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515127872
https://github.com/scverse/scanpy/issues/749#issuecomment-515127872:88,Integrability,message,message,88,"Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515127872
https://github.com/scverse/scanpy/issues/749#issuecomment-515138442:84,Availability,error,error,84,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply!; Unfortunately , no visible exception... My code is as follows:. ```py; import velocyto as vcy; import numpy as np; import scanpy as sc; import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""); S = vlm.S; S=S.transpose(); adata = anndata.AnnData(S); print(adata.X); print(adata.obs); print(adata.var). sc.pp.neighbors(adata, n_neighbors=100); adata.uns['iroot'] = 0; print(adata.uns); sc.tl.dpt(adata, n_branchings=2); sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'); ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb; WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`.; Falling back to preprocessing with `sc.pp.pca` and default params.; /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:; def make_euclidean_tree(data, indices, rng_state, leaf_size=30):; <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size); ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))); [2] Du",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442
https://github.com/scverse/scanpy/issues/749#issuecomment-515138442:666,Availability,error,error,666,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply!; Unfortunately , no visible exception... My code is as follows:. ```py; import velocyto as vcy; import numpy as np; import scanpy as sc; import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""); S = vlm.S; S=S.transpose(); adata = anndata.AnnData(S); print(adata.X); print(adata.obs); print(adata.var). sc.pp.neighbors(adata, n_neighbors=100); adata.uns['iroot'] = 0; print(adata.uns); sc.tl.dpt(adata, n_branchings=2); sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'); ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb; WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`.; Falling back to preprocessing with `sc.pp.pca` and default params.; /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:; def make_euclidean_tree(data, indices, rng_state, leaf_size=30):; <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size); ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))); [2] Du",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442
https://github.com/scverse/scanpy/issues/749#issuecomment-515138442:3210,Availability,error,errors,3210,"f_size=30):; <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size); ^. @numba.jit(); /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""make_euclidean_tree"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:; @numba.jit(); def make_euclidean_tree(data, indices, rng_state, leaf_size=30):; ^. self.func_ir.loc)); /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: ; Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:; @numba.jit(); def make_euclidean_tree(data, indices, rng_state, leaf_size=30):; ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)); /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: ; The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:; @numba.njit(parallel=True); def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):; ^. current_graph, n_vertices, n_neighbors, max_candidates, rng_state; /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:588: NumbaPerformanceWarning: ; The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442
https://github.com/scverse/scanpy/issues/749#issuecomment-515138442:5664,Availability,error,errors,5664,"hon3.6/site-packages/umap/umap_.py"", line 467:; def fuzzy_simplicial_set(; <source elided>; if knn_indices is None or knn_dists is None:; knn_indices, knn_dists, _ = nearest_neighbors(; ^. @numba.jit(); /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:; @numba.jit(); def fuzzy_simplicial_set(; ^. self.func_ir.loc)); /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: ; Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:; @numba.jit(); def fuzzy_simplicial_set(; ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)); OrderedDict([('neighbors', {'params': {'n_neighbors': 100, 'method': 'umap', 'metric': 'euclidean'}, 'distances': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'; 	with 1803087 stored elements in Compressed Sparse Row format>, 'connectivities': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'; 	with 2667882 stored elements in Compressed Sparse Row format>}), ('iroot', 0)]); WARNING: Trying to run `tl.dpt` without prior call of `tl.diffmap`. Falling back to `tl.diffmap` with default parameters.; WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`); WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`); WARNING: detected group with only [] cells; ```. </details>; <details><summary>Traceback</summary>. ```pytb; ValueError Traceback (most recent call last); ~/dif",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442
https://github.com/scverse/scanpy/issues/749#issuecomment-515138442:26,Integrability,message,message,26,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply!; Unfortunately , no visible exception... My code is as follows:. ```py; import velocyto as vcy; import numpy as np; import scanpy as sc; import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""); S = vlm.S; S=S.transpose(); adata = anndata.AnnData(S); print(adata.X); print(adata.obs); print(adata.var). sc.pp.neighbors(adata, n_neighbors=100); adata.uns['iroot'] = 0; print(adata.uns); sc.tl.dpt(adata, n_branchings=2); sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'); ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb; WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`.; Falling back to preprocessing with `sc.pp.pca` and default params.; /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:; def make_euclidean_tree(data, indices, rng_state, leaf_size=30):; <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size); ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))); [2] Du",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442
https://github.com/scverse/scanpy/issues/749#issuecomment-515138442:90,Integrability,message,message,90,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply!; Unfortunately , no visible exception... My code is as follows:. ```py; import velocyto as vcy; import numpy as np; import scanpy as sc; import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""); S = vlm.S; S=S.transpose(); adata = anndata.AnnData(S); print(adata.X); print(adata.obs); print(adata.var). sc.pp.neighbors(adata, n_neighbors=100); adata.uns['iroot'] = 0; print(adata.uns); sc.tl.dpt(adata, n_branchings=2); sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'); ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb; WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`.; Falling back to preprocessing with `sc.pp.pca` and default params.; /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:; def make_euclidean_tree(data, indices, rng_state, leaf_size=30):; <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size); ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))); [2] Du",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442
https://github.com/scverse/scanpy/issues/749#issuecomment-515138442:672,Integrability,message,message,672,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply!; Unfortunately , no visible exception... My code is as follows:. ```py; import velocyto as vcy; import numpy as np; import scanpy as sc; import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""); S = vlm.S; S=S.transpose(); adata = anndata.AnnData(S); print(adata.X); print(adata.obs); print(adata.var). sc.pp.neighbors(adata, n_neighbors=100); adata.uns['iroot'] = 0; print(adata.uns); sc.tl.dpt(adata, n_branchings=2); sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'); ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb; WARNING: You’re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`.; Falling back to preprocessing with `sc.pp.pca` and default params.; /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:; def make_euclidean_tree(data, indices, rng_state, leaf_size=30):; <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size); ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))); [2] Du",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442
https://github.com/scverse/scanpy/issues/749#issuecomment-515138442:2848,Safety,detect,detected,2848,"node = make_euclidean_tree(data, left_indices, rng_state, leaf_size); ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))); [2] During: typing of call at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:; def make_euclidean_tree(data, indices, rng_state, leaf_size=30):; <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size); ^. @numba.jit(); /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""make_euclidean_tree"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:; @numba.jit(); def make_euclidean_tree(data, indices, rng_state, leaf_size=30):; ^. self.func_ir.loc)); /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: ; Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 451:; @numba.jit(); def make_euclidean_tree(data, indices, rng_state, leaf_size=30):; ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)); /home/liz3/env/lib/python3.6/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: ; The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""env/lib/python3.6/site-packages/umap/utils.py"", line 409:; @numba.njit(parallel=True); def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):; ^. current_gra",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442
https://github.com/scverse/scanpy/issues/749#issuecomment-515138442:5343,Safety,detect,detected,5343,"def nn_descent(; ^. self.func_ir.loc)); /home/liz3/env/lib/python3.6/site-packages/umap/umap_.py:349: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""fuzzy_simplicial_set"" failed type inference due to: Untyped global name 'nearest_neighbors': cannot determine Numba type of <class 'function'>. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 467:; def fuzzy_simplicial_set(; <source elided>; if knn_indices is None or knn_dists is None:; knn_indices, knn_dists, _ = nearest_neighbors(; ^. @numba.jit(); /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:725: NumbaWarning: Function ""fuzzy_simplicial_set"" was compiled in object mode without forceobj=True. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:; @numba.jit(); def fuzzy_simplicial_set(; ^. self.func_ir.loc)); /home/liz3/env/lib/python3.6/site-packages/numba/compiler.py:734: NumbaDeprecationWarning: ; Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:; @numba.jit(); def fuzzy_simplicial_set(; ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)); OrderedDict([('neighbors', {'params': {'n_neighbors': 100, 'method': 'umap', 'metric': 'euclidean'}, 'distances': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'; 	with 1803087 stored elements in Compressed Sparse Row format>, 'connectivities': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'; 	with 2667882 stored elements in Compressed Sparse Row format>}), ('iroot', 0)]); WARNING: Trying to run `tl.dpt` without prior call of `tl.diffmap`. Falling back to `tl.diffmap` with default parameters.; WARNING: shifting branching point away from maximal ken",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442
https://github.com/scverse/scanpy/issues/749#issuecomment-515138442:6514,Safety,detect,detected,6514,"ation visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""env/lib/python3.6/site-packages/umap/umap_.py"", line 350:; @numba.jit(); def fuzzy_simplicial_set(; ^. warnings.warn(errors.NumbaDeprecationWarning(msg, self.func_ir.loc)); OrderedDict([('neighbors', {'params': {'n_neighbors': 100, 'method': 'umap', 'metric': 'euclidean'}, 'distances': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'; 	with 1803087 stored elements in Compressed Sparse Row format>, 'connectivities': <18213x18213 sparse matrix of type '<class 'numpy.float64'>'; 	with 2667882 stored elements in Compressed Sparse Row format>}), ('iroot', 0)]); WARNING: Trying to run `tl.dpt` without prior call of `tl.diffmap`. Falling back to `tl.diffmap` with default parameters.; WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`); WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`); WARNING: detected group with only [] cells; ```. </details>; <details><summary>Traceback</summary>. ```pytb; ValueError Traceback (most recent call last); ~/diffusion_map.py in <module>; 57 adata.uns['iroot'] = 0; 58 print(adata.uns); ---> 59 sc.tl.dpt(adata, n_branchings=2); 60 sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in dpt(adata, n_dcs, n_branchings, min_group_size, allow_kendall_tau_shift, copy); 128 # detect branchings and partition the data into segments; 129 if n_branchings > 0:; --> 130 dpt.branchings_segments(); 131 adata.obs['dpt_groups'] = pd.Categorical(; 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in branchings_segments(self); 187 for each segment.; 188 """"""; --> 189 self.detect_branchings(); 190 self.postprocess_segments(); 191 self.set_segs_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442
https://github.com/scverse/scanpy/issues/749#issuecomment-515138442:6992,Safety,detect,detect,6992,"arse matrix of type '<class 'numpy.float64'>'; 	with 2667882 stored elements in Compressed Sparse Row format>}), ('iroot', 0)]); WARNING: Trying to run `tl.dpt` without prior call of `tl.diffmap`. Falling back to `tl.diffmap` with default parameters.; WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`); WARNING: shifting branching point away from maximal kendall-tau correlation (suppress this with `allow_kendall_tau_shift=False`); WARNING: detected group with only [] cells; ```. </details>; <details><summary>Traceback</summary>. ```pytb; ValueError Traceback (most recent call last); ~/diffusion_map.py in <module>; 57 adata.uns['iroot'] = 0; 58 print(adata.uns); ---> 59 sc.tl.dpt(adata, n_branchings=2); 60 sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in dpt(adata, n_dcs, n_branchings, min_group_size, allow_kendall_tau_shift, copy); 128 # detect branchings and partition the data into segments; 129 if n_branchings > 0:; --> 130 dpt.branchings_segments(); 131 adata.obs['dpt_groups'] = pd.Categorical(; 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in branchings_segments(self); 187 for each segment.; 188 """"""; --> 189 self.detect_branchings(); 190 self.postprocess_segments(); 191 self.set_segs_names(). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branchings(self); 262 segs_connects,; 263 segs_undecided,; --> 264 segs_adjacency, iseg, tips3); 265 # store as class members; 266 self.segs = segs. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3); 476 # branching on the segment, return the list ssegs of segments that; 477 # are defined by splitting this segment; --> 478 result = self._detect_branching(Dseg, tips3, seg); 479 ssegs, ssegs_tips, ssegs_adjacenc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442
https://github.com/scverse/scanpy/issues/749#issuecomment-515138442:8233,Safety,detect,detected,8233,"anchings and partition the data into segments; 129 if n_branchings > 0:; --> 130 dpt.branchings_segments(); 131 adata.obs['dpt_groups'] = pd.Categorical(; 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in branchings_segments(self); 187 for each segment.; 188 """"""; --> 189 self.detect_branchings(); 190 self.postprocess_segments(); 191 self.set_segs_names(). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branchings(self); 262 segs_connects,; 263 segs_undecided,; --> 264 segs_adjacency, iseg, tips3); 265 # store as class members; 266 self.segs = segs. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3); 476 # branching on the segment, return the list ssegs of segments that; 477 # are defined by splitting this segment; --> 478 result = self._detect_branching(Dseg, tips3, seg); 479 ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk = result; 480 # map back to global indices. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in _detect_branching(self, Dseg, tips, seg_reference); 646 if len(np.flatnonzero(newseg)) <= 1:; 647 logg.warning(f'detected group with only {np.flatnonzero(newseg)} cells'); --> 648 secondtip = newseg[np.argmax(Dseg[tips[inewseg]][newseg])]; 649 ssegs_tips.append([tips[inewseg], secondtip]); 650 undecided_cells = np.arange(Dseg.shape[0], dtype=int)[nonunique]. ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in argmax(a, axis, out); 1101 ; 1102 """"""; -> 1103 return _wrapfunc(a, 'argmax', axis=axis, out=out); 1104 ; 1105 . ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds); 54 def _wrapfunc(obj, method, *args, **kwds):; 55 try:; ---> 56 return getattr(obj, method)(*args, **kwds); 57 ; 58 # An AttributeError occurs if the object does not have. ValueError: attempt to get argmax of an empty sequence; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442
https://github.com/scverse/scanpy/issues/749#issuecomment-515138442:8218,Testability,log,logg,8218,"anchings and partition the data into segments; 129 if n_branchings > 0:; --> 130 dpt.branchings_segments(); 131 adata.obs['dpt_groups'] = pd.Categorical(; 132 values=dpt.segs_names.astype('U'),. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in branchings_segments(self); 187 for each segment.; 188 """"""; --> 189 self.detect_branchings(); 190 self.postprocess_segments(); 191 self.set_segs_names(). ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branchings(self); 262 segs_connects,; 263 segs_undecided,; --> 264 segs_adjacency, iseg, tips3); 265 # store as class members; 266 self.segs = segs. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3); 476 # branching on the segment, return the list ssegs of segments that; 477 # are defined by splitting this segment; --> 478 result = self._detect_branching(Dseg, tips3, seg); 479 ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk = result; 480 # map back to global indices. ~/env/lib/python3.6/site-packages/scanpy/tools/_dpt.py in _detect_branching(self, Dseg, tips, seg_reference); 646 if len(np.flatnonzero(newseg)) <= 1:; 647 logg.warning(f'detected group with only {np.flatnonzero(newseg)} cells'); --> 648 secondtip = newseg[np.argmax(Dseg[tips[inewseg]][newseg])]; 649 ssegs_tips.append([tips[inewseg], secondtip]); 650 undecided_cells = np.arange(Dseg.shape[0], dtype=int)[nonunique]. ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in argmax(a, axis, out); 1101 ; 1102 """"""; -> 1103 return _wrapfunc(a, 'argmax', axis=axis, out=out); 1104 ; 1105 . ~/env/lib/python3.6/site-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds); 54 def _wrapfunc(obj, method, *args, **kwds):; 55 try:; ---> 56 return getattr(obj, method)(*args, **kwds); 57 ; 58 # An AttributeError occurs if the object does not have. ValueError: attempt to get argmax of an empty sequence; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442
https://github.com/scverse/scanpy/issues/749#issuecomment-528179543:19,Availability,error,error,19,The Numba parallel error also occurred to me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-528179543
https://github.com/scverse/scanpy/issues/749#issuecomment-635348051:284,Performance,queue,queue,284,"This issue is still persistent. I've created a colab notebook that shows the issue on a dataset we subsample to 6000 cells:. https://colab.research.google.com/drive/1QrnDFZ7nDNOLx9gr92eknhKShd2aTIdN. @gokceneraslan can you please throw a ""bug"" tag on this issue so it gets put in the queue?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-635348051
https://github.com/scverse/scanpy/issues/749#issuecomment-635762909:111,Availability,error,error,111,"[Here's the `AnnData` object](https://cloudstor.aarnet.edu.au/plus/s/oYjEB26gWJdaA4R) which will reproduce the error if you call: `sc.tl.dpt(adata, n_branchings=N)` where N > 3. @falexwolf, maybe you could help with diagnosis here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-635762909
https://github.com/scverse/scanpy/issues/751#issuecomment-515740613:553,Availability,error,error,553,"sudo rm /media/ubuntu/d0b69706-4d42-40a3-b531-382041477d35/home/cns/biosoft/cellranger/cellranger-3.0.2/deng2_count_myself -fr. At 2019-07-27 18:48:50, ""Cristian"" <notifications@github.com> wrote:. Good day!. I have been trying to run the single cell tutorial but have had some issues concatenating several datasets. I am able to read successfully the first data set. However, once I want to load the other datasets, there is a problem concatenating the files. This happens in the first loop to load all the datasets. If I run only one dataset the same error (unsupported operand type(s) for +: 'int' and 'str') showed up when I plot some data quality summary plots:. For instance:; p1 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac') p2 = sc.pl.scatter(adata[adata.obs['n_counts']<10000], 'n_counts', 'n_genes', color='mt_frac'); adata = adata[adata.obs['mt_frac'] < 0.2] print('Number of cells after MT filter: {:d}'.format(adata.n_obs)); sc.pp.filter_cells(adata, min_genes = 700) print('Number of cells after gene filter: {:d}'.format(adata.n_obs)). I am using data generated by 10x V3 and CellRanger v3.0.1. I really do not know where the problem is. I really appreciate any advice/help to solve this issue. Thanks in advance. —; You are receiving this because you are subscribed to this thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/751#issuecomment-515740613
https://github.com/scverse/scanpy/issues/751#issuecomment-515740613:392,Performance,load,load,392,"sudo rm /media/ubuntu/d0b69706-4d42-40a3-b531-382041477d35/home/cns/biosoft/cellranger/cellranger-3.0.2/deng2_count_myself -fr. At 2019-07-27 18:48:50, ""Cristian"" <notifications@github.com> wrote:. Good day!. I have been trying to run the single cell tutorial but have had some issues concatenating several datasets. I am able to read successfully the first data set. However, once I want to load the other datasets, there is a problem concatenating the files. This happens in the first loop to load all the datasets. If I run only one dataset the same error (unsupported operand type(s) for +: 'int' and 'str') showed up when I plot some data quality summary plots:. For instance:; p1 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac') p2 = sc.pl.scatter(adata[adata.obs['n_counts']<10000], 'n_counts', 'n_genes', color='mt_frac'); adata = adata[adata.obs['mt_frac'] < 0.2] print('Number of cells after MT filter: {:d}'.format(adata.n_obs)); sc.pp.filter_cells(adata, min_genes = 700) print('Number of cells after gene filter: {:d}'.format(adata.n_obs)). I am using data generated by 10x V3 and CellRanger v3.0.1. I really do not know where the problem is. I really appreciate any advice/help to solve this issue. Thanks in advance. —; You are receiving this because you are subscribed to this thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/751#issuecomment-515740613
https://github.com/scverse/scanpy/issues/751#issuecomment-515740613:495,Performance,load,load,495,"sudo rm /media/ubuntu/d0b69706-4d42-40a3-b531-382041477d35/home/cns/biosoft/cellranger/cellranger-3.0.2/deng2_count_myself -fr. At 2019-07-27 18:48:50, ""Cristian"" <notifications@github.com> wrote:. Good day!. I have been trying to run the single cell tutorial but have had some issues concatenating several datasets. I am able to read successfully the first data set. However, once I want to load the other datasets, there is a problem concatenating the files. This happens in the first loop to load all the datasets. If I run only one dataset the same error (unsupported operand type(s) for +: 'int' and 'str') showed up when I plot some data quality summary plots:. For instance:; p1 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac') p2 = sc.pl.scatter(adata[adata.obs['n_counts']<10000], 'n_counts', 'n_genes', color='mt_frac'); adata = adata[adata.obs['mt_frac'] < 0.2] print('Number of cells after MT filter: {:d}'.format(adata.n_obs)); sc.pp.filter_cells(adata, min_genes = 700) print('Number of cells after gene filter: {:d}'.format(adata.n_obs)). I am using data generated by 10x V3 and CellRanger v3.0.1. I really do not know where the problem is. I really appreciate any advice/help to solve this issue. Thanks in advance. —; You are receiving this because you are subscribed to this thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/751#issuecomment-515740613
https://github.com/scverse/scanpy/issues/752#issuecomment-517984501:41,Availability,avail,available,41,@ivirshup I fixed the build it should be available now. The issue can be closed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/752#issuecomment-517984501
https://github.com/scverse/scanpy/issues/753#issuecomment-517530320:108,Safety,safe,safe,108,This is a consequence of our rec-arrays where we have to specify the length of strings. I think it would be safe to calculate the maximum length of a gene name instead of using a hard coded value.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/753#issuecomment-517530320
https://github.com/scverse/scanpy/issues/753#issuecomment-522930652:143,Energy Efficiency,reduce,reduce,143,"It'll take a little doing, but it's certainly do-able. Something like this should do it:. ```python; import numpy as np; from functools import reduce. def concat(arrays: ""list[np.recarray]""):; names = arrays[0].dtype.names; dtypes = [dict(a.dtype.descr) for a in arrays]; assert all(arrays[0].dtype.names == a.dtype.names for a in arrays[1:]), ""All arrays should have same names""; ; offset = 0; out_dtypes = {}; for k in names:; out_dtype = reduce(np.result_type, (dtype[k] for dtype in dtypes)); out_dtypes[k] = (out_dtype, offset); offset += out_dtype.alignment. out_recarray = np.recarray(sum(map(len, arrays)), dtype=out_dtypes) ; np.concatenate(arrays, out=out_recarray); ; return out_recarray; ```. Maybe the solution should happen upstream though. . Do we concatenate recarrays often?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/753#issuecomment-522930652
https://github.com/scverse/scanpy/issues/753#issuecomment-522930652:441,Energy Efficiency,reduce,reduce,441,"It'll take a little doing, but it's certainly do-able. Something like this should do it:. ```python; import numpy as np; from functools import reduce. def concat(arrays: ""list[np.recarray]""):; names = arrays[0].dtype.names; dtypes = [dict(a.dtype.descr) for a in arrays]; assert all(arrays[0].dtype.names == a.dtype.names for a in arrays[1:]), ""All arrays should have same names""; ; offset = 0; out_dtypes = {}; for k in names:; out_dtype = reduce(np.result_type, (dtype[k] for dtype in dtypes)); out_dtypes[k] = (out_dtype, offset); offset += out_dtype.alignment. out_recarray = np.recarray(sum(map(len, arrays)), dtype=out_dtypes) ; np.concatenate(arrays, out=out_recarray); ; return out_recarray; ```. Maybe the solution should happen upstream though. . Do we concatenate recarrays often?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/753#issuecomment-522930652
https://github.com/scverse/scanpy/issues/753#issuecomment-522930652:272,Testability,assert,assert,272,"It'll take a little doing, but it's certainly do-able. Something like this should do it:. ```python; import numpy as np; from functools import reduce. def concat(arrays: ""list[np.recarray]""):; names = arrays[0].dtype.names; dtypes = [dict(a.dtype.descr) for a in arrays]; assert all(arrays[0].dtype.names == a.dtype.names for a in arrays[1:]), ""All arrays should have same names""; ; offset = 0; out_dtypes = {}; for k in names:; out_dtype = reduce(np.result_type, (dtype[k] for dtype in dtypes)); out_dtypes[k] = (out_dtype, offset); offset += out_dtype.alignment. out_recarray = np.recarray(sum(map(len, arrays)), dtype=out_dtypes) ; np.concatenate(arrays, out=out_recarray); ; return out_recarray; ```. Maybe the solution should happen upstream though. . Do we concatenate recarrays often?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/753#issuecomment-522930652
https://github.com/scverse/scanpy/issues/754#issuecomment-517541034:721,Testability,log,logfoldchanges,721,"A quick reproducible example:. ```python; import scanpy as sc; pbmc = sc.datasets.pbmc68k_reduced(); pbmc.X = pbmc.raw.X; sc.tl.rank_genes_groups( ; pbmc, ; groupby=""bulk_labels"", ; groups=[""CD14+ Monocyte"", ""Dendritic""], ; reference=""Dendritic"", ; n_genes=pbmc.shape[1], ; method='wilcoxon' ; ) ; md_d = ( ; sc.get.rank_genes_groups_df(pbmc, group=""CD14+ Monocyte"") ; .set_index(""names"", drop=False) ; ) ; ; sc.tl.rank_genes_groups( ; pbmc, ; groupby=""bulk_labels"", ; groups=[""CD14+ Monocyte"", ""Dendritic""], ; reference=""CD14+ Monocyte"", ; n_genes=pbmc.shape[1], ; method='wilcoxon' ; ) ; md_m = ( ; sc.get.rank_genes_groups_df(pbmc, group=""Dendritic"") ; .set_index(""names"", drop=False) ; ). md_d.head(); # scores names logfoldchanges pvals pvals_adj; # names ; # FTL 13.163277 FTL 1.600541 1.427571e-39 1.092092e-36; # AIF1 12.768205 AIF1 1.882886 2.467807e-37 9.439361e-35; # FCGR3A 12.733917 FCGR3A 4.500901 3.831234e-37 9.769647e-35; # PSAP 12.576810 PSAP 1.998426 2.832393e-36 5.416951e-34; # FCER1G 12.152568 FCER1G 1.596950 5.559192e-34 8.505565e-32; md_m.tail()[::-1]; # scores names logfoldchanges pvals pvals_adj; # names ; # FTL -12.616215 FTL -1.600541 1.718871e-36 1.314936e-33; # FCGR3A -12.204766 FCGR3A -4.500901 2.931495e-34 7.919483e-32; # AIF1 -12.176620 AIF1 -1.882886 4.140906e-34 7.919483e-32; # PSAP -12.115210 PSAP -1.998426 8.773953e-34 1.342415e-31; # FCER1G -11.519019 FCER1G -1.596950 1.058089e-30 8.094380e-29; ```. I think the log fold changes are pretty close, and those small changes could be occurring due to different order of operations and the use of single precision. I'm not to worried about these. . Could someone more familiar with the differential expression code comment about p-value correctness? @falexwolf @a-munoz-rojas? A few of the values look pretty different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/754#issuecomment-517541034
https://github.com/scverse/scanpy/issues/754#issuecomment-517541034:1093,Testability,log,logfoldchanges,1093,"A quick reproducible example:. ```python; import scanpy as sc; pbmc = sc.datasets.pbmc68k_reduced(); pbmc.X = pbmc.raw.X; sc.tl.rank_genes_groups( ; pbmc, ; groupby=""bulk_labels"", ; groups=[""CD14+ Monocyte"", ""Dendritic""], ; reference=""Dendritic"", ; n_genes=pbmc.shape[1], ; method='wilcoxon' ; ) ; md_d = ( ; sc.get.rank_genes_groups_df(pbmc, group=""CD14+ Monocyte"") ; .set_index(""names"", drop=False) ; ) ; ; sc.tl.rank_genes_groups( ; pbmc, ; groupby=""bulk_labels"", ; groups=[""CD14+ Monocyte"", ""Dendritic""], ; reference=""CD14+ Monocyte"", ; n_genes=pbmc.shape[1], ; method='wilcoxon' ; ) ; md_m = ( ; sc.get.rank_genes_groups_df(pbmc, group=""Dendritic"") ; .set_index(""names"", drop=False) ; ). md_d.head(); # scores names logfoldchanges pvals pvals_adj; # names ; # FTL 13.163277 FTL 1.600541 1.427571e-39 1.092092e-36; # AIF1 12.768205 AIF1 1.882886 2.467807e-37 9.439361e-35; # FCGR3A 12.733917 FCGR3A 4.500901 3.831234e-37 9.769647e-35; # PSAP 12.576810 PSAP 1.998426 2.832393e-36 5.416951e-34; # FCER1G 12.152568 FCER1G 1.596950 5.559192e-34 8.505565e-32; md_m.tail()[::-1]; # scores names logfoldchanges pvals pvals_adj; # names ; # FTL -12.616215 FTL -1.600541 1.718871e-36 1.314936e-33; # FCGR3A -12.204766 FCGR3A -4.500901 2.931495e-34 7.919483e-32; # AIF1 -12.176620 AIF1 -1.882886 4.140906e-34 7.919483e-32; # PSAP -12.115210 PSAP -1.998426 8.773953e-34 1.342415e-31; # FCER1G -11.519019 FCER1G -1.596950 1.058089e-30 8.094380e-29; ```. I think the log fold changes are pretty close, and those small changes could be occurring due to different order of operations and the use of single precision. I'm not to worried about these. . Could someone more familiar with the differential expression code comment about p-value correctness? @falexwolf @a-munoz-rojas? A few of the values look pretty different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/754#issuecomment-517541034
https://github.com/scverse/scanpy/issues/754#issuecomment-517541034:1458,Testability,log,log,1458,"A quick reproducible example:. ```python; import scanpy as sc; pbmc = sc.datasets.pbmc68k_reduced(); pbmc.X = pbmc.raw.X; sc.tl.rank_genes_groups( ; pbmc, ; groupby=""bulk_labels"", ; groups=[""CD14+ Monocyte"", ""Dendritic""], ; reference=""Dendritic"", ; n_genes=pbmc.shape[1], ; method='wilcoxon' ; ) ; md_d = ( ; sc.get.rank_genes_groups_df(pbmc, group=""CD14+ Monocyte"") ; .set_index(""names"", drop=False) ; ) ; ; sc.tl.rank_genes_groups( ; pbmc, ; groupby=""bulk_labels"", ; groups=[""CD14+ Monocyte"", ""Dendritic""], ; reference=""CD14+ Monocyte"", ; n_genes=pbmc.shape[1], ; method='wilcoxon' ; ) ; md_m = ( ; sc.get.rank_genes_groups_df(pbmc, group=""Dendritic"") ; .set_index(""names"", drop=False) ; ). md_d.head(); # scores names logfoldchanges pvals pvals_adj; # names ; # FTL 13.163277 FTL 1.600541 1.427571e-39 1.092092e-36; # AIF1 12.768205 AIF1 1.882886 2.467807e-37 9.439361e-35; # FCGR3A 12.733917 FCGR3A 4.500901 3.831234e-37 9.769647e-35; # PSAP 12.576810 PSAP 1.998426 2.832393e-36 5.416951e-34; # FCER1G 12.152568 FCER1G 1.596950 5.559192e-34 8.505565e-32; md_m.tail()[::-1]; # scores names logfoldchanges pvals pvals_adj; # names ; # FTL -12.616215 FTL -1.600541 1.718871e-36 1.314936e-33; # FCGR3A -12.204766 FCGR3A -4.500901 2.931495e-34 7.919483e-32; # AIF1 -12.176620 AIF1 -1.882886 4.140906e-34 7.919483e-32; # PSAP -12.115210 PSAP -1.998426 8.773953e-34 1.342415e-31; # FCER1G -11.519019 FCER1G -1.596950 1.058089e-30 8.094380e-29; ```. I think the log fold changes are pretty close, and those small changes could be occurring due to different order of operations and the use of single precision. I'm not to worried about these. . Could someone more familiar with the differential expression code comment about p-value correctness? @falexwolf @a-munoz-rojas? A few of the values look pretty different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/754#issuecomment-517541034
https://github.com/scverse/scanpy/issues/756#issuecomment-516324433:159,Performance,cache,caches,159,"The biggest ones are in order:. 1. [ ] `numba`: Hard to defer. We’d have to create our own `jit` decorator returning a callable object that numba-compiles and caches the real function on its first invocation; 2. ~~`pandas`~~: Used all over the place, not feasible to defer; 3. [x] `sklearn.metrics`: Easy to defer I think, let’s start with this.; 4. [ ] `matplotlib.pyplot`: Shouldn’t be used in a library at all. It exists to import the kitchen sink in order to be low-friction for interactive use. Hard to do since we rely on it a lot, but we should do it.; 5. [x] `networkx`: Used in DPT, paga and plotting. Pretty easy. We use pandas all over the place, and it’s hard to defer loading numba as it works with decorators. /edit: shaved off another 2/5 in a7729bc61ac569a718075edb4466852b0b4a696a via `sklearn.metrics`, `scipy.stats`, and `networkx`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-516324433
https://github.com/scverse/scanpy/issues/756#issuecomment-516324433:681,Performance,load,loading,681,"The biggest ones are in order:. 1. [ ] `numba`: Hard to defer. We’d have to create our own `jit` decorator returning a callable object that numba-compiles and caches the real function on its first invocation; 2. ~~`pandas`~~: Used all over the place, not feasible to defer; 3. [x] `sklearn.metrics`: Easy to defer I think, let’s start with this.; 4. [ ] `matplotlib.pyplot`: Shouldn’t be used in a library at all. It exists to import the kitchen sink in order to be low-friction for interactive use. Hard to do since we rely on it a lot, but we should do it.; 5. [x] `networkx`: Used in DPT, paga and plotting. Pretty easy. We use pandas all over the place, and it’s hard to defer loading numba as it works with decorators. /edit: shaved off another 2/5 in a7729bc61ac569a718075edb4466852b0b4a696a via `sklearn.metrics`, `scipy.stats`, and `networkx`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-516324433
https://github.com/scverse/scanpy/issues/756#issuecomment-516404460:514,Integrability,depend,depending,514,"I don't think we should bother with numba, since it'll likely be a pretty core requirement once we can start transitioning to `pydata/sparse`. For `pyplot`, does `matplotlib` also take a while to import? Management of environment variables is a good reason not to defer that import. If we're already using `h5py`, could we drop `tables` as a requirement?. I think bad import times are only really noticeable for interactive use, since any script using scanpy will likely take longer to run. Do import times change depending on interactive environment? I wouldn't be surprised if different code ran when importing something like matplotlib in a notebook vs in a script.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-516404460
https://github.com/scverse/scanpy/issues/756#issuecomment-516404460:230,Modifiability,variab,variables,230,"I don't think we should bother with numba, since it'll likely be a pretty core requirement once we can start transitioning to `pydata/sparse`. For `pyplot`, does `matplotlib` also take a while to import? Management of environment variables is a good reason not to defer that import. If we're already using `h5py`, could we drop `tables` as a requirement?. I think bad import times are only really noticeable for interactive use, since any script using scanpy will likely take longer to run. Do import times change depending on interactive environment? I wouldn't be surprised if different code ran when importing something like matplotlib in a notebook vs in a script.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-516404460
https://github.com/scverse/scanpy/issues/756#issuecomment-522595568:102,Modifiability,variab,variables,102,"Matplotlib takes a while but less time. Can you please point me to what you mean with the environment variables?. No idea about tables, @falexwolf wrote the sim module I think and it’s not commonly used …. I don’t think import times change noticably, but I didn’t measure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-522595568
https://github.com/scverse/scanpy/issues/756#issuecomment-522822622:278,Integrability,depend,depending,278,"I've gotten complaints from Matplotlib about calling `mpl.use`, to set the backend after importing `pyplot` ([relevant matplotlib docs](https://matplotlib.org/tutorials/introductory/usage.html#what-is-a-backend)). I think it would be unintuitive if packages behaved differently depending on what functions had been called. In general, matplotlib has a lot of state and messing with it has only brought me pain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-522822622
https://github.com/scverse/scanpy/issues/756#issuecomment-523026212:420,Modifiability,config,configure,420,"That’s exactly backwards: I find it annoying if packages modify state on import. We already jump through hoops in our testing framework to work around our misbehavior:. https://github.com/theislab/scanpy/blob/681ce93e7e58956cb78ef81bc165558b84d6ebb0/scanpy/tests/conftest.py#L4-L6. `import matplotlib.pyplot [as plt]` means “I’m an end user who just opened a notebook and I want the kitchen sink, give me everything and configure everything”. Libraries shouldn’t do it and scanpy is one. When we still had `scanpy.api` there would have been a case for importing pyplot there, as `scanpy.api` was for interactive use. Now we don’t have any excuses.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-523026212
https://github.com/scverse/scanpy/issues/756#issuecomment-523026212:118,Testability,test,testing,118,"That’s exactly backwards: I find it annoying if packages modify state on import. We already jump through hoops in our testing framework to work around our misbehavior:. https://github.com/theislab/scanpy/blob/681ce93e7e58956cb78ef81bc165558b84d6ebb0/scanpy/tests/conftest.py#L4-L6. `import matplotlib.pyplot [as plt]` means “I’m an end user who just opened a notebook and I want the kitchen sink, give me everything and configure everything”. Libraries shouldn’t do it and scanpy is one. When we still had `scanpy.api` there would have been a case for importing pyplot there, as `scanpy.api` was for interactive use. Now we don’t have any excuses.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-523026212
https://github.com/scverse/scanpy/issues/756#issuecomment-523026212:257,Testability,test,tests,257,"That’s exactly backwards: I find it annoying if packages modify state on import. We already jump through hoops in our testing framework to work around our misbehavior:. https://github.com/theislab/scanpy/blob/681ce93e7e58956cb78ef81bc165558b84d6ebb0/scanpy/tests/conftest.py#L4-L6. `import matplotlib.pyplot [as plt]` means “I’m an end user who just opened a notebook and I want the kitchen sink, give me everything and configure everything”. Libraries shouldn’t do it and scanpy is one. When we still had `scanpy.api` there would have been a case for importing pyplot there, as `scanpy.api` was for interactive use. Now we don’t have any excuses.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-523026212
https://github.com/scverse/scanpy/issues/756#issuecomment-523738527:495,Integrability,depend,depend,495,"I agree that it's bad behavior to modify state on import. I think it's worse to modify state after a function is called, save a few cases where it's obvious that will happen. I think it takes less time to figure out why my plot suddenly looks different if it's based on imports than which functions were called prior. I think if we could make all of our plots without importing `pyplot` that would be great. I'm not sure how feasible this is. Not only do we use `pyplot` a lot, but libraries we depend on for plots (like `seaborn`) import `pyplot`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-523738527
https://github.com/scverse/scanpy/issues/757#issuecomment-516793637:253,Modifiability,variab,variable,253,"Reproducible example:. ```python; import scanpy as sc; import scanpy.external as ice; from itertools import cycle. pbmc = sc.datasets.pbmc68k_reduced(); sce.pp.mnn_correct(pbmc, batch_key=""phase""); ```. It looks like `mnn_correct` is only returning one variable, through its documentation looks like it should return three. @chriscainx, could you offer some guidance here?. As a workaround for now, you could just call `mnnpy.mnn_correct` with the same signature you've been using. It'll return a one-tuple with a modified anndata object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/757#issuecomment-516793637
https://github.com/scverse/scanpy/issues/757#issuecomment-516793637:358,Usability,guid,guidance,358,"Reproducible example:. ```python; import scanpy as sc; import scanpy.external as ice; from itertools import cycle. pbmc = sc.datasets.pbmc68k_reduced(); sce.pp.mnn_correct(pbmc, batch_key=""phase""); ```. It looks like `mnn_correct` is only returning one variable, through its documentation looks like it should return three. @chriscainx, could you offer some guidance here?. As a workaround for now, you could just call `mnnpy.mnn_correct` with the same signature you've been using. It'll return a one-tuple with a modified anndata object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/757#issuecomment-516793637
https://github.com/scverse/scanpy/issues/757#issuecomment-523455880:88,Availability,error,errors,88,"@ivirshup Sorry I need to correct my previous answer. `mnnpy.mnn_correct` is not giving errors, but is returning a tuple. Check my issue here https://github.com/chriscainx/mnnpy/issues/27",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/757#issuecomment-523455880
https://github.com/scverse/scanpy/issues/757#issuecomment-542159460:23,Availability,error,error,23,I also got the similar error. ![image](https://user-images.githubusercontent.com/49429496/66826207-8652c580-ef7e-11e9-9168-5c19aa666354.png); ![image](https://user-images.githubusercontent.com/49429496/66826292-bd28db80-ef7e-11e9-801e-1d2dfbf01cb8.png),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/757#issuecomment-542159460
https://github.com/scverse/scanpy/issues/758#issuecomment-517145132:19,Availability,error,error,19,"It looks like this error occurs whenever `batch_key` is specified and `inplace=False`. MCVE:. ```python; import scanpy as sc; pbmc = sc.datasets.pbmc68k_reduced(); pbmc.X = pbmc.raw.X # So we have reasonable values to calculate on. # These do not throw an error:; sc.pp.highly_variable_genes(pbmc, batch_key=""phase""); sc.pp.highly_variable_genes(pbmc, inplace=False). # This throws an error; sc.pp.highly_variable_genes(pbmc, batch_key=""phase"", inplace=False); ```. This does raise the question of what `inplace=False` should return for the batch case. I'd think a recarray (maybe this should change to a dataframe?) with metrics for each of the batches. You could end up with columns with names like: `means_{batch1}`, `dispersions_{batch1}` etc. @danielStrobl, what were you expecting this to return?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/758#issuecomment-517145132
https://github.com/scverse/scanpy/issues/758#issuecomment-517145132:256,Availability,error,error,256,"It looks like this error occurs whenever `batch_key` is specified and `inplace=False`. MCVE:. ```python; import scanpy as sc; pbmc = sc.datasets.pbmc68k_reduced(); pbmc.X = pbmc.raw.X # So we have reasonable values to calculate on. # These do not throw an error:; sc.pp.highly_variable_genes(pbmc, batch_key=""phase""); sc.pp.highly_variable_genes(pbmc, inplace=False). # This throws an error; sc.pp.highly_variable_genes(pbmc, batch_key=""phase"", inplace=False); ```. This does raise the question of what `inplace=False` should return for the batch case. I'd think a recarray (maybe this should change to a dataframe?) with metrics for each of the batches. You could end up with columns with names like: `means_{batch1}`, `dispersions_{batch1}` etc. @danielStrobl, what were you expecting this to return?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/758#issuecomment-517145132
https://github.com/scverse/scanpy/issues/758#issuecomment-517145132:385,Availability,error,error,385,"It looks like this error occurs whenever `batch_key` is specified and `inplace=False`. MCVE:. ```python; import scanpy as sc; pbmc = sc.datasets.pbmc68k_reduced(); pbmc.X = pbmc.raw.X # So we have reasonable values to calculate on. # These do not throw an error:; sc.pp.highly_variable_genes(pbmc, batch_key=""phase""); sc.pp.highly_variable_genes(pbmc, inplace=False). # This throws an error; sc.pp.highly_variable_genes(pbmc, batch_key=""phase"", inplace=False); ```. This does raise the question of what `inplace=False` should return for the batch case. I'd think a recarray (maybe this should change to a dataframe?) with metrics for each of the batches. You could end up with columns with names like: `means_{batch1}`, `dispersions_{batch1}` etc. @danielStrobl, what were you expecting this to return?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/758#issuecomment-517145132
https://github.com/scverse/scanpy/issues/762#issuecomment-517618114:170,Modifiability,layers,layers,170,"@coh-racng I would like to add that for your specific intention the best way is to load the `plot_scatter` function that accepts `basis` as parameter and works well with layers. The code should be:; ```PYTHON; from scanpy._plotting.scatterplots import plot_scatter`; plot_scatter(adata, basis='<name>'....); ```. @ivirshup, @falexwolf I think we should add `plot_scatter` to the API maybe renaming it `plot_embedding` to help users like @coh-racng. Currently we have two different ways to make scatter plots: One for embeddings (`plot_scatter`) and other more generic for obs and vars (`sc.pl.scatter`) that accepts `x` and `y` parameters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/762#issuecomment-517618114
https://github.com/scverse/scanpy/issues/762#issuecomment-517618114:83,Performance,load,load,83,"@coh-racng I would like to add that for your specific intention the best way is to load the `plot_scatter` function that accepts `basis` as parameter and works well with layers. The code should be:; ```PYTHON; from scanpy._plotting.scatterplots import plot_scatter`; plot_scatter(adata, basis='<name>'....); ```. @ivirshup, @falexwolf I think we should add `plot_scatter` to the API maybe renaming it `plot_embedding` to help users like @coh-racng. Currently we have two different ways to make scatter plots: One for embeddings (`plot_scatter`) and other more generic for obs and vars (`sc.pl.scatter`) that accepts `x` and `y` parameters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/762#issuecomment-517618114
https://github.com/scverse/scanpy/issues/762#issuecomment-522915275:0,Deployability,Update,Update,0,"Update: The initial issue has been fixed, we don't have a more generic embedding plotting function yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/762#issuecomment-522915275
https://github.com/scverse/scanpy/issues/763#issuecomment-517842921:26,Availability,down,downstream,26,"Either that, or allow the downstream code to gracefully handle `inf` values.; It is the binning procedure for both 'seurat' and 'cell_ranger' that seem to be a problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763#issuecomment-517842921
https://github.com/scverse/scanpy/issues/763#issuecomment-517851311:72,Availability,fault,fault,72,"Sorry, just realizing that this function expects logarithmized data. My fault.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763#issuecomment-517851311
https://github.com/scverse/scanpy/issues/763#issuecomment-517851311:49,Testability,log,logarithmized,49,"Sorry, just realizing that this function expects logarithmized data. My fault.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763#issuecomment-517851311
https://github.com/scverse/scanpy/issues/763#issuecomment-517929825:10,Availability,error,error,10,"Still the error message could be a lot better. I’ve made the same mistake,; it’s easy to forget to log the data. On Fri 2 Aug 2019 at 23:36, Stephen Fleming <notifications@github.com>; wrote:. > Closed #763 <https://github.com/theislab/scanpy/issues/763>.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/763?email_source=notifications&email_token=AACL4TL6QHUQMHIBKEQT5GLQCSSFFA5CNFSM4IJBAFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGOS3M3XBA#event-2530851716>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TM5VZDC544TAQPK7NDQCSSFFANCNFSM4IJBAFAA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763#issuecomment-517929825
https://github.com/scverse/scanpy/issues/763#issuecomment-517929825:16,Integrability,message,message,16,"Still the error message could be a lot better. I’ve made the same mistake,; it’s easy to forget to log the data. On Fri 2 Aug 2019 at 23:36, Stephen Fleming <notifications@github.com>; wrote:. > Closed #763 <https://github.com/theislab/scanpy/issues/763>.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/763?email_source=notifications&email_token=AACL4TL6QHUQMHIBKEQT5GLQCSSFFA5CNFSM4IJBAFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGOS3M3XBA#event-2530851716>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TM5VZDC544TAQPK7NDQCSSFFANCNFSM4IJBAFAA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763#issuecomment-517929825
https://github.com/scverse/scanpy/issues/763#issuecomment-517929825:99,Testability,log,log,99,"Still the error message could be a lot better. I’ve made the same mistake,; it’s easy to forget to log the data. On Fri 2 Aug 2019 at 23:36, Stephen Fleming <notifications@github.com>; wrote:. > Closed #763 <https://github.com/theislab/scanpy/issues/763>.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/763?email_source=notifications&email_token=AACL4TL6QHUQMHIBKEQT5GLQCSSFFA5CNFSM4IJBAFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGOS3M3XBA#event-2530851716>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TM5VZDC544TAQPK7NDQCSSFFANCNFSM4IJBAFAA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763#issuecomment-517929825
https://github.com/scverse/scanpy/issues/763#issuecomment-1137813331:9,Availability,error,error,9,"The same error in `sc.pp.highly_variable_genes` can pop up also if you forget to `sc.pp.filter_genes(adata, min_cells=0)` before running normalization and logging. Some informative error messages could for sure save some time here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763#issuecomment-1137813331
https://github.com/scverse/scanpy/issues/763#issuecomment-1137813331:181,Availability,error,error,181,"The same error in `sc.pp.highly_variable_genes` can pop up also if you forget to `sc.pp.filter_genes(adata, min_cells=0)` before running normalization and logging. Some informative error messages could for sure save some time here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763#issuecomment-1137813331
https://github.com/scverse/scanpy/issues/763#issuecomment-1137813331:187,Integrability,message,messages,187,"The same error in `sc.pp.highly_variable_genes` can pop up also if you forget to `sc.pp.filter_genes(adata, min_cells=0)` before running normalization and logging. Some informative error messages could for sure save some time here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763#issuecomment-1137813331
https://github.com/scverse/scanpy/issues/763#issuecomment-1137813331:155,Testability,log,logging,155,"The same error in `sc.pp.highly_variable_genes` can pop up also if you forget to `sc.pp.filter_genes(adata, min_cells=0)` before running normalization and logging. Some informative error messages could for sure save some time here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763#issuecomment-1137813331
https://github.com/scverse/scanpy/issues/763#issuecomment-1309646072:87,Availability,error,error,87,do not `sc.pp.scale(adata)` before use `sc.pp.highly_variable_genes` will not show the error,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763#issuecomment-1309646072
https://github.com/scverse/scanpy/issues/763#issuecomment-1854543723:37,Testability,Log,Log-transform,37,"Thanks, all I had to do was:. ```; # Log-transform the data; sc.pp.log1p(adata); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763#issuecomment-1854543723
https://github.com/scverse/scanpy/pull/764#issuecomment-517982481:35,Testability,test,test,35,Thanks for the PR! Could you add a test too?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/764#issuecomment-517982481
https://github.com/scverse/scanpy/pull/764#issuecomment-522894347:48,Testability,test,test,48,"@coh-racng, I've merged your fix in #790 with a test added. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/764#issuecomment-522894347
https://github.com/scverse/scanpy/issues/767#issuecomment-519071187:357,Usability,simpl,simple,357,"Could you tell us about how you've found it useful, or point us towards some literature on it being used? I think this would be easy enough to implement, but when I tried a naive implementation the results weren't that compelling. It's very possible I should've played around with the parameters more. @flying-sheep, do you have thoughts on this?. Here's a simple implementation:. ```python; def ica(adata, n_components, inplace=True, **kwargs): ; from sklearn.decomposition import FastICA ; ica_transformer = FastICA(n_components=n_components, **kwargs) ; x_ica = ica_transformer.fit_transform(adata.X) ; if inplace:; adata.obsm[""X_ica""] = x_ica ; adata.varm[""ICs""] = ica_transformer.components_.T ; else:; return ica_transformer ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-519071187
https://github.com/scverse/scanpy/issues/767#issuecomment-519089834:474,Modifiability,variab,variable,474,"Hi !; To answer about how it is useful, we are using ICA in our lab to a dataset of more than 100k cells, with a lot of complexity, and the main advantage of ICA against PCA is that it helps us detecting small populations of cells. As these small populations are not accounting for a lot of variance within the dataset, using a treshold on PCs, we discarded the PCs that would allow the separate them.; Another advantage is that we do not make use of a selection of ""highly variable genes"" anymore, and use all genes expressed in more than 100 cells for the whole analysis... Doing the same and applying PCA gave us quite poor results.. . We made use of Seurat implementation.. and I tried fastICA from sklearn once but I couldn't obtain similar results... I have not looked thoroughly into seurat's code tough... . Hope it helps ! ; Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-519089834
https://github.com/scverse/scanpy/issues/767#issuecomment-519089834:194,Safety,detect,detecting,194,"Hi !; To answer about how it is useful, we are using ICA in our lab to a dataset of more than 100k cells, with a lot of complexity, and the main advantage of ICA against PCA is that it helps us detecting small populations of cells. As these small populations are not accounting for a lot of variance within the dataset, using a treshold on PCs, we discarded the PCs that would allow the separate them.; Another advantage is that we do not make use of a selection of ""highly variable genes"" anymore, and use all genes expressed in more than 100 cells for the whole analysis... Doing the same and applying PCA gave us quite poor results.. . We made use of Seurat implementation.. and I tried fastICA from sklearn once but I couldn't obtain similar results... I have not looked thoroughly into seurat's code tough... . Hope it helps ! ; Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-519089834
https://github.com/scverse/scanpy/issues/767#issuecomment-519397103:98,Availability,toler,tolerance,98,"I looked through the R `fastica` package, and I think one of the main differences was the default tolerance. I've changed that and results seem a bit better. I don't think I have a great reference point to evaluate it though. Any chance you could provide a vignette of ICA being used with single cell data, so we can see if the python version can recapitulate the results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-519397103
https://github.com/scverse/scanpy/issues/767#issuecomment-526496705:17,Deployability,update,update,17,@chris-rands Any update on this? Does the code from @ivirshup gives you any meaningful results?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-526496705
https://github.com/scverse/scanpy/issues/767#issuecomment-540457004:169,Availability,down,downstream,169,"Thanks for the feedback and sorry for the delay. The code snippet using `sklearn.decomposition.FastICA` gave me quite similar results to PCA for my data in terms of the downstream UMAP visualisations (when I simply embedded 50 components in the neighbourhood graph). One difference is that the ICA was slower to compute. I am not confident to create a vignette, as I'm unclear what the 'correct' results should look like. I have not tried the `picard` implementation yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-540457004
https://github.com/scverse/scanpy/issues/767#issuecomment-540457004:15,Usability,feedback,feedback,15,"Thanks for the feedback and sorry for the delay. The code snippet using `sklearn.decomposition.FastICA` gave me quite similar results to PCA for my data in terms of the downstream UMAP visualisations (when I simply embedded 50 components in the neighbourhood graph). One difference is that the ICA was slower to compute. I am not confident to create a vignette, as I'm unclear what the 'correct' results should look like. I have not tried the `picard` implementation yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-540457004
https://github.com/scverse/scanpy/issues/767#issuecomment-540457004:208,Usability,simpl,simply,208,"Thanks for the feedback and sorry for the delay. The code snippet using `sklearn.decomposition.FastICA` gave me quite similar results to PCA for my data in terms of the downstream UMAP visualisations (when I simply embedded 50 components in the neighbourhood graph). One difference is that the ICA was slower to compute. I am not confident to create a vignette, as I'm unclear what the 'correct' results should look like. I have not tried the `picard` implementation yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-540457004
https://github.com/scverse/scanpy/issues/767#issuecomment-540475625:253,Performance,perform,performed,253,"Hi, I tried the snippet, with fastICA and picard, and with a number of cells higher than 30,000, the whitening step cannot be completed. This seems be due to some Lapack limitations. ; `ValueError: Too large work array required -- computation cannot be performed with standard 32-bit LAPACK.`; I don't know how to get around this.... ; Best, ; Chloé",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-540475625
https://github.com/scverse/scanpy/issues/767#issuecomment-552756716:400,Performance,load,loadings,400,"Sorry for the late response, completley forgot to post my response here. @Fougere87, did that whitening issue occur with picard as well? I saw that with sklearn. I think we could get around that by whitening ourselves with ARPACK. Picard and sklearn look pretty similar to me in a quick comparison. Below are top 16/30 components (ranked by Geary's C, autocorrelation on the connectivity graph) cell loadings on the pbmc3k dataset. The umap and connectivity matrix here were computed on top of a PCA – which I should maybe do differently. However I think the results are similar enough that it's probably not of consequence. <details>; <summary> sklearn FastICA </summary>. ![sklearn_ica](https://user-images.githubusercontent.com/8238804/68647787-d53a4d80-0572-11ea-8b95-cde9122824f1.png). </details>. <details>; <summary> picard ICA </summary>. ![picard_ica2](https://user-images.githubusercontent.com/8238804/68647808-e5eac380-0572-11ea-8485-71a770849cc9.png). </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-552756716
https://github.com/scverse/scanpy/issues/769#issuecomment-519000488:33,Availability,error,error,33,I have also got exactly the same error !,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769#issuecomment-519000488
https://github.com/scverse/scanpy/issues/769#issuecomment-519055058:29,Deployability,release,release,29,"There hasn't actually been a release of UMAP since the pull request that should fix this (https://github.com/lmcinnes/umap/pull/261). I think I see what happened here, so I've opened a PR to fix it here. For now, this can be worked around by running:. ```python; sc.tl.umap(adata, init_pos=sc.tl._utils.get_init_pos_from_paga(adata)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769#issuecomment-519055058
https://github.com/scverse/scanpy/issues/769#issuecomment-519061562:119,Availability,error,error,119,"Hello @ivirshup thanks for this!. Quick question (still very new to python). Upon following your suggestion I get this error:; AttributeError: module 'scanpy.api.tl' has no attribute '_utils'. I then proceeded to install utils (pip install utils), and then; import utils. But still doesn't work. I assume it's because I'm not loading it correctly into the environment for scanpy to use but I don't know how?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769#issuecomment-519061562
https://github.com/scverse/scanpy/issues/769#issuecomment-519061562:213,Deployability,install,install,213,"Hello @ivirshup thanks for this!. Quick question (still very new to python). Upon following your suggestion I get this error:; AttributeError: module 'scanpy.api.tl' has no attribute '_utils'. I then proceeded to install utils (pip install utils), and then; import utils. But still doesn't work. I assume it's because I'm not loading it correctly into the environment for scanpy to use but I don't know how?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769#issuecomment-519061562
https://github.com/scverse/scanpy/issues/769#issuecomment-519061562:232,Deployability,install,install,232,"Hello @ivirshup thanks for this!. Quick question (still very new to python). Upon following your suggestion I get this error:; AttributeError: module 'scanpy.api.tl' has no attribute '_utils'. I then proceeded to install utils (pip install utils), and then; import utils. But still doesn't work. I assume it's because I'm not loading it correctly into the environment for scanpy to use but I don't know how?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769#issuecomment-519061562
https://github.com/scverse/scanpy/issues/769#issuecomment-519061562:326,Performance,load,loading,326,"Hello @ivirshup thanks for this!. Quick question (still very new to python). Upon following your suggestion I get this error:; AttributeError: module 'scanpy.api.tl' has no attribute '_utils'. I then proceeded to install utils (pip install utils), and then; import utils. But still doesn't work. I assume it's because I'm not loading it correctly into the environment for scanpy to use but I don't know how?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769#issuecomment-519061562
https://github.com/scverse/scanpy/issues/769#issuecomment-559832485:15,Availability,error,error,15,I got the same error with scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.2 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1; But I am so glad to find answer here and thanks a lot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769#issuecomment-559832485
https://github.com/scverse/scanpy/issues/769#issuecomment-559832485:130,Usability,learn,learn,130,I got the same error with scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.2 pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.2 python-igraph==0.7.1 louvain==0.6.1; But I am so glad to find answer here and thanks a lot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769#issuecomment-559832485
https://github.com/scverse/scanpy/issues/770#issuecomment-519007245:90,Availability,error,error,90,"What version of bbknn are you using? I think it might be out of date if you didn't get an error from `save_knn=False`. Also, this works for me:. ```python; import scanpy as sc; import scanpy.external as sce. pbmc = sc.datasets.pbmc68k_reduced(); sce.pp.bbknn(pbmc, batch_key='bulk_labels'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/770#issuecomment-519007245
https://github.com/scverse/scanpy/issues/770#issuecomment-521728652:152,Deployability,install,install,152,"Hi, ; I faced the same problem. I solved it by using the development version of scanpy : ; git clone https://github.com/theislab/scanpy; cd scanpy; pip install -e . For bbknn i just pip installed it . Then : ; bbknn.bbknn(adata_bbknn, batch_key=""Batches""). Hope this can help,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/770#issuecomment-521728652
https://github.com/scverse/scanpy/issues/770#issuecomment-521728652:186,Deployability,install,installed,186,"Hi, ; I faced the same problem. I solved it by using the development version of scanpy : ; git clone https://github.com/theislab/scanpy; cd scanpy; pip install -e . For bbknn i just pip installed it . Then : ; bbknn.bbknn(adata_bbknn, batch_key=""Batches""). Hope this can help,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/770#issuecomment-521728652
https://github.com/scverse/scanpy/issues/775#issuecomment-519626141:392,Usability,clear,clear,392,"> So if I understand correctly you want to use quantile scaling to translate values to colour? If that is what you're suggesting, I'm not sure I'm such a fan of that idea. With quantile scaling you would lose all sense of gradient in your e.g. expression values. I would instead opt for trimming and scaling. The trimming could be done via a quantile threshold though. OK, I thought code was clear enough, here is more information :). vmin and vmax are used for determining the lowest and highest values of the colormap (see https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.colors.Normalize.html). By default, these values are mapped to the minimum and maximum of the color vector of the scatter plot (e.g. gene expression). Right now, we can define only one vmin/vmax value in a sc.pl.* call (e.g. `sc.pl.umap(ad, color=..., vmax=2.0)`). But when we plot multiple genes (`sc.pl.umap(ad, color=['a', 'b'], vmax=2.0)`), setting vmax to a specific value sometimes does not make sense because each gene might have a different outlier range. . What I propose is the flexibility to use quantiles to set vmin/vmax e.g. `sc.pl.umap(ad, color=['a', 'b'], vmax_quantile=0.99)` where vmax values will be calculated per panel (i.e. per gene) by Scanpy. I mean it's simply winsorization, nothing fancy :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775#issuecomment-519626141
https://github.com/scverse/scanpy/issues/775#issuecomment-519626141:1257,Usability,simpl,simply,1257,"> So if I understand correctly you want to use quantile scaling to translate values to colour? If that is what you're suggesting, I'm not sure I'm such a fan of that idea. With quantile scaling you would lose all sense of gradient in your e.g. expression values. I would instead opt for trimming and scaling. The trimming could be done via a quantile threshold though. OK, I thought code was clear enough, here is more information :). vmin and vmax are used for determining the lowest and highest values of the colormap (see https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.colors.Normalize.html). By default, these values are mapped to the minimum and maximum of the color vector of the scatter plot (e.g. gene expression). Right now, we can define only one vmin/vmax value in a sc.pl.* call (e.g. `sc.pl.umap(ad, color=..., vmax=2.0)`). But when we plot multiple genes (`sc.pl.umap(ad, color=['a', 'b'], vmax=2.0)`), setting vmax to a specific value sometimes does not make sense because each gene might have a different outlier range. . What I propose is the flexibility to use quantiles to set vmin/vmax e.g. `sc.pl.umap(ad, color=['a', 'b'], vmax_quantile=0.99)` where vmax values will be calculated per panel (i.e. per gene) by Scanpy. I mean it's simply winsorization, nothing fancy :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775#issuecomment-519626141
https://github.com/scverse/scanpy/issues/775#issuecomment-520202242:438,Modifiability,flexible,flexible,438,"I think this would be good. I can think of cases where I wouldn't want every gene to have the same quantile normalization (e.g. when plotting markers for populations whose frequencies differ by orders of magnitude). In addition to including a quantile argument, would you also include a ""vectorized"" vmax, vmin argument?. ```python; sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=[2.0, 3.0]); ```. If you want to get even more flexible about how cutoffs can be chosen, we could also allow `vmin` and `vmax` to be callables. That way you could get the behavior of specifying a quantile from:. ```python; from functools import partial. sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=partial(np.quantile, q=.99)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775#issuecomment-520202242
https://github.com/scverse/scanpy/issues/775#issuecomment-521550044:741,Integrability,depend,depending,741,"I sympathize with the problem which I tend to solve by plotting individual scatterplots, each one with its specific vmax. But I will be happy to have a better output. I like the idea of using quantile but I would avoid an increasing list of parameters. Thus @ivirshup suggestion to use `functools.partial` seems better. I like the flexibility it provides and I think we should implement it, but I don't know if this would be difficult to document and explain to the user that just would like to compute the quantile. An idea would be to allow some encoding for vmax as for example `vmax='q99'` which would be interpreted as np.quantile. My suggestion is to . - add vectorized vmax and vmin; - each entry of vmax or vmin would be interpreted depending on the data type. Besides a number, if it is a string then it is interpreted as for example quantile if it starts with 'q' or as a function if the type is `partial`. The following options would then be valid:. ```PYTHON; sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=[4., 3.]); sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=['q80', 'q90']). from functools import partial; sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=[partial(np.mean), partial(np.median)]). # combination; sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2"", ""gene3""], ; vmax=[4., 'q85', partial(np.percentile, q=90]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775#issuecomment-521550044
https://github.com/scverse/scanpy/issues/775#issuecomment-521550044:213,Safety,avoid,avoid,213,"I sympathize with the problem which I tend to solve by plotting individual scatterplots, each one with its specific vmax. But I will be happy to have a better output. I like the idea of using quantile but I would avoid an increasing list of parameters. Thus @ivirshup suggestion to use `functools.partial` seems better. I like the flexibility it provides and I think we should implement it, but I don't know if this would be difficult to document and explain to the user that just would like to compute the quantile. An idea would be to allow some encoding for vmax as for example `vmax='q99'` which would be interpreted as np.quantile. My suggestion is to . - add vectorized vmax and vmin; - each entry of vmax or vmin would be interpreted depending on the data type. Besides a number, if it is a string then it is interpreted as for example quantile if it starts with 'q' or as a function if the type is `partial`. The following options would then be valid:. ```PYTHON; sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=[4., 3.]); sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=['q80', 'q90']). from functools import partial; sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=[partial(np.mean), partial(np.median)]). # combination; sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2"", ""gene3""], ; vmax=[4., 'q85', partial(np.percentile, q=90]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775#issuecomment-521550044
https://github.com/scverse/scanpy/pull/776#issuecomment-521567791:55,Testability,test,test,55,@gokceneraslan Thanks for looking at this. Can you add test for this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/776#issuecomment-521567791
https://github.com/scverse/scanpy/issues/778#issuecomment-521108926:40,Security,access,accessed,40,"Whoa, I had no idea you could have ever accessed elements of obsm and varm as attributes. That probably won't be the case for current and future versions of `anndata`, since `obsm` should be a `Mapping` subclass. What versions of scanpy and anndata are you using?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/778#issuecomment-521108926
https://github.com/scverse/scanpy/issues/778#issuecomment-522895754:7,Deployability,update,update,7,"If you update to more recent releases, you won't be able to access elements of `obsm` or `varm` like an attribute. It should all be through `.__getitem__` (e.g. `adata.obsm[""X_tsne""]`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/778#issuecomment-522895754
https://github.com/scverse/scanpy/issues/778#issuecomment-522895754:29,Deployability,release,releases,29,"If you update to more recent releases, you won't be able to access elements of `obsm` or `varm` like an attribute. It should all be through `.__getitem__` (e.g. `adata.obsm[""X_tsne""]`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/778#issuecomment-522895754
https://github.com/scverse/scanpy/issues/778#issuecomment-522895754:60,Security,access,access,60,"If you update to more recent releases, you won't be able to access elements of `obsm` or `varm` like an attribute. It should all be through `.__getitem__` (e.g. `adata.obsm[""X_tsne""]`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/778#issuecomment-522895754
https://github.com/scverse/scanpy/issues/779#issuecomment-524128498:565,Deployability,release,release,565,"Haven't tried again but I have a suggestion. Since umap (or pynndescent) is a critical component of scanpy, I think it'd be great to run our tests against both ""stable"" and ""development"" branches of umap. However in order for this to happen, umap needs proper naming for the development and stable branches. Right now, there are master, 0.3dev and 0.4dev, therefore the names are version-dependent. . Does it make sense to file a bug report in umap repo? It'd be a lot easier to run test against two major branches of umap without changing the names in every major release. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/779#issuecomment-524128498
https://github.com/scverse/scanpy/issues/779#issuecomment-524128498:388,Integrability,depend,dependent,388,"Haven't tried again but I have a suggestion. Since umap (or pynndescent) is a critical component of scanpy, I think it'd be great to run our tests against both ""stable"" and ""development"" branches of umap. However in order for this to happen, umap needs proper naming for the development and stable branches. Right now, there are master, 0.3dev and 0.4dev, therefore the names are version-dependent. . Does it make sense to file a bug report in umap repo? It'd be a lot easier to run test against two major branches of umap without changing the names in every major release. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/779#issuecomment-524128498
https://github.com/scverse/scanpy/issues/779#issuecomment-524128498:141,Testability,test,tests,141,"Haven't tried again but I have a suggestion. Since umap (or pynndescent) is a critical component of scanpy, I think it'd be great to run our tests against both ""stable"" and ""development"" branches of umap. However in order for this to happen, umap needs proper naming for the development and stable branches. Right now, there are master, 0.3dev and 0.4dev, therefore the names are version-dependent. . Does it make sense to file a bug report in umap repo? It'd be a lot easier to run test against two major branches of umap without changing the names in every major release. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/779#issuecomment-524128498
https://github.com/scverse/scanpy/issues/779#issuecomment-524128498:483,Testability,test,test,483,"Haven't tried again but I have a suggestion. Since umap (or pynndescent) is a critical component of scanpy, I think it'd be great to run our tests against both ""stable"" and ""development"" branches of umap. However in order for this to happen, umap needs proper naming for the development and stable branches. Right now, there are master, 0.3dev and 0.4dev, therefore the names are version-dependent. . Does it make sense to file a bug report in umap repo? It'd be a lot easier to run test against two major branches of umap without changing the names in every major release. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/779#issuecomment-524128498
https://github.com/scverse/scanpy/issues/779#issuecomment-524177435:38,Deployability,release,release,38,"Do we know when UMAP 0.4 is due for a release? I wouldn't want to put too much effort into ensuring compatibility with something unstable. If it's really useful now, maybe it's worth it, but it could change again before something gets released.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/779#issuecomment-524177435
https://github.com/scverse/scanpy/issues/779#issuecomment-524177435:235,Deployability,release,released,235,"Do we know when UMAP 0.4 is due for a release? I wouldn't want to put too much effort into ensuring compatibility with something unstable. If it's really useful now, maybe it's worth it, but it could change again before something gets released.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/779#issuecomment-524177435
https://github.com/scverse/scanpy/issues/779#issuecomment-524178694:40,Deployability,release,released,40,"I don't know when it's due, but if it's released tomorrow scanpy will stop working and that scares me 😑",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/779#issuecomment-524178694
https://github.com/scverse/scanpy/issues/780#issuecomment-521669600:387,Modifiability,variab,variable,387,"Hi Samuele,. `covariates` argument refers to the additional covariates (biological or technical) that are used in the model fit. It's the `mod` parameter in the R function combat (https://www.rdocumentation.org/packages/sva/versions/3.20.0/topics/ComBat) and `X` in equation 2.1 in Johnson et al. 2007, https://academic.oup.com/biostatistics/article/8/1/118/252073. Since only the batch variable is ""regressed out"" from the gene expression, adding extra covariates changes the way batch effect coefficient is estimated. By the way, https://scanpy.discourse.group is a better place to ask questions and start such discussions :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/780#issuecomment-521669600
https://github.com/scverse/scanpy/issues/780#issuecomment-521671691:2,Modifiability,extend,extended,2,"I extended the documentation a bit now, see https://github.com/theislab/scanpy/commit/c7e58e3a8e32b7f395e25267cd3cba684d6d40c4.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/780#issuecomment-521671691
https://github.com/scverse/scanpy/pull/784#issuecomment-522566850:22,Testability,test,tests,22,Did anyone run scanpy tests with umap 0.4 branch?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/784#issuecomment-522566850
https://github.com/scverse/scanpy/issues/786#issuecomment-522249082:8,Deployability,install,installing,8,"Hi, try installing python-igraph from the wheel here - https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph; And after that install louvain via pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786#issuecomment-522249082
https://github.com/scverse/scanpy/issues/786#issuecomment-522249082:129,Deployability,install,install,129,"Hi, try installing python-igraph from the wheel here - https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph; And after that install louvain via pip",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786#issuecomment-522249082
https://github.com/scverse/scanpy/issues/786#issuecomment-522570296:47,Deployability,install,installing,47,"Yup, not a bug with scanpy, but a problem with installing louvain",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786#issuecomment-522570296
https://github.com/scverse/scanpy/issues/786#issuecomment-522896826:87,Deployability,install,install,87,"I've talked to some other people who had trouble with this. If there's a better way to install it, I think we should mention it in the docs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786#issuecomment-522896826
https://github.com/scverse/scanpy/issues/787#issuecomment-524200077:45,Availability,avail,available,45,"Also, can you replicate the problem with the available Scanpy datasets?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787#issuecomment-524200077
https://github.com/scverse/scanpy/issues/787#issuecomment-531732473:260,Integrability,depend,dependency,260,"None, there is no problem on our side right? @fidelram said in https://github.com/theislab/scanpy/pull/661#issuecomment-496144015 to wait for matplotlib/matplotlib#14298 to be fixed. It seems to be in matplotlib’s 3.1.2 milestone, so we can maybe just set the dependency to “matplotlib == 3.0.0 or matplotlib >= 3.1.2”",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787#issuecomment-531732473
https://github.com/scverse/scanpy/issues/787#issuecomment-532125401:125,Availability,failure,failures,125,"It would be good to have an open issue here for why we pin matplotlib to a lower version. If I try upgrading it, I get a few failures in the tests for heat maps as well as 3d plotting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787#issuecomment-532125401
https://github.com/scverse/scanpy/issues/787#issuecomment-532125401:141,Testability,test,tests,141,"It would be good to have an open issue here for why we pin matplotlib to a lower version. If I try upgrading it, I get a few failures in the tests for heat maps as well as 3d plotting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787#issuecomment-532125401
https://github.com/scverse/scanpy/pull/789#issuecomment-522966587:51,Availability,down,down,51,Thank you! IDK how I missed that it’s used further down.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/789#issuecomment-522966587
https://github.com/scverse/scanpy/pull/791#issuecomment-523073188:12,Testability,test,test,12,"The failing test is here https://travis-ci.org/theislab/scanpy/jobs/574409535#L386. However test_var_df should not be affected by this PR, and this test is also failing on master,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/791#issuecomment-523073188
https://github.com/scverse/scanpy/pull/791#issuecomment-523073188:148,Testability,test,test,148,"The failing test is here https://travis-ci.org/theislab/scanpy/jobs/574409535#L386. However test_var_df should not be affected by this PR, and this test is also failing on master,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/791#issuecomment-523073188
https://github.com/scverse/scanpy/pull/791#issuecomment-523152544:24,Availability,fault,fault,24,"Yes, sorry, that was my fault. Rerunning the tests for this PR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/791#issuecomment-523152544
https://github.com/scverse/scanpy/pull/791#issuecomment-523152544:45,Testability,test,tests,45,"Yes, sorry, that was my fault. Rerunning the tests for this PR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/791#issuecomment-523152544
https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:81,Deployability,integrat,integrating,81,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420
https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:81,Integrability,integrat,integrating,81,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420
https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:305,Modifiability,extend,extending,305,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420
https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:735,Performance,optimiz,optimized,735,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420
https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:192,Usability,simpl,simplification,192,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420
https://github.com/scverse/scanpy/issues/792#issuecomment-523843131:206,Availability,robust,robust,206,"It's **a** thing, not yet **the** thing. For directed PAGA follow [this](https://github.com/theislab/paga/issues/11) while always double checking with your single cell velocities as it is not yet perfectly robust.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523843131
https://github.com/scverse/scanpy/issues/793#issuecomment-526320133:29,Deployability,release,released,29,Thanks. I merged your PR and released a new package. Closing this issue now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793#issuecomment-526320133
https://github.com/scverse/scanpy/pull/794#issuecomment-523541089:1234,Availability,error,error,1234,"> 'outline' could be good. We already have font outline that works similarly. Great!. > Can you take a look at the new type hints that I added. I am not sure if I did it right?. The typing is pretty good! There’s one rule I follow, which is to be specific:. - If your function has parameter `a`, does `for elem in a`, and expects `elem`s to be `str`s, you can say `Iterable[str]`. If you use `a[i]`, say `Sequence[str]`. You don’t want to artificially limit the user by saying you need a `List[str]` if a `Tuple[str]` can be passed or even any `Iterator[str]` is sufficient.; - If you say what you *return*, be concrete, e.g. `List[str]`. You know what exact type you return.; - If you accept a callable, specify its signature: `Callable[[ArgType1, ArgType2], RetType]`. There’s nothing more annoying than to dive into the code because the library doesn’t specify what kind of function you can supply. So you should change. - `callable`→`Callable[[???], ?]`; - `Sequence`→`Sequence[?]`; - `Optional[dict]`→`Optional[Mapping[?, ?]]`. Also stylewise: Once `(` and `)` aren on separate lines, never have anything after `(`, and before `)`:. ```py; def _get_vmin_vmax(; […]; color_vector: Sequence[float],; ):; '''; […]; ```. ```py; logg.error(; ""The parameter […]""; […]; ""of plots.""; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-523541089
https://github.com/scverse/scanpy/pull/794#issuecomment-523541089:1229,Testability,log,logg,1229,"> 'outline' could be good. We already have font outline that works similarly. Great!. > Can you take a look at the new type hints that I added. I am not sure if I did it right?. The typing is pretty good! There’s one rule I follow, which is to be specific:. - If your function has parameter `a`, does `for elem in a`, and expects `elem`s to be `str`s, you can say `Iterable[str]`. If you use `a[i]`, say `Sequence[str]`. You don’t want to artificially limit the user by saying you need a `List[str]` if a `Tuple[str]` can be passed or even any `Iterator[str]` is sufficient.; - If you say what you *return*, be concrete, e.g. `List[str]`. You know what exact type you return.; - If you accept a callable, specify its signature: `Callable[[ArgType1, ArgType2], RetType]`. There’s nothing more annoying than to dive into the code because the library doesn’t specify what kind of function you can supply. So you should change. - `callable`→`Callable[[???], ?]`; - `Sequence`→`Sequence[?]`; - `Optional[dict]`→`Optional[Mapping[?, ?]]`. Also stylewise: Once `(` and `)` aren on separate lines, never have anything after `(`, and before `)`:. ```py; def _get_vmin_vmax(; […]; color_vector: Sequence[float],; ):; '''; […]; ```. ```py; logg.error(; ""The parameter […]""; […]; ""of plots.""; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-523541089
https://github.com/scverse/scanpy/pull/794#issuecomment-523589188:38,Usability,guid,guidelines,38,@flying-sheep I quite like your style guidelines. It might be a good idea to designate a particular function that does it well and is complex enough to include all of these options. That way it's easy to follow style when writing a new function. Something like a contributors template.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-523589188
https://github.com/scverse/scanpy/pull/794#issuecomment-523732596:1245,Availability,Avail,Available,1245,"This looks great!. A few ideas:. * For having an outline to separate overlapping clusters, I don't think I like that one of the outlines would be plotted over the other cluster. In the plots shown above (https://github.com/theislab/scanpy/pull/794#issuecomment-523515331) I think the upper image is less clear about the extent of the overlap than the lower one, and suggests a greater importance of group `3`. Maybe there could be some indication of ambiguity for the region of overlap?; * For the string based quantile selection, is there another package which allows writing operations like this? My concern is that string based DSLs can get messy. It would be nice to make sure we're choosing a unambiguous spec which we can extend in the future and use in other functions. An example of a spec would be SQL reduction operations (like `PERCENTILE_DISC`), but hopefully there would be something less verbose.; * For the basis argument, could we not require the key in `obsm` start with `X_`? I'm thinking the key would just go through a check like:. ```python; if basis in adata.obsm:; basis_key = basis; elif f""X_{basis}"" in adata.obsm:; basis_key = f""X_{basis}""; else:; raise KeyError(; f""Could not find entry in `obsm` for '{basis}'.\n""; f""Available keys are: {list(adata.obsm.keys())}.""; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-523732596
https://github.com/scverse/scanpy/pull/794#issuecomment-523732596:728,Modifiability,extend,extend,728,"This looks great!. A few ideas:. * For having an outline to separate overlapping clusters, I don't think I like that one of the outlines would be plotted over the other cluster. In the plots shown above (https://github.com/theislab/scanpy/pull/794#issuecomment-523515331) I think the upper image is less clear about the extent of the overlap than the lower one, and suggests a greater importance of group `3`. Maybe there could be some indication of ambiguity for the region of overlap?; * For the string based quantile selection, is there another package which allows writing operations like this? My concern is that string based DSLs can get messy. It would be nice to make sure we're choosing a unambiguous spec which we can extend in the future and use in other functions. An example of a spec would be SQL reduction operations (like `PERCENTILE_DISC`), but hopefully there would be something less verbose.; * For the basis argument, could we not require the key in `obsm` start with `X_`? I'm thinking the key would just go through a check like:. ```python; if basis in adata.obsm:; basis_key = basis; elif f""X_{basis}"" in adata.obsm:; basis_key = f""X_{basis}""; else:; raise KeyError(; f""Could not find entry in `obsm` for '{basis}'.\n""; f""Available keys are: {list(adata.obsm.keys())}.""; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-523732596
https://github.com/scverse/scanpy/pull/794#issuecomment-523732596:304,Usability,clear,clear,304,"This looks great!. A few ideas:. * For having an outline to separate overlapping clusters, I don't think I like that one of the outlines would be plotted over the other cluster. In the plots shown above (https://github.com/theislab/scanpy/pull/794#issuecomment-523515331) I think the upper image is less clear about the extent of the overlap than the lower one, and suggests a greater importance of group `3`. Maybe there could be some indication of ambiguity for the region of overlap?; * For the string based quantile selection, is there another package which allows writing operations like this? My concern is that string based DSLs can get messy. It would be nice to make sure we're choosing a unambiguous spec which we can extend in the future and use in other functions. An example of a spec would be SQL reduction operations (like `PERCENTILE_DISC`), but hopefully there would be something less verbose.; * For the basis argument, could we not require the key in `obsm` start with `X_`? I'm thinking the key would just go through a check like:. ```python; if basis in adata.obsm:; basis_key = basis; elif f""X_{basis}"" in adata.obsm:; basis_key = f""X_{basis}""; else:; raise KeyError(; f""Could not find entry in `obsm` for '{basis}'.\n""; f""Available keys are: {list(adata.obsm.keys())}.""; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-523732596
https://github.com/scverse/scanpy/pull/794#issuecomment-523883673:64,Usability,simpl,simplify,64,"> I ended up with […] Does it looks ok to you?. Yeah! You could simplify though:. ```py; VMinMax = Union[str, float, Callable[[Sequence[float]], float]]. def embedding(; ...; vmin: Union[VMinMax, Sequence[VMinMax], None] = None,; vmax: Union[VMinMax, Sequence[VMinMax], None] = None,; ...; ): ... def _get_vmin_vmax(; vmin: Sequence[VMinMax],; vmax: Sequence[VMinMax],; ...; ) -> ...: ...; ```. > It's not a big deal but if we use numbers in 0-100 range for vmax-vmin, it's a percentile, so p80 makes more sense. you can also be more or less precise: `q001`→`0.001`, `q9`→`0.9`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-523883673
https://github.com/scverse/scanpy/pull/794#issuecomment-524280917:434,Availability,robust,robust,434,"@ivirshup I looked around and could not find any example in other packages that use input strings as in this case. I agree that the current solution is not ideal, but other solutions seem more complicated. For example, we can add a new parameter called `percentile` that then interprets `vmin` and `vmax` as percentiles/quantiles. However, this limits the option to mix different types of values. . In seaborn they use the parameter `robust` to set vmin and vmax as the .02 and .98 quantiles but the quantiles can not be changed. . Unless we have a strong opinion against this solution I suggest we keep it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-524280917
https://github.com/scverse/scanpy/pull/794#issuecomment-524364576:34,Testability,test,test,34,"Looks great! Optimally we’d add a test image like the bottom one in https://github.com/theislab/scanpy/pull/794#issuecomment-523515331, but I’d be up for merging this as-is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-524364576
https://github.com/scverse/scanpy/pull/794#issuecomment-524373551:289,Usability,clear,clear,289,"> For having an outline to separate overlapping clusters, I don't think I like that one of the outlines would be plotted over the other cluster. In the plots shown above ([#794 (comment)](https://github.com/theislab/scanpy/pull/794#issuecomment-523515331)) I think the upper image is less clear about the extent of the overlap than the lower one, and suggests a greater importance of group `3`. Maybe there could be some indication of ambiguity for the region of overlap?. @ivirshup I see your point regarding the accurate representation of what is shown. My thoughts were more along the lines of making it easy to distinguish clusters. That would be particularly useful when you have >20 clusters that are hard to distinguish by colours alone. And the overlaps may not be as important for a large-scale overview.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-524373551
https://github.com/scverse/scanpy/pull/794#issuecomment-531248281:92,Availability,error,error,92,I am surprised that this is coming out. I thought I had solved the issue as I don't get the error. Thanks @flying-sheep for addressing this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-531248281
https://github.com/scverse/scanpy/pull/797#issuecomment-524381733:4,Testability,test,test,4,The test is failing because `scipy.stats` is not a wanted import. Let me know how you'd like to deal with that,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-524381733
https://github.com/scverse/scanpy/pull/797#issuecomment-526651900:58,Usability,clear,clear,58,@fidelram lemme know if the new comments make things more clear,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-526651900
https://github.com/scverse/scanpy/pull/797#issuecomment-536570108:106,Testability,test,test,106,"@ivirshup @flying-sheep I would like to merge this code but I have some questions:. - Do you know why the test is failing?; - Currently, this is located in `preprocessing`, but I think that the right place is `external.pp` would you agree on that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-536570108
https://github.com/scverse/scanpy/pull/797#issuecomment-536861482:22,Testability,test,test,22,"> Do you know why the test is failing?. @flying-sheep and I have put some effort into reducing import times for scanpy (they were getting up to a few seconds). This can most effectively be done by deferring the import of slow-to-import packages until they're actually needed. Phil added a test which blacklists top level import of some of particularly egregious packages (scipy and seaboarn for example). Here, the imports for `argrelextrema` and `gaussian_kde` are pretty slow (about half a second). These should be moved inside `_demultiplex_per_barcode` function instead of being top level. > Currently, this is located in preprocessing, but I think that the right place is external.pp would you agree on that?. I think I would be fine with this going in preprocessing, since it isn't tied to another tool/ code base. I'm also not too familiar with the current state of demultiplexing techniques so I'd be fine to defer to you on this. Could you elaborate on why you think it should go in external?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-536861482
https://github.com/scverse/scanpy/pull/797#issuecomment-536861482:289,Testability,test,test,289,"> Do you know why the test is failing?. @flying-sheep and I have put some effort into reducing import times for scanpy (they were getting up to a few seconds). This can most effectively be done by deferring the import of slow-to-import packages until they're actually needed. Phil added a test which blacklists top level import of some of particularly egregious packages (scipy and seaboarn for example). Here, the imports for `argrelextrema` and `gaussian_kde` are pretty slow (about half a second). These should be moved inside `_demultiplex_per_barcode` function instead of being top level. > Currently, this is located in preprocessing, but I think that the right place is external.pp would you agree on that?. I think I would be fine with this going in preprocessing, since it isn't tied to another tool/ code base. I'm also not too familiar with the current state of demultiplexing techniques so I'd be fine to defer to you on this. Could you elaborate on why you think it should go in external?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-536861482
https://github.com/scverse/scanpy/pull/797#issuecomment-537023624:158,Availability,error,errors,158,@ivirshup : Thanks for the background explanation. . @njbernstein Can you move the import statements inside the `_demultiplex_per_barcode` to remove the test errors? . I think the tool should go to `external` to point out that this is based on a method that we have not tested and thus the responsibility of its accuracy and implementation lies on the external contributor.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537023624
https://github.com/scverse/scanpy/pull/797#issuecomment-537023624:153,Testability,test,test,153,@ivirshup : Thanks for the background explanation. . @njbernstein Can you move the import statements inside the `_demultiplex_per_barcode` to remove the test errors? . I think the tool should go to `external` to point out that this is based on a method that we have not tested and thus the responsibility of its accuracy and implementation lies on the external contributor.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537023624
https://github.com/scverse/scanpy/pull/797#issuecomment-537023624:270,Testability,test,tested,270,@ivirshup : Thanks for the background explanation. . @njbernstein Can you move the import statements inside the `_demultiplex_per_barcode` to remove the test errors? . I think the tool should go to `external` to point out that this is based on a method that we have not tested and thus the responsibility of its accuracy and implementation lies on the external contributor.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537023624
https://github.com/scverse/scanpy/pull/797#issuecomment-537474116:46,Testability,test,test,46,"Hmm, I’m pretty happy with my self-documented test code:. ```py; def test_deferred_imports(imported_modules):; slow_to_import = {; 'umap', # neighbors, tl.umap; 'seaborn', # plotting; 'sklearn.metrics', # neighbors; 'scipy.stats', # tools._embedding_density; 'networkx', # diffmap, paga, plotting._utils; # TODO: 'matplotlib.pyplot',; # TODO (maybe): 'numba',; }; falsely_imported = slow_to_import & imported_modules; > assert not falsely_imported; E AssertionError: assert not {'scipy.stats'}; ```. Do you think this could be clearer?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537474116
https://github.com/scverse/scanpy/pull/797#issuecomment-537474116:420,Testability,assert,assert,420,"Hmm, I’m pretty happy with my self-documented test code:. ```py; def test_deferred_imports(imported_modules):; slow_to_import = {; 'umap', # neighbors, tl.umap; 'seaborn', # plotting; 'sklearn.metrics', # neighbors; 'scipy.stats', # tools._embedding_density; 'networkx', # diffmap, paga, plotting._utils; # TODO: 'matplotlib.pyplot',; # TODO (maybe): 'numba',; }; falsely_imported = slow_to_import & imported_modules; > assert not falsely_imported; E AssertionError: assert not {'scipy.stats'}; ```. Do you think this could be clearer?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537474116
https://github.com/scverse/scanpy/pull/797#issuecomment-537474116:451,Testability,Assert,AssertionError,451,"Hmm, I’m pretty happy with my self-documented test code:. ```py; def test_deferred_imports(imported_modules):; slow_to_import = {; 'umap', # neighbors, tl.umap; 'seaborn', # plotting; 'sklearn.metrics', # neighbors; 'scipy.stats', # tools._embedding_density; 'networkx', # diffmap, paga, plotting._utils; # TODO: 'matplotlib.pyplot',; # TODO (maybe): 'numba',; }; falsely_imported = slow_to_import & imported_modules; > assert not falsely_imported; E AssertionError: assert not {'scipy.stats'}; ```. Do you think this could be clearer?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537474116
https://github.com/scverse/scanpy/pull/797#issuecomment-537474116:467,Testability,assert,assert,467,"Hmm, I’m pretty happy with my self-documented test code:. ```py; def test_deferred_imports(imported_modules):; slow_to_import = {; 'umap', # neighbors, tl.umap; 'seaborn', # plotting; 'sklearn.metrics', # neighbors; 'scipy.stats', # tools._embedding_density; 'networkx', # diffmap, paga, plotting._utils; # TODO: 'matplotlib.pyplot',; # TODO (maybe): 'numba',; }; falsely_imported = slow_to_import & imported_modules; > assert not falsely_imported; E AssertionError: assert not {'scipy.stats'}; ```. Do you think this could be clearer?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537474116
https://github.com/scverse/scanpy/pull/797#issuecomment-537474116:527,Usability,clear,clearer,527,"Hmm, I’m pretty happy with my self-documented test code:. ```py; def test_deferred_imports(imported_modules):; slow_to_import = {; 'umap', # neighbors, tl.umap; 'seaborn', # plotting; 'sklearn.metrics', # neighbors; 'scipy.stats', # tools._embedding_density; 'networkx', # diffmap, paga, plotting._utils; # TODO: 'matplotlib.pyplot',; # TODO (maybe): 'numba',; }; falsely_imported = slow_to_import & imported_modules; > assert not falsely_imported; E AssertionError: assert not {'scipy.stats'}; ```. Do you think this could be clearer?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537474116
https://github.com/scverse/scanpy/pull/797#issuecomment-537510120:82,Availability,error,error,82,"@flying-sheep I think the output is clear once you know what is about. Since this error may happen to future contributions that are not aware of the efforts to reduce import times, I think is better to be explicit. Something like: ""Slow import detected (scipy.stats). Please check that slow-to-import packages are not in top level calls but inside the functions that require them"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120
https://github.com/scverse/scanpy/pull/797#issuecomment-537510120:160,Energy Efficiency,reduce,reduce,160,"@flying-sheep I think the output is clear once you know what is about. Since this error may happen to future contributions that are not aware of the efforts to reduce import times, I think is better to be explicit. Something like: ""Slow import detected (scipy.stats). Please check that slow-to-import packages are not in top level calls but inside the functions that require them"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120
https://github.com/scverse/scanpy/pull/797#issuecomment-537510120:244,Safety,detect,detected,244,"@flying-sheep I think the output is clear once you know what is about. Since this error may happen to future contributions that are not aware of the efforts to reduce import times, I think is better to be explicit. Something like: ""Slow import detected (scipy.stats). Please check that slow-to-import packages are not in top level calls but inside the functions that require them"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120
https://github.com/scverse/scanpy/pull/797#issuecomment-537510120:36,Usability,clear,clear,36,"@flying-sheep I think the output is clear once you know what is about. Since this error may happen to future contributions that are not aware of the efforts to reduce import times, I think is better to be explicit. Something like: ""Slow import detected (scipy.stats). Please check that slow-to-import packages are not in top level calls but inside the functions that require them"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797#issuecomment-537510120
https://github.com/scverse/scanpy/issues/800#issuecomment-524526783:150,Availability,error,error,150,Minor addendum that I'm not sure is worth it's own issue. I had thought we were gonna use `q` instead of `p` for the prefix of the string values. The error that raised was:. ```python; ValueError: could not convert string to float: 'q99'; ```. I think we should probably recommend the correct usage in the error instead.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800#issuecomment-524526783
https://github.com/scverse/scanpy/issues/800#issuecomment-524526783:306,Availability,error,error,306,Minor addendum that I'm not sure is worth it's own issue. I had thought we were gonna use `q` instead of `p` for the prefix of the string values. The error that raised was:. ```python; ValueError: could not convert string to float: 'q99'; ```. I think we should probably recommend the correct usage in the error instead.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800#issuecomment-524526783
https://github.com/scverse/scanpy/issues/800#issuecomment-526476496:128,Availability,error,error,128,"Thanks for the fix!. For the `p` vs `q` thing, I was just thinking if the user passes a string that doesn't start with `p`, the error message could be something like `""ValueError: Couldn't understand string value '{passed_val}' for vmax. Percentile cutoffs can be specified like 'p99' (99th percentile).""`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800#issuecomment-526476496
https://github.com/scverse/scanpy/issues/800#issuecomment-526476496:134,Integrability,message,message,134,"Thanks for the fix!. For the `p` vs `q` thing, I was just thinking if the user passes a string that doesn't start with `p`, the error message could be something like `""ValueError: Couldn't understand string value '{passed_val}' for vmax. Percentile cutoffs can be specified like 'p99' (99th percentile).""`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800#issuecomment-526476496
https://github.com/scverse/scanpy/issues/800#issuecomment-526495713:8,Availability,error,error,8,"Now the error is something like: The value is not valid, please use a valid number a percentile as `pN` or a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800#issuecomment-526495713
https://github.com/scverse/scanpy/issues/800#issuecomment-526890943:58,Availability,error,error,58,That's good too. Right now it's throwing a warning not an error. Is that meant to happen?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800#issuecomment-526890943
https://github.com/scverse/scanpy/issues/909#issuecomment-551772902:447,Availability,error,error,447,"You can’t supply a string here, as mentioned in the docstring:. > `root`: If choosing a tree layout, this is the index of the root node or a list of root node indices. If this is a non-empty vector then the supplied node IDs are used as the roots of the trees (or a single tree if the graph is connected). If this is `None` or an empty list, the root vertices are automatically calculated based on topological sorting. @falexwolf the code for the error indicates that supplying a string is intended, but not properly implemented. I quickly whipped up #910, please finish it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/909#issuecomment-551772902
https://github.com/scverse/scanpy/issues/909#issuecomment-1188988922:184,Availability,error,error,184,"I do want to know why this [jupyter notebook](https://nbviewer.org/github/theislab/paga/blob/master/planaria/planaria.ipynb) can use 'neoblast 1' as root, a string you know, but cause error now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/909#issuecomment-1188988922
https://github.com/scverse/scanpy/pull/805#issuecomment-527405641:150,Performance,load,loadings,150,"Is it that useful to see it by default? Why would you want to know unimportant genes?. IMHO it would be more interesting to know genes that have high loadings in another early PC but not in this one, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805#issuecomment-527405641
https://github.com/scverse/scanpy/pull/805#issuecomment-527409146:12,Performance,load,loadings,12,Hi negative loadings are also important genes as you can see on the example of `LTB` and `HLA-DRA`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805#issuecomment-527409146
https://github.com/scverse/scanpy/pull/805#issuecomment-528244541:80,Availability,avail,available,80,"I think it’s pretty impossible to know if they’ll render – Depends on the fonts available on the system and the way the font rendering stack falls back to other fonts. My approach would be to check which systems have the problem, and if it’s only some Linux server or some obsolete stuff like e.g. Windows versions up to Vista, ignore it. If it’s a commonly used and still supported desktop OS / Linux distribution, we have to deal with it. The reason I excluded Linux servers is that server admins often set up things minimalistically, excluding “GUI stuff” so trying to support those highly heterogenous systems will only bring pain. When people want better fonts, then fontconfig is happy to provide them with the means to do so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805#issuecomment-528244541
https://github.com/scverse/scanpy/pull/805#issuecomment-528244541:59,Integrability,Depend,Depends,59,"I think it’s pretty impossible to know if they’ll render – Depends on the fonts available on the system and the way the font rendering stack falls back to other fonts. My approach would be to check which systems have the problem, and if it’s only some Linux server or some obsolete stuff like e.g. Windows versions up to Vista, ignore it. If it’s a commonly used and still supported desktop OS / Linux distribution, we have to deal with it. The reason I excluded Linux servers is that server admins often set up things minimalistically, excluding “GUI stuff” so trying to support those highly heterogenous systems will only bring pain. When people want better fonts, then fontconfig is happy to provide them with the means to do so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805#issuecomment-528244541
https://github.com/scverse/scanpy/pull/805#issuecomment-547000682:96,Deployability,update,updated,96,"> Why do we change Matplotlib ""font.sans-serif"" anyway?. No idea, I don’t even like Arial. Alex updated the fonts last in 6c68b8ba2821f27bd0b8f499a1d543dff9cc51b2, and setting the fonts happened in the initial commit:. https://github.com/theislab/scanpy/blob/c22e48abe45a6ccca5918bbf689637caa4b31250/scanpy/plotting.py#L605. @falexwolf do you recall why you did that? Can we just remove that line?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805#issuecomment-547000682
https://github.com/scverse/scanpy/pull/806#issuecomment-527404432:28,Testability,test,test,28,Great! Will this change the test pics? If not LGTM.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/806#issuecomment-527404432
https://github.com/scverse/scanpy/pull/806#issuecomment-527568439:21,Testability,test,test,21,"There is no plotting test for PAGA, I think. It'd be great to have some :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/806#issuecomment-527568439
https://github.com/scverse/scanpy/pull/806#issuecomment-527687681:20,Testability,test,tests,20,Added PAGA plotting tests and merged.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/806#issuecomment-527687681
https://github.com/scverse/scanpy/issues/807#issuecomment-527179520:54,Deployability,install,installed,54,for the record: @odorea had the distribution “igraph” installed which contains the “jgraph” package. Scanpy needs the distribution “python-igraph” containing hte package “igraph”. … which is all confusing and annoying so I appreciate that “jgraph” has changed its name away from “igraph”.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-527179520
https://github.com/scverse/scanpy/issues/807#issuecomment-534443201:27,Availability,error,error,27,"Hi, I encountered the same error as odorea. But it could not be solved by renaming `igraph` to `jgraph`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-534443201
https://github.com/scverse/scanpy/issues/807#issuecomment-534457495:22,Deployability,install,install,22,"Hi @YiweiNiu,. Please install `python-igraph` and not `igraph` via pip. So run `pip install python-igraph` and get rid of the `igraph` package. You may have to update you `pip` as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-534457495
https://github.com/scverse/scanpy/issues/807#issuecomment-534457495:84,Deployability,install,install,84,"Hi @YiweiNiu,. Please install `python-igraph` and not `igraph` via pip. So run `pip install python-igraph` and get rid of the `igraph` package. You may have to update you `pip` as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-534457495
https://github.com/scverse/scanpy/issues/807#issuecomment-534457495:160,Deployability,update,update,160,"Hi @YiweiNiu,. Please install `python-igraph` and not `igraph` via pip. So run `pip install python-igraph` and get rid of the `igraph` package. You may have to update you `pip` as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-534457495
https://github.com/scverse/scanpy/issues/807#issuecomment-534505271:68,Safety,avoid,avoid,68,"Is there a way to blacklist igraph and umap packages in setup.py to avoid; further confusion?. On Tue, Sep 24, 2019, 12:13 PM Yiwei Niu <notifications@github.com> wrote:. > @LuckyMD <https://github.com/LuckyMD> Thank you very much! It woks now.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/807?email_source=notifications&email_token=AAIWNB5YP7V74NDJXNEZJ6DQLHR4HA5CNFSM4ISZVL6KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD7N3DPA#issuecomment-534491580>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAIWNBZCSV4KT2AYXIS43BTQLHR4HANCNFSM4ISZVL6A>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-534505271
https://github.com/scverse/scanpy/issues/807#issuecomment-534561181:52,Deployability,install,installed,52,"scanpy could just check on import if one of them is installed instead of the package that should be installed, and raise a nicely phrased ImportError.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-534561181
https://github.com/scverse/scanpy/issues/807#issuecomment-534561181:100,Deployability,install,installed,100,"scanpy could just check on import if one of them is installed instead of the package that should be installed, and raise a nicely phrased ImportError.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-534561181
https://github.com/scverse/scanpy/issues/807#issuecomment-638548074:111,Availability,error,error,111,"@LuckyMD . Hi, LuckyMD. I tried pip uninstalling igraph and pip installing python-igraph and got the following error:. ""Installing collected packages: texttable, python-igraph; ERROR: Could not install packages due to an EnvironmentError: [Errno 5] Input/output error: '/home/blahblah/miniconda2/envs/funkyLab/lib/python3.7/site-packages/igraph/drawing/__pycache__' "". It doesn't say anything more. Do you have an idea of what the problem may be? . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-638548074
https://github.com/scverse/scanpy/issues/807#issuecomment-638548074:177,Availability,ERROR,ERROR,177,"@LuckyMD . Hi, LuckyMD. I tried pip uninstalling igraph and pip installing python-igraph and got the following error:. ""Installing collected packages: texttable, python-igraph; ERROR: Could not install packages due to an EnvironmentError: [Errno 5] Input/output error: '/home/blahblah/miniconda2/envs/funkyLab/lib/python3.7/site-packages/igraph/drawing/__pycache__' "". It doesn't say anything more. Do you have an idea of what the problem may be? . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-638548074
https://github.com/scverse/scanpy/issues/807#issuecomment-638548074:262,Availability,error,error,262,"@LuckyMD . Hi, LuckyMD. I tried pip uninstalling igraph and pip installing python-igraph and got the following error:. ""Installing collected packages: texttable, python-igraph; ERROR: Could not install packages due to an EnvironmentError: [Errno 5] Input/output error: '/home/blahblah/miniconda2/envs/funkyLab/lib/python3.7/site-packages/igraph/drawing/__pycache__' "". It doesn't say anything more. Do you have an idea of what the problem may be? . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-638548074
https://github.com/scverse/scanpy/issues/807#issuecomment-638548074:64,Deployability,install,installing,64,"@LuckyMD . Hi, LuckyMD. I tried pip uninstalling igraph and pip installing python-igraph and got the following error:. ""Installing collected packages: texttable, python-igraph; ERROR: Could not install packages due to an EnvironmentError: [Errno 5] Input/output error: '/home/blahblah/miniconda2/envs/funkyLab/lib/python3.7/site-packages/igraph/drawing/__pycache__' "". It doesn't say anything more. Do you have an idea of what the problem may be? . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-638548074
https://github.com/scverse/scanpy/issues/807#issuecomment-638548074:120,Deployability,Install,Installing,120,"@LuckyMD . Hi, LuckyMD. I tried pip uninstalling igraph and pip installing python-igraph and got the following error:. ""Installing collected packages: texttable, python-igraph; ERROR: Could not install packages due to an EnvironmentError: [Errno 5] Input/output error: '/home/blahblah/miniconda2/envs/funkyLab/lib/python3.7/site-packages/igraph/drawing/__pycache__' "". It doesn't say anything more. Do you have an idea of what the problem may be? . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-638548074
https://github.com/scverse/scanpy/issues/807#issuecomment-638548074:194,Deployability,install,install,194,"@LuckyMD . Hi, LuckyMD. I tried pip uninstalling igraph and pip installing python-igraph and got the following error:. ""Installing collected packages: texttable, python-igraph; ERROR: Could not install packages due to an EnvironmentError: [Errno 5] Input/output error: '/home/blahblah/miniconda2/envs/funkyLab/lib/python3.7/site-packages/igraph/drawing/__pycache__' "". It doesn't say anything more. Do you have an idea of what the problem may be? . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-638548074
https://github.com/scverse/scanpy/issues/807#issuecomment-640070940:52,Performance,load,loaded,52,"Hi @o0stsou0o ,; Could it be that you have `igraph` loaded somewhere while you were uninstalling? Not sure why you can't remove `igraph` otherwise.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-640070940
https://github.com/scverse/scanpy/issues/807#issuecomment-640195237:58,Deployability,install,installing,58,"Hi @LuckyMD,. Thank you. Oddly, I went to sleep and tried installing 'python-igraph' again the next day and it worked just fine. I can only assume pip was being moody the first day. Thank you again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-640195237
https://github.com/scverse/scanpy/issues/808#issuecomment-527131466:134,Availability,error,error,134,"> anndata.AnnData(). Thank you for you suggestion! I failed all other methods, including anndata2ri. God know why I encounter so much error. And I've posted issue in the specific place.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/808#issuecomment-527131466
https://github.com/scverse/scanpy/pull/809#issuecomment-528385710:16,Testability,test,tests,16,"We already have tests for heatmap and matrixplot and it seems they are unaffected. We process the color vector differently there, I think. I'm not entirely sure how that happened and for how long clustermap is broken. Maybe a recent change in seaborn or pandas triggered it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809#issuecomment-528385710
https://github.com/scverse/scanpy/issues/810#issuecomment-528250968:8,Usability,simpl,simply,8,"Did you simply set the verbosity to a higher level to get timings or did you have to use profiling or modify the code? If the latter, it might be helpful for more people to do more fine-grained timings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/810#issuecomment-528250968
https://github.com/scverse/scanpy/issues/810#issuecomment-528383146:124,Integrability,depend,depend,124,"My intuition would be neighbor finding would take more time as dataset size increases. What exact fraction of the time will depend a lot on number of samples, number of features, and possibly distance metric. If you're investigating yourself, I think trying `line_profiler`'s `%lprun` on `umap.UMAP.fit` would be a good bet. I'd also bet that they'd have a better idea over at `UMAP` or Pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/810#issuecomment-528383146
https://github.com/scverse/scanpy/issues/810#issuecomment-528383146:3,Usability,intuit,intuition,3,"My intuition would be neighbor finding would take more time as dataset size increases. What exact fraction of the time will depend a lot on number of samples, number of features, and possibly distance metric. If you're investigating yourself, I think trying `line_profiler`'s `%lprun` on `umap.UMAP.fit` would be a good bet. I'd also bet that they'd have a better idea over at `UMAP` or Pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/810#issuecomment-528383146
https://github.com/scverse/scanpy/issues/810#issuecomment-528642504:11,Testability,test,tested,11,"hi, I just tested a few runs where I added a few lines of code within the compute_neighbors() and compute_connectivites() functions and get the following (in seconds) (thanks for the suggestion using line_profiler, i'll give that a shot later:; ![image](https://user-images.githubusercontent.com/35950152/64389655-20694900-d076-11e9-8b39-e0ee75c36db9.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/810#issuecomment-528642504
https://github.com/scverse/scanpy/pull/812#issuecomment-537020598:4,Deployability,update,update,4,Any update on this? Can you add a test (probably reusing the example already in the method docstring)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812#issuecomment-537020598
https://github.com/scverse/scanpy/pull/812#issuecomment-537020598:34,Testability,test,test,34,Any update on this? Can you add a test (probably reusing the example already in the method docstring)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812#issuecomment-537020598
https://github.com/scverse/scanpy/pull/812#issuecomment-537410912:82,Testability,test,tests,82,"@fidelram, we are still working paper/preprint. I will post it soon. . I will add tests. So in order for the test to work should I add my library in the requirements.txt? What I observed is that other external packages are not included in project requirements.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812#issuecomment-537410912
https://github.com/scverse/scanpy/pull/812#issuecomment-537410912:109,Testability,test,test,109,"@fidelram, we are still working paper/preprint. I will post it soon. . I will add tests. So in order for the test to work should I add my library in the requirements.txt? What I observed is that other external packages are not included in project requirements.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812#issuecomment-537410912
https://github.com/scverse/scanpy/pull/812#issuecomment-537465652:106,Deployability,install,install,106,"@PrimozGodec, probably don't add this to `requirements.txt`, since the requirement should be optional for install. I think instead you should mark it with something like:. ```python; from importlib.util import find_spec. @pytest.mark.skipif(find_spec('pointannotator') is None, reason=""pointannotator not installed""); ```. You can add a requirement for the package to this line in `setup.py`: https://github.com/theislab/scanpy/blob/d8f32c040f3a5f4fc07998b269796ca58de84b40/setup.py#L41. Maybe we should eventually have a second requirements file for CI testing, like we do for anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812#issuecomment-537465652
https://github.com/scverse/scanpy/pull/812#issuecomment-537465652:305,Deployability,install,installed,305,"@PrimozGodec, probably don't add this to `requirements.txt`, since the requirement should be optional for install. I think instead you should mark it with something like:. ```python; from importlib.util import find_spec. @pytest.mark.skipif(find_spec('pointannotator') is None, reason=""pointannotator not installed""); ```. You can add a requirement for the package to this line in `setup.py`: https://github.com/theislab/scanpy/blob/d8f32c040f3a5f4fc07998b269796ca58de84b40/setup.py#L41. Maybe we should eventually have a second requirements file for CI testing, like we do for anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812#issuecomment-537465652
https://github.com/scverse/scanpy/pull/812#issuecomment-537465652:554,Testability,test,testing,554,"@PrimozGodec, probably don't add this to `requirements.txt`, since the requirement should be optional for install. I think instead you should mark it with something like:. ```python; from importlib.util import find_spec. @pytest.mark.skipif(find_spec('pointannotator') is None, reason=""pointannotator not installed""); ```. You can add a requirement for the package to this line in `setup.py`: https://github.com/theislab/scanpy/blob/d8f32c040f3a5f4fc07998b269796ca58de84b40/setup.py#L41. Maybe we should eventually have a second requirements file for CI testing, like we do for anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812#issuecomment-537465652
https://github.com/scverse/scanpy/pull/812#issuecomment-537955879:13,Testability,test,tests,13,I added unit tests and reformated the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812#issuecomment-537955879
https://github.com/scverse/scanpy/pull/812#issuecomment-538909554:58,Testability,test,tests,58,"Thank you. We’re using pytest though, so please write the tests that way:. 1. Remove the class and make all its methods top-level functions; 2. Make `setUp` into `fixture`s; 3. Just use `assert`. ```py; @pytest.fixture; def markers():; return pd.DataFrame(; ...; ). @pytest.fixture; def adata():; ...; return AnnData(data.values, var=data.columns.values). def test_remove_empty_column(adata, markers):; ...; annotations = annotator(adata, markers, num_genes=20); ...; assert len(annotations) == len(self.anndata); ...; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812#issuecomment-538909554
https://github.com/scverse/scanpy/pull/812#issuecomment-538909554:187,Testability,assert,assert,187,"Thank you. We’re using pytest though, so please write the tests that way:. 1. Remove the class and make all its methods top-level functions; 2. Make `setUp` into `fixture`s; 3. Just use `assert`. ```py; @pytest.fixture; def markers():; return pd.DataFrame(; ...; ). @pytest.fixture; def adata():; ...; return AnnData(data.values, var=data.columns.values). def test_remove_empty_column(adata, markers):; ...; annotations = annotator(adata, markers, num_genes=20); ...; assert len(annotations) == len(self.anndata); ...; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812#issuecomment-538909554
https://github.com/scverse/scanpy/pull/812#issuecomment-538909554:468,Testability,assert,assert,468,"Thank you. We’re using pytest though, so please write the tests that way:. 1. Remove the class and make all its methods top-level functions; 2. Make `setUp` into `fixture`s; 3. Just use `assert`. ```py; @pytest.fixture; def markers():; return pd.DataFrame(; ...; ). @pytest.fixture; def adata():; ...; return AnnData(data.values, var=data.columns.values). def test_remove_empty_column(adata, markers):; ...; annotations = annotator(adata, markers, num_genes=20); ...; assert len(annotations) == len(self.anndata); ...; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812#issuecomment-538909554
https://github.com/scverse/scanpy/pull/816#issuecomment-531486992:128,Energy Efficiency,efficient,efficient,128,"So what’s the difference between normalize_per_cell and normalize_total?. If they’re replacable, why don’t we just use the more efficient one?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/816#issuecomment-531486992
https://github.com/scverse/scanpy/pull/816#issuecomment-531900088:50,Energy Efficiency,efficient,efficient,50,"@flying-sheep `normalize_total` is newer and more efficient. And we should use it instead of `normalize_per_cell`, yes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/816#issuecomment-531900088
https://github.com/scverse/scanpy/pull/816#issuecomment-532120452:4,Testability,test,tests,4,"The tests are failing because there’s some master fixes missing from your branch, so this is fine!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/816#issuecomment-532120452
https://github.com/scverse/scanpy/issues/817#issuecomment-528735020:67,Modifiability,variab,variables,67,"Do you have values in `adata.raw`? Raw can have a different set of variables than `adata.var`, which could be causing the issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/817#issuecomment-528735020
https://github.com/scverse/scanpy/pull/819#issuecomment-529235054:406,Performance,optimiz,optimization,406,"> I think it's really good to record the key added. I have a couple questions about the quality score.; > ; > * Should it still be called a modularity score if we aren't using the modularity quality function?. If another flavor is used, we do not record or print anything about quality, so it's ok. But there is a chance that user supplies a partition type from Louvain/Leiden that is not using modularity optimization (e.g. CPM or Surprize). In this case, we do not print the term `scaled modularity` anymore. > * Do you have some use cases for recording the modularity score? My impression was that it may not have much interpretable meaning, especially between different graphs.; > . To me, scaled modularity is like any statistical measure which gives a rough idea about a concept, like correlation or silhouette coef. It's far from conclusive just by itself, but it gives a ""feeling"" of how ""well-clustered"" the data is (and how good we are at finding them). Without complementing it with other measures, it's not more than just a ""feeling"" :). > Also, should this stuff be mirrored to `leiden`?. It's done.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529235054
https://github.com/scverse/scanpy/pull/819#issuecomment-529329056:198,Performance,optimiz,optimization,198,"> If another flavor is used, we do not record or print anything about quality, so it's ok. But there is a chance that user supplies a partition type from Louvain/Leiden that is not using modularity optimization (e.g. CPM or Surprize). In this case, we do not print the term scaled modularity anymore. By default we don't use modularity as the quality function, we use the `RBConfigurationVertexPartition`. I believe that we should get a quality score from the leiden and louvain packages regardless of quality function though. Could we use the term `quality_score` and also store `quality_function` used in params like: `.uns[key_added][""params""][""quality_function""] = partition_type.__name__`?. > To me, scaled modularity is like any statistical measure which gives a rough idea about a concept, like correlation or silhouette coef. It's far from conclusive just by itself, but it gives a ""feeling"" of how ""well-clustered"" the data is (and how good we are at finding them). Without complementing it with other measures, it's not more than just a ""feeling"" :). A couple follow up points on this and @LuckyMD's points. * I don't actually know how different the quality score can be for different solutions. Any chance you have some stats on quality scores from multiple clusterings? I'm mostly wondering if ""good"" clusterings are associated with high quality scores.; * I think if a user sees a value like ""quality"" they could ascribe more meaning to it than it deserves. I think we should add some docs about what it means, and how to interpret it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529329056
https://github.com/scverse/scanpy/pull/819#issuecomment-529377195:579,Performance,optimiz,optimized,579,">A couple follow up points on this and @LuckyMD's points:; > I don't actually know how different the quality score can be for different solutions. Any chance you have some stats on quality scores from multiple clusterings? I'm mostly wondering if ""good"" clusterings are associated with high quality scores.; > I think if a user sees a value like ""quality"" they could ascribe more meaning to it than it deserves. I think we should add some docs about what it means, and how to interpret it. I'm not sure I entirely understand your point. The quality score is modularity, which is optimized. Thus a ""good"" partition is a high quality score by definition. Or what are you referring to as ""good""? . I have looked at some stats for communities in protein-protein interaction networks, and the quality of the communities can change dramatically with louvain output there (could find the link if you think it's relevant). However, modularity as a score is fairly degenerate toward the optimal score and therefore the value often doesn't change that much between the optimized partitions. I'm not sure how this is on the knn graphs though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529377195
https://github.com/scverse/scanpy/pull/819#issuecomment-529377195:1059,Performance,optimiz,optimized,1059,">A couple follow up points on this and @LuckyMD's points:; > I don't actually know how different the quality score can be for different solutions. Any chance you have some stats on quality scores from multiple clusterings? I'm mostly wondering if ""good"" clusterings are associated with high quality scores.; > I think if a user sees a value like ""quality"" they could ascribe more meaning to it than it deserves. I think we should add some docs about what it means, and how to interpret it. I'm not sure I entirely understand your point. The quality score is modularity, which is optimized. Thus a ""good"" partition is a high quality score by definition. Or what are you referring to as ""good""? . I have looked at some stats for communities in protein-protein interaction networks, and the quality of the communities can change dramatically with louvain output there (could find the link if you think it's relevant). However, modularity as a score is fairly degenerate toward the optimal score and therefore the value often doesn't change that much between the optimized partitions. I'm not sure how this is on the knn graphs though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529377195
https://github.com/scverse/scanpy/pull/819#issuecomment-529494088:2951,Availability,avail,available,2951,"cs.io/en/latest/reference.html#rbconfigurationvertexpartition)). I account for 1) in the code but using a resolution parameter other than 1.0 would lead to values different than modularity due to 2). Right now, for example, you can get a perfect quality (=1.0) by just setting the resolution to 0.0 :D I don't think that'd mislead users though. After all, that's what the algorithm uses for optimization. I can think of two solutions. We can report typical modularity regardless of the `partition_type`, namely:. ```; modularity_part = leidenalg.ModularityVertexPartition(g, initial_membership=part.membership); q = modularity_part.quality(); ```. or we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. It's in the ""hint"" verbosity level anyway. Regarding the suggestion to record `partition_type.__name__`, I think it's a good idea. I'd record it in the `uns[uns_key]['partition_type']` though, not in `quality_function`. > > To me, scaled modularity is like any statistical measure which gives a rough idea about a concept, like correlation or silhouette coef. It's far from conclusive just by itself, but it gives a ""feeling"" of how ""well-clustered"" the data is (and how good we are at finding them). Without complementing it with other measures, it's not more than just a ""feeling"" :); > ; > A couple follow up points on this and @LuckyMD's points; > ; > * I don't actually know how different the quality score can be for different solutions. Any chance you have some stats on quality scores from multiple clusterings? I'm mostly wondering if ""good"" clusterings are associated with high quality scores.; > * I think if a user sees a value like ""quality"" they could ascribe more meaning to it than it deserves. I think we should add some docs about what it means, and how to interpret it. That makes sense. Maybe calculating typical modularity and using the term modularity is safe enough, since definition of modularity is available everywhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529494088
https://github.com/scverse/scanpy/pull/819#issuecomment-529494088:824,Performance,optimiz,optimization,824,"> By default we don't use modularity as the quality function, we use the `RBConfigurationVertexPartition`. I believe that we should get a quality score from the leiden and louvain packages regardless of quality function though. Could we use the term `quality_score` and also store `quality_function` used in params like: `.uns[key_added][""params""][""quality_function""] = partition_type.__name__`?; > . Strictly speaking, you are correct that the quality function of `RBConfigurationVertexPartition` is not exactly the same as modularity, although it is called unscaled modularity in the [code](https://github.com/vtraag/louvain-igraph/blob/master/src/RBConfigurationVertexPartition.cpp#L123). . There are two main differences between RBConfigurationVertexPartition and ModularityVertexPartition which uses typical modularity optimization. 1) Scaling by the number of edges and 2) the resolution parameter (as it's written [here in the note](https://louvain-igraph.readthedocs.io/en/latest/reference.html#rbconfigurationvertexpartition)). I account for 1) in the code but using a resolution parameter other than 1.0 would lead to values different than modularity due to 2). Right now, for example, you can get a perfect quality (=1.0) by just setting the resolution to 0.0 :D I don't think that'd mislead users though. After all, that's what the algorithm uses for optimization. I can think of two solutions. We can report typical modularity regardless of the `partition_type`, namely:. ```; modularity_part = leidenalg.ModularityVertexPartition(g, initial_membership=part.membership); q = modularity_part.quality(); ```. or we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. It's in the ""hint"" verbosity level anyway. Regarding the suggestion to record `partition_type.__name__`, I think it's a good idea. I'd record it in the `uns[uns_key]['partition_type']` though, not in `quality_function`. > > To me, scaled modularity is like any statistical m",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529494088
https://github.com/scverse/scanpy/pull/819#issuecomment-529494088:1363,Performance,optimiz,optimization,1363,"][""quality_function""] = partition_type.__name__`?; > . Strictly speaking, you are correct that the quality function of `RBConfigurationVertexPartition` is not exactly the same as modularity, although it is called unscaled modularity in the [code](https://github.com/vtraag/louvain-igraph/blob/master/src/RBConfigurationVertexPartition.cpp#L123). . There are two main differences between RBConfigurationVertexPartition and ModularityVertexPartition which uses typical modularity optimization. 1) Scaling by the number of edges and 2) the resolution parameter (as it's written [here in the note](https://louvain-igraph.readthedocs.io/en/latest/reference.html#rbconfigurationvertexpartition)). I account for 1) in the code but using a resolution parameter other than 1.0 would lead to values different than modularity due to 2). Right now, for example, you can get a perfect quality (=1.0) by just setting the resolution to 0.0 :D I don't think that'd mislead users though. After all, that's what the algorithm uses for optimization. I can think of two solutions. We can report typical modularity regardless of the `partition_type`, namely:. ```; modularity_part = leidenalg.ModularityVertexPartition(g, initial_membership=part.membership); q = modularity_part.quality(); ```. or we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. It's in the ""hint"" verbosity level anyway. Regarding the suggestion to record `partition_type.__name__`, I think it's a good idea. I'd record it in the `uns[uns_key]['partition_type']` though, not in `quality_function`. > > To me, scaled modularity is like any statistical measure which gives a rough idea about a concept, like correlation or silhouette coef. It's far from conclusive just by itself, but it gives a ""feeling"" of how ""well-clustered"" the data is (and how good we are at finding them). Without complementing it with other measures, it's not more than just a ""feeling"" :); > ; > A couple follow up points ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529494088
https://github.com/scverse/scanpy/pull/819#issuecomment-529494088:2904,Safety,safe,safe,2904,"cs.io/en/latest/reference.html#rbconfigurationvertexpartition)). I account for 1) in the code but using a resolution parameter other than 1.0 would lead to values different than modularity due to 2). Right now, for example, you can get a perfect quality (=1.0) by just setting the resolution to 0.0 :D I don't think that'd mislead users though. After all, that's what the algorithm uses for optimization. I can think of two solutions. We can report typical modularity regardless of the `partition_type`, namely:. ```; modularity_part = leidenalg.ModularityVertexPartition(g, initial_membership=part.membership); q = modularity_part.quality(); ```. or we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. It's in the ""hint"" verbosity level anyway. Regarding the suggestion to record `partition_type.__name__`, I think it's a good idea. I'd record it in the `uns[uns_key]['partition_type']` though, not in `quality_function`. > > To me, scaled modularity is like any statistical measure which gives a rough idea about a concept, like correlation or silhouette coef. It's far from conclusive just by itself, but it gives a ""feeling"" of how ""well-clustered"" the data is (and how good we are at finding them). Without complementing it with other measures, it's not more than just a ""feeling"" :); > ; > A couple follow up points on this and @LuckyMD's points; > ; > * I don't actually know how different the quality score can be for different solutions. Any chance you have some stats on quality scores from multiple clusterings? I'm mostly wondering if ""good"" clusterings are associated with high quality scores.; > * I think if a user sees a value like ""quality"" they could ascribe more meaning to it than it deserves. I think we should add some docs about what it means, and how to interpret it. That makes sense. Maybe calculating typical modularity and using the term modularity is safe enough, since definition of modularity is available everywhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529494088
https://github.com/scverse/scanpy/pull/819#issuecomment-529791688:307,Performance,optimiz,optimization,307,"On the definition of modularity, I did go back over some literature and saw that modularity sometimes takes multiple meanings within a paper. It can be either the [specific quality function](https://leidenalg.readthedocs.io/en/latest/reference.html#modularityvertexpartition), or when used like ""modularity optimization"" can refer to the whole class of partition optimizing algorithms (which are generic wrt quality function) like `louvain` ([I like section IV F of this paper for an overview](https://arxiv.org/abs/1608.00163v2)). @LuckyMD. > The quality score is modularity, which is optimized. Thus a ""good"" partition is a high quality score by definition. Or what are you referring to as ""good""?. I think of the quality function/score as being determined by the `partition_type`. . To me, a good partition is one that seperates data points into discrete groups which reflect some true underlying structure. I put this in quotes since it’s ill-defined, however we can tell when it’s definitely not true. A high quality score for a partitioning is just a high quality score for a partitioning. @gokceneraslan . > we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. Personally, I would just report the quality metric calculated by the quality function used. To me, the point of returning this value would be to know if the optimization went well, which is probably best measured by looking at the optimized value. This would also simplify the code a bunch. I think there's another case for trying to tell if it's a ""good"" partitioning, but I think that should be handled seperatly. > Regarding the suggestion to record partition_type.__name__, I think it's a good idea. I'd record it in the uns[uns_key]['partition_type'] though, not in quality_function. That's reasonable. Just to be sure, we'd keep it in `uns[uns_key][""params""]['partition_type']` like it is now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529791688
https://github.com/scverse/scanpy/pull/819#issuecomment-529791688:363,Performance,optimiz,optimizing,363,"On the definition of modularity, I did go back over some literature and saw that modularity sometimes takes multiple meanings within a paper. It can be either the [specific quality function](https://leidenalg.readthedocs.io/en/latest/reference.html#modularityvertexpartition), or when used like ""modularity optimization"" can refer to the whole class of partition optimizing algorithms (which are generic wrt quality function) like `louvain` ([I like section IV F of this paper for an overview](https://arxiv.org/abs/1608.00163v2)). @LuckyMD. > The quality score is modularity, which is optimized. Thus a ""good"" partition is a high quality score by definition. Or what are you referring to as ""good""?. I think of the quality function/score as being determined by the `partition_type`. . To me, a good partition is one that seperates data points into discrete groups which reflect some true underlying structure. I put this in quotes since it’s ill-defined, however we can tell when it’s definitely not true. A high quality score for a partitioning is just a high quality score for a partitioning. @gokceneraslan . > we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. Personally, I would just report the quality metric calculated by the quality function used. To me, the point of returning this value would be to know if the optimization went well, which is probably best measured by looking at the optimized value. This would also simplify the code a bunch. I think there's another case for trying to tell if it's a ""good"" partitioning, but I think that should be handled seperatly. > Regarding the suggestion to record partition_type.__name__, I think it's a good idea. I'd record it in the uns[uns_key]['partition_type'] though, not in quality_function. That's reasonable. Just to be sure, we'd keep it in `uns[uns_key][""params""]['partition_type']` like it is now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529791688
https://github.com/scverse/scanpy/pull/819#issuecomment-529791688:586,Performance,optimiz,optimized,586,"On the definition of modularity, I did go back over some literature and saw that modularity sometimes takes multiple meanings within a paper. It can be either the [specific quality function](https://leidenalg.readthedocs.io/en/latest/reference.html#modularityvertexpartition), or when used like ""modularity optimization"" can refer to the whole class of partition optimizing algorithms (which are generic wrt quality function) like `louvain` ([I like section IV F of this paper for an overview](https://arxiv.org/abs/1608.00163v2)). @LuckyMD. > The quality score is modularity, which is optimized. Thus a ""good"" partition is a high quality score by definition. Or what are you referring to as ""good""?. I think of the quality function/score as being determined by the `partition_type`. . To me, a good partition is one that seperates data points into discrete groups which reflect some true underlying structure. I put this in quotes since it’s ill-defined, however we can tell when it’s definitely not true. A high quality score for a partitioning is just a high quality score for a partitioning. @gokceneraslan . > we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. Personally, I would just report the quality metric calculated by the quality function used. To me, the point of returning this value would be to know if the optimization went well, which is probably best measured by looking at the optimized value. This would also simplify the code a bunch. I think there's another case for trying to tell if it's a ""good"" partitioning, but I think that should be handled seperatly. > Regarding the suggestion to record partition_type.__name__, I think it's a good idea. I'd record it in the uns[uns_key]['partition_type'] though, not in quality_function. That's reasonable. Just to be sure, we'd keep it in `uns[uns_key][""params""]['partition_type']` like it is now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529791688
https://github.com/scverse/scanpy/pull/819#issuecomment-529791688:1376,Performance,optimiz,optimization,1376,"On the definition of modularity, I did go back over some literature and saw that modularity sometimes takes multiple meanings within a paper. It can be either the [specific quality function](https://leidenalg.readthedocs.io/en/latest/reference.html#modularityvertexpartition), or when used like ""modularity optimization"" can refer to the whole class of partition optimizing algorithms (which are generic wrt quality function) like `louvain` ([I like section IV F of this paper for an overview](https://arxiv.org/abs/1608.00163v2)). @LuckyMD. > The quality score is modularity, which is optimized. Thus a ""good"" partition is a high quality score by definition. Or what are you referring to as ""good""?. I think of the quality function/score as being determined by the `partition_type`. . To me, a good partition is one that seperates data points into discrete groups which reflect some true underlying structure. I put this in quotes since it’s ill-defined, however we can tell when it’s definitely not true. A high quality score for a partitioning is just a high quality score for a partitioning. @gokceneraslan . > we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. Personally, I would just report the quality metric calculated by the quality function used. To me, the point of returning this value would be to know if the optimization went well, which is probably best measured by looking at the optimized value. This would also simplify the code a bunch. I think there's another case for trying to tell if it's a ""good"" partitioning, but I think that should be handled seperatly. > Regarding the suggestion to record partition_type.__name__, I think it's a good idea. I'd record it in the uns[uns_key]['partition_type'] though, not in quality_function. That's reasonable. Just to be sure, we'd keep it in `uns[uns_key][""params""]['partition_type']` like it is now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529791688
https://github.com/scverse/scanpy/pull/819#issuecomment-529791688:1450,Performance,optimiz,optimized,1450,"On the definition of modularity, I did go back over some literature and saw that modularity sometimes takes multiple meanings within a paper. It can be either the [specific quality function](https://leidenalg.readthedocs.io/en/latest/reference.html#modularityvertexpartition), or when used like ""modularity optimization"" can refer to the whole class of partition optimizing algorithms (which are generic wrt quality function) like `louvain` ([I like section IV F of this paper for an overview](https://arxiv.org/abs/1608.00163v2)). @LuckyMD. > The quality score is modularity, which is optimized. Thus a ""good"" partition is a high quality score by definition. Or what are you referring to as ""good""?. I think of the quality function/score as being determined by the `partition_type`. . To me, a good partition is one that seperates data points into discrete groups which reflect some true underlying structure. I put this in quotes since it’s ill-defined, however we can tell when it’s definitely not true. A high quality score for a partitioning is just a high quality score for a partitioning. @gokceneraslan . > we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. Personally, I would just report the quality metric calculated by the quality function used. To me, the point of returning this value would be to know if the optimization went well, which is probably best measured by looking at the optimized value. This would also simplify the code a bunch. I think there's another case for trying to tell if it's a ""good"" partitioning, but I think that should be handled seperatly. > Regarding the suggestion to record partition_type.__name__, I think it's a good idea. I'd record it in the uns[uns_key]['partition_type'] though, not in quality_function. That's reasonable. Just to be sure, we'd keep it in `uns[uns_key][""params""]['partition_type']` like it is now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529791688
https://github.com/scverse/scanpy/pull/819#issuecomment-529791688:1483,Usability,simpl,simplify,1483,"On the definition of modularity, I did go back over some literature and saw that modularity sometimes takes multiple meanings within a paper. It can be either the [specific quality function](https://leidenalg.readthedocs.io/en/latest/reference.html#modularityvertexpartition), or when used like ""modularity optimization"" can refer to the whole class of partition optimizing algorithms (which are generic wrt quality function) like `louvain` ([I like section IV F of this paper for an overview](https://arxiv.org/abs/1608.00163v2)). @LuckyMD. > The quality score is modularity, which is optimized. Thus a ""good"" partition is a high quality score by definition. Or what are you referring to as ""good""?. I think of the quality function/score as being determined by the `partition_type`. . To me, a good partition is one that seperates data points into discrete groups which reflect some true underlying structure. I put this in quotes since it’s ill-defined, however we can tell when it’s definitely not true. A high quality score for a partitioning is just a high quality score for a partitioning. @gokceneraslan . > we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. Personally, I would just report the quality metric calculated by the quality function used. To me, the point of returning this value would be to know if the optimization went well, which is probably best measured by looking at the optimized value. This would also simplify the code a bunch. I think there's another case for trying to tell if it's a ""good"" partitioning, but I think that should be handled seperatly. > Regarding the suggestion to record partition_type.__name__, I think it's a good idea. I'd record it in the uns[uns_key]['partition_type'] though, not in quality_function. That's reasonable. Just to be sure, we'd keep it in `uns[uns_key][""params""]['partition_type']` like it is now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529791688
https://github.com/scverse/scanpy/pull/819#issuecomment-529929977:171,Performance,optimiz,optimization,171,"@ivirshup . > Personally, I would just report the quality metric calculated by the quality function used. To me, the point of returning this value would be to know if the optimization went well, which is probably best measured by looking at the optimized value. The quality score returned by RBConfigurationVertexPartition is unscaled modularity, so it's something like `41726.23`. So how would one interpret that? I don't think there is any point in reporting raw RBConfigurationVertexPartition quality value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529929977
https://github.com/scverse/scanpy/pull/819#issuecomment-529929977:245,Performance,optimiz,optimized,245,"@ivirshup . > Personally, I would just report the quality metric calculated by the quality function used. To me, the point of returning this value would be to know if the optimization went well, which is probably best measured by looking at the optimized value. The quality score returned by RBConfigurationVertexPartition is unscaled modularity, so it's something like `41726.23`. So how would one interpret that? I don't think there is any point in reporting raw RBConfigurationVertexPartition quality value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529929977
https://github.com/scverse/scanpy/pull/819#issuecomment-531683547:793,Performance,optimiz,optimization,793,"Sorry for the long delay – I was away on a retreat. By the way, congrats on the spatial letter!. I'm not sure I understand why you'd report the standard modularity if the partitioning was done with multi resolution modularity or some other quality function. To me, this makes the metric being pretty disconnected from the computation that was run. It seems likely that there could be non-proportional relationships between the whatever quality function is used and unscaled modularity. For example there could be a case where: partitioning A has higher unscaled modularity than partitioning B, but B has higher multi resolution modularity quality with resolution .8 than A. If the multi resolution modularity is what was run, I think the logging should reflect that run resulted in a ""better"" optimization. Separately, if it's meant to assess the quality of the clustering, why not calculate something like silhouette? From my perspective, it would be because that's separate enough from the process of clustering that it should be run separately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-531683547
https://github.com/scverse/scanpy/pull/819#issuecomment-531683547:738,Testability,log,logging,738,"Sorry for the long delay – I was away on a retreat. By the way, congrats on the spatial letter!. I'm not sure I understand why you'd report the standard modularity if the partitioning was done with multi resolution modularity or some other quality function. To me, this makes the metric being pretty disconnected from the computation that was run. It seems likely that there could be non-proportional relationships between the whatever quality function is used and unscaled modularity. For example there could be a case where: partitioning A has higher unscaled modularity than partitioning B, but B has higher multi resolution modularity quality with resolution .8 than A. If the multi resolution modularity is what was run, I think the logging should reflect that run resulted in a ""better"" optimization. Separately, if it's meant to assess the quality of the clustering, why not calculate something like silhouette? From my perspective, it would be because that's separate enough from the process of clustering that it should be run separately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-531683547
https://github.com/scverse/scanpy/pull/819#issuecomment-557824199:150,Deployability,update,update,150,"I think the PR is now good as is. I will be quite busy for some time, therefore I won't have time for further discussions. Please feel free to close, update or merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-557824199
https://github.com/scverse/scanpy/pull/820#issuecomment-529927212:1273,Security,access,access,1273,"> This sounds familiar, but I don't remember where it happens. Could you point me towards that?; > ; > Never mind, I think I found it in [`fuzzy_simplical_set`](https://github.com/lmcinnes/umap/blob/439db748b9959b53d6678b6fdc6cb18e8f49c6c6/umap/umap_.py#L566-L574), . Yes, exactly. > but made me think of another question:; > ; > Do we want to save the indices and distances as a pair of arrays, . What do you mean? If we merge this, we will have `adata.uns['neighbors']['knn_indices']` and `adata.uns['neighbors']['distances']` (only if it's requested by the user). Is this what you mean? . > or just save a sparse matrix of the original distances? I think the latter would be easier to work with. I'm not following. `adata.uns['neighbors']['distances']` encodes a different type of information since it represents the graph structure and it's symmetrized. `knn_indices` on the other hand represents the ""raw"" output of kNN method. Just like `adata.uns['neighbors']['rp_forest']`, it's additional information about the kNN. It can be used for other things like kNN classifiers or building other types of graphs like mutual kNN as I mentioned. Just to clarify, I don't propose using knn_indices as a replacement of distance or connectivitiy matrix. It's just to be able to access more details of the kNN construction. Furthermore, it's optional and it's False by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/820#issuecomment-529927212
https://github.com/scverse/scanpy/issues/821#issuecomment-529213147:198,Deployability,integrat,integrated,198,"Hi @sygongcode,. Are you referring to differential expression testing between conditions? You can do that with `sc.tl.rank_genes_groups()` or in a more advanced way using `diffxpy`, which is easily integrated with `scanpy`. You can find it [here](https://github.com/theislab/diffxpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821#issuecomment-529213147
https://github.com/scverse/scanpy/issues/821#issuecomment-529213147:198,Integrability,integrat,integrated,198,"Hi @sygongcode,. Are you referring to differential expression testing between conditions? You can do that with `sc.tl.rank_genes_groups()` or in a more advanced way using `diffxpy`, which is easily integrated with `scanpy`. You can find it [here](https://github.com/theislab/diffxpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821#issuecomment-529213147
https://github.com/scverse/scanpy/issues/821#issuecomment-529213147:62,Testability,test,testing,62,"Hi @sygongcode,. Are you referring to differential expression testing between conditions? You can do that with `sc.tl.rank_genes_groups()` or in a more advanced way using `diffxpy`, which is easily integrated with `scanpy`. You can find it [here](https://github.com/theislab/diffxpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821#issuecomment-529213147
https://github.com/scverse/scanpy/issues/821#issuecomment-529218989:206,Deployability,integrat,integrated,206,"> Hi @sygongcode,; > ; > Are you referring to differential expression testing between conditions? You can do that with `sc.tl.rank_genes_groups()` or in a more advanced way using `diffxpy`, which is easily integrated with `scanpy`. You can find it [here](https://github.com/theislab/diffxpy). Yes, that is what I want to do. Thank you so much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821#issuecomment-529218989
https://github.com/scverse/scanpy/issues/821#issuecomment-529218989:206,Integrability,integrat,integrated,206,"> Hi @sygongcode,; > ; > Are you referring to differential expression testing between conditions? You can do that with `sc.tl.rank_genes_groups()` or in a more advanced way using `diffxpy`, which is easily integrated with `scanpy`. You can find it [here](https://github.com/theislab/diffxpy). Yes, that is what I want to do. Thank you so much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821#issuecomment-529218989
https://github.com/scverse/scanpy/issues/821#issuecomment-529218989:70,Testability,test,testing,70,"> Hi @sygongcode,; > ; > Are you referring to differential expression testing between conditions? You can do that with `sc.tl.rank_genes_groups()` or in a more advanced way using `diffxpy`, which is easily integrated with `scanpy`. You can find it [here](https://github.com/theislab/diffxpy). Yes, that is what I want to do. Thank you so much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821#issuecomment-529218989
https://github.com/scverse/scanpy/pull/822#issuecomment-577743290:57,Integrability,depend,dependencies,57,I can’t reproduce this locally and would like to get the dependencies unlocked. Where’s the bug report at h5py?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/822#issuecomment-577743290
https://github.com/scverse/scanpy/pull/823#issuecomment-718745640:342,Testability,test,test,342,"Hey guys @LuckyMD, @Koncopd, @falexwolf, @flying-sheep. I think this feature would be very useful to have in Scanpy, but this PR has been sort of forgotten. . I would be up to take care of this, but it would be my first contribution and I'd like some advice on how to move forward on this. I take it the main issue with the PR is the missing test for the scran normalization, is that correct?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/823#issuecomment-718745640
https://github.com/scverse/scanpy/pull/823#issuecomment-718863510:74,Testability,test,tests,74,"A student of @mbuttner started this, and yes, I believe it's only missing tests. I would probably make a small test case and record the results of the `R` version on there, and use it as a test for this. You can check out the `testing/` directory for tests. . To start, just fetch this branch and you should be able to commit to it directly as a member of theislab github.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/823#issuecomment-718863510
https://github.com/scverse/scanpy/pull/823#issuecomment-718863510:111,Testability,test,test,111,"A student of @mbuttner started this, and yes, I believe it's only missing tests. I would probably make a small test case and record the results of the `R` version on there, and use it as a test for this. You can check out the `testing/` directory for tests. . To start, just fetch this branch and you should be able to commit to it directly as a member of theislab github.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/823#issuecomment-718863510
https://github.com/scverse/scanpy/pull/823#issuecomment-718863510:189,Testability,test,test,189,"A student of @mbuttner started this, and yes, I believe it's only missing tests. I would probably make a small test case and record the results of the `R` version on there, and use it as a test for this. You can check out the `testing/` directory for tests. . To start, just fetch this branch and you should be able to commit to it directly as a member of theislab github.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/823#issuecomment-718863510
https://github.com/scverse/scanpy/pull/823#issuecomment-718863510:227,Testability,test,testing,227,"A student of @mbuttner started this, and yes, I believe it's only missing tests. I would probably make a small test case and record the results of the `R` version on there, and use it as a test for this. You can check out the `testing/` directory for tests. . To start, just fetch this branch and you should be able to commit to it directly as a member of theislab github.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/823#issuecomment-718863510
https://github.com/scverse/scanpy/pull/823#issuecomment-718863510:251,Testability,test,tests,251,"A student of @mbuttner started this, and yes, I believe it's only missing tests. I would probably make a small test case and record the results of the `R` version on there, and use it as a test for this. You can check out the `testing/` directory for tests. . To start, just fetch this branch and you should be able to commit to it directly as a member of theislab github.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/823#issuecomment-718863510
https://github.com/scverse/scanpy/pull/823#issuecomment-721033044:47,Testability,test,tests,47,Unfortunately the PR was not just missing some tests. There are quite a lot of missing parts and the code is not correct. This will take much more effort than expected. I will try working on it on my own branch and close this PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/823#issuecomment-721033044
https://github.com/scverse/scanpy/pull/824#issuecomment-529560265:34,Energy Efficiency,efficient,efficiently,34,"> I think this could be done more efficiently by using the index returned from `filter_genes(..., inplace=False)` in `_highly_variable_genes_single_batch` and instead of the whole data frame merging you add in the current version of your changes. I guess that would depend if you want to have a `filter_genes` call in the HVG selection function every time, or whether you only want it in there in a case where `filter_genes` normally doesn't work. You typically use it on the whole dataset, but not per batch. Another issue atm is that if you set the verbosity high, then the `filter_genes()` call gives you an output, which is not really intended as the user can't see the function call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-529560265
https://github.com/scverse/scanpy/pull/824#issuecomment-529560265:266,Integrability,depend,depend,266,"> I think this could be done more efficiently by using the index returned from `filter_genes(..., inplace=False)` in `_highly_variable_genes_single_batch` and instead of the whole data frame merging you add in the current version of your changes. I guess that would depend if you want to have a `filter_genes` call in the HVG selection function every time, or whether you only want it in there in a case where `filter_genes` normally doesn't work. You typically use it on the whole dataset, but not per batch. Another issue atm is that if you set the verbosity high, then the `filter_genes()` call gives you an output, which is not really intended as the user can't see the function call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-529560265
https://github.com/scverse/scanpy/pull/824#issuecomment-529840088:200,Availability,error,error,200,"This is mainly a fix for cases when multiple genes have zero variance. The output in that case was that `sc.pp.highly_variable_genes()` failed as the bin boundaries were too close to one another. The error I got is:; ```; ValueError: Bin edges must be unique: array([ -inf, 9.99999996e-13, 9.99999996e-13, 3.71624832e-03,; 4.50723944e-03, 5.04237041e-03, 7.96065722e-03, 9.17631686e-03,; 1.15813100e-02, 1.34968273e-02, 1.62843971e-02, 1.89858746e-02,; 2.27864407e-02, 2.76163086e-02, 3.43018658e-02, 4.27573830e-02,; 5.52219763e-02, 7.87758350e-02, 1.42211060e-01, 3.10728383e+00,; inf]).; You can drop duplicate edges by setting the 'duplicates' kwarg; ```. I figured the best way forward would be to exclude those genes from the function, rather than changing the bins in the `_highly_variable_genes_single_batch()` function with the `duplicates` argument as suggested in the error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-529840088
https://github.com/scverse/scanpy/pull/824#issuecomment-529840088:879,Availability,error,error,879,"This is mainly a fix for cases when multiple genes have zero variance. The output in that case was that `sc.pp.highly_variable_genes()` failed as the bin boundaries were too close to one another. The error I got is:; ```; ValueError: Bin edges must be unique: array([ -inf, 9.99999996e-13, 9.99999996e-13, 3.71624832e-03,; 4.50723944e-03, 5.04237041e-03, 7.96065722e-03, 9.17631686e-03,; 1.15813100e-02, 1.34968273e-02, 1.62843971e-02, 1.89858746e-02,; 2.27864407e-02, 2.76163086e-02, 3.43018658e-02, 4.27573830e-02,; 5.52219763e-02, 7.87758350e-02, 1.42211060e-01, 3.10728383e+00,; inf]).; You can drop duplicate edges by setting the 'duplicates' kwarg; ```. I figured the best way forward would be to exclude those genes from the function, rather than changing the bins in the `_highly_variable_genes_single_batch()` function with the `duplicates` argument as suggested in the error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-529840088
https://github.com/scverse/scanpy/pull/824#issuecomment-529851678:171,Availability,error,error,171,"> This is mainly a fix for cases when multiple genes have zero variance. Could you add that as the test case? When some genes aren't expressed in a batch you won't get an error. > the best way forward would be to exclude those genes from the function. I think the approach of masking out the non-expressed genes sounds reasonable, since that's what you'd probably do if it were just one dataset. I'd definitely defer to @gokceneraslan on any more about the appropriateness of the approach.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-529851678
https://github.com/scverse/scanpy/pull/824#issuecomment-529851678:276,Availability,mask,masking,276,"> This is mainly a fix for cases when multiple genes have zero variance. Could you add that as the test case? When some genes aren't expressed in a batch you won't get an error. > the best way forward would be to exclude those genes from the function. I think the approach of masking out the non-expressed genes sounds reasonable, since that's what you'd probably do if it were just one dataset. I'd definitely defer to @gokceneraslan on any more about the appropriateness of the approach.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-529851678
https://github.com/scverse/scanpy/pull/824#issuecomment-529851678:99,Testability,test,test,99,"> This is mainly a fix for cases when multiple genes have zero variance. Could you add that as the test case? When some genes aren't expressed in a batch you won't get an error. > the best way forward would be to exclude those genes from the function. I think the approach of masking out the non-expressed genes sounds reasonable, since that's what you'd probably do if it were just one dataset. I'd definitely defer to @gokceneraslan on any more about the appropriateness of the approach.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-529851678
https://github.com/scverse/scanpy/pull/824#issuecomment-530426113:52,Availability,error,error,52,"I had to use the pbmc3k dataset for testing, as the error doesn't occur on blobs or pbmc68k_reduced. To test I need sufficient genes that have 0 variance in a subset of the cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-530426113
https://github.com/scverse/scanpy/pull/824#issuecomment-530426113:36,Testability,test,testing,36,"I had to use the pbmc3k dataset for testing, as the error doesn't occur on blobs or pbmc68k_reduced. To test I need sufficient genes that have 0 variance in a subset of the cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-530426113
https://github.com/scverse/scanpy/pull/824#issuecomment-530426113:104,Testability,test,test,104,"I had to use the pbmc3k dataset for testing, as the error doesn't occur on blobs or pbmc68k_reduced. To test I need sufficient genes that have 0 variance in a subset of the cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-530426113
https://github.com/scverse/scanpy/pull/824#issuecomment-530432997:63,Testability,test,test,63,Cancel that @flying-sheep sheep helped me find a way around to test with `pbmc68k_reduced`. This should speed up Travis again.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-530432997
https://github.com/scverse/scanpy/pull/824#issuecomment-530443204:25,Modifiability,refactor,refactor,25,Works for me! I’d say we refactor the helper function in a separate PR,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-530443204
https://github.com/scverse/scanpy/issues/828#issuecomment-560072919:796,Availability,mask,masks,796,"As an update, I've been using this helper function to consistently handle this:. ```python. def _choose_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):; """"""; Choose array aligned with obs annotation.; """"""; is_layer = layer is not None; is_raw = use_raw is not False; is_obsm = obsm is not None; is_obsp = obsp is not None; choices_made = sum((is_layer, is_raw, is_obsm, is_obsp)); assert choices_made <= 1; if choices_made == 0:; return adata.X; elif is_layer:; return adata.layers[layer]; elif use_raw:; return adata.raw.X; elif is_obsm:; return adata.obsm[obsm]; elif is_obsp:; return adata.obsp[obsp]; else:; assert False, (; ""That was unexpected. Please report this bug at:\n\n\t""; "" https://github.com/theislab/scanpy/issues""; ); ```. This could use support for variable masks like `use_highly_variable`. Also the error message should be better. I think a collection of helper functions like this should go in to a utils module (`sc.utils.argutils`?) which could be public so it's easier to use in `scanpy`-like packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919
https://github.com/scverse/scanpy/issues/828#issuecomment-560072919:839,Availability,error,error,839,"As an update, I've been using this helper function to consistently handle this:. ```python. def _choose_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):; """"""; Choose array aligned with obs annotation.; """"""; is_layer = layer is not None; is_raw = use_raw is not False; is_obsm = obsm is not None; is_obsp = obsp is not None; choices_made = sum((is_layer, is_raw, is_obsm, is_obsp)); assert choices_made <= 1; if choices_made == 0:; return adata.X; elif is_layer:; return adata.layers[layer]; elif use_raw:; return adata.raw.X; elif is_obsm:; return adata.obsm[obsm]; elif is_obsp:; return adata.obsp[obsp]; else:; assert False, (; ""That was unexpected. Please report this bug at:\n\n\t""; "" https://github.com/theislab/scanpy/issues""; ); ```. This could use support for variable masks like `use_highly_variable`. Also the error message should be better. I think a collection of helper functions like this should go in to a utils module (`sc.utils.argutils`?) which could be public so it's easier to use in `scanpy`-like packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919
https://github.com/scverse/scanpy/issues/828#issuecomment-560072919:6,Deployability,update,update,6,"As an update, I've been using this helper function to consistently handle this:. ```python. def _choose_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):; """"""; Choose array aligned with obs annotation.; """"""; is_layer = layer is not None; is_raw = use_raw is not False; is_obsm = obsm is not None; is_obsp = obsp is not None; choices_made = sum((is_layer, is_raw, is_obsm, is_obsp)); assert choices_made <= 1; if choices_made == 0:; return adata.X; elif is_layer:; return adata.layers[layer]; elif use_raw:; return adata.raw.X; elif is_obsm:; return adata.obsm[obsm]; elif is_obsp:; return adata.obsp[obsp]; else:; assert False, (; ""That was unexpected. Please report this bug at:\n\n\t""; "" https://github.com/theislab/scanpy/issues""; ); ```. This could use support for variable masks like `use_highly_variable`. Also the error message should be better. I think a collection of helper functions like this should go in to a utils module (`sc.utils.argutils`?) which could be public so it's easier to use in `scanpy`-like packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919
https://github.com/scverse/scanpy/issues/828#issuecomment-560072919:845,Integrability,message,message,845,"As an update, I've been using this helper function to consistently handle this:. ```python. def _choose_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):; """"""; Choose array aligned with obs annotation.; """"""; is_layer = layer is not None; is_raw = use_raw is not False; is_obsm = obsm is not None; is_obsp = obsp is not None; choices_made = sum((is_layer, is_raw, is_obsm, is_obsp)); assert choices_made <= 1; if choices_made == 0:; return adata.X; elif is_layer:; return adata.layers[layer]; elif use_raw:; return adata.raw.X; elif is_obsm:; return adata.obsm[obsm]; elif is_obsp:; return adata.obsp[obsp]; else:; assert False, (; ""That was unexpected. Please report this bug at:\n\n\t""; "" https://github.com/theislab/scanpy/issues""; ); ```. This could use support for variable masks like `use_highly_variable`. Also the error message should be better. I think a collection of helper functions like this should go in to a utils module (`sc.utils.argutils`?) which could be public so it's easier to use in `scanpy`-like packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919
https://github.com/scverse/scanpy/issues/828#issuecomment-560072919:495,Modifiability,layers,layers,495,"As an update, I've been using this helper function to consistently handle this:. ```python. def _choose_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):; """"""; Choose array aligned with obs annotation.; """"""; is_layer = layer is not None; is_raw = use_raw is not False; is_obsm = obsm is not None; is_obsp = obsp is not None; choices_made = sum((is_layer, is_raw, is_obsm, is_obsp)); assert choices_made <= 1; if choices_made == 0:; return adata.X; elif is_layer:; return adata.layers[layer]; elif use_raw:; return adata.raw.X; elif is_obsm:; return adata.obsm[obsm]; elif is_obsp:; return adata.obsp[obsp]; else:; assert False, (; ""That was unexpected. Please report this bug at:\n\n\t""; "" https://github.com/theislab/scanpy/issues""; ); ```. This could use support for variable masks like `use_highly_variable`. Also the error message should be better. I think a collection of helper functions like this should go in to a utils module (`sc.utils.argutils`?) which could be public so it's easier to use in `scanpy`-like packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919
https://github.com/scverse/scanpy/issues/828#issuecomment-560072919:787,Modifiability,variab,variable,787,"As an update, I've been using this helper function to consistently handle this:. ```python. def _choose_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):; """"""; Choose array aligned with obs annotation.; """"""; is_layer = layer is not None; is_raw = use_raw is not False; is_obsm = obsm is not None; is_obsp = obsp is not None; choices_made = sum((is_layer, is_raw, is_obsm, is_obsp)); assert choices_made <= 1; if choices_made == 0:; return adata.X; elif is_layer:; return adata.layers[layer]; elif use_raw:; return adata.raw.X; elif is_obsm:; return adata.obsm[obsm]; elif is_obsp:; return adata.obsp[obsp]; else:; assert False, (; ""That was unexpected. Please report this bug at:\n\n\t""; "" https://github.com/theislab/scanpy/issues""; ); ```. This could use support for variable masks like `use_highly_variable`. Also the error message should be better. I think a collection of helper functions like this should go in to a utils module (`sc.utils.argutils`?) which could be public so it's easier to use in `scanpy`-like packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919
https://github.com/scverse/scanpy/issues/828#issuecomment-560072919:401,Testability,assert,assert,401,"As an update, I've been using this helper function to consistently handle this:. ```python. def _choose_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):; """"""; Choose array aligned with obs annotation.; """"""; is_layer = layer is not None; is_raw = use_raw is not False; is_obsm = obsm is not None; is_obsp = obsp is not None; choices_made = sum((is_layer, is_raw, is_obsm, is_obsp)); assert choices_made <= 1; if choices_made == 0:; return adata.X; elif is_layer:; return adata.layers[layer]; elif use_raw:; return adata.raw.X; elif is_obsm:; return adata.obsm[obsm]; elif is_obsp:; return adata.obsp[obsp]; else:; assert False, (; ""That was unexpected. Please report this bug at:\n\n\t""; "" https://github.com/theislab/scanpy/issues""; ); ```. This could use support for variable masks like `use_highly_variable`. Also the error message should be better. I think a collection of helper functions like this should go in to a utils module (`sc.utils.argutils`?) which could be public so it's easier to use in `scanpy`-like packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919
https://github.com/scverse/scanpy/issues/828#issuecomment-560072919:632,Testability,assert,assert,632,"As an update, I've been using this helper function to consistently handle this:. ```python. def _choose_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):; """"""; Choose array aligned with obs annotation.; """"""; is_layer = layer is not None; is_raw = use_raw is not False; is_obsm = obsm is not None; is_obsp = obsp is not None; choices_made = sum((is_layer, is_raw, is_obsm, is_obsp)); assert choices_made <= 1; if choices_made == 0:; return adata.X; elif is_layer:; return adata.layers[layer]; elif use_raw:; return adata.raw.X; elif is_obsm:; return adata.obsm[obsm]; elif is_obsp:; return adata.obsp[obsp]; else:; assert False, (; ""That was unexpected. Please report this bug at:\n\n\t""; "" https://github.com/theislab/scanpy/issues""; ); ```. This could use support for variable masks like `use_highly_variable`. Also the error message should be better. I think a collection of helper functions like this should go in to a utils module (`sc.utils.argutils`?) which could be public so it's easier to use in `scanpy`-like packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919
https://github.com/scverse/scanpy/pull/830#issuecomment-534438279:71,Availability,error,errors,71,"I think this looks good and simple enough. Could you please fix the CI errors?. Also there’s 3 added optional deps: cuml, cudf, and cugraph. I assume they’re all different CUDA packages. Could you add them into an `extra` in setup.py?. The RAPIDS umap branch doesn’t use a metric argument. Does it support metrics other than euclidean?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830#issuecomment-534438279
https://github.com/scverse/scanpy/pull/830#issuecomment-534438279:28,Usability,simpl,simple,28,"I think this looks good and simple enough. Could you please fix the CI errors?. Also there’s 3 added optional deps: cuml, cudf, and cugraph. I assume they’re all different CUDA packages. Could you add them into an `extra` in setup.py?. The RAPIDS umap branch doesn’t use a metric argument. Does it support metrics other than euclidean?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830#issuecomment-534438279
https://github.com/scverse/scanpy/pull/830#issuecomment-534981775:58,Availability,error,errors,58,@flying-sheep thanks for taking a look. I've fixed the CI errors now and added the deps as suggested.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830#issuecomment-534981775
https://github.com/scverse/scanpy/pull/830#issuecomment-535090728:116,Testability,test,test,116,"The packages do need a GPU, unfortunately. There is no way to fallback to run on a CPU, so I don't think Travis can test them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830#issuecomment-535090728
https://github.com/scverse/scanpy/pull/831#issuecomment-531750992:61,Testability,benchmark,benchmark,61,"Thanks, that option is very useful and works perfectly. mini-benchmark on my data (compared to non-compressed): ; gzip: 35% memory, 3x runtime; lzf: 66% memory, 2x runtime",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/831#issuecomment-531750992
https://github.com/scverse/scanpy/issues/832#issuecomment-530529513:198,Availability,down,downgrade,198,"Hi Dylan,. This is an issue with the new h5py package, which @ivirshup already fixed on master (https://github.com/theislab/scanpy/commit/928d475a8e2d2901c5744c3afc75e2d5a1b65f29). For now, you can downgrade your h5py package to 2.9.0 using `pip install h5py==2.9.0` as a workaround.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-530529513
https://github.com/scverse/scanpy/issues/832#issuecomment-530529513:246,Deployability,install,install,246,"Hi Dylan,. This is an issue with the new h5py package, which @ivirshup already fixed on master (https://github.com/theislab/scanpy/commit/928d475a8e2d2901c5744c3afc75e2d5a1b65f29). For now, you can downgrade your h5py package to 2.9.0 using `pip install h5py==2.9.0` as a workaround.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-530529513
https://github.com/scverse/scanpy/issues/832#issuecomment-530530037:81,Deployability,pipeline,pipeline,81,@gokceneraslan - thanks for the fast response. This broke our (cellxgene) travis pipeline as well. Do you have any info on eta for a fix/workaround other than pinning the module version? TY!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-530530037
https://github.com/scverse/scanpy/issues/832#issuecomment-530659556:300,Deployability,pipeline,pipeline,300,"Just a heads up, there is a remaining issue on anndata master where reading older files with h5py 2.10.0 results in bytestring indexes. > On Sep 12, 2019, at 05:28, Bruce Martin <notifications@github.com> wrote:; > ; > @gokceneraslan - thanks for the fast response. This broke our (cellxgene) travis pipeline as well. Do you have any info on eta for a fix/workaround other than pinning the module version? TY!; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-530659556
https://github.com/scverse/scanpy/issues/832#issuecomment-544968526:20,Availability,error,error,20,"I'm having the same error with `h5py==2.9.0`. Cellxgene doesn't seem to be working with the object that I created the object with scanpy `1.4.3+116.g0075c62`. I can however load it again with that version. But when I downgrade to 1.3.7 (recommendation from @mbuttner who had the same cellxgene issue) I can no longer load the object and get the above error. Back in the 1.4.3 dev version scanpy it no longer writes the object after loading, and gives me the following error:; ```; In [23]: adata.write(""cellxgene.h5ad"") ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-33b15d710f71> in <module>; ----> 1 adata.write(""cellxgene.h5ad""). ~/new_anndata/anndata/anndata/core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 2222 compression=compression,; 2223 compression_opts=compression_opts,; -> 2224 force_dense=force_dense,; 2225 ); 2226 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_h5ad(filepath, adata, force_dense, dataset_kwargs, **kwargs); 90 write_attribute(f, ""varp"", adata.varp, dataset_kwargs); 91 write_attribute(f, ""layers"", adata.layers, dataset_kwargs); ---> 92 write_attribute(f, ""uns"", adata.uns, dataset_kwargs); 93 write_attribute(f, ""raw"", adata.raw, dataset_kwargs); 94 if adata.isbacked:. ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 204 for sub_key, sub_value in value.items():; --> 205 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs); 206 ; 207 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526
https://github.com/scverse/scanpy/issues/832#issuecomment-544968526:217,Availability,down,downgrade,217,"I'm having the same error with `h5py==2.9.0`. Cellxgene doesn't seem to be working with the object that I created the object with scanpy `1.4.3+116.g0075c62`. I can however load it again with that version. But when I downgrade to 1.3.7 (recommendation from @mbuttner who had the same cellxgene issue) I can no longer load the object and get the above error. Back in the 1.4.3 dev version scanpy it no longer writes the object after loading, and gives me the following error:; ```; In [23]: adata.write(""cellxgene.h5ad"") ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-33b15d710f71> in <module>; ----> 1 adata.write(""cellxgene.h5ad""). ~/new_anndata/anndata/anndata/core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 2222 compression=compression,; 2223 compression_opts=compression_opts,; -> 2224 force_dense=force_dense,; 2225 ); 2226 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_h5ad(filepath, adata, force_dense, dataset_kwargs, **kwargs); 90 write_attribute(f, ""varp"", adata.varp, dataset_kwargs); 91 write_attribute(f, ""layers"", adata.layers, dataset_kwargs); ---> 92 write_attribute(f, ""uns"", adata.uns, dataset_kwargs); 93 write_attribute(f, ""raw"", adata.raw, dataset_kwargs); 94 if adata.isbacked:. ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 204 for sub_key, sub_value in value.items():; --> 205 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs); 206 ; 207 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526
https://github.com/scverse/scanpy/issues/832#issuecomment-544968526:351,Availability,error,error,351,"I'm having the same error with `h5py==2.9.0`. Cellxgene doesn't seem to be working with the object that I created the object with scanpy `1.4.3+116.g0075c62`. I can however load it again with that version. But when I downgrade to 1.3.7 (recommendation from @mbuttner who had the same cellxgene issue) I can no longer load the object and get the above error. Back in the 1.4.3 dev version scanpy it no longer writes the object after loading, and gives me the following error:; ```; In [23]: adata.write(""cellxgene.h5ad"") ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-33b15d710f71> in <module>; ----> 1 adata.write(""cellxgene.h5ad""). ~/new_anndata/anndata/anndata/core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 2222 compression=compression,; 2223 compression_opts=compression_opts,; -> 2224 force_dense=force_dense,; 2225 ); 2226 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_h5ad(filepath, adata, force_dense, dataset_kwargs, **kwargs); 90 write_attribute(f, ""varp"", adata.varp, dataset_kwargs); 91 write_attribute(f, ""layers"", adata.layers, dataset_kwargs); ---> 92 write_attribute(f, ""uns"", adata.uns, dataset_kwargs); 93 write_attribute(f, ""raw"", adata.raw, dataset_kwargs); 94 if adata.isbacked:. ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 204 for sub_key, sub_value in value.items():; --> 205 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs); 206 ; 207 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526
https://github.com/scverse/scanpy/issues/832#issuecomment-544968526:468,Availability,error,error,468,"I'm having the same error with `h5py==2.9.0`. Cellxgene doesn't seem to be working with the object that I created the object with scanpy `1.4.3+116.g0075c62`. I can however load it again with that version. But when I downgrade to 1.3.7 (recommendation from @mbuttner who had the same cellxgene issue) I can no longer load the object and get the above error. Back in the 1.4.3 dev version scanpy it no longer writes the object after loading, and gives me the following error:; ```; In [23]: adata.write(""cellxgene.h5ad"") ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-33b15d710f71> in <module>; ----> 1 adata.write(""cellxgene.h5ad""). ~/new_anndata/anndata/anndata/core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 2222 compression=compression,; 2223 compression_opts=compression_opts,; -> 2224 force_dense=force_dense,; 2225 ); 2226 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_h5ad(filepath, adata, force_dense, dataset_kwargs, **kwargs); 90 write_attribute(f, ""varp"", adata.varp, dataset_kwargs); 91 write_attribute(f, ""layers"", adata.layers, dataset_kwargs); ---> 92 write_attribute(f, ""uns"", adata.uns, dataset_kwargs); 93 write_attribute(f, ""raw"", adata.raw, dataset_kwargs); 94 if adata.isbacked:. ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 204 for sub_key, sub_value in value.items():; --> 205 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs); 206 ; 207 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526
https://github.com/scverse/scanpy/issues/832#issuecomment-544968526:1171,Modifiability,layers,layers,1171,"2`. I can however load it again with that version. But when I downgrade to 1.3.7 (recommendation from @mbuttner who had the same cellxgene issue) I can no longer load the object and get the above error. Back in the 1.4.3 dev version scanpy it no longer writes the object after loading, and gives me the following error:; ```; In [23]: adata.write(""cellxgene.h5ad"") ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-33b15d710f71> in <module>; ----> 1 adata.write(""cellxgene.h5ad""). ~/new_anndata/anndata/anndata/core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 2222 compression=compression,; 2223 compression_opts=compression_opts,; -> 2224 force_dense=force_dense,; 2225 ); 2226 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_h5ad(filepath, adata, force_dense, dataset_kwargs, **kwargs); 90 write_attribute(f, ""varp"", adata.varp, dataset_kwargs); 91 write_attribute(f, ""layers"", adata.layers, dataset_kwargs); ---> 92 write_attribute(f, ""uns"", adata.uns, dataset_kwargs); 93 write_attribute(f, ""raw"", adata.raw, dataset_kwargs); 94 if adata.isbacked:. ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 204 for sub_key, sub_value in value.items():; --> 205 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs); 206 ; 207 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526
https://github.com/scverse/scanpy/issues/832#issuecomment-544968526:1186,Modifiability,layers,layers,1186,"owngrade to 1.3.7 (recommendation from @mbuttner who had the same cellxgene issue) I can no longer load the object and get the above error. Back in the 1.4.3 dev version scanpy it no longer writes the object after loading, and gives me the following error:; ```; In [23]: adata.write(""cellxgene.h5ad"") ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-33b15d710f71> in <module>; ----> 1 adata.write(""cellxgene.h5ad""). ~/new_anndata/anndata/anndata/core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 2222 compression=compression,; 2223 compression_opts=compression_opts,; -> 2224 force_dense=force_dense,; 2225 ); 2226 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_h5ad(filepath, adata, force_dense, dataset_kwargs, **kwargs); 90 write_attribute(f, ""varp"", adata.varp, dataset_kwargs); 91 write_attribute(f, ""layers"", adata.layers, dataset_kwargs); ---> 92 write_attribute(f, ""uns"", adata.uns, dataset_kwargs); 93 write_attribute(f, ""raw"", adata.raw, dataset_kwargs); 94 if adata.isbacked:. ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 204 for sub_key, sub_value in value.items():; --> 205 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs); 206 ; 207 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, da",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526
https://github.com/scverse/scanpy/issues/832#issuecomment-544968526:173,Performance,load,load,173,"I'm having the same error with `h5py==2.9.0`. Cellxgene doesn't seem to be working with the object that I created the object with scanpy `1.4.3+116.g0075c62`. I can however load it again with that version. But when I downgrade to 1.3.7 (recommendation from @mbuttner who had the same cellxgene issue) I can no longer load the object and get the above error. Back in the 1.4.3 dev version scanpy it no longer writes the object after loading, and gives me the following error:; ```; In [23]: adata.write(""cellxgene.h5ad"") ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-33b15d710f71> in <module>; ----> 1 adata.write(""cellxgene.h5ad""). ~/new_anndata/anndata/anndata/core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 2222 compression=compression,; 2223 compression_opts=compression_opts,; -> 2224 force_dense=force_dense,; 2225 ); 2226 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_h5ad(filepath, adata, force_dense, dataset_kwargs, **kwargs); 90 write_attribute(f, ""varp"", adata.varp, dataset_kwargs); 91 write_attribute(f, ""layers"", adata.layers, dataset_kwargs); ---> 92 write_attribute(f, ""uns"", adata.uns, dataset_kwargs); 93 write_attribute(f, ""raw"", adata.raw, dataset_kwargs); 94 if adata.isbacked:. ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 204 for sub_key, sub_value in value.items():; --> 205 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs); 206 ; 207 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526
https://github.com/scverse/scanpy/issues/832#issuecomment-544968526:317,Performance,load,load,317,"I'm having the same error with `h5py==2.9.0`. Cellxgene doesn't seem to be working with the object that I created the object with scanpy `1.4.3+116.g0075c62`. I can however load it again with that version. But when I downgrade to 1.3.7 (recommendation from @mbuttner who had the same cellxgene issue) I can no longer load the object and get the above error. Back in the 1.4.3 dev version scanpy it no longer writes the object after loading, and gives me the following error:; ```; In [23]: adata.write(""cellxgene.h5ad"") ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-33b15d710f71> in <module>; ----> 1 adata.write(""cellxgene.h5ad""). ~/new_anndata/anndata/anndata/core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 2222 compression=compression,; 2223 compression_opts=compression_opts,; -> 2224 force_dense=force_dense,; 2225 ); 2226 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_h5ad(filepath, adata, force_dense, dataset_kwargs, **kwargs); 90 write_attribute(f, ""varp"", adata.varp, dataset_kwargs); 91 write_attribute(f, ""layers"", adata.layers, dataset_kwargs); ---> 92 write_attribute(f, ""uns"", adata.uns, dataset_kwargs); 93 write_attribute(f, ""raw"", adata.raw, dataset_kwargs); 94 if adata.isbacked:. ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 204 for sub_key, sub_value in value.items():; --> 205 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs); 206 ; 207 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526
https://github.com/scverse/scanpy/issues/832#issuecomment-544968526:432,Performance,load,loading,432,"I'm having the same error with `h5py==2.9.0`. Cellxgene doesn't seem to be working with the object that I created the object with scanpy `1.4.3+116.g0075c62`. I can however load it again with that version. But when I downgrade to 1.3.7 (recommendation from @mbuttner who had the same cellxgene issue) I can no longer load the object and get the above error. Back in the 1.4.3 dev version scanpy it no longer writes the object after loading, and gives me the following error:; ```; In [23]: adata.write(""cellxgene.h5ad"") ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-33b15d710f71> in <module>; ----> 1 adata.write(""cellxgene.h5ad""). ~/new_anndata/anndata/anndata/core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 2222 compression=compression,; 2223 compression_opts=compression_opts,; -> 2224 force_dense=force_dense,; 2225 ); 2226 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_h5ad(filepath, adata, force_dense, dataset_kwargs, **kwargs); 90 write_attribute(f, ""varp"", adata.varp, dataset_kwargs); 91 write_attribute(f, ""layers"", adata.layers, dataset_kwargs); ---> 92 write_attribute(f, ""uns"", adata.uns, dataset_kwargs); 93 write_attribute(f, ""raw"", adata.raw, dataset_kwargs); 94 if adata.isbacked:. ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 204 for sub_key, sub_value in value.items():; --> 205 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs); 206 ; 207 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526
https://github.com/scverse/scanpy/issues/832#issuecomment-544968526:3771,Testability,log,logical,3771," value.items():; --> 205 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs); 206 ; 207 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_array(f, key, value, dataset_kwargs); 152 elif value.dtype.names is not None:; 153 value = _to_hdf5_vlen_strings(value); --> 154 f.create_dataset(key, data=value, **dataset_kwargs); 155 ; 156 . ~/new_anndata/anndata/anndata/h5py/h5sparse.py in create_dataset(self, name, data, chunk_size, **kwargs); 151 if not isinstance(data, SparseDataset) and not ss.issparse(data):; 152 return self.h5py_group.create_dataset(; --> 153 name=name, data=data, **kwargs; 154 ); 155 if self.force_dense:. ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/h5py/_hl/group.py in create_dataset(self, name, shape, dtype, data, **kwds); 134 ; 135 with phil:; --> 136 dsid = dataset.make_new_dset(self, shape, dtype, data, **kwds); 137 dset = dataset.Dataset(dsid); 138 if name is not None:. ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/h5py/_hl/dataset.py in make_new_dset(parent, shape, dtype, data, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl); 116 else:; 117 dtype = numpy.dtype(dtype); --> 118 tid = h5t.py_create(dtype, logical=1); 119 ; 120 # Legacy. h5py/h5t.pyx in h5py.h5t.py_create(). h5py/h5t.pyx in h5py.h5t.py_create(). h5py/h5t.pyx in h5py.h5t.py_create(). h5py/h5t.pyx in h5py.h5t._c_compound(). h5py/h5t.pyx in h5py.h5t.py_create(). h5py/h5t.pyx in h5py.h5t.py_create(). TypeError: Object dtype dtype('O') has no native HDF5 equivalent; ```. Everything however seems to work fine when I throw out the rank_genes_groups results from `adata.uns`. Edit: actually cellxgene still isn't working, but I could at least save again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526
https://github.com/scverse/scanpy/issues/832#issuecomment-545345438:359,Modifiability,variab,variable,359,"@LuckyMD . I can replicate that with:. ```python; import scanpy as sc; pbmc = sc.datasets.pbmc68k_reduced(); pbmc.write(""tmp.h5ad""); fromdisk = sc.read(""tmp.h5ad"") # Do we read okay; fromdisk.write(pbmc) # Can we round trip; ```. Some context around this, and my current thinking on a solution:. * h5py doesn't do fixed length unicode strings; * h5py does do variable length unicode strings, pretty much anywhere; * zarr doesn't do variable length strings in structured arrays; * We probably don't actually want to use fixed length unicode strings much. Bytestrings, more likely.; * We can probably just add another element type to allow special handling for these. I think it'd be fine to not do `np.str_` type arrays.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-545345438
https://github.com/scverse/scanpy/issues/832#issuecomment-545345438:432,Modifiability,variab,variable,432,"@LuckyMD . I can replicate that with:. ```python; import scanpy as sc; pbmc = sc.datasets.pbmc68k_reduced(); pbmc.write(""tmp.h5ad""); fromdisk = sc.read(""tmp.h5ad"") # Do we read okay; fromdisk.write(pbmc) # Can we round trip; ```. Some context around this, and my current thinking on a solution:. * h5py doesn't do fixed length unicode strings; * h5py does do variable length unicode strings, pretty much anywhere; * zarr doesn't do variable length strings in structured arrays; * We probably don't actually want to use fixed length unicode strings much. Bytestrings, more likely.; * We can probably just add another element type to allow special handling for these. I think it'd be fine to not do `np.str_` type arrays.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-545345438
https://github.com/scverse/scanpy/issues/832#issuecomment-545906897:33,Modifiability,variab,variable-length,33,"It is, too bad numpy has no good variable-length string array type. When would bytes make sense? Bytes just mean “data, but I don’t know its structure or am about to write it to disk”",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-545906897
https://github.com/scverse/scanpy/issues/833#issuecomment-531440165:55,Availability,error,error,55,"I think I found a way around it. The issue here is the error is thrown when the new louvain groups are created by the adata.obs['louvain_colors'] are not updated until the plotting sc.pl function is run. Therefore, when you try to slice anything, it throws out an index out of bounds error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833#issuecomment-531440165
https://github.com/scverse/scanpy/issues/833#issuecomment-531440165:284,Availability,error,error,284,"I think I found a way around it. The issue here is the error is thrown when the new louvain groups are created by the adata.obs['louvain_colors'] are not updated until the plotting sc.pl function is run. Therefore, when you try to slice anything, it throws out an index out of bounds error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833#issuecomment-531440165
https://github.com/scverse/scanpy/issues/833#issuecomment-531440165:154,Deployability,update,updated,154,"I think I found a way around it. The issue here is the error is thrown when the new louvain groups are created by the adata.obs['louvain_colors'] are not updated until the plotting sc.pl function is run. Therefore, when you try to slice anything, it throws out an index out of bounds error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833#issuecomment-531440165
https://github.com/scverse/scanpy/issues/833#issuecomment-531482480:138,Availability,error,error,138,Thank you! If you add a few more details we can fix this quickly: Which call will update the groups but not the color and which call will error out with which stack trace? Please add the the traceback to your comment this:. ````md; ```python; sc.tl.something(adata); ```. ```pytb; XError Traceback (most recent call last); ....; XError: some message.; ```; ````,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833#issuecomment-531482480
https://github.com/scverse/scanpy/issues/833#issuecomment-531482480:82,Deployability,update,update,82,Thank you! If you add a few more details we can fix this quickly: Which call will update the groups but not the color and which call will error out with which stack trace? Please add the the traceback to your comment this:. ````md; ```python; sc.tl.something(adata); ```. ```pytb; XError Traceback (most recent call last); ....; XError: some message.; ```; ````,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833#issuecomment-531482480
https://github.com/scverse/scanpy/issues/833#issuecomment-531482480:342,Integrability,message,message,342,Thank you! If you add a few more details we can fix this quickly: Which call will update the groups but not the color and which call will error out with which stack trace? Please add the the traceback to your comment this:. ````md; ```python; sc.tl.something(adata); ```. ```pytb; XError Traceback (most recent call last); ....; XError: some message.; ```; ````,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833#issuecomment-531482480
https://github.com/scverse/scanpy/issues/836#issuecomment-539649582:18,Performance,load,loadings,18,"Scanpy stores the loadings for each PC in the `adata.varm['PCs']` slot. The order is the same is `obs_names`, but you can use pandas functions like `sort_values` to look at the top genes or do something like `np.argsort` or `scipy.stats.rankdata` on the columns (the PCs) to get their ranks. ```python; import scanpy as sc; import numpy as np. pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.pca(pbmc, svd_solver='arpack', random_state=0); # Get loadings for each gene for each PC; df_loadings = pd.DataFrame(pbmc.varm['PCs'], index=pbmc.var_names); # get rank of each loading for each PC; df_rankings = pd.DataFrame((-1 * df_loadings.values).argsort(0).argsort(0), index=df_loadings.index, columns=df_loadings.columns); # c.f. with df_loadings.apply(scipy.stats.rankdata, axis=0); # evaluate ; print(""Top loadings for PC1...""); print(df_loadings[0].sort_values().tail()); print(""Rank of PTPRCAP for first 5 PCs...""); print(df_rankings.loc[""PTPRCAP""].head()); sc.pl.pca_loadings(pbmc). # alternatively, you can do SVD or PCA manually with scipy, numpy, sklearn, etc.; # from sklearn.decomposition import PCA; # pc = PCA(n_components=50, svd_solver='arpack', random_state=0).fit(pbmc.X); # pc.components_.T has the loadings; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/836#issuecomment-539649582
https://github.com/scverse/scanpy/issues/836#issuecomment-539649582:442,Performance,load,loadings,442,"Scanpy stores the loadings for each PC in the `adata.varm['PCs']` slot. The order is the same is `obs_names`, but you can use pandas functions like `sort_values` to look at the top genes or do something like `np.argsort` or `scipy.stats.rankdata` on the columns (the PCs) to get their ranks. ```python; import scanpy as sc; import numpy as np. pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.pca(pbmc, svd_solver='arpack', random_state=0); # Get loadings for each gene for each PC; df_loadings = pd.DataFrame(pbmc.varm['PCs'], index=pbmc.var_names); # get rank of each loading for each PC; df_rankings = pd.DataFrame((-1 * df_loadings.values).argsort(0).argsort(0), index=df_loadings.index, columns=df_loadings.columns); # c.f. with df_loadings.apply(scipy.stats.rankdata, axis=0); # evaluate ; print(""Top loadings for PC1...""); print(df_loadings[0].sort_values().tail()); print(""Rank of PTPRCAP for first 5 PCs...""); print(df_rankings.loc[""PTPRCAP""].head()); sc.pl.pca_loadings(pbmc). # alternatively, you can do SVD or PCA manually with scipy, numpy, sklearn, etc.; # from sklearn.decomposition import PCA; # pc = PCA(n_components=50, svd_solver='arpack', random_state=0).fit(pbmc.X); # pc.components_.T has the loadings; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/836#issuecomment-539649582
https://github.com/scverse/scanpy/issues/836#issuecomment-539649582:565,Performance,load,loading,565,"Scanpy stores the loadings for each PC in the `adata.varm['PCs']` slot. The order is the same is `obs_names`, but you can use pandas functions like `sort_values` to look at the top genes or do something like `np.argsort` or `scipy.stats.rankdata` on the columns (the PCs) to get their ranks. ```python; import scanpy as sc; import numpy as np. pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.pca(pbmc, svd_solver='arpack', random_state=0); # Get loadings for each gene for each PC; df_loadings = pd.DataFrame(pbmc.varm['PCs'], index=pbmc.var_names); # get rank of each loading for each PC; df_rankings = pd.DataFrame((-1 * df_loadings.values).argsort(0).argsort(0), index=df_loadings.index, columns=df_loadings.columns); # c.f. with df_loadings.apply(scipy.stats.rankdata, axis=0); # evaluate ; print(""Top loadings for PC1...""); print(df_loadings[0].sort_values().tail()); print(""Rank of PTPRCAP for first 5 PCs...""); print(df_rankings.loc[""PTPRCAP""].head()); sc.pl.pca_loadings(pbmc). # alternatively, you can do SVD or PCA manually with scipy, numpy, sklearn, etc.; # from sklearn.decomposition import PCA; # pc = PCA(n_components=50, svd_solver='arpack', random_state=0).fit(pbmc.X); # pc.components_.T has the loadings; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/836#issuecomment-539649582
https://github.com/scverse/scanpy/issues/836#issuecomment-539649582:802,Performance,load,loadings,802,"Scanpy stores the loadings for each PC in the `adata.varm['PCs']` slot. The order is the same is `obs_names`, but you can use pandas functions like `sort_values` to look at the top genes or do something like `np.argsort` or `scipy.stats.rankdata` on the columns (the PCs) to get their ranks. ```python; import scanpy as sc; import numpy as np. pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.pca(pbmc, svd_solver='arpack', random_state=0); # Get loadings for each gene for each PC; df_loadings = pd.DataFrame(pbmc.varm['PCs'], index=pbmc.var_names); # get rank of each loading for each PC; df_rankings = pd.DataFrame((-1 * df_loadings.values).argsort(0).argsort(0), index=df_loadings.index, columns=df_loadings.columns); # c.f. with df_loadings.apply(scipy.stats.rankdata, axis=0); # evaluate ; print(""Top loadings for PC1...""); print(df_loadings[0].sort_values().tail()); print(""Rank of PTPRCAP for first 5 PCs...""); print(df_rankings.loc[""PTPRCAP""].head()); sc.pl.pca_loadings(pbmc). # alternatively, you can do SVD or PCA manually with scipy, numpy, sklearn, etc.; # from sklearn.decomposition import PCA; # pc = PCA(n_components=50, svd_solver='arpack', random_state=0).fit(pbmc.X); # pc.components_.T has the loadings; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/836#issuecomment-539649582
https://github.com/scverse/scanpy/issues/836#issuecomment-539649582:1210,Performance,load,loadings,1210,"Scanpy stores the loadings for each PC in the `adata.varm['PCs']` slot. The order is the same is `obs_names`, but you can use pandas functions like `sort_values` to look at the top genes or do something like `np.argsort` or `scipy.stats.rankdata` on the columns (the PCs) to get their ranks. ```python; import scanpy as sc; import numpy as np. pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.pca(pbmc, svd_solver='arpack', random_state=0); # Get loadings for each gene for each PC; df_loadings = pd.DataFrame(pbmc.varm['PCs'], index=pbmc.var_names); # get rank of each loading for each PC; df_rankings = pd.DataFrame((-1 * df_loadings.values).argsort(0).argsort(0), index=df_loadings.index, columns=df_loadings.columns); # c.f. with df_loadings.apply(scipy.stats.rankdata, axis=0); # evaluate ; print(""Top loadings for PC1...""); print(df_loadings[0].sort_values().tail()); print(""Rank of PTPRCAP for first 5 PCs...""); print(df_rankings.loc[""PTPRCAP""].head()); sc.pl.pca_loadings(pbmc). # alternatively, you can do SVD or PCA manually with scipy, numpy, sklearn, etc.; # from sklearn.decomposition import PCA; # pc = PCA(n_components=50, svd_solver='arpack', random_state=0).fit(pbmc.X); # pc.components_.T has the loadings; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/836#issuecomment-539649582
https://github.com/scverse/scanpy/issues/838#issuecomment-532502522:199,Availability,error,error,199,"@ivirshup ; Hi, thank for your help.; When I ran this code (sc.tl.rank_genes_groups(adata, ""comparison"", reference=""B"", n_genes=adata.shape[1]); the previous three code worked perfectly), I got some error. I don't know whether they came from the scanpy module or something else. ```python; >>> import scanpy as sc; >>> sc.tl.rank_genes_groups(adata, ""comparison"", reference=""B"", n_genes=adata.shape[1]); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/scanpy/tools/_rank_genes_groups.py"", line 120, in rank_genes_groups; groups_order += [reference]; TypeError: must be str, not list. >>> import scanpy.api as sc; >>> sc.tl.rank_genes_groups(adata, ""comparison"", reference=""B"", n_genes=adata.shape[1]); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/scanpy/tools/_rank_genes_groups.py"", line 120, in rank_genes_groups; groups_order += [reference]; TypeError: must be str, not list. >>> import scanpy.external as sc; >>> sc.tl.rank_genes_groups(adata, ""comparison"", reference=""B"", n_genes=adata.shape[1]); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; AttributeError: module 'scanpy.external.tl' has no attribute 'rank_genes_groups'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532502522
https://github.com/scverse/scanpy/issues/838#issuecomment-532633720:263,Deployability,release,release,263,"Hi! You shouldn’t need to try here: Read the documentation at https://scanpy.rtfd.io to figure out if what you want is in `scanpy` or `scanpy.external`. `scanpy.api` is deprecated, you should never use it. The code @ivirshup gave you will work in the next scanpy release, as scanpy 1.4.4 still has the bug #346. Please upgrade to scanpy from git master and try again. You can do that via:. ```bash; pip install git+https://github.com/theislab/scanpy.git; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532633720
https://github.com/scverse/scanpy/issues/838#issuecomment-532633720:319,Deployability,upgrade,upgrade,319,"Hi! You shouldn’t need to try here: Read the documentation at https://scanpy.rtfd.io to figure out if what you want is in `scanpy` or `scanpy.external`. `scanpy.api` is deprecated, you should never use it. The code @ivirshup gave you will work in the next scanpy release, as scanpy 1.4.4 still has the bug #346. Please upgrade to scanpy from git master and try again. You can do that via:. ```bash; pip install git+https://github.com/theislab/scanpy.git; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532633720
https://github.com/scverse/scanpy/issues/838#issuecomment-532633720:403,Deployability,install,install,403,"Hi! You shouldn’t need to try here: Read the documentation at https://scanpy.rtfd.io to figure out if what you want is in `scanpy` or `scanpy.external`. `scanpy.api` is deprecated, you should never use it. The code @ivirshup gave you will work in the next scanpy release, as scanpy 1.4.4 still has the bug #346. Please upgrade to scanpy from git master and try again. You can do that via:. ```bash; pip install git+https://github.com/theislab/scanpy.git; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532633720
https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:80,Availability,error,errors,80,"@flying-sheep Hi, I tried to install the new version of scanpy, but encountered errors. first, I tried your code ; ```; pip install git+https://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027
https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:481,Availability,error,error,481,"@flying-sheep Hi, I tried to install the new version of scanpy, but encountered errors. first, I tried your code ; ```; pip install git+https://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027
https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:834,Availability,down,download,834,"@flying-sheep Hi, I tried to install the new version of scanpy, but encountered errors. first, I tried your code ; ```; pip install git+https://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027
https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:1117,Availability,error,error,1117,"github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressing objects: 100% (109/109), done.; Receiving objects: 3% (577/14992), 156.00 KiB | 3.00 KiB/s; fatal: The remote end hung up unexpectedly MiB | ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027
https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:1168,Availability,Down,Download,1168,"github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressing objects: 100% (109/109), done.; Receiving objects: 3% (577/14992), 156.00 KiB | 3.00 KiB/s; fatal: The remote end hung up unexpectedly MiB | ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027
https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:1464,Availability,Down,Downloading,1464,"slab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressing objects: 100% (109/109), done.; Receiving objects: 3% (577/14992), 156.00 KiB | 3.00 KiB/s; fatal: The remote end hung up unexpectedly MiB | 28.00 KiB/s; fatal: early EOF; fatal: index-pack failed; ```. however, i can successfully install scanpy 1.4.4 with. ```; pip install scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027
https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:1589,Availability,down,downloaded,1589,"slab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressing objects: 100% (109/109), done.; Receiving objects: 3% (577/14992), 156.00 KiB | 3.00 KiB/s; fatal: The remote end hung up unexpectedly MiB | 28.00 KiB/s; fatal: early EOF; fatal: index-pack failed; ```. however, i can successfully install scanpy 1.4.4 with. ```; pip install scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027
https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:29,Deployability,install,install,29,"@flying-sheep Hi, I tried to install the new version of scanpy, but encountered errors. first, I tried your code ; ```; pip install git+https://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027
https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:124,Deployability,install,install,124,"@flying-sheep Hi, I tried to install the new version of scanpy, but encountered errors. first, I tried your code ; ```; pip install git+https://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027
https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:536,Deployability,install,install,536,"@flying-sheep Hi, I tried to install the new version of scanpy, but encountered errors. first, I tried your code ; ```; pip install git+https://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027
https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:1029,Deployability,install,install,1029,"nstall the new version of scanpy, but encountered errors. first, I tried your code ; ```; pip install git+https://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressing objects: 100% (109/109), d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027
https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:1346,Deployability,install,install,1346,"slab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressing objects: 100% (109/109), done.; Receiving objects: 3% (577/14992), 156.00 KiB | 3.00 KiB/s; fatal: The remote end hung up unexpectedly MiB | 28.00 KiB/s; fatal: early EOF; fatal: index-pack failed; ```. however, i can successfully install scanpy 1.4.4 with. ```; pip install scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027
https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:2235,Deployability,install,install,2235,"slab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressing objects: 100% (109/109), done.; Receiving objects: 3% (577/14992), 156.00 KiB | 3.00 KiB/s; fatal: The remote end hung up unexpectedly MiB | 28.00 KiB/s; fatal: early EOF; fatal: index-pack failed; ```. however, i can successfully install scanpy 1.4.4 with. ```; pip install scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027
https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:2271,Deployability,install,install,2271,"slab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressing objects: 100% (109/109), done.; Receiving objects: 3% (577/14992), 156.00 KiB | 3.00 KiB/s; fatal: The remote end hung up unexpectedly MiB | 28.00 KiB/s; fatal: early EOF; fatal: index-pack failed; ```. however, i can successfully install scanpy 1.4.4 with. ```; pip install scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027
https://github.com/scverse/scanpy/issues/838#issuecomment-532985027:1678,Safety,detect,detect,1678,"slab/scanpy.git to /tmp/pip-_z2v8och-build; fatal: Unable to find remote helper for 'https'; Command ""git clone -q https://github.com/theislab/scanpy.git /tmp/pip-_z2v8och-build"" failed with error code 128 in None; ```. second, I tried; ```; pip install git+git://github.com/theislab/scanpy.git ; ```; I got ouput as:; ```; Collecting git+git://github.com/theislab/scanpy.git; Cloning git://github.com/theislab/scanpy.git to /tmp/pip-2jry40l_-build; ```; and there was no more information and I have to stop it with ""ctrl+C"". third, I tried to download the zip and `cd` to that directory and used . ```; python setup.py build; ```. I got ouput as:. ```; importlib_metadata.PackageNotFoundError: scanpy; ```. after this, I tried . ```; pip install -e .; ```. I got ouput as:. ```; Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```. I searched the relative information in GitHub/Scanpy, but still have no solution for my situation. the following was another failed code. ``` ; pip install https://github.com/theislab/scanpy.git; ```. output:. ```; Collecting https://github.com/theislab/scanpy.git; Downloading https://github.com/theislab/scanpy.git; \ 143kB 442kB/s; Cannot unpack file /tmp/pip-chtzh_a9-unpack/scanpy.git (downloaded from /tmp/pip-xolhyav7-build, content-type: text/html; charset=utf-8); cannot detect archive format; Cannot determine archive format of /tmp/pip-xolhyav7-build; ```. and i also tried. ```; git clone --recursive git://github.com/theislab/scanpy.git; ```. output:. ```; Cloning into 'scanpy'...; remote: Enumerating objects: 122, done.; remote: Counting objects: 100% (122/122), done.; remote: Compressing objects: 100% (109/109), done.; Receiving objects: 3% (577/14992), 156.00 KiB | 3.00 KiB/s; fatal: The remote end hung up unexpectedly MiB | 28.00 KiB/s; fatal: early EOF; fatal: index-pack failed; ```. however, i can successfully install scanpy 1.4.4 with. ```; pip install scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-532985027
https://github.com/scverse/scanpy/issues/838#issuecomment-533014138:460,Availability,down,downloadable,460,"> pip install git+git://github.com/theislab/scanpy.git . should have worked, there seems to be a problem with your git installation or internet. > importlib_metadata.PackageNotFoundError: scanpy. That’s my mistake, seems like a broke installing from .zips (not that anyone tried so far, installing from git is more convenient). > pip install https://github.com/theislab/scanpy.git. This won’t work, `pip install http...` means “install me a `sdist` or `wheel` downloadable from that URL”, but there’s no sdist or wheel at that URL, but a git repository. > git clone --recursive git://github.com/theislab/scanpy.git. again, this should work, probably a problem with your internet or git installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533014138
https://github.com/scverse/scanpy/issues/838#issuecomment-533014138:6,Deployability,install,install,6,"> pip install git+git://github.com/theislab/scanpy.git . should have worked, there seems to be a problem with your git installation or internet. > importlib_metadata.PackageNotFoundError: scanpy. That’s my mistake, seems like a broke installing from .zips (not that anyone tried so far, installing from git is more convenient). > pip install https://github.com/theislab/scanpy.git. This won’t work, `pip install http...` means “install me a `sdist` or `wheel` downloadable from that URL”, but there’s no sdist or wheel at that URL, but a git repository. > git clone --recursive git://github.com/theislab/scanpy.git. again, this should work, probably a problem with your internet or git installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533014138
https://github.com/scverse/scanpy/issues/838#issuecomment-533014138:119,Deployability,install,installation,119,"> pip install git+git://github.com/theislab/scanpy.git . should have worked, there seems to be a problem with your git installation or internet. > importlib_metadata.PackageNotFoundError: scanpy. That’s my mistake, seems like a broke installing from .zips (not that anyone tried so far, installing from git is more convenient). > pip install https://github.com/theislab/scanpy.git. This won’t work, `pip install http...` means “install me a `sdist` or `wheel` downloadable from that URL”, but there’s no sdist or wheel at that URL, but a git repository. > git clone --recursive git://github.com/theislab/scanpy.git. again, this should work, probably a problem with your internet or git installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533014138
https://github.com/scverse/scanpy/issues/838#issuecomment-533014138:234,Deployability,install,installing,234,"> pip install git+git://github.com/theislab/scanpy.git . should have worked, there seems to be a problem with your git installation or internet. > importlib_metadata.PackageNotFoundError: scanpy. That’s my mistake, seems like a broke installing from .zips (not that anyone tried so far, installing from git is more convenient). > pip install https://github.com/theislab/scanpy.git. This won’t work, `pip install http...` means “install me a `sdist` or `wheel` downloadable from that URL”, but there’s no sdist or wheel at that URL, but a git repository. > git clone --recursive git://github.com/theislab/scanpy.git. again, this should work, probably a problem with your internet or git installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533014138
https://github.com/scverse/scanpy/issues/838#issuecomment-533014138:287,Deployability,install,installing,287,"> pip install git+git://github.com/theislab/scanpy.git . should have worked, there seems to be a problem with your git installation or internet. > importlib_metadata.PackageNotFoundError: scanpy. That’s my mistake, seems like a broke installing from .zips (not that anyone tried so far, installing from git is more convenient). > pip install https://github.com/theislab/scanpy.git. This won’t work, `pip install http...` means “install me a `sdist` or `wheel` downloadable from that URL”, but there’s no sdist or wheel at that URL, but a git repository. > git clone --recursive git://github.com/theislab/scanpy.git. again, this should work, probably a problem with your internet or git installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533014138
https://github.com/scverse/scanpy/issues/838#issuecomment-533014138:334,Deployability,install,install,334,"> pip install git+git://github.com/theislab/scanpy.git . should have worked, there seems to be a problem with your git installation or internet. > importlib_metadata.PackageNotFoundError: scanpy. That’s my mistake, seems like a broke installing from .zips (not that anyone tried so far, installing from git is more convenient). > pip install https://github.com/theislab/scanpy.git. This won’t work, `pip install http...` means “install me a `sdist` or `wheel` downloadable from that URL”, but there’s no sdist or wheel at that URL, but a git repository. > git clone --recursive git://github.com/theislab/scanpy.git. again, this should work, probably a problem with your internet or git installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533014138
https://github.com/scverse/scanpy/issues/838#issuecomment-533014138:404,Deployability,install,install,404,"> pip install git+git://github.com/theislab/scanpy.git . should have worked, there seems to be a problem with your git installation or internet. > importlib_metadata.PackageNotFoundError: scanpy. That’s my mistake, seems like a broke installing from .zips (not that anyone tried so far, installing from git is more convenient). > pip install https://github.com/theislab/scanpy.git. This won’t work, `pip install http...` means “install me a `sdist` or `wheel` downloadable from that URL”, but there’s no sdist or wheel at that URL, but a git repository. > git clone --recursive git://github.com/theislab/scanpy.git. again, this should work, probably a problem with your internet or git installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533014138
https://github.com/scverse/scanpy/issues/838#issuecomment-533014138:428,Deployability,install,install,428,"> pip install git+git://github.com/theislab/scanpy.git . should have worked, there seems to be a problem with your git installation or internet. > importlib_metadata.PackageNotFoundError: scanpy. That’s my mistake, seems like a broke installing from .zips (not that anyone tried so far, installing from git is more convenient). > pip install https://github.com/theislab/scanpy.git. This won’t work, `pip install http...` means “install me a `sdist` or `wheel` downloadable from that URL”, but there’s no sdist or wheel at that URL, but a git repository. > git clone --recursive git://github.com/theislab/scanpy.git. again, this should work, probably a problem with your internet or git installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533014138
https://github.com/scverse/scanpy/issues/838#issuecomment-533014138:686,Deployability,install,installation,686,"> pip install git+git://github.com/theislab/scanpy.git . should have worked, there seems to be a problem with your git installation or internet. > importlib_metadata.PackageNotFoundError: scanpy. That’s my mistake, seems like a broke installing from .zips (not that anyone tried so far, installing from git is more convenient). > pip install https://github.com/theislab/scanpy.git. This won’t work, `pip install http...` means “install me a `sdist` or `wheel` downloadable from that URL”, but there’s no sdist or wheel at that URL, but a git repository. > git clone --recursive git://github.com/theislab/scanpy.git. again, this should work, probably a problem with your internet or git installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533014138
https://github.com/scverse/scanpy/issues/838#issuecomment-533015846:115,Deployability,install,install,115,What you can do is. 1. go into the folder from the extracted zip; 2. `git init`; 3. `git tag v1.4.5.dev0`; 4. `pip install -e .`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533015846
https://github.com/scverse/scanpy/issues/838#issuecomment-533019090:305,Availability,Down,Download,305,"@flying-sheep I got the similar result. ```python; >>> scanpy-master]$ ls; conftest.py CONTRIBUTING.md docs LICENSE MANIFEST.in pyproject.toml pytest.ini README.rst requirements.txt scanpy setup.py; >>> scanpy-master]$ git init; Reinitialized existing Git repository in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/.git/; >>> scanpy-master]$ git tag v1.4.5.dev0; fatal: Failed to resolve 'HEAD' as a valid ref.; >>> scanpy-master]$ pip install -e .; Obtaining file:///public-supool/home/wuhaoda/Scanpy/Download/scanpy-master; Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 25, in <module>; from setuptools_scm import get_version; ModuleNotFoundError: No module named 'setuptools_scm'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 29, in <module>; __version__ = version(__name__); File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/_utils.py"", line 29, in version; return version(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 438, in version; return distribution(package).version; File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 411, in distribution; return Distribution.from_name(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 184, in from_name; raise PackageNotFoundError(name); importlib_metadata.PackageNotFoundError: scanpy. -----------------------",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533019090
https://github.com/scverse/scanpy/issues/838#issuecomment-533019090:515,Availability,Down,Download,515,"@flying-sheep I got the similar result. ```python; >>> scanpy-master]$ ls; conftest.py CONTRIBUTING.md docs LICENSE MANIFEST.in pyproject.toml pytest.ini README.rst requirements.txt scanpy setup.py; >>> scanpy-master]$ git init; Reinitialized existing Git repository in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/.git/; >>> scanpy-master]$ git tag v1.4.5.dev0; fatal: Failed to resolve 'HEAD' as a valid ref.; >>> scanpy-master]$ pip install -e .; Obtaining file:///public-supool/home/wuhaoda/Scanpy/Download/scanpy-master; Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 25, in <module>; from setuptools_scm import get_version; ModuleNotFoundError: No module named 'setuptools_scm'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 29, in <module>; __version__ = version(__name__); File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/_utils.py"", line 29, in version; return version(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 438, in version; return distribution(package).version; File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 411, in distribution; return Distribution.from_name(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 184, in from_name; raise PackageNotFoundError(name); importlib_metadata.PackageNotFoundError: scanpy. -----------------------",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533019090
https://github.com/scverse/scanpy/issues/838#issuecomment-533019090:672,Availability,Down,Download,672,"@flying-sheep I got the similar result. ```python; >>> scanpy-master]$ ls; conftest.py CONTRIBUTING.md docs LICENSE MANIFEST.in pyproject.toml pytest.ini README.rst requirements.txt scanpy setup.py; >>> scanpy-master]$ git init; Reinitialized existing Git repository in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/.git/; >>> scanpy-master]$ git tag v1.4.5.dev0; fatal: Failed to resolve 'HEAD' as a valid ref.; >>> scanpy-master]$ pip install -e .; Obtaining file:///public-supool/home/wuhaoda/Scanpy/Download/scanpy-master; Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 25, in <module>; from setuptools_scm import get_version; ModuleNotFoundError: No module named 'setuptools_scm'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 29, in <module>; __version__ = version(__name__); File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/_utils.py"", line 29, in version; return version(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 438, in version; return distribution(package).version; File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 411, in distribution; return Distribution.from_name(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 184, in from_name; raise PackageNotFoundError(name); importlib_metadata.PackageNotFoundError: scanpy. -----------------------",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533019090
https://github.com/scverse/scanpy/issues/838#issuecomment-533019090:1017,Availability,Down,Download,1017,"@flying-sheep I got the similar result. ```python; >>> scanpy-master]$ ls; conftest.py CONTRIBUTING.md docs LICENSE MANIFEST.in pyproject.toml pytest.ini README.rst requirements.txt scanpy setup.py; >>> scanpy-master]$ git init; Reinitialized existing Git repository in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/.git/; >>> scanpy-master]$ git tag v1.4.5.dev0; fatal: Failed to resolve 'HEAD' as a valid ref.; >>> scanpy-master]$ pip install -e .; Obtaining file:///public-supool/home/wuhaoda/Scanpy/Download/scanpy-master; Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 25, in <module>; from setuptools_scm import get_version; ModuleNotFoundError: No module named 'setuptools_scm'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 29, in <module>; __version__ = version(__name__); File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/_utils.py"", line 29, in version; return version(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 438, in version; return distribution(package).version; File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 411, in distribution; return Distribution.from_name(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 184, in from_name; raise PackageNotFoundError(name); importlib_metadata.PackageNotFoundError: scanpy. -----------------------",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533019090
https://github.com/scverse/scanpy/issues/838#issuecomment-533019090:1156,Availability,Down,Download,1156,"FEST.in pyproject.toml pytest.ini README.rst requirements.txt scanpy setup.py; >>> scanpy-master]$ git init; Reinitialized existing Git repository in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/.git/; >>> scanpy-master]$ git tag v1.4.5.dev0; fatal: Failed to resolve 'HEAD' as a valid ref.; >>> scanpy-master]$ pip install -e .; Obtaining file:///public-supool/home/wuhaoda/Scanpy/Download/scanpy-master; Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 25, in <module>; from setuptools_scm import get_version; ModuleNotFoundError: No module named 'setuptools_scm'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 29, in <module>; __version__ = version(__name__); File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/_utils.py"", line 29, in version; return version(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 438, in version; return distribution(package).version; File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 411, in distribution; return Distribution.from_name(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 184, in from_name; raise PackageNotFoundError(name); importlib_metadata.PackageNotFoundError: scanpy. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Dow",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533019090
https://github.com/scverse/scanpy/issues/838#issuecomment-533019090:1296,Availability,Down,Download,1296,"est.ini README.rst requirements.txt scanpy setup.py; >>> scanpy-master]$ git init; Reinitialized existing Git repository in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/.git/; >>> scanpy-master]$ git tag v1.4.5.dev0; fatal: Failed to resolve 'HEAD' as a valid ref.; >>> scanpy-master]$ pip install -e .; Obtaining file:///public-supool/home/wuhaoda/Scanpy/Download/scanpy-master; Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 25, in <module>; from setuptools_scm import get_version; ModuleNotFoundError: No module named 'setuptools_scm'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 29, in <module>; __version__ = version(__name__); File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/_utils.py"", line 29, in version; return version(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 438, in version; return distribution(package).version; File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 411, in distribution; return Distribution.from_name(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 184, in from_name; raise PackageNotFoundError(name); importlib_metadata.PackageNotFoundError: scanpy. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533019090
https://github.com/scverse/scanpy/issues/838#issuecomment-533019090:2067,Availability,error,error,2067,"est.ini README.rst requirements.txt scanpy setup.py; >>> scanpy-master]$ git init; Reinitialized existing Git repository in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/.git/; >>> scanpy-master]$ git tag v1.4.5.dev0; fatal: Failed to resolve 'HEAD' as a valid ref.; >>> scanpy-master]$ pip install -e .; Obtaining file:///public-supool/home/wuhaoda/Scanpy/Download/scanpy-master; Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 25, in <module>; from setuptools_scm import get_version; ModuleNotFoundError: No module named 'setuptools_scm'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 29, in <module>; __version__ = version(__name__); File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/_utils.py"", line 29, in version; return version(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 438, in version; return distribution(package).version; File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 411, in distribution; return Distribution.from_name(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 184, in from_name; raise PackageNotFoundError(name); importlib_metadata.PackageNotFoundError: scanpy. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533019090
https://github.com/scverse/scanpy/issues/838#issuecomment-533019090:2118,Availability,Down,Download,2118,"est.ini README.rst requirements.txt scanpy setup.py; >>> scanpy-master]$ git init; Reinitialized existing Git repository in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/.git/; >>> scanpy-master]$ git tag v1.4.5.dev0; fatal: Failed to resolve 'HEAD' as a valid ref.; >>> scanpy-master]$ pip install -e .; Obtaining file:///public-supool/home/wuhaoda/Scanpy/Download/scanpy-master; Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 25, in <module>; from setuptools_scm import get_version; ModuleNotFoundError: No module named 'setuptools_scm'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 29, in <module>; __version__ = version(__name__); File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/_utils.py"", line 29, in version; return version(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 438, in version; return distribution(package).version; File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 411, in distribution; return Distribution.from_name(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 184, in from_name; raise PackageNotFoundError(name); importlib_metadata.PackageNotFoundError: scanpy. ----------------------------------------. Command ""python setup.py egg_info"" failed with error code 1 in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533019090
https://github.com/scverse/scanpy/issues/838#issuecomment-533019090:449,Deployability,install,install,449,"@flying-sheep I got the similar result. ```python; >>> scanpy-master]$ ls; conftest.py CONTRIBUTING.md docs LICENSE MANIFEST.in pyproject.toml pytest.ini README.rst requirements.txt scanpy setup.py; >>> scanpy-master]$ git init; Reinitialized existing Git repository in /public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/.git/; >>> scanpy-master]$ git tag v1.4.5.dev0; fatal: Failed to resolve 'HEAD' as a valid ref.; >>> scanpy-master]$ pip install -e .; Obtaining file:///public-supool/home/wuhaoda/Scanpy/Download/scanpy-master; Complete output from command python setup.py egg_info:; Traceback (most recent call last):; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 25, in <module>; from setuptools_scm import get_version; ModuleNotFoundError: No module named 'setuptools_scm'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/__init__.py"", line 29, in <module>; __version__ = version(__name__); File ""/public-supool/home/wuhaoda/Scanpy/Download/scanpy-master/scanpy/_utils.py"", line 29, in version; return version(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 438, in version; return distribution(package).version; File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 411, in distribution; return Distribution.from_name(package); File ""/public-supool/home/wuhaoda/anaconda2/envs/Grim3.6.8/lib/python3.6/site-packages/importlib_metadata/__init__.py"", line 184, in from_name; raise PackageNotFoundError(name); importlib_metadata.PackageNotFoundError: scanpy. -----------------------",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838#issuecomment-533019090
https://github.com/scverse/scanpy/pull/839#issuecomment-531489355:52,Testability,test,tests,52,"I think separating static analysis from running the tests is the way to go (in #841 I added black checking as a separate step.). Also mypy is very strict, so we might have to fix *a lot*.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/839#issuecomment-531489355
https://github.com/scverse/scanpy/pull/839#issuecomment-531642595:95,Testability,stub,stubs,95,I'm not sure the benefits from using mypy would outweigh the pain in using unstable numpy type-stubs,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/839#issuecomment-531642595
https://github.com/scverse/scanpy/pull/839#issuecomment-531701000:132,Testability,test,testing,132,"It’s both bullshit that that numpy/numpy#2776 is unfixed since 2012 and Python doesn’t have instance checks for collections without testing for all the mixed-in methods. What we mostly want to test for is if something is iterable and/or indexable. For “sized iterable”, this is possible via `isinstance(x, cabc.Collection)`, but everything more complex has all those mixin methods that are checked for …. At least what we have now is better than `isinstance(x, list)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/839#issuecomment-531701000
https://github.com/scverse/scanpy/pull/839#issuecomment-531701000:193,Testability,test,test,193,"It’s both bullshit that that numpy/numpy#2776 is unfixed since 2012 and Python doesn’t have instance checks for collections without testing for all the mixed-in methods. What we mostly want to test for is if something is iterable and/or indexable. For “sized iterable”, this is possible via `isinstance(x, cabc.Collection)`, but everything more complex has all those mixin methods that are checked for …. At least what we have now is better than `isinstance(x, list)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/839#issuecomment-531701000
https://github.com/scverse/scanpy/pull/839#issuecomment-531701106:261,Testability,assert,assert,261,"Actually, I call bullshit even more:. ```py; class seq_array(np.ndarray):; def __reversed__(self):; return iter(self[::-1]); def index(self, value) -> int:; return np.in1d(self, value).nonzero()[0]; def count(self, value) -> int:; return (self == value).sum(). assert issubclass(seq_array, cabc.Collection); assert issubclass(seq_array, cabc.Reversible); for meth in ""__contains__ __iter__ __reversed__ index count"".split():; assert hasattr(seq_array, meth), meth; print(issubclass(seq_array, cabc.Sequence)); ```. prints `False`. wat.jpg. /edit: Hilarious. Sequence doesn’t implement `__subclasscheck__`, so only things that are `Sequence.register`ed are considered sequences.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/839#issuecomment-531701106
https://github.com/scverse/scanpy/pull/839#issuecomment-531701106:308,Testability,assert,assert,308,"Actually, I call bullshit even more:. ```py; class seq_array(np.ndarray):; def __reversed__(self):; return iter(self[::-1]); def index(self, value) -> int:; return np.in1d(self, value).nonzero()[0]; def count(self, value) -> int:; return (self == value).sum(). assert issubclass(seq_array, cabc.Collection); assert issubclass(seq_array, cabc.Reversible); for meth in ""__contains__ __iter__ __reversed__ index count"".split():; assert hasattr(seq_array, meth), meth; print(issubclass(seq_array, cabc.Sequence)); ```. prints `False`. wat.jpg. /edit: Hilarious. Sequence doesn’t implement `__subclasscheck__`, so only things that are `Sequence.register`ed are considered sequences.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/839#issuecomment-531701106
https://github.com/scverse/scanpy/pull/839#issuecomment-531701106:426,Testability,assert,assert,426,"Actually, I call bullshit even more:. ```py; class seq_array(np.ndarray):; def __reversed__(self):; return iter(self[::-1]); def index(self, value) -> int:; return np.in1d(self, value).nonzero()[0]; def count(self, value) -> int:; return (self == value).sum(). assert issubclass(seq_array, cabc.Collection); assert issubclass(seq_array, cabc.Reversible); for meth in ""__contains__ __iter__ __reversed__ index count"".split():; assert hasattr(seq_array, meth), meth; print(issubclass(seq_array, cabc.Sequence)); ```. prints `False`. wat.jpg. /edit: Hilarious. Sequence doesn’t implement `__subclasscheck__`, so only things that are `Sequence.register`ed are considered sequences.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/839#issuecomment-531701106
https://github.com/scverse/scanpy/pull/839#issuecomment-533874937:227,Availability,error,errors,227,"Of course it is, but it’s a sequence of sequences (… of sequences of sequences …):. ```py; >>> list(iter(np.array([[1,2],[3,5]]))) ; [array([1, 2]), array([3, 5])]; ```. Yes, 0D-arrays aren’t sequences, but I’m OK with runtime errors if you pass one of those here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/839#issuecomment-533874937
https://github.com/scverse/scanpy/issues/842#issuecomment-531668446:33,Testability,test,test,33,"What's happening is in the first test you're still comparing each group to all other cells. Here's a quick example of that:. ```python; import scanpy as sc; import numpy as np. adata = sc.datasets.pbmc68k_reduced(); adata.X = adata.raw.X.copy(); sc.tl.leiden(adata); a = sc.tl.rank_genes_groups( ; adata=adata, ; groupby='leiden', ; use_raw=False, ; method='t-test', ; groups=['1', '2'], ; n_genes=adata.shape[1], ; copy=True ; ); b = sc.tl.rank_genes_groups( ; adata=adata, ; groupby='leiden', ; use_raw=False, ; method='t-test', ; # groups=['1', '2'], ; n_genes=adata.shape[1], ; copy=True ; ). assert np.all(sc.get.rank_genes_groups_df(a, ""1"") == sc.get.rank_genes_groups_df(b, ""1"")); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531668446
https://github.com/scverse/scanpy/issues/842#issuecomment-531668446:360,Testability,test,test,360,"What's happening is in the first test you're still comparing each group to all other cells. Here's a quick example of that:. ```python; import scanpy as sc; import numpy as np. adata = sc.datasets.pbmc68k_reduced(); adata.X = adata.raw.X.copy(); sc.tl.leiden(adata); a = sc.tl.rank_genes_groups( ; adata=adata, ; groupby='leiden', ; use_raw=False, ; method='t-test', ; groups=['1', '2'], ; n_genes=adata.shape[1], ; copy=True ; ); b = sc.tl.rank_genes_groups( ; adata=adata, ; groupby='leiden', ; use_raw=False, ; method='t-test', ; # groups=['1', '2'], ; n_genes=adata.shape[1], ; copy=True ; ). assert np.all(sc.get.rank_genes_groups_df(a, ""1"") == sc.get.rank_genes_groups_df(b, ""1"")); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531668446
https://github.com/scverse/scanpy/issues/842#issuecomment-531668446:524,Testability,test,test,524,"What's happening is in the first test you're still comparing each group to all other cells. Here's a quick example of that:. ```python; import scanpy as sc; import numpy as np. adata = sc.datasets.pbmc68k_reduced(); adata.X = adata.raw.X.copy(); sc.tl.leiden(adata); a = sc.tl.rank_genes_groups( ; adata=adata, ; groupby='leiden', ; use_raw=False, ; method='t-test', ; groups=['1', '2'], ; n_genes=adata.shape[1], ; copy=True ; ); b = sc.tl.rank_genes_groups( ; adata=adata, ; groupby='leiden', ; use_raw=False, ; method='t-test', ; # groups=['1', '2'], ; n_genes=adata.shape[1], ; copy=True ; ). assert np.all(sc.get.rank_genes_groups_df(a, ""1"") == sc.get.rank_genes_groups_df(b, ""1"")); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531668446
https://github.com/scverse/scanpy/issues/842#issuecomment-531668446:597,Testability,assert,assert,597,"What's happening is in the first test you're still comparing each group to all other cells. Here's a quick example of that:. ```python; import scanpy as sc; import numpy as np. adata = sc.datasets.pbmc68k_reduced(); adata.X = adata.raw.X.copy(); sc.tl.leiden(adata); a = sc.tl.rank_genes_groups( ; adata=adata, ; groupby='leiden', ; use_raw=False, ; method='t-test', ; groups=['1', '2'], ; n_genes=adata.shape[1], ; copy=True ; ); b = sc.tl.rank_genes_groups( ; adata=adata, ; groupby='leiden', ; use_raw=False, ; method='t-test', ; # groups=['1', '2'], ; n_genes=adata.shape[1], ; copy=True ; ). assert np.all(sc.get.rank_genes_groups_df(a, ""1"") == sc.get.rank_genes_groups_df(b, ""1"")); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531668446
https://github.com/scverse/scanpy/issues/842#issuecomment-531820364:167,Usability,clear,clear,167,"Thanks, however I think the [documentation](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.rank_genes_groups.html#scanpy.tl.rank_genes_groups) is not perfectly clear about it:. > groups : str, Iterable[str]; Subset of groups, e.g. ['g1', 'g2', 'g3'], to which comparison shall be restricted, or 'all' (default), for all groups. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531820364
https://github.com/scverse/scanpy/issues/842#issuecomment-531833776:41,Usability,clear,clearer,41,Sorry about that. Any idea how we can be clearer?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531833776
https://github.com/scverse/scanpy/issues/842#issuecomment-531838315:698,Integrability,interface,interface,698,"No worries. It can be something like:. > Subset of groups, e.g. ['g1', 'g2', 'g3'], for which the list of DE genes should be computed. Each group of cells is always compared to the remaining cells, even if they don't belong to the subset of groups. However, it would be probably more useful to change the API and to implement subsetting of the groups through the 'groups' parameter. I think this would be more intuitive, also together with the use of the 'reference' parameter.; In particular, because retrieving of DE genes for specific groups can be more easily done with the [get](https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html#scanpy.get.rank_genes_groups_df) interface.; But there may be other use cases I haven't considered in which the actual implementation may be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531838315
https://github.com/scverse/scanpy/issues/842#issuecomment-531838315:410,Usability,intuit,intuitive,410,"No worries. It can be something like:. > Subset of groups, e.g. ['g1', 'g2', 'g3'], for which the list of DE genes should be computed. Each group of cells is always compared to the remaining cells, even if they don't belong to the subset of groups. However, it would be probably more useful to change the API and to implement subsetting of the groups through the 'groups' parameter. I think this would be more intuitive, also together with the use of the 'reference' parameter.; In particular, because retrieving of DE genes for specific groups can be more easily done with the [get](https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html#scanpy.get.rank_genes_groups_df) interface.; But there may be other use cases I haven't considered in which the actual implementation may be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531838315
https://github.com/scverse/scanpy/issues/843#issuecomment-532086499:149,Deployability,release,release,149,"It looks like the underlying issue (`np.zeros` was being called with a heterogeneous `shape` tuple) has been marked to be resolved in the next numba release. We could implement a workaround here where we force the dtype, though numba does have a pretty fast release cadence. @fkoegel, if we implemented a fix here would you be able to test it for us on master branches?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/843#issuecomment-532086499
https://github.com/scverse/scanpy/issues/843#issuecomment-532086499:258,Deployability,release,release,258,"It looks like the underlying issue (`np.zeros` was being called with a heterogeneous `shape` tuple) has been marked to be resolved in the next numba release. We could implement a workaround here where we force the dtype, though numba does have a pretty fast release cadence. @fkoegel, if we implemented a fix here would you be able to test it for us on master branches?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/843#issuecomment-532086499
https://github.com/scverse/scanpy/issues/843#issuecomment-532086499:335,Testability,test,test,335,"It looks like the underlying issue (`np.zeros` was being called with a heterogeneous `shape` tuple) has been marked to be resolved in the next numba release. We could implement a workaround here where we force the dtype, though numba does have a pretty fast release cadence. @fkoegel, if we implemented a fix here would you be able to test it for us on master branches?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/843#issuecomment-532086499
https://github.com/scverse/scanpy/issues/843#issuecomment-542784036:34,Availability,error,errors,34,@ivirshup I am getting same numba errors on windows 10 machine. I can test the workaround if you provide a fix. Currently to make the function working I set `percent_top=None`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/843#issuecomment-542784036
https://github.com/scverse/scanpy/issues/843#issuecomment-542784036:70,Testability,test,test,70,@ivirshup I am getting same numba errors on windows 10 machine. I can test the workaround if you provide a fix. Currently to make the function working I set `percent_top=None`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/843#issuecomment-542784036
https://github.com/scverse/scanpy/pull/844#issuecomment-532173941:43,Testability,benchmark,benchmarks,43,"Just a heads up, I would like to run a few benchmarks on this before merging.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-532173941
https://github.com/scverse/scanpy/pull/844#issuecomment-534015240:0,Testability,Benchmark,Benchmark,0,"Benchmark results from my laptop:. | | Parallel | Single |; |-|--------|--------|; | Compilation | ~12s | ~5s |; | Run (3k cells, 37k genes) | ~70ms | ~120ms | ; | Run (50k cells, 35k genes) | ~3.8s | ~8.1s | ; | Run (370k cells, 33k genes) | ~13.7s | ~17.4 s |. So... probably worth it? I recall the difference being more pronounced with larger dataset sizes, but that was with a different numba version and maybe a different machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534015240
https://github.com/scverse/scanpy/pull/844#issuecomment-534067279:4,Availability,down,downside,4,"> A downside of this is it takes a really long time to compile on first run, which might be off-putting. Right, numba only compiles stuff when first run (because otherwise it can’t know the types) so this doesn’t slow down importing scanpy. > So... probably worth it?. We could wrap it in a function that checks the number of cells and only compiles this to faster code when necessary. If we do this in a generic way we could even defer importing numpy, saving on import duration (although there probably isn’t much functionality without running numpy-driven functions)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534067279
https://github.com/scverse/scanpy/pull/844#issuecomment-534067279:218,Availability,down,down,218,"> A downside of this is it takes a really long time to compile on first run, which might be off-putting. Right, numba only compiles stuff when first run (because otherwise it can’t know the types) so this doesn’t slow down importing scanpy. > So... probably worth it?. We could wrap it in a function that checks the number of cells and only compiles this to faster code when necessary. If we do this in a generic way we could even defer importing numpy, saving on import duration (although there probably isn’t much functionality without running numpy-driven functions)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534067279
https://github.com/scverse/scanpy/pull/844#issuecomment-534067279:278,Integrability,wrap,wrap,278,"> A downside of this is it takes a really long time to compile on first run, which might be off-putting. Right, numba only compiles stuff when first run (because otherwise it can’t know the types) so this doesn’t slow down importing scanpy. > So... probably worth it?. We could wrap it in a function that checks the number of cells and only compiles this to faster code when necessary. If we do this in a generic way we could even defer importing numpy, saving on import duration (although there probably isn’t much functionality without running numpy-driven functions)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534067279
https://github.com/scverse/scanpy/pull/844#issuecomment-534371715:341,Deployability,install,install,341,"> We could wrap it in a function that checks the number of cells and only compiles this to faster code when necessary. So that's what this PR would replace. The reason I thought this could be replaced is that `numba` now allows on-disk cacheing of parallelized functions. This means that the function would only have to be compiled once per install. That cache only get's invalidated if function's source code get's modified, so this shouldn't cause too much pain for development testing times. I've added a note to the documentation mentioning this, so I think it's fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534371715
https://github.com/scverse/scanpy/pull/844#issuecomment-534371715:11,Integrability,wrap,wrap,11,"> We could wrap it in a function that checks the number of cells and only compiles this to faster code when necessary. So that's what this PR would replace. The reason I thought this could be replaced is that `numba` now allows on-disk cacheing of parallelized functions. This means that the function would only have to be compiled once per install. That cache only get's invalidated if function's source code get's modified, so this shouldn't cause too much pain for development testing times. I've added a note to the documentation mentioning this, so I think it's fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534371715
https://github.com/scverse/scanpy/pull/844#issuecomment-534371715:236,Performance,cache,cacheing,236,"> We could wrap it in a function that checks the number of cells and only compiles this to faster code when necessary. So that's what this PR would replace. The reason I thought this could be replaced is that `numba` now allows on-disk cacheing of parallelized functions. This means that the function would only have to be compiled once per install. That cache only get's invalidated if function's source code get's modified, so this shouldn't cause too much pain for development testing times. I've added a note to the documentation mentioning this, so I think it's fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534371715
https://github.com/scverse/scanpy/pull/844#issuecomment-534371715:355,Performance,cache,cache,355,"> We could wrap it in a function that checks the number of cells and only compiles this to faster code when necessary. So that's what this PR would replace. The reason I thought this could be replaced is that `numba` now allows on-disk cacheing of parallelized functions. This means that the function would only have to be compiled once per install. That cache only get's invalidated if function's source code get's modified, so this shouldn't cause too much pain for development testing times. I've added a note to the documentation mentioning this, so I think it's fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534371715
https://github.com/scverse/scanpy/pull/844#issuecomment-534371715:480,Testability,test,testing,480,"> We could wrap it in a function that checks the number of cells and only compiles this to faster code when necessary. So that's what this PR would replace. The reason I thought this could be replaced is that `numba` now allows on-disk cacheing of parallelized functions. This means that the function would only have to be compiled once per install. That cache only get's invalidated if function's source code get's modified, so this shouldn't cause too much pain for development testing times. I've added a note to the documentation mentioning this, so I think it's fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534371715
https://github.com/scverse/scanpy/pull/844#issuecomment-534485862:18,Deployability,install,install,18,"OK! A global, per-install cache. Where is it stored?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534485862
https://github.com/scverse/scanpy/pull/844#issuecomment-534485862:26,Performance,cache,cache,26,"OK! A global, per-install cache. Where is it stored?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534485862
https://github.com/scverse/scanpy/pull/847#issuecomment-532191481:91,Availability,checkpoint,checkpoints,91,"@ivirshup I think writing a file for uploading it to the web, for read caches, and for for checkpoints of a pipeline has different requirements. I think a `h5ad_compression` or even `hdf5_compression` setting could have its place, but separately from the `cache_compression`. We’ll have to think about naming though. Maybe we want to namespace our settings like matplotlib’s rcparams?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/847#issuecomment-532191481
https://github.com/scverse/scanpy/pull/847#issuecomment-532191481:108,Deployability,pipeline,pipeline,108,"@ivirshup I think writing a file for uploading it to the web, for read caches, and for for checkpoints of a pipeline has different requirements. I think a `h5ad_compression` or even `hdf5_compression` setting could have its place, but separately from the `cache_compression`. We’ll have to think about naming though. Maybe we want to namespace our settings like matplotlib’s rcparams?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/847#issuecomment-532191481
https://github.com/scverse/scanpy/pull/847#issuecomment-532191481:71,Performance,cache,caches,71,"@ivirshup I think writing a file for uploading it to the web, for read caches, and for for checkpoints of a pipeline has different requirements. I think a `h5ad_compression` or even `hdf5_compression` setting could have its place, but separately from the `cache_compression`. We’ll have to think about naming though. Maybe we want to namespace our settings like matplotlib’s rcparams?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/847#issuecomment-532191481
https://github.com/scverse/scanpy/issues/849#issuecomment-725928803:52,Deployability,update,updates,52,"This seems to be working now. Given that matplotlib updates are known to introduce bugs, I'm not sure we want to pin matplotlib to `3.3.3`. I'm concerned that's too strong of a restriction, since it's likely there's someone out there who needs to use an older version. Right now this is causing CI to fail, since there's a canary test that fails when the function works 😆.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/849#issuecomment-725928803
https://github.com/scverse/scanpy/issues/849#issuecomment-725928803:323,Deployability,canary,canary,323,"This seems to be working now. Given that matplotlib updates are known to introduce bugs, I'm not sure we want to pin matplotlib to `3.3.3`. I'm concerned that's too strong of a restriction, since it's likely there's someone out there who needs to use an older version. Right now this is causing CI to fail, since there's a canary test that fails when the function works 😆.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/849#issuecomment-725928803
https://github.com/scverse/scanpy/issues/849#issuecomment-725928803:330,Testability,test,test,330,"This seems to be working now. Given that matplotlib updates are known to introduce bugs, I'm not sure we want to pin matplotlib to `3.3.3`. I'm concerned that's too strong of a restriction, since it's likely there's someone out there who needs to use an older version. Right now this is causing CI to fail, since there's a canary test that fails when the function works 😆.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/849#issuecomment-725928803
https://github.com/scverse/scanpy/issues/850#issuecomment-532633516:44,Deployability,update,updated,44,"@flying-sheep sorry for the trouble, I have updated my code and output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/850#issuecomment-532633516
https://github.com/scverse/scanpy/issues/850#issuecomment-532654224:319,Availability,down,download,319,"@flying-sheep sorry, I didn't use Jupyter notebook. one is because i am a fresh man in python. second is that our engineer of the lab server told us that we don't have ""??? some image software"" due to the limited memory. (I think, he means we could use R, but we cannot see the figure like Rstudio. we have to save it, download it to our PC, and view the figure.) I would try Juputer tomorrow~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/850#issuecomment-532654224
https://github.com/scverse/scanpy/issues/850#issuecomment-532657402:302,Deployability,install,install,302,"Well, it makes working with all this easier, specifically if you can just see your figures inline instead of popup windows. You can try out jupyterlab here to get a small tutorial: https://mybinder.org/v2/gh/jupyterlab/jupyterlab-demo/try.jupyter.org?urlpath=lab. If you think you’d like it, just `pip install jupyterlab` and start it with `jupyter lab`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/850#issuecomment-532657402
https://github.com/scverse/scanpy/issues/851#issuecomment-533674280:0,Deployability,update,update,0,"update on this, seems like this is an issue when the package is installed through conda,. reinstalled the package using pip and everything works!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851#issuecomment-533674280
https://github.com/scverse/scanpy/issues/851#issuecomment-533674280:64,Deployability,install,installed,64,"update on this, seems like this is an issue when the package is installed through conda,. reinstalled the package using pip and everything works!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851#issuecomment-533674280
https://github.com/scverse/scanpy/issues/851#issuecomment-533797158:34,Energy Efficiency,reduce,reduced,34,some of the datasets like pbmc68k-reduced also seem to have an issue loading in conda.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851#issuecomment-533797158
https://github.com/scverse/scanpy/issues/851#issuecomment-533797158:69,Performance,load,loading,69,some of the datasets like pbmc68k-reduced also seem to have an issue loading in conda.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851#issuecomment-533797158
https://github.com/scverse/scanpy/issues/851#issuecomment-533874424:85,Deployability,release,releases,85,"Actually I've always been using the conda package and never had any issues. ; Github releases are watched by the bioconda-bot, so it should never be out of date, either.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851#issuecomment-533874424
https://github.com/scverse/scanpy/issues/852#issuecomment-534002026:152,Availability,fault,faulty,152,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-534002026
https://github.com/scverse/scanpy/issues/852#issuecomment-534002026:202,Deployability,install,installation,202,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-534002026
https://github.com/scverse/scanpy/issues/852#issuecomment-534002026:293,Usability,simpl,simple,293,"I’ve only found this problem in the wild when people tried to create a figure with a dimension of size 0. It implies that either matplotlib passes some faulty instructions to libpng or that your libpng installation is broken. It’s very unlikely that it’s a problem with scanpy. Does something simple with matplotlib work? Just `pyplot.scatter([0,1], [0,1])` or so?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-534002026
https://github.com/scverse/scanpy/issues/852#issuecomment-534634792:49,Availability,error,error,49,Hello ; I did what you told me .. and I got this error. I changed the version of those . ![image](https://user-images.githubusercontent.com/48261734/65530250-302dbd80-debd-11e9-8026-761cd8571849.png). **matplotlib==3.1.1**. ![image](https://user-images.githubusercontent.com/48261734/65530196-17250c80-debd-11e9-8c19-986293e57d92.png),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-534634792
https://github.com/scverse/scanpy/issues/852#issuecomment-534975285:150,Availability,error,errors,150,"This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. . Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-534975285
https://github.com/scverse/scanpy/issues/852#issuecomment-534975285:184,Availability,error,errors,184,"This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. . Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-534975285
https://github.com/scverse/scanpy/issues/852#issuecomment-534975285:291,Availability,error,error,291,"This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings. . Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-534975285
https://github.com/scverse/scanpy/issues/852#issuecomment-534977199:348,Availability,error,errors,348,I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-534977199
https://github.com/scverse/scanpy/issues/852#issuecomment-534977199:478,Availability,error,error,478,I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-534977199
https://github.com/scverse/scanpy/issues/852#issuecomment-535093343:350,Availability,error,errors,350,> I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error?. my system type is 64 bit,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-535093343
https://github.com/scverse/scanpy/issues/852#issuecomment-535093343:480,Availability,error,error,480,> I just had another thought... it seems like this might have to do with the initial `sc.tl.paga()` call. There you get a runtime warning about overflow in long scalars. Maybe check if you get any meaningful output in `adata.uns['paga/connectivities']` or `adata.uns['paga/connectivities_tree']`. It might just be all `nan` in there due to the above errors. Could it be that you have a 32-bit windows version and the code is trying to use 64-bit floats? Maybe that's the overflow error?. my system type is 64 bit,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-535093343
https://github.com/scverse/scanpy/issues/852#issuecomment-535106890:152,Availability,error,errors,152,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings.; > ; > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?. matplotlib is working well with my windows and I tried to run ; pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-535106890
https://github.com/scverse/scanpy/issues/852#issuecomment-535106890:191,Availability,error,errors,191,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings.; > ; > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?. matplotlib is working well with my windows and I tried to run ; pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-535106890
https://github.com/scverse/scanpy/issues/852#issuecomment-535106890:298,Availability,error,error,298,"> This is not the case study code. I think this comes from the PAGA tutorial. So I can't really say whether this is normal. I typically don't have PAGA errors or warnings.; > ; > Most of the errors seem to be deprecation warnings, so that should be fine... but the ""finite values"" on posx and posy error I haven't come across. It looks like this is on Windows. Is there a matplotlib issue with windows?. matplotlib is working well with my windows and I tried to run ; pyplot.scatter([0,1], [0,1]) it works correctly. . Maybe, this issue is caused by the difference of versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-535106890
https://github.com/scverse/scanpy/issues/852#issuecomment-540751918:8,Deployability,install,install,8,can you install matplotlib 3.0? We have seen an unrelated problem with matplotlib 3.1 and thus we recommend not to install it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-540751918
https://github.com/scverse/scanpy/issues/852#issuecomment-540751918:115,Deployability,install,install,115,can you install matplotlib 3.0? We have seen an unrelated problem with matplotlib 3.1 and thus we recommend not to install it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852#issuecomment-540751918
https://github.com/scverse/scanpy/issues/853#issuecomment-534487124:209,Deployability,install,installs,209,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853#issuecomment-534487124
https://github.com/scverse/scanpy/issues/853#issuecomment-534487124:274,Testability,log,logging,274,"Yes, the `tables` module is provided by the PyPI package `pytables`. That’s confusing so most PyPI packages are named the same as the Python packages they contain. Another notable exception is `Pillow`, which installs a module named `PIL`. Please post the output of `scanpy.logging.print_versions()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853#issuecomment-534487124
https://github.com/scverse/scanpy/issues/853#issuecomment-539798622:60,Availability,error,error,60,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py; import scanpy; ```. Here is what the computer showed after I ran this code:. ```pytb; AttributeError Traceback (most recent call last); <ipython-input-2-135279188441> in <module>; ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>; 34 # the actual API; 35 from ._settings import settings, Verbosity # start with settings as several tools are using it; ---> 36 from . import tools as tl; 37 from . import preprocessing as pp; 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>; 23 ; 24 from .. import _utils; ---> 25 from .. import readwrite; 26 from .._settings import settings; 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>; 7 import numpy as np; 8 import pandas as pd; ----> 9 import tables; 10 import anndata; 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>; 91 ; 92 # Necessary imports to get versions stored on the cython extension; ---> 93 from .utilsextension import (; 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,; 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>; 122 from .flavor import restrict_flavors; 123 from .description import *; --> 124 from .filters import Filters; 125 ; 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>; 27 from tables.req_versions import min_blosc_bitshuffle_version; 28 ;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853#issuecomment-539798622
https://github.com/scverse/scanpy/issues/853#issuecomment-539798622:2197,Deployability,update,update,2197,"t); <ipython-input-2-135279188441> in <module>; ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>; 34 # the actual API; 35 from ._settings import settings, Verbosity # start with settings as several tools are using it; ---> 36 from . import tools as tl; 37 from . import preprocessing as pp; 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>; 23 ; 24 from .. import _utils; ---> 25 from .. import readwrite; 26 from .._settings import settings; 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>; 7 import numpy as np; 8 import pandas as pd; ----> 9 import tables; 10 import anndata; 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>; 91 ; 92 # Necessary imports to get versions stored on the cython extension; ---> 93 from .utilsextension import (; 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,; 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>; 122 from .flavor import restrict_flavors; 123 from .description import *; --> 124 from .filters import Filters; 125 ; 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>; 27 from tables.req_versions import min_blosc_bitshuffle_version; 28 ; ---> 29 blosc_version = LooseVersion(tables.which_lib_version(""blosc"")[1]); 30 ; 31 . AttributeError: module 'tables' has no attribute 'which_lib_version'; ```. I used Jupyter Notebook. Should I update any package? Thank you so much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853#issuecomment-539798622
https://github.com/scverse/scanpy/issues/853#issuecomment-539798622:66,Integrability,message,message,66,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py; import scanpy; ```. Here is what the computer showed after I ran this code:. ```pytb; AttributeError Traceback (most recent call last); <ipython-input-2-135279188441> in <module>; ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>; 34 # the actual API; 35 from ._settings import settings, Verbosity # start with settings as several tools are using it; ---> 36 from . import tools as tl; 37 from . import preprocessing as pp; 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>; 23 ; 24 from .. import _utils; ---> 25 from .. import readwrite; 26 from .._settings import settings; 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>; 7 import numpy as np; 8 import pandas as pd; ----> 9 import tables; 10 import anndata; 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>; 91 ; 92 # Necessary imports to get versions stored on the cython extension; ---> 93 from .utilsextension import (; 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,; 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>; 122 from .flavor import restrict_flavors; 123 from .description import *; --> 124 from .filters import Filters; 125 ; 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>; 27 from tables.req_versions import min_blosc_bitshuffle_version; 28 ;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853#issuecomment-539798622
https://github.com/scverse/scanpy/issues/853#issuecomment-539798622:1037,Testability,log,logging,1037,"it still shows the error message. . Here is my code. . ```py; import scanpy; ```. Here is what the computer showed after I ran this code:. ```pytb; AttributeError Traceback (most recent call last); <ipython-input-2-135279188441> in <module>; ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>; 34 # the actual API; 35 from ._settings import settings, Verbosity # start with settings as several tools are using it; ---> 36 from . import tools as tl; 37 from . import preprocessing as pp; 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>; 23 ; 24 from .. import _utils; ---> 25 from .. import readwrite; 26 from .._settings import settings; 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>; 7 import numpy as np; 8 import pandas as pd; ----> 9 import tables; 10 import anndata; 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>; 91 ; 92 # Necessary imports to get versions stored on the cython extension; ---> 93 from .utilsextension import (; 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,; 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>; 122 from .flavor import restrict_flavors; 123 from .description import *; --> 124 from .filters import Filters; 125 ; 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>; 27 from tables.req_versions import min_blosc_bitshuffle_version; 28 ; ---> 29 blosc_version = LooseVersion(ta",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853#issuecomment-539798622
https://github.com/scverse/scanpy/issues/853#issuecomment-539798622:1048,Testability,log,logg,1048,"it still shows the error message. . Here is my code. . ```py; import scanpy; ```. Here is what the computer showed after I ran this code:. ```pytb; AttributeError Traceback (most recent call last); <ipython-input-2-135279188441> in <module>; ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>; 34 # the actual API; 35 from ._settings import settings, Verbosity # start with settings as several tools are using it; ---> 36 from . import tools as tl; 37 from . import preprocessing as pp; 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>; 23 ; 24 from .. import _utils; ---> 25 from .. import readwrite; 26 from .._settings import settings; 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>; 7 import numpy as np; 8 import pandas as pd; ----> 9 import tables; 10 import anndata; 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>; 91 ; 92 # Necessary imports to get versions stored on the cython extension; ---> 93 from .utilsextension import (; 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,; 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>; 122 from .flavor import restrict_flavors; 123 from .description import *; --> 124 from .filters import Filters; 125 ; 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>; 27 from tables.req_versions import min_blosc_bitshuffle_version; 28 ; ---> 29 blosc_version = LooseVersion(ta",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853#issuecomment-539798622
https://github.com/scverse/scanpy/issues/855#issuecomment-537134057:60,Deployability,install,installing,60,Thanks for the suggestion. I actually solved the problem by installing a local miniconda with newer version of Python 3. Thank you.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855#issuecomment-537134057
https://github.com/scverse/scanpy/issues/856#issuecomment-537518323:87,Performance,load,loaded,87,"Hi @k3yavi, I would be great to have a tutorial in which an alevin generated matrix is loaded into scanpy. Your suggestion is to have such a tutorial hosted by scanpy or you plan to add it to your list of tutorials?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856#issuecomment-537518323
https://github.com/scverse/scanpy/issues/856#issuecomment-538028764:847,Availability,down,downstream,847,"Hi @fidelram ,. Thanks for the response.; I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ?. Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856#issuecomment-538028764
https://github.com/scverse/scanpy/issues/856#issuecomment-538028764:113,Energy Efficiency,efficient,efficient,113,"Hi @fidelram ,. Thanks for the response.; I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ?. Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856#issuecomment-538028764
https://github.com/scverse/scanpy/issues/856#issuecomment-538028764:230,Energy Efficiency,efficient,efficiently,230,"Hi @fidelram ,. Thanks for the response.; I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ?. Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856#issuecomment-538028764
https://github.com/scverse/scanpy/issues/856#issuecomment-538028764:713,Energy Efficiency,efficient,efficient,713,"Hi @fidelram ,. Thanks for the response.; I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ?. Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856#issuecomment-538028764
https://github.com/scverse/scanpy/issues/856#issuecomment-538028764:872,Energy Efficiency,efficient,efficient,872,"Hi @fidelram ,. Thanks for the response.; I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ?. Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856#issuecomment-538028764
https://github.com/scverse/scanpy/issues/856#issuecomment-538028764:547,Performance,load,load,547,"Hi @fidelram ,. Thanks for the response.; I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ?. Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856#issuecomment-538028764
https://github.com/scverse/scanpy/issues/856#issuecomment-538301097:223,Availability,avail,available,223,@k3yavi The right place to ask those questions is on the AnnData repository where all the read and write functions are located. [Here](https://icb-anndata.readthedocs-hosted.com/en/stable/api.html#reading) you can find the available read options. You are welcome to contribute! Would be great if AnnData can read EDS files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856#issuecomment-538301097
https://github.com/scverse/scanpy/pull/857#issuecomment-537310661:211,Integrability,depend,dependency,211,Remaining Qs:. * Does this need support for nans?; * This code is based off `sklearn.utils.sparsefuncs.mean_variance_axis`. Do we need a copy of the sklearn license here? Do we already include that since it's a dependency?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857#issuecomment-537310661
https://github.com/scverse/scanpy/pull/857#issuecomment-537406138:123,Testability,log,logic,123,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py; if not issparse(X):; mean = np.mean(X, axis=0, dtype=np.float64); mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64); var = mean_sq - mean ** 2; elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):; from sklearn.preprocessing import StandardScaler; scaler = StandardScaler(with_mean=False).partial_fit(X); mean, var = scaler.mean_, scaler.var_; else:; mean, var = sparse_mean_variance_axis(X, axis=0) ; ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py; if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):; from sklearn.preprocessing import StandardScaler; scaler = StandardScaler(with_mean=False).partial_fit(X); mean, var = scaler.mean_, scaler.var_; else:; mean, var = sparse_mean_variance_axis(X, axis=0) ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857#issuecomment-537406138
https://github.com/scverse/scanpy/pull/857#issuecomment-537406138:38,Usability,learn,learn,38,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py; if not issparse(X):; mean = np.mean(X, axis=0, dtype=np.float64); mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64); var = mean_sq - mean ** 2; elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):; from sklearn.preprocessing import StandardScaler; scaler = StandardScaler(with_mean=False).partial_fit(X); mean, var = scaler.mean_, scaler.var_; else:; mean, var = sparse_mean_variance_axis(X, axis=0) ; ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py; if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):; from sklearn.preprocessing import StandardScaler; scaler = StandardScaler(with_mean=False).partial_fit(X); mean, var = scaler.mean_, scaler.var_; else:; mean, var = sparse_mean_variance_axis(X, axis=0) ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857#issuecomment-537406138
https://github.com/scverse/scanpy/pull/857#issuecomment-537406138:51,Usability,learn,learn,51,"The original issue seems to be scikit-learn/scikit-learn#2969. It says that issues arise if there’s a 64 bit index, so the logic should look like. ```py; if not issparse(X):; mean = np.mean(X, axis=0, dtype=np.float64); mean_sq = np.multiply(X, X).mean(axis=0, dtype=np.float64); var = mean_sq - mean ** 2; elif STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):; from sklearn.preprocessing import StandardScaler; scaler = StandardScaler(with_mean=False).partial_fit(X); mean, var = scaler.mean_, scaler.var_; else:; mean, var = sparse_mean_variance_axis(X, axis=0) ; ```. But actually StandardScaler handles dense matrices just fine, so why not. ```py; if not issparse(X) or STANDARD_SCALER_FIXED or np.issubdtype(X.indices.dtype, np.int32):; from sklearn.preprocessing import StandardScaler; scaler = StandardScaler(with_mean=False).partial_fit(X); mean, var = scaler.mean_, scaler.var_; else:; mean, var = sparse_mean_variance_axis(X, axis=0) ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857#issuecomment-537406138
https://github.com/scverse/scanpy/issues/859#issuecomment-538903445:434,Usability,simpl,simply,434,"Hi! Welcome to the community. For questions like this, https://scanpy.discourse.group/ would be the ideal place!. Generally: If you can’t find what you search in the regular anndata or scanpy API docs, you can always try [`scanpy.external`](https://icb-scanpy.readthedocs-hosted.com/en/stable/external/index.html), where you should e.g. find answers for your first question. I don’t think we have a tutorial for this yet, though. For simply `concatenate`ing multiple `AnnData` objects, the anndata docs should help you out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859#issuecomment-538903445
https://github.com/scverse/scanpy/issues/859#issuecomment-545962554:290,Deployability,integrat,integration,290,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:; https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859#issuecomment-545962554
https://github.com/scverse/scanpy/issues/859#issuecomment-545962554:504,Deployability,integrat,integration,504,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:; https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859#issuecomment-545962554
https://github.com/scverse/scanpy/issues/859#issuecomment-545962554:290,Integrability,integrat,integration,290,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:; https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859#issuecomment-545962554
https://github.com/scverse/scanpy/issues/859#issuecomment-545962554:454,Integrability,interface,interface,454,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:; https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859#issuecomment-545962554
https://github.com/scverse/scanpy/issues/859#issuecomment-545962554:504,Integrability,integrat,integration,504,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:; https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859#issuecomment-545962554
https://github.com/scverse/scanpy/issues/859#issuecomment-565168151:620,Performance,perform,perform,620,"@LuckyMD . Thank you very much for pointing me in the right direction!! I was able to merge the my anndata objects using ```anndata.concatenate()``` than I created the trajectory analysis for the identified clusters using ```sc.pl.paga```. As explained in my initial post, I have data from 12 samples and 3 treatments. When I created my anndata object, I labeled the batch categories using the following command:. ```adata_merged = adata_ESTRUS.concatenate(adata_DIESTRUS, adata_PROESTRUS,batch_categories=['ESTRUS','DIESTRUS','PROESTRUS'])```. Now, I would like choose a cell type (one specific identified clusters) to perform trajectory analysis according to treatment (in my case batch_category). Do you have any suggestions on how to do that?. Thanks again!!. Joao",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859#issuecomment-565168151
https://github.com/scverse/scanpy/pull/862#issuecomment-540005836:57,Availability,error,errors,57,"Thanks for this contribution! Can you look at the travis errors? See https://github.com/theislab/scanpy/pull/797#issuecomment-536861482 for background. To fix formatting errors you can use https://github.com/psf/black. Furthermore, can you add a small example to the docstring? (see for example: https://github.com/theislab/scanpy/blob/master/scanpy/external/tl/_palantir.py)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-540005836
https://github.com/scverse/scanpy/pull/862#issuecomment-540005836:170,Availability,error,errors,170,"Thanks for this contribution! Can you look at the travis errors? See https://github.com/theislab/scanpy/pull/797#issuecomment-536861482 for background. To fix formatting errors you can use https://github.com/psf/black. Furthermore, can you add a small example to the docstring? (see for example: https://github.com/theislab/scanpy/blob/master/scanpy/external/tl/_palantir.py)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-540005836
https://github.com/scverse/scanpy/pull/862#issuecomment-541174606:54,Availability,error,errors,54,"Thanks for getting back to me. I fixed the formatting errors and moved trimap to scanpy external. ; trimap is no longer imported by default, so the overall import time is unaffected. ; I also added an example to the docstring. Please let me know if further fixes are required.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-541174606
https://github.com/scverse/scanpy/pull/862#issuecomment-561830094:95,Availability,error,error,95,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-561830094
https://github.com/scverse/scanpy/pull/862#issuecomment-561830094:187,Testability,test,test,187,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-561830094
https://github.com/scverse/scanpy/pull/862#issuecomment-561830094:38,Usability,feedback,feedback,38,"Thanks @flying-sheep for the thorough feedback! I made the changes. There is still a Travis CI error about slow_to_import modules. Since trimap is now in external, I am now sure how this test is being affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-561830094
https://github.com/scverse/scanpy/pull/862#issuecomment-562052370:146,Deployability,update,updated,146,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-562052370
https://github.com/scverse/scanpy/pull/862#issuecomment-562052370:185,Testability,test,test,185,"Very weird. Importing `scipy.sparse` shouldn’t import `scipy.stats`, and you didn’t add any other imports. Could be that this is a change in some updated version of something. Does the test fail for you? Then you could find out how it gets imported using `import-profiler`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-562052370
https://github.com/scverse/scanpy/pull/862#issuecomment-562259348:252,Testability,test,tests,252,Thanks @flying-sheep! I haven't added any new imports other those for type hints and copying anndata: [https://github.com/eamid/scanpy/compare/9ae6c19...b7ed705](https://github.com/eamid/scanpy/compare/9ae6c19...b7ed705). My previous commit passed the tests. Could this be because of recent commits on your side?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-562259348
https://github.com/scverse/scanpy/pull/862#issuecomment-562545044:160,Integrability,depend,dependency,160,"To help debug it, can one of you do what I suggested? I don’t have the problem locally and if one of you does see it, it would be much easier to find out which dependency started to import that module.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-562545044
https://github.com/scverse/scanpy/issues/863#issuecomment-661497061:1000,Availability,down,down,1000,"this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167.; - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained; - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?!; - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863#issuecomment-661497061
https://github.com/scverse/scanpy/issues/863#issuecomment-661497061:1898,Availability,down,downregulated,1898,"anges from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167.; - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained; - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?!; - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863#issuecomment-661497061
https://github.com/scverse/scanpy/issues/863#issuecomment-661497061:2099,Availability,down,downregulated,2099,"anges from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167.; - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained; - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?!; - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863#issuecomment-661497061
https://github.com/scverse/scanpy/issues/863#issuecomment-661497061:1949,Deployability,update,update,1949,"anges from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167.; - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained; - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?!; - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(fold_change_matrix)) > np.log2(min_fold_change))` (similar to @gianasco's suggestion, but handles downregulated fold changes more appropriately). I noted this issue separately in #1325",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863#issuecomment-661497061
https://github.com/scverse/scanpy/issues/863#issuecomment-661497061:469,Testability,log,logfoldchanges,469,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167.; - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained; - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?!; - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863#issuecomment-661497061
https://github.com/scverse/scanpy/issues/863#issuecomment-661497061:969,Testability,log,log,969,"I want to second this issue!! I just spent many hours digging into the source code to figure out why `filter_rank_genes_groups` was filtering out genes that reported really high fold changes from `rank_genes_groups`, only to discover the discrepancy in the fold change calculation. Here is an example of how confusing this inconsistency can be:. - I run `rank_genes_groups` and see that many marker genes have high log2 fold changes in `adata.uns['rank_genes_groups']['logfoldchanges'][<cluster_string>]`. For example, gene X has a fold change of -27.720167.; - Then, I run `filter_rank_genes_groups` -- and none of these genes with high negative fold changes are retained; - There are two issues here: one is that negative fold changes don't get retained at all. [This is the issue I notice first, and report in #1325]. I fix that in my fork of the repo (solution below), but STILL these genes are removed when filtering for a min absolute fold change of 1.5 (0.58 on log scale)... ?!; - This boils down to the inconsistency in fold change calculation. Mean expression of gene X within my cluster of interest is 0, and outside it is 0.1997576. `np.log2((0 + 1e-9)/(0.1997576 + 1e-9)) = -27.720167`, as reported originally by `rank_genes_groups`. As a user, I completely expect this gene to pass my threshold. `filter_rank_genes_groups`, however, calculates fold change as `np.log2(np.exp(0)/np.exp(0.199758)) = -0.288189`, which does NOT pass my fold change threshold, thus it gets filtered out. All this happens silently of course [the only number I have seen is a whopping fold change of -27] leaving me utterly confused. I'm not sure which is more correct (though -27 seems pretty inflated to me given the raw numbers), but it would make a lot more sense for it to at least be consistent, especially so that `filter_rank_genes_groups` could give expected results. p.s. Here is my fix to retain downregulated genes in `filter_rank_genes_groups`: update the third condition to `(np.absolute(np.log2(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863#issuecomment-661497061
https://github.com/scverse/scanpy/issues/863#issuecomment-776459742:43,Deployability,update,updated,43,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. ; - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to; - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863#issuecomment-776459742
https://github.com/scverse/scanpy/issues/863#issuecomment-776459742:156,Deployability,update,updated,156,"For all those asking whether this has been updated, I'm not a contributor to this repo but from reading the source code, it looks like this has indeed been updated in the latest version of scanpy. From what I can tell, . - `np.log2((expm1_func(mean_in_cluster) + 1e-9) / (expm1_func(mean_out_cluster) + 1e-9))` is now consistently used to calculate and filter fold changes, in `rank_genes_groups` and `filter_rank_genes_groups` respectively. ; - The default value for arg `min_fold_change=2` has also been changed from 2 to 1, which makes sense. i.e. won't filter out genes based on fold change unless the user explicitly asks it to; - Still no fix for the issue where negative fold changes get automatically filtered out in `filter_rank_genes_groups`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863#issuecomment-776459742
https://github.com/scverse/scanpy/issues/864#issuecomment-1109443073:30,Testability,log,logFC,30,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864#issuecomment-1109443073
https://github.com/scverse/scanpy/issues/864#issuecomment-1109443073:188,Testability,log,loged,188,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864#issuecomment-1109443073
https://github.com/scverse/scanpy/issues/864#issuecomment-1109443073:437,Testability,log,logFC,437,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864#issuecomment-1109443073
https://github.com/scverse/scanpy/issues/864#issuecomment-1109443073:252,Usability,intuit,intuitive,252,"I found this problem too. Now logFC is still calculated in this way, that I am not satisfied with. When we are talking about average fold change of gene expression, the fold change of non-loged average expression is expected. In this way people get an intuitive feeling about how many times a gene is expressed compared with another group. **So the expm() step must be done before the mean() step.** Swap this order not only changes the logFC vaule, but also loses the biological meaning and doesn't make any sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864#issuecomment-1109443073
https://github.com/scverse/scanpy/pull/865#issuecomment-552292197:259,Availability,down,downstream,259,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-552292197
https://github.com/scverse/scanpy/pull/865#issuecomment-552292197:148,Performance,perform,performance,148,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-552292197
https://github.com/scverse/scanpy/pull/865#issuecomment-552292197:297,Testability,test,tested,297,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-552292197
https://github.com/scverse/scanpy/pull/865#issuecomment-552814583:56,Availability,down,downstream,56,"> If we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now. This is quite a compelling argument for me (as I was one of the people who reported an issue like this). If an integer matrix is generally returned, then one would have to ensure all other functions will work with this data type as intended (sc.pp.log1p for example). Otherwise this would be backward-breaking.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-552814583
https://github.com/scverse/scanpy/pull/865#issuecomment-552814583:94,Testability,test,tested,94,"> If we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now. This is quite a compelling argument for me (as I was one of the people who reported an issue like this). If an integer matrix is generally returned, then one would have to ensure all other functions will work with this data type as intended (sc.pp.log1p for example). Otherwise this would be backward-breaking.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-552814583
https://github.com/scverse/scanpy/pull/865#issuecomment-552929823:787,Deployability,pipeline,pipelines,787,"I would just like to add that the issue with `normalize_total` can arise not only from the `downsample_counts` function. . In my case, I am working on `.loom` files generated with *velocyto* - I want to be able to estimate RNA velocity in the end. That means I have 'spliced' and 'unspliced' layers in my anndata object. I wanted to use `normalize_total` on all the layers, which should be possible by setting parameter `layers='all'`. However, I was getting a TypeError, as in #435 . The workaround described at the end of that issue solved it for me. My point is just that fixing only `downsample_counts` is not enough and functions that work on layers should accept integer data. I think that my case is not that uncommon and will happen more often as people use scVelo with velocyto pipelines. . Alternatively, you should warn people about it in the tutorials and make them convert everything to float.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-552929823
https://github.com/scverse/scanpy/pull/865#issuecomment-552929823:292,Modifiability,layers,layers,292,"I would just like to add that the issue with `normalize_total` can arise not only from the `downsample_counts` function. . In my case, I am working on `.loom` files generated with *velocyto* - I want to be able to estimate RNA velocity in the end. That means I have 'spliced' and 'unspliced' layers in my anndata object. I wanted to use `normalize_total` on all the layers, which should be possible by setting parameter `layers='all'`. However, I was getting a TypeError, as in #435 . The workaround described at the end of that issue solved it for me. My point is just that fixing only `downsample_counts` is not enough and functions that work on layers should accept integer data. I think that my case is not that uncommon and will happen more often as people use scVelo with velocyto pipelines. . Alternatively, you should warn people about it in the tutorials and make them convert everything to float.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-552929823
https://github.com/scverse/scanpy/pull/865#issuecomment-552929823:366,Modifiability,layers,layers,366,"I would just like to add that the issue with `normalize_total` can arise not only from the `downsample_counts` function. . In my case, I am working on `.loom` files generated with *velocyto* - I want to be able to estimate RNA velocity in the end. That means I have 'spliced' and 'unspliced' layers in my anndata object. I wanted to use `normalize_total` on all the layers, which should be possible by setting parameter `layers='all'`. However, I was getting a TypeError, as in #435 . The workaround described at the end of that issue solved it for me. My point is just that fixing only `downsample_counts` is not enough and functions that work on layers should accept integer data. I think that my case is not that uncommon and will happen more often as people use scVelo with velocyto pipelines. . Alternatively, you should warn people about it in the tutorials and make them convert everything to float.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-552929823
https://github.com/scverse/scanpy/pull/865#issuecomment-552929823:421,Modifiability,layers,layers,421,"I would just like to add that the issue with `normalize_total` can arise not only from the `downsample_counts` function. . In my case, I am working on `.loom` files generated with *velocyto* - I want to be able to estimate RNA velocity in the end. That means I have 'spliced' and 'unspliced' layers in my anndata object. I wanted to use `normalize_total` on all the layers, which should be possible by setting parameter `layers='all'`. However, I was getting a TypeError, as in #435 . The workaround described at the end of that issue solved it for me. My point is just that fixing only `downsample_counts` is not enough and functions that work on layers should accept integer data. I think that my case is not that uncommon and will happen more often as people use scVelo with velocyto pipelines. . Alternatively, you should warn people about it in the tutorials and make them convert everything to float.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-552929823
https://github.com/scverse/scanpy/pull/865#issuecomment-552929823:648,Modifiability,layers,layers,648,"I would just like to add that the issue with `normalize_total` can arise not only from the `downsample_counts` function. . In my case, I am working on `.loom` files generated with *velocyto* - I want to be able to estimate RNA velocity in the end. That means I have 'spliced' and 'unspliced' layers in my anndata object. I wanted to use `normalize_total` on all the layers, which should be possible by setting parameter `layers='all'`. However, I was getting a TypeError, as in #435 . The workaround described at the end of that issue solved it for me. My point is just that fixing only `downsample_counts` is not enough and functions that work on layers should accept integer data. I think that my case is not that uncommon and will happen more often as people use scVelo with velocyto pipelines. . Alternatively, you should warn people about it in the tutorials and make them convert everything to float.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-552929823
https://github.com/scverse/scanpy/pull/865#issuecomment-558138634:141,Availability,down,downcast,141,"We should definitely maintain the type in layers, and that means maintaining the type in .X makes sense too. We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-558138634
https://github.com/scverse/scanpy/pull/865#issuecomment-558138634:42,Modifiability,layers,layers,42,"We should definitely maintain the type in layers, and that means maintaining the type in .X makes sense too. We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-558138634
https://github.com/scverse/scanpy/pull/865#issuecomment-558449713:310,Availability,down,downcast,310,"Hah, I've gotten much better at numba since I wrote this function. I figured out I can just get the core part to work on floats and don't have to worry about casting between types. Makes this a much easier decision. Now floats aren't converted to integers in the first place. > We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64. I think we can be a little flexible on this, and just generally follow numpy promotion rules (except for when they're bad).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-558449713
https://github.com/scverse/scanpy/pull/865#issuecomment-558449713:451,Modifiability,flexible,flexible,451,"Hah, I've gotten much better at numba since I wrote this function. I figured out I can just get the core part to work on floats and don't have to worry about casting between types. Makes this a much easier decision. Now floats aren't converted to integers in the first place. > We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64. I think we can be a little flexible on this, and just generally follow numpy promotion rules (except for when they're bad).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-558449713
https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:1695,Availability,avail,available,1695,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230
https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:1810,Availability,avail,available,1810,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230
https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:276,Deployability,release,release,276,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230
https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:1638,Modifiability,variab,variable,1638,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230
https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:969,Performance,load,load,969,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230
https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:1520,Usability,simpl,simpler,1520,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230
https://github.com/scverse/scanpy/issues/868#issuecomment-540691814:518,Deployability,release,release,518,"OK, seems like I misunderstood the point about zero inflation here. You just meant “large number of zeroes” as in “pretty sparse” then?. A factor of 10 isn’t that bad for something that’s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our – currently slow but we’ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540691814
https://github.com/scverse/scanpy/issues/868#issuecomment-540691814:290,Modifiability,enhance,enhancement,290,"OK, seems like I misunderstood the point about zero inflation here. You just meant “large number of zeroes” as in “pretty sparse” then?. A factor of 10 isn’t that bad for something that’s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our – currently slow but we’ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540691814
https://github.com/scverse/scanpy/issues/868#issuecomment-540691814:231,Performance,bottleneck,bottleneck,231,"OK, seems like I misunderstood the point about zero inflation here. You just meant “large number of zeroes” as in “pretty sparse” then?. A factor of 10 isn’t that bad for something that’s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our – currently slow but we’ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540691814
https://github.com/scverse/scanpy/issues/868#issuecomment-541384867:50,Availability,avail,available,50,A rough implementation of glmpca in python is now available here: https://github.com/willtownes/glmpca-py . I will try to get it organized as an installable package tomorrow and add unit tests. Issues/ pull requests welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-541384867
https://github.com/scverse/scanpy/issues/868#issuecomment-541384867:145,Deployability,install,installable,145,A rough implementation of glmpca in python is now available here: https://github.com/willtownes/glmpca-py . I will try to get it organized as an installable package tomorrow and add unit tests. Issues/ pull requests welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-541384867
https://github.com/scverse/scanpy/issues/868#issuecomment-541384867:187,Testability,test,tests,187,A rough implementation of glmpca in python is now available here: https://github.com/willtownes/glmpca-py . I will try to get it organized as an installable package tomorrow and add unit tests. Issues/ pull requests welcome.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-541384867
https://github.com/scverse/scanpy/issues/868#issuecomment-541505929:19,Availability,avail,available,19,The package is now available on [pypi](https://pypi.org/project/glmpca/0.1.0/) and there is an [automated test suite](https://github.com/willtownes/glmpca-py/blob/master/tests/glmpca_tests.py).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-541505929
https://github.com/scverse/scanpy/issues/868#issuecomment-541505929:106,Testability,test,test,106,The package is now available on [pypi](https://pypi.org/project/glmpca/0.1.0/) and there is an [automated test suite](https://github.com/willtownes/glmpca-py/blob/master/tests/glmpca_tests.py).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-541505929
https://github.com/scverse/scanpy/issues/868#issuecomment-541505929:170,Testability,test,tests,170,The package is now available on [pypi](https://pypi.org/project/glmpca/0.1.0/) and there is an [automated test suite](https://github.com/willtownes/glmpca-py/blob/master/tests/glmpca_tests.py).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-541505929
https://github.com/scverse/scanpy/issues/868#issuecomment-592476723:106,Integrability,protocol,protocols,106,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-592476723
https://github.com/scverse/scanpy/issues/868#issuecomment-592476723:477,Performance,perform,performance,477,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-592476723
https://github.com/scverse/scanpy/issues/868#issuecomment-592476723:126,Testability,log,log-transforming,126,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-592476723
https://github.com/scverse/scanpy/issues/868#issuecomment-592503518:89,Testability,log,log-transform,89,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions.; One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-592503518
https://github.com/scverse/scanpy/issues/868#issuecomment-592503518:334,Testability,log,log-transforming,334,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions.; One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-592503518
https://github.com/scverse/scanpy/issues/868#issuecomment-592503518:361,Testability,log,log,361,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions.; One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-592503518
https://github.com/scverse/scanpy/issues/868#issuecomment-592503518:1015,Testability,log,log,1015,"My point is: The paper suggests that the reason for the zero inflation idea might be the log-transform, so we should offer a better path from counts to distances. Our current path is to go via normally distributed “expression” values which can be used to calculate distances. Something like fold changes, i.e. what we currently do by log-transforming (because `log(count) = foldchange`):. > counts (→ normalization) → expressions (→ normalization) (→ embedding) → distances → analyses. We use PCA as a latent space embedding here for efficiency purposes but it’s not required, we could calculate distances directly from expressions.; One alternative is to stay with (possibly bias-normalized) counts, and create our latent space from those directly using a suitable model (like GLM-PCA):. > counts (→ normalization) → embedding → distances → analyses. The other alternative is to offer something like SCTransform and stay with our original path, bu better. ---. All of this is of course only super important if the log transform turns out to be maximally problematic (which the amount of successful data analyses using it doesn’t suggest), but I think offering alternatives will definitely be very beneficial!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-592503518
https://github.com/scverse/scanpy/issues/868#issuecomment-593125190:1346,Availability,down,down,1346,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190
https://github.com/scverse/scanpy/issues/868#issuecomment-593125190:1110,Modifiability,variab,variable,1110,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190
https://github.com/scverse/scanpy/issues/868#issuecomment-593125190:645,Testability,log,log-transforming,645,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190
https://github.com/scverse/scanpy/issues/868#issuecomment-593125190:1397,Usability,simpl,simple,1397,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190
https://github.com/scverse/scanpy/issues/868#issuecomment-593266979:207,Availability,avail,available,207,"We can compute any kind of residual in principle (NB right now, Poisson + log normal soon) in batchglm, this is also not restricted to categorical covariates and automatically selects whether closed form is available. The implmentation is numerically very stable. I think it could be a potential backend for this, @willtownes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-593266979
https://github.com/scverse/scanpy/issues/868#issuecomment-593266979:74,Testability,log,log,74,"We can compute any kind of residual in principle (NB right now, Poisson + log normal soon) in batchglm, this is also not restricted to categorical covariates and automatically selects whether closed form is available. The implmentation is numerically very stable. I think it could be a potential backend for this, @willtownes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-593266979
https://github.com/scverse/scanpy/pull/869#issuecomment-540517150:562,Availability,error,error,562,"Here is the summary:; * `_set_default_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications; * `_set_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications; * `_validate_palette()` function added, extracting the relevant code from `scatterplots.py:_get_color_values()`; * `adjust_palette()` was removed as this functionality is replicated to some extent by `_set_default_colors_for_categorical_obs()`; * `add_colors_for_categorical_sample_annotation()` was simplified by using the above functions. FYI: the error from `sc.pl.paga` that I had was caused because the expected output of `adjust_palette()` was `Cycler` but the function actually returned the original type of `palette` which could be `ListedColorMap`, `cabc.Sequence` or `Cycler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869#issuecomment-540517150
https://github.com/scverse/scanpy/pull/869#issuecomment-540517150:512,Usability,simpl,simplified,512,"Here is the summary:; * `_set_default_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications; * `_set_colors_for_categorical_obs()` moved from scatterplots to _utils. No modifications; * `_validate_palette()` function added, extracting the relevant code from `scatterplots.py:_get_color_values()`; * `adjust_palette()` was removed as this functionality is replicated to some extent by `_set_default_colors_for_categorical_obs()`; * `add_colors_for_categorical_sample_annotation()` was simplified by using the above functions. FYI: the error from `sc.pl.paga` that I had was caused because the expected output of `adjust_palette()` was `Cycler` but the function actually returned the original type of `palette` which could be `ListedColorMap`, `cabc.Sequence` or `Cycler`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869#issuecomment-540517150
https://github.com/scverse/scanpy/issues/870#issuecomment-541467843:88,Integrability,Depend,Depending,88,"Hi,. If `adata` is your anndata object, you can get the expression data from `adata.X`. Depending on what you did to the object before, that will contain differently pre-processed data. If you want it for ""Gene A"", then you can use `adata[:,'Gene A'].X`to get the expression value of Gene A in all cells. You can of course put that into a pandas dataframe via `pd.DataFrame()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870#issuecomment-541467843
https://github.com/scverse/scanpy/issues/871#issuecomment-544294477:29,Deployability,upgrade,upgrade,29,I am using 1.4.3. I tried to upgrade to 1.4.4 but I was having a lot of problem with dependencies. Wonder it 1.4.3 is recent enough? Thank you so much.; scanpy 1.4.3 py_0 bioconda,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871#issuecomment-544294477
https://github.com/scverse/scanpy/issues/871#issuecomment-544294477:85,Integrability,depend,dependencies,85,I am using 1.4.3. I tried to upgrade to 1.4.4 but I was having a lot of problem with dependencies. Wonder it 1.4.3 is recent enough? Thank you so much.; scanpy 1.4.3 py_0 bioconda,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871#issuecomment-544294477
https://github.com/scverse/scanpy/issues/871#issuecomment-545908616:45,Deployability,install,installed,45,"What dependency problems do you have? If you installed everything through conda, you should just be able to update it with conda…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871#issuecomment-545908616
https://github.com/scverse/scanpy/issues/871#issuecomment-545908616:108,Deployability,update,update,108,"What dependency problems do you have? If you installed everything through conda, you should just be able to update it with conda…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871#issuecomment-545908616
https://github.com/scverse/scanpy/issues/871#issuecomment-545908616:5,Integrability,depend,dependency,5,"What dependency problems do you have? If you installed everything through conda, you should just be able to update it with conda…",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871#issuecomment-545908616
https://github.com/scverse/scanpy/issues/872#issuecomment-558999208:32,Performance,perform,performance,32,"This sounds interesting. If the performance is acceptable, it might make a good addition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-558999208
https://github.com/scverse/scanpy/issues/872#issuecomment-559179146:119,Security,validat,validation,119,"It's more along the line of selecting number of PCs for denoising but MCV (https://github.com/czbiohub/molecular-cross-validation) is also interesting here. It helps with hyperparameter selection based on reconstruction loss on the hold out ""molecules"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-559179146
https://github.com/scverse/scanpy/issues/872#issuecomment-559186183:67,Modifiability,variab,variable,67,"Yeah, I assumed it might be useful for other algorithms that use a variable number of PCs as input as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-559186183
https://github.com/scverse/scanpy/issues/872#issuecomment-559334707:281,Modifiability,variab,variable,281,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-559334707
https://github.com/scverse/scanpy/issues/872#issuecomment-559334707:417,Modifiability,variab,variable,417,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-559334707
https://github.com/scverse/scanpy/issues/872#issuecomment-559334707:290,Performance,load,loadings,290,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-559334707
https://github.com/scverse/scanpy/issues/872#issuecomment-559334707:569,Testability,log,log,569,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-559334707
https://github.com/scverse/scanpy/issues/872#issuecomment-590110278:132,Availability,robust,robustly,132,I recently had a discussion with @LisaSikkema about how much you can overshoot here. The suggestion was made to use 100 PCs. Can we robustly compute that many or do the numerical methods break down when too little variance is represented by a PC? I recall @falexwolf mentioning this at some point.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-590110278
https://github.com/scverse/scanpy/issues/872#issuecomment-590110278:193,Availability,down,down,193,I recently had a discussion with @LisaSikkema about how much you can overshoot here. The suggestion was made to use 100 PCs. Can we robustly compute that many or do the numerical methods break down when too little variance is represented by a PC? I recall @falexwolf mentioning this at some point.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-590110278
https://github.com/scverse/scanpy/issues/872#issuecomment-822057877:186,Modifiability,variab,variability,186,"@chris-rands my two cents here: Using two many PCs (50 or 100) might not be such a good idea because most PCs above the first 15-20 (rough figure) are likely to represent non-biological variability (e.g., batch effects), so their inclusion in the analysis might lead to the identification of clusters that are not biologically-relevant but are rather technical noise. That's why it is recommended to use either a Jackstraw plot or an elbow plot to identify the optimal number of PCs to avoid including PCs that correspond to technical variation rather than biological heterogeneity. Therefore, overshooting the number of PCs used from 10 to 20 is not that big a deal, but using 50 is NOT ideal by any means.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822057877
https://github.com/scverse/scanpy/issues/872#issuecomment-822057877:486,Safety,avoid,avoid,486,"@chris-rands my two cents here: Using two many PCs (50 or 100) might not be such a good idea because most PCs above the first 15-20 (rough figure) are likely to represent non-biological variability (e.g., batch effects), so their inclusion in the analysis might lead to the identification of clusters that are not biologically-relevant but are rather technical noise. That's why it is recommended to use either a Jackstraw plot or an elbow plot to identify the optimal number of PCs to avoid including PCs that correspond to technical variation rather than biological heterogeneity. Therefore, overshooting the number of PCs used from 10 to 20 is not that big a deal, but using 50 is NOT ideal by any means.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822057877
https://github.com/scverse/scanpy/issues/872#issuecomment-822286073:111,Availability,down,downstream,111,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically.; > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well?. I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type?. Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822286073
https://github.com/scverse/scanpy/issues/872#issuecomment-822286073:353,Availability,down,downstream,353,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically.; > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well?. I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type?. Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822286073
https://github.com/scverse/scanpy/issues/872#issuecomment-822286073:705,Availability,down,downstream,705,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically.; > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well?. I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type?. Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822286073
https://github.com/scverse/scanpy/issues/872#issuecomment-822286073:987,Modifiability,variab,variability,987,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically.; > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well?. I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type?. Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822286073
https://github.com/scverse/scanpy/issues/872#issuecomment-822286073:1051,Modifiability,variab,variability,1051,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically.; > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well?. I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type?. Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822286073
https://github.com/scverse/scanpy/issues/872#issuecomment-822286073:1117,Modifiability,variab,variability,1117,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically.; > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well?. I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type?. Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822286073
https://github.com/scverse/scanpy/issues/872#issuecomment-822286073:1311,Modifiability,variab,variability,1311,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically.; > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well?. I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type?. Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822286073
https://github.com/scverse/scanpy/issues/872#issuecomment-822286073:342,Performance,perform,performing,342,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically.; > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well?. I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type?. Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822286073
https://github.com/scverse/scanpy/issues/872#issuecomment-822621611:932,Deployability,integrat,integration,932,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph?. I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822621611
https://github.com/scverse/scanpy/issues/872#issuecomment-822621611:932,Integrability,integrat,integration,932,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph?. I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822621611
https://github.com/scverse/scanpy/issues/872#issuecomment-822621611:638,Performance,load,loadings,638,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph?. I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822621611
https://github.com/scverse/scanpy/issues/873#issuecomment-542637756:652,Deployability,Integrat,Integrate,652,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```; sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle; sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None); #OR; import mnnpy; mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata); ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. ; ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch); ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873#issuecomment-542637756
https://github.com/scverse/scanpy/issues/873#issuecomment-542637756:652,Integrability,Integrat,Integrate,652,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```; sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle; sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None); #OR; import mnnpy; mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata); ```. Here，I attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002，and 009 were grouped together on left , most part of sample 003 was on the right. ; ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch); ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873#issuecomment-542637756
https://github.com/scverse/scanpy/issues/873#issuecomment-542686602:24,Deployability,integrat,integration,24,"Hey!. You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:; `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873#issuecomment-542686602
https://github.com/scverse/scanpy/issues/873#issuecomment-542686602:24,Integrability,integrat,integration,24,"Hey!. You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:; `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873#issuecomment-542686602
https://github.com/scverse/scanpy/issues/873#issuecomment-543582157:145,Deployability,integrat,integrated,145,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873#issuecomment-543582157
https://github.com/scverse/scanpy/issues/873#issuecomment-543582157:145,Integrability,integrat,integrated,145,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873#issuecomment-543582157
https://github.com/scverse/scanpy/issues/873#issuecomment-544365236:119,Availability,error,error,119,"You just saw in your output line [7], that you get back a tuple from `mnn_correct()`. This is also what it says in the error you get. Thus, `adata[0]` is your anndata object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873#issuecomment-544365236
https://github.com/scverse/scanpy/issues/874#issuecomment-544246058:16,Deployability,install,install,16,"Hi, how did you install everything? With conda or pip? Does reinstalling MulticoreTSNE and/or scikit-learn help?. This is most certainly not scanpy’s problem, we don’t have any compiled code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874#issuecomment-544246058
https://github.com/scverse/scanpy/issues/874#issuecomment-544246058:101,Usability,learn,learn,101,"Hi, how did you install everything? With conda or pip? Does reinstalling MulticoreTSNE and/or scikit-learn help?. This is most certainly not scanpy’s problem, we don’t have any compiled code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874#issuecomment-544246058
https://github.com/scverse/scanpy/issues/875#issuecomment-545958927:336,Usability,simpl,simplify,336,"Hi! thank you for the praise! I’m afraid this is a documentation bug!. We had a bit of an issue with documented parameters not matching real ones. It’s fixed in the [latest docs](https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html) where `scatter` doesn’t say it has a `kwargs` or `vmax` parameter. We should simplify our plotting. @VolkerBergen has some nice plotting functions, but implemented them as part of scvelo, not scanpy as he should have :stuck_out_tongue_winking_eye:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875#issuecomment-545958927
https://github.com/scverse/scanpy/issues/876#issuecomment-545947984:212,Deployability,Install,Installing,212,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545947984
https://github.com/scverse/scanpy/issues/876#issuecomment-545947984:308,Deployability,install,installs,308,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545947984
https://github.com/scverse/scanpy/issues/876#issuecomment-545947984:374,Deployability,install,installations,374,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545947984
https://github.com/scverse/scanpy/issues/876#issuecomment-545947984:487,Deployability,install,installation,487,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545947984
https://github.com/scverse/scanpy/issues/876#issuecomment-545947984:569,Usability,guid,guides,569,"Well, @biocondabot doesn’t know this. Please report in https://github.com/bioconda/bioconda-recipes/blob/master/recipes/scanpy/meta.yaml. PS: Since I know from which company you are, some free consulting :wink:: Installing conda inside of docker images wouldn’t be a pain I’d be willing to go through. Conda installs a whole parallel universe of native libraries and python installations. A container is much easier to debug and much lighter on the resources if you use a regular python installation and pip. I’d rather [host a small PyPI](https://packaging.python.org/guides/hosting-your-own-index/) to hold precompiled wheels of louvain-igraph and so on than use conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545947984
https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:51,Deployability,release,release,51,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170
https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:415,Deployability,update,updates,415,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170
https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:292,Energy Efficiency,drain,draining,292,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170
https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:243,Integrability,depend,dependencies,243,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170
https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:423,Integrability,depend,dependencies,423,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170
https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:389,Usability,simpl,simply,389,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didn’t check the dependencies changing (very understandable, it’s draining to search where they’re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and I’m sure they’ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170
https://github.com/scverse/scanpy/issues/882#issuecomment-545433846:148,Deployability,pipeline,pipelines,148,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py; samples = []; for sample in range(1, 10):; s = read(; path / f'{sample}.matrix.mtx',; cache=cache,; cache_compression=cache_compression,; ).T; genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); s.var_names = genes[0]; s.var['gene_symbols'] = genes[1].values; s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; samples.append(s); adata = AnnData.concatenate(samples); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846
https://github.com/scverse/scanpy/issues/882#issuecomment-545433846:270,Energy Efficiency,adapt,adapt,270,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py; samples = []; for sample in range(1, 10):; s = read(; path / f'{sample}.matrix.mtx',; cache=cache,; cache_compression=cache_compression,; ).T; genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); s.var_names = genes[0]; s.var['gene_symbols'] = genes[1].values; s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; samples.append(s); adata = AnnData.concatenate(samples); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846
https://github.com/scverse/scanpy/issues/882#issuecomment-545433846:270,Modifiability,adapt,adapt,270,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py; samples = []; for sample in range(1, 10):; s = read(; path / f'{sample}.matrix.mtx',; cache=cache,; cache_compression=cache_compression,; ).T; genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); s.var_names = genes[0]; s.var['gene_symbols'] = genes[1].values; s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; samples.append(s); adata = AnnData.concatenate(samples); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846
https://github.com/scverse/scanpy/issues/882#issuecomment-545433846:548,Performance,cache,cache,548,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py; samples = []; for sample in range(1, 10):; s = read(; path / f'{sample}.matrix.mtx',; cache=cache,; cache_compression=cache_compression,; ).T; genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); s.var_names = genes[0]; s.var['gene_symbols'] = genes[1].values; s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; samples.append(s); adata = AnnData.concatenate(samples); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846
https://github.com/scverse/scanpy/issues/882#issuecomment-545433846:554,Performance,cache,cache,554,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py; samples = []; for sample in range(1, 10):; s = read(; path / f'{sample}.matrix.mtx',; cache=cache,; cache_compression=cache_compression,; ).T; genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); s.var_names = genes[0]; s.var['gene_symbols'] = genes[1].values; s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; samples.append(s); adata = AnnData.concatenate(samples); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846
https://github.com/scverse/scanpy/issues/882#issuecomment-545433846:241,Safety,predict,predict,241,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py; samples = []; for sample in range(1, 10):; s = read(; path / f'{sample}.matrix.mtx',; cache=cache,; cache_compression=cache_compression,; ).T; genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); s.var_names = genes[0]; s.var['gene_symbols'] = genes[1].values; s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; samples.append(s); adata = AnnData.concatenate(samples); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846
https://github.com/scverse/scanpy/issues/882#issuecomment-551408602:284,Performance,Load,Load,284,"I thought this would be useful. I recently got a few datasets that were renamed and/or in a different folder structure and I thought it would be good if one could specify that. Something like . ````; def read(folder,mtx_file=None,features_file=None,...):; if mtx_file is not None:; # Load mtx file; else: ; # Fall back to load from folder; ````. Again, thank you so much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-551408602
https://github.com/scverse/scanpy/issues/882#issuecomment-551408602:322,Performance,load,load,322,"I thought this would be useful. I recently got a few datasets that were renamed and/or in a different folder structure and I thought it would be good if one could specify that. Something like . ````; def read(folder,mtx_file=None,features_file=None,...):; if mtx_file is not None:; # Load mtx file; else: ; # Fall back to load from folder; ````. Again, thank you so much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-551408602
https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694:427,Availability,reliab,reliably,427,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming?; If yes, scanpy should indeed be able to deal with this;; If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:; > ; > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341; > ; > Took me 3 minutes:; > ; > ```python; > samples = []; > for sample in range(1, 10):; > s = read(; > path / f'{sample}.matrix.mtx',; > cache=cache,; > cache_compression=cache_compression,; > ).T; > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); > s.var_names = genes[0]; > s.var['gene_symbols'] = genes[1].values; > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; > samples.append(s); > adata = AnnData.concatenate(samples); > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694
https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694:270,Deployability,pipeline,pipeline,270,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming?; If yes, scanpy should indeed be able to deal with this;; If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:; > ; > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341; > ; > Took me 3 minutes:; > ; > ```python; > samples = []; > for sample in range(1, 10):; > s = read(; > path / f'{sample}.matrix.mtx',; > cache=cache,; > cache_compression=cache_compression,; > ).T; > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); > s.var_names = genes[0]; > s.var['gene_symbols'] = genes[1].values; > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; > samples.append(s); > adata = AnnData.concatenate(samples); > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694
https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694:674,Deployability,pipeline,pipelines,674,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming?; If yes, scanpy should indeed be able to deal with this;; If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:; > ; > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341; > ; > Took me 3 minutes:; > ; > ```python; > samples = []; > for sample in range(1, 10):; > s = read(; > path / f'{sample}.matrix.mtx',; > cache=cache,; > cache_compression=cache_compression,; > ).T; > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); > s.var_names = genes[0]; > s.var['gene_symbols'] = genes[1].values; > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; > samples.append(s); > adata = AnnData.concatenate(samples); > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694
https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694:796,Energy Efficiency,adapt,adapt,796,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming?; If yes, scanpy should indeed be able to deal with this;; If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:; > ; > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341; > ; > Took me 3 minutes:; > ; > ```python; > samples = []; > for sample in range(1, 10):; > s = read(; > path / f'{sample}.matrix.mtx',; > cache=cache,; > cache_compression=cache_compression,; > ).T; > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); > s.var_names = genes[0]; > s.var['gene_symbols'] = genes[1].values; > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; > samples.append(s); > adata = AnnData.concatenate(samples); > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694
https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694:796,Modifiability,adapt,adapt,796,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming?; If yes, scanpy should indeed be able to deal with this;; If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:; > ; > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341; > ; > Took me 3 minutes:; > ; > ```python; > samples = []; > for sample in range(1, 10):; > s = read(; > path / f'{sample}.matrix.mtx',; > cache=cache,; > cache_compression=cache_compression,; > ).T; > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); > s.var_names = genes[0]; > s.var['gene_symbols'] = genes[1].values; > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; > samples.append(s); > adata = AnnData.concatenate(samples); > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694
https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694:1106,Performance,cache,cache,1106,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming?; If yes, scanpy should indeed be able to deal with this;; If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:; > ; > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341; > ; > Took me 3 minutes:; > ; > ```python; > samples = []; > for sample in range(1, 10):; > s = read(; > path / f'{sample}.matrix.mtx',; > cache=cache,; > cache_compression=cache_compression,; > ).T; > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); > s.var_names = genes[0]; > s.var['gene_symbols'] = genes[1].values; > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; > samples.append(s); > adata = AnnData.concatenate(samples); > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694
https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694:1112,Performance,cache,cache,1112,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming?; If yes, scanpy should indeed be able to deal with this;; If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:; > ; > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341; > ; > Took me 3 minutes:; > ; > ```python; > samples = []; > for sample in range(1, 10):; > s = read(; > path / f'{sample}.matrix.mtx',; > cache=cache,; > cache_compression=cache_compression,; > ).T; > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); > s.var_names = genes[0]; > s.var['gene_symbols'] = genes[1].values; > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; > samples.append(s); > adata = AnnData.concatenate(samples); > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694
https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694:767,Safety,predict,predict,767,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming?; If yes, scanpy should indeed be able to deal with this;; If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:; > ; > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341; > ; > Took me 3 minutes:; > ; > ```python; > samples = []; > for sample in range(1, 10):; > s = read(; > path / f'{sample}.matrix.mtx',; > cache=cache,; > cache_compression=cache_compression,; > ).T; > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); > s.var_names = genes[0]; > s.var['gene_symbols'] = genes[1].values; > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; > samples.append(s); > adata = AnnData.concatenate(samples); > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694
https://github.com/scverse/scanpy/issues/882#issuecomment-2002523593:36,Deployability,pipeline,pipeline,36,"> Is there a widely used processing pipeline which does not adhere to this file naming?. STARsolo generates cell-ranger compatible output, and when multiple multi-mapper resolution strategies are enabled, it will write multiple matrix.mtx.gz files, with different names. e.g: `STARsolo ... --soloMultiMappers Unique EM PropUnique Rescue Uniform` yields:. ```; barcodes.tsv.gz; features.tsv.gz; matrix.mtx.gz; UniqueAndMult-EM.mtx.gz; UniqueAndMult-PropUnique.mtx.gz; UniqueAndMult-Rescue.mtx.gz; UniqueAndMult-Uniform.mtx.gz; ```. Each of these `*.mtx.gz` files matches the same format as `matrix.mtx.gz` and can be read in the same way. (They all share the `*.tsv.gz` files). . A 3-parameter version of the `read_10x_mtx()` function would be my vote as the most flexible option.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-2002523593
https://github.com/scverse/scanpy/issues/882#issuecomment-2002523593:763,Modifiability,flexible,flexible,763,"> Is there a widely used processing pipeline which does not adhere to this file naming?. STARsolo generates cell-ranger compatible output, and when multiple multi-mapper resolution strategies are enabled, it will write multiple matrix.mtx.gz files, with different names. e.g: `STARsolo ... --soloMultiMappers Unique EM PropUnique Rescue Uniform` yields:. ```; barcodes.tsv.gz; features.tsv.gz; matrix.mtx.gz; UniqueAndMult-EM.mtx.gz; UniqueAndMult-PropUnique.mtx.gz; UniqueAndMult-Rescue.mtx.gz; UniqueAndMult-Uniform.mtx.gz; ```. Each of these `*.mtx.gz` files matches the same format as `matrix.mtx.gz` and can be read in the same way. (They all share the `*.tsv.gz` files). . A 3-parameter version of the `read_10x_mtx()` function would be my vote as the most flexible option.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-2002523593
https://github.com/scverse/scanpy/issues/883#issuecomment-545478497:426,Availability,ERROR,ERROR,426,"Not sure if still required (the code in the anndata repo is close to this), but here is a minimal example:; ```py; import scanpy as sc; adata = sc.datasets.pbmc3k(); h5adfile = 'pbmc3k.h5ad'; adata.write(h5adfile); a1 = sc.read_h5ad(h5adfile); a2 = sc.read_h5ad(h5adfile, backed='r+'); sc.tl.score_genes(a1, ['KIR3DL2-1', 'AL590523.1', 'CT476828.1']); # OK; sc.tl.score_genes(a2, ['KIR3DL2-1', 'AL590523.1', 'CT476828.1']); # ERROR ; ```. thanks..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883#issuecomment-545478497
https://github.com/scverse/scanpy/issues/884#issuecomment-545328026:59,Usability,guid,guides,59,"Hi @a-munoz-rojas, please use [fenced code blocks](https://guides.github.com/features/mastering-markdown/#GitHub-flavored-markdown) for code. The “language” for tracebacks is “pytb”: ```` ```pytb````. @ivirshup do you know what’s happening here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884#issuecomment-545328026
https://github.com/scverse/scanpy/issues/884#issuecomment-554084273:409,Modifiability,variab,variables,409,"Thanks for your replies and tips on the using fenced code blocks! I did verify that removing `adata.varm` before saving the `.raw` object gets rid of this problem. . On a tangentially related note, is there an easy way to restore a new adata object from a `.raw` object? There are some cases where I would like to re-do some analysis on the full, `.raw` object that still retains the original non-transformed variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884#issuecomment-554084273
https://github.com/scverse/scanpy/issues/887#issuecomment-545561632:99,Testability,test,testing,99,"minimal reproducible example:; ```; import scanpy as sc ; adata = sc.datasets.blobs() ; adata.uns['testing'] = {'a':[1,2,4], 'b':[""1"",""2"",""3""]} ; adata.write('test.h5ad') ; ... storing 'blobs' as categorical. a2 = sc.read('test.h5ad') ; a2.uns ; Out: {'testing': {'a': array([1, 2, 4]), 'b': array(['1', '2', '3'], dtype=object)}}. adata.uns ; Out: OrderedDict([('testing', {'a': [1, 2, 4], 'b': ['1', '2', '3']})]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887#issuecomment-545561632
https://github.com/scverse/scanpy/issues/887#issuecomment-545561632:159,Testability,test,test,159,"minimal reproducible example:; ```; import scanpy as sc ; adata = sc.datasets.blobs() ; adata.uns['testing'] = {'a':[1,2,4], 'b':[""1"",""2"",""3""]} ; adata.write('test.h5ad') ; ... storing 'blobs' as categorical. a2 = sc.read('test.h5ad') ; a2.uns ; Out: {'testing': {'a': array([1, 2, 4]), 'b': array(['1', '2', '3'], dtype=object)}}. adata.uns ; Out: OrderedDict([('testing', {'a': [1, 2, 4], 'b': ['1', '2', '3']})]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887#issuecomment-545561632
https://github.com/scverse/scanpy/issues/887#issuecomment-545561632:223,Testability,test,test,223,"minimal reproducible example:; ```; import scanpy as sc ; adata = sc.datasets.blobs() ; adata.uns['testing'] = {'a':[1,2,4], 'b':[""1"",""2"",""3""]} ; adata.write('test.h5ad') ; ... storing 'blobs' as categorical. a2 = sc.read('test.h5ad') ; a2.uns ; Out: {'testing': {'a': array([1, 2, 4]), 'b': array(['1', '2', '3'], dtype=object)}}. adata.uns ; Out: OrderedDict([('testing', {'a': [1, 2, 4], 'b': ['1', '2', '3']})]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887#issuecomment-545561632
https://github.com/scverse/scanpy/issues/887#issuecomment-545561632:253,Testability,test,testing,253,"minimal reproducible example:; ```; import scanpy as sc ; adata = sc.datasets.blobs() ; adata.uns['testing'] = {'a':[1,2,4], 'b':[""1"",""2"",""3""]} ; adata.write('test.h5ad') ; ... storing 'blobs' as categorical. a2 = sc.read('test.h5ad') ; a2.uns ; Out: {'testing': {'a': array([1, 2, 4]), 'b': array(['1', '2', '3'], dtype=object)}}. adata.uns ; Out: OrderedDict([('testing', {'a': [1, 2, 4], 'b': ['1', '2', '3']})]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887#issuecomment-545561632
https://github.com/scverse/scanpy/issues/887#issuecomment-545561632:364,Testability,test,testing,364,"minimal reproducible example:; ```; import scanpy as sc ; adata = sc.datasets.blobs() ; adata.uns['testing'] = {'a':[1,2,4], 'b':[""1"",""2"",""3""]} ; adata.write('test.h5ad') ; ... storing 'blobs' as categorical. a2 = sc.read('test.h5ad') ; a2.uns ; Out: {'testing': {'a': array([1, 2, 4]), 'b': array(['1', '2', '3'], dtype=object)}}. adata.uns ; Out: OrderedDict([('testing', {'a': [1, 2, 4], 'b': ['1', '2', '3']})]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887#issuecomment-545561632
https://github.com/scverse/scanpy/issues/887#issuecomment-545561793:161,Usability,learn,learn,161,"oh, and versions:; ```; scanpy==1.4.3+116.g0075c62 ; anndata==0.6.22.post2.dev80+g72c2bde ; umap==0.3.9 ; numpy==1.17.2 ; scipy==1.3.0 ; pandas==0.24.1 ; scikit-learn==0.21.3 ; statsmodels==0.10.0rc2 ; ython-igraph==0.7.1 ; louvain==0.6.1; ```. and h5py version 2.9.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887#issuecomment-545561793
https://github.com/scverse/scanpy/issues/887#issuecomment-545948736:66,Modifiability,extend,extend,66,Okay... scvelo uses `adata.uns['velocity_settings']['embeddings'].extend()` which only works on lists. @VolkerBergen shall i report this again in the `scvelo` repo or is this sufficient for you?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887#issuecomment-545948736
https://github.com/scverse/scanpy/issues/888#issuecomment-547828011:35,Deployability,release,release,35,"Well, we *really* need to get that release out of the door. But on the other hand, as @gokceneraslan said the current behavior is a bug, so we can change it now!. Gökçen, do you still plan on consolidating those PRs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/888#issuecomment-547828011
https://github.com/scverse/scanpy/issues/889#issuecomment-590643431:1070,Availability,error,errors,1070,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects; ```pytb; data.obs.dtypes; ClusterID int32; ClusterName object; RNA_snn_res_0_5 object; nCount_RNA float32; nFeature_RNA int32; orig_ident object; percent_mt float32; seurat_clusters object; louvain category; dtype: object; ```. As a quick fix, I think you can do something like this:; ```python; adata = data.copy(); obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]; adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') ; sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True); ```; Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889#issuecomment-590643431
https://github.com/scverse/scanpy/issues/889#issuecomment-590643431:214,Deployability,continuous,continuous,214,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects; ```pytb; data.obs.dtypes; ClusterID int32; ClusterName object; RNA_snn_res_0_5 object; nCount_RNA float32; nFeature_RNA int32; orig_ident object; percent_mt float32; seurat_clusters object; louvain category; dtype: object; ```. As a quick fix, I think you can do something like this:; ```python; adata = data.copy(); obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]; adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') ; sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True); ```; Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889#issuecomment-590643431
https://github.com/scverse/scanpy/issues/889#issuecomment-590643431:133,Modifiability,variab,variable,133,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects; ```pytb; data.obs.dtypes; ClusterID int32; ClusterName object; RNA_snn_res_0_5 object; nCount_RNA float32; nFeature_RNA int32; orig_ident object; percent_mt float32; seurat_clusters object; louvain category; dtype: object; ```. As a quick fix, I think you can do something like this:; ```python; adata = data.copy(); obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]; adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') ; sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True); ```; Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889#issuecomment-590643431
https://github.com/scverse/scanpy/issues/889#issuecomment-590643431:225,Modifiability,variab,variable,225,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects; ```pytb; data.obs.dtypes; ClusterID int32; ClusterName object; RNA_snn_res_0_5 object; nCount_RNA float32; nFeature_RNA int32; orig_ident object; percent_mt float32; seurat_clusters object; louvain category; dtype: object; ```. As a quick fix, I think you can do something like this:; ```python; adata = data.copy(); obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]; adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') ; sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True); ```; Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889#issuecomment-590643431
https://github.com/scverse/scanpy/issues/889#issuecomment-590643431:351,Modifiability,variab,variables,351,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects; ```pytb; data.obs.dtypes; ClusterID int32; ClusterName object; RNA_snn_res_0_5 object; nCount_RNA float32; nFeature_RNA int32; orig_ident object; percent_mt float32; seurat_clusters object; louvain category; dtype: object; ```. As a quick fix, I think you can do something like this:; ```python; adata = data.copy(); obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]; adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') ; sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True); ```; Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889#issuecomment-590643431
https://github.com/scverse/scanpy/issues/889#issuecomment-590643431:1042,Safety,avoid,avoid,1042,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects; ```pytb; data.obs.dtypes; ClusterID int32; ClusterName object; RNA_snn_res_0_5 object; nCount_RNA float32; nFeature_RNA int32; orig_ident object; percent_mt float32; seurat_clusters object; louvain category; dtype: object; ```. As a quick fix, I think you can do something like this:; ```python; adata = data.copy(); obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]; adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') ; sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True); ```; Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889#issuecomment-590643431
https://github.com/scverse/scanpy/pull/893#issuecomment-546319097:94,Testability,test,test,94,"Great! Can’t say I understand the #890-related fix though: What went wrong before? Is there a test for it?. Please use 4 space indentation, not visual indentation. Basically, running `black` on the any newly changed code should yield minimal changes. Feel free to remove a file where you changed a lot from here:. https://github.com/theislab/scanpy/blob/b3933ac185f9af3908261e939fc5df2336f1932e/pyproject.toml#L9-L13. Then everything will be done automatically. You’ll just have to go through the changes and fix ugly ones like black making `some = code[:] # comment` into `some = (\n code\n) # comment` instead of `#comment\nsome = code`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893#issuecomment-546319097
https://github.com/scverse/scanpy/pull/893#issuecomment-546330077:45,Testability,test,tested,45,"@flying-sheep For #890 probably we had never tested that combination of parameters because the output was a broking image. . If I understand you correctly, black can by applied to only some lines? Apparently PyCharm can be used with black, do you have any experience?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893#issuecomment-546330077
https://github.com/scverse/scanpy/pull/893#issuecomment-546341102:62,Deployability,continuous,continuous,62,"One question related to #891, is there any plotting order for continuous values like higher expression plotted on top? That's more controversial but sometimes dropout et al might obscure cells expressing a gene. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893#issuecomment-546341102
https://github.com/scverse/scanpy/pull/893#issuecomment-546448504:24,Testability,test,tested,24,"> probably we had never tested that combination of parameters because the output was a broking image. got it!. > If I understand you correctly, black can by applied to only some lines? Apparently PyCharm can be used with black, do you have any experience?. AFAIK black can’t be applied to some lines only. Thus my suggestion to just run it on files where you changed a lot. [about pycharm](https://black.readthedocs.io/en/stable/editor_integration.html)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893#issuecomment-546448504
https://github.com/scverse/scanpy/issues/895#issuecomment-546839182:246,Testability,log,logging,246,"As in most Python projects, functions that aren’t in the docs are considered private and not for use from different packages. This is therefore a bug in `desc`: eleozzr/desc#13. As a workaround, you can of course assign the function: `scanpy.api.logging.msg = lambda ...: ...`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895#issuecomment-546839182
https://github.com/scverse/scanpy/pull/896#issuecomment-547361635:99,Integrability,wrap,wrap,99,"Thank you! As it stands, this destroys backwards compatibility. But fear not:. 1. Add [`legacy-api-wrap`](https://pypi.org/project/legacy-api-wrap/) to requirements.txt and docs/requirements.txt; 2. Use it to change the API to 1-2 positional parameters (i.e. either `def magic(adata: ..., *, name_list: ... = None, ...)` or with the `*` one parameter to the right); 3. While you’re at it, change `name_list`’s type to `Union[Literal['all_genes', 'pca_only'], Sequence[str], None]`. You can do `from .._compat import Literal` or so (maybe a dot less or more)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896#issuecomment-547361635
https://github.com/scverse/scanpy/pull/896#issuecomment-547361635:142,Integrability,wrap,wrap,142,"Thank you! As it stands, this destroys backwards compatibility. But fear not:. 1. Add [`legacy-api-wrap`](https://pypi.org/project/legacy-api-wrap/) to requirements.txt and docs/requirements.txt; 2. Use it to change the API to 1-2 positional parameters (i.e. either `def magic(adata: ..., *, name_list: ... = None, ...)` or with the `*` one parameter to the right); 3. While you’re at it, change `name_list`’s type to `Union[Literal['all_genes', 'pca_only'], Sequence[str], None]`. You can do `from .._compat import Literal` or so (maybe a dot less or more)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896#issuecomment-547361635
https://github.com/scverse/scanpy/pull/896#issuecomment-547448945:7,Usability,learn,learned,7,"Wow, I learned something new! Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896#issuecomment-547448945
https://github.com/scverse/scanpy/issues/897#issuecomment-556743231:93,Modifiability,config,config,93,"I don't think so, not unless you call `sc.set_figure_params()`. But this modifies the global config.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897#issuecomment-556743231
https://github.com/scverse/scanpy/pull/899#issuecomment-548282891:28,Testability,test,test,28,"Thank you, judging from the test pics, those look like very useful changes!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/899#issuecomment-548282891
https://github.com/scverse/scanpy/issues/900#issuecomment-549045842:59,Deployability,install,install,59,This seems to be a problem related to h5py build.; You can install h5py from the wheel [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#h5py) or use conda package manager.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900#issuecomment-549045842
https://github.com/scverse/scanpy/pull/903#issuecomment-555603337:435,Integrability,depend,depending,435,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names; - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers; - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903#issuecomment-555603337
https://github.com/scverse/scanpy/pull/903#issuecomment-555603337:366,Modifiability,layers,layers,366,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names; - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers; - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903#issuecomment-555603337
https://github.com/scverse/scanpy/issues/905#issuecomment-552922125:31,Usability,clear,clear,31,"Maybe the documentation is not clear; ```; counts_per_cell_after : float or None, optional (default: None); If None, after normalization, each cell has a total count equal to the median of the counts_per_cell before normalization.; ```; I thought that only ""counts_per_cell_after"" would multiply by the median, but the argument is:; ```; sc.pp.normalize_per_cell( # normalize with total UMI count per cell; adata, key_n_counts='n_counts_all'); ```; not ""counts_per_cell_after"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905#issuecomment-552922125
https://github.com/scverse/scanpy/issues/905#issuecomment-557090359:31,Usability,clear,clear,31,"For me the documentation seems clear, but the example may be confusing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905#issuecomment-557090359
https://github.com/scverse/scanpy/issues/906#issuecomment-551413589:7,Deployability,install,installed,7,"Yes, I installed the development version of scanpy, and it worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/906#issuecomment-551413589
https://github.com/scverse/scanpy/issues/913#issuecomment-552997835:50,Testability,test,tested,50,"I actually had not set that attribute. But I just tested it now. ```; sc.settings.n_jobs = 15; sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11); ```. OR. ```; sc.settings.n_jobs = 15; with parallel_backend('threading', n_jobs=15):; sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11); ```. and in either case it only uses one cpu and takes the same amount of time as above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-552997835
https://github.com/scverse/scanpy/issues/913#issuecomment-553010671:147,Deployability,install,install,147,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something?. I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553010671
https://github.com/scverse/scanpy/issues/913#issuecomment-553010671:241,Deployability,install,installed,241,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something?. I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553010671
https://github.com/scverse/scanpy/issues/913#issuecomment-553010671:40,Usability,learn,learn,40,"the latest umap version on pypi is umap-learn 0.3.10 which is the version I have and also the version on Github as far as I can tell. Do I need to install in another way or from a development branch or something?. I did not have pynndescent installed but just added it: pynndescent-0.3.3. However, it still is not using parallelization in the KNN calculation with either of the commands above",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553010671
https://github.com/scverse/scanpy/issues/913#issuecomment-553019440:155,Availability,error,error,155,"OK I install umap 0.4 . ```; pip install git+git://github.com/lmcinnes/umap@0.4dev; ```. However, it doesn't seem to run any faster and actually throws an error now. ```; sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11); ```; gives; ```; AttributeError Traceback (most recent call last); <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy); 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,; 94 method=method, metric=metric, metric_kwds=metric_kwds,; ---> 95 random_state=random_state,; 96 ); 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 681 knn_distances,; 682 self._adata.shape[0],; --> 683 self.n_neighbors,; 684 ); 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors); 323 ; --> 324 return distances, connectivities.tocsr(); 325 ; 326 . AttributeError: 'tuple' object has no attribute 'tocsr'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553019440
https://github.com/scverse/scanpy/issues/913#issuecomment-553019440:5,Deployability,install,install,5,"OK I install umap 0.4 . ```; pip install git+git://github.com/lmcinnes/umap@0.4dev; ```. However, it doesn't seem to run any faster and actually throws an error now. ```; sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11); ```; gives; ```; AttributeError Traceback (most recent call last); <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy); 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,; 94 method=method, metric=metric, metric_kwds=metric_kwds,; ---> 95 random_state=random_state,; 96 ); 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 681 knn_distances,; 682 self._adata.shape[0],; --> 683 self.n_neighbors,; 684 ); 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors); 323 ; --> 324 return distances, connectivities.tocsr(); 325 ; 326 . AttributeError: 'tuple' object has no attribute 'tocsr'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553019440
https://github.com/scverse/scanpy/issues/913#issuecomment-553019440:33,Deployability,install,install,33,"OK I install umap 0.4 . ```; pip install git+git://github.com/lmcinnes/umap@0.4dev; ```. However, it doesn't seem to run any faster and actually throws an error now. ```; sc.pp.neighbors(adata_B, n_neighbors=100, n_pcs=11); ```; gives; ```; AttributeError Traceback (most recent call last); <timed eval> in <module>. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy); 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,; 94 method=method, metric=metric, metric_kwds=metric_kwds,; ---> 95 random_state=random_state,; 96 ); 97 adata.uns['neighbors'] = {}. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 681 knn_distances,; 682 self._adata.shape[0],; --> 683 self.n_neighbors,; 684 ); 685 # overwrite the umap connectivities if method is 'gauss'. /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors); 323 ; --> 324 return distances, connectivities.tocsr(); 325 ; 326 . AttributeError: 'tuple' object has no attribute 'tocsr'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553019440
https://github.com/scverse/scanpy/issues/913#issuecomment-553030310:21,Deployability,install,installed,21,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8; umap-learn 0.4.0; pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```; sc.settings.n_jobs = 15; with parallel_backend('threading', n_jobs=15):; sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12); ```. gives the warning. ```; /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: ; The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:; @numba.njit(parallel=True); def nn_descent(; ^. self.func_ir.loc)); ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553030310
https://github.com/scverse/scanpy/issues/913#issuecomment-553030310:87,Usability,learn,learn,87,"So now I have scanpy installed from master branch:. scanpy 1.4.5.dev175+g64f04d8; umap-learn 0.4.0; pynndescent 0.3.3. but still no luck with any of the commands above with or without first specifying sc.settings.n_jobs = 15. ```; sc.settings.n_jobs = 15; with parallel_backend('threading', n_jobs=15):; sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12); ```. gives the warning. ```; /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: ; The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:; @numba.njit(parallel=True); def nn_descent(; ^. self.func_ir.loc)); ```. and now takes 1min 29s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553030310
https://github.com/scverse/scanpy/issues/913#issuecomment-553087327:190,Deployability,install,installed,190,"Ah I see. Sorry about the red herring with `sc.settings.n_jobs`, all those warnings seem to come directly from umap code. So according to @tomwhite, only umap 0.4 and pynndescent need to be installed and it should automatically be parallelized. Scanpy doesn’t effect this in any way. You should probably report this to the umap repo. Please write here once you openend an issue there. The only thing we can to is to call umap in a way that respects our settings once this is fixed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553087327
https://github.com/scverse/scanpy/issues/913#issuecomment-553195318:161,Deployability,install,installation,161,"Leland replied that the parallelization isn't fully implement even in umap 0.4 but that a temporary work around is as below (which is what I'm doing in my local installation):. If you just want to make use of 16 cores then the other option is in umap/umap_.py where it calls pynndescent add an extra option n_jobs=-1, which will turn on the (slightly memory intensive) threaded implementation of nndescent that exists in pynndescent v0.3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553195318
https://github.com/scverse/scanpy/issues/913#issuecomment-553392140:140,Deployability,install,install,140,"I've written up a demo of how to run parallel NN and UMAP here: https://github.com/theislab/scanpy_usage/pull/17. The trick for NN is to a) install pyndescent and b) call the NN algortihm from within a joblib parallel context manager:. ```python; from joblib import parallel_backend; with parallel_backend('threading', n_jobs=16):; sc.pp.neighbors(adata); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553392140
https://github.com/scverse/scanpy/issues/913#issuecomment-553415534:69,Deployability,install,installed,69,"Yes @flying-sheep is right. Even with UMAP 0.4 and pynndescent 0.3.3 installed, the parallel_backend code did not change the number of CPUs used for the core nn_descent step, so the runtime was approximately the same as just running sc.pp.neighbors(adata) without the joblib parallel context manager",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553415534
https://github.com/scverse/scanpy/issues/913#issuecomment-553420798:208,Performance,optimiz,optimized,208,"Ah yes - sorry I missed the context manager bit from earlier in this thread. I tried running this earlier today using the script in theislab/scanpy_usage#17, and for 130K cells NN took 53s unoptimized vs 35s optimized (32 cores). (Not a proportionate speedup, but still worthwhile.) The UMAP speedup shown in that script is significant too. I see that there is a UMAP issue to discuss this further.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553420798
https://github.com/scverse/scanpy/issues/914#issuecomment-553986902:1254,Availability,error,errors,1254,"TCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',; 'CGCGCCTTGTCA', 'AACCTTTGATGG',; ...; 'TATCTGTAATCA', 'ATGGGTGAACAG', 'TCCGATAGTGGA', 'TTGTCAATCTCT',; 'CGGTGGCTGAGT', 'TCCATATCAGGG', 'TGGCGTTAGTAT', 'TTCTTCTGGTTT',; 'ACTATGGCTGGT', 'CGTGAACCCTGT'],; dtype='object', length=1500); >>> print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 1, in <genexpr>; NameError: name 'adata00a' is not defined; >>> print(adata.var_names, adata); Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',; 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',; 'CGCGCCTTGTCA', 'AACCTTTGATGG',; ...; 'GCCCAGGTGCCA', 'GACGAAACCATG', 'CCCGCCCAAGTT', 'GTGCGTTAAGTG',; 'TCCTACCTGTAC', 'CACACTGATGAT', 'CAGCATACTCCN', 'AGAAACCTTGGG',; 'GACATAAATCAG', 'AGGGGTGACGAC'],; dtype='object', length=3971) AnnData object with n_obs × n_vars = 119446 × 3971 ; obs: 'batch', 'tech'; ```; The `adata00a` threw some errors as it hadn't been defined yet, and I wan't totally sure what you were trying to reference there, but I tried with both just `adata` and `day00a` and it still threw errors (not defined and ValueError respectively). Assuming that you just wanted the var_names for each day I just ran them separately and also got the cell barcodes:. ```; >>> print(day01.var_names); Index(['CCGTCATCTTCT', 'GATTCGATTTCC', 'TTTTGGCCGTTA', 'TGAGTTTTTATN',; 'CCGTCTCTACTN', 'ATAAGTTGCTTG', 'CCGTCGCAAGGT', 'ACACCTTGGAAA',; 'AGCCCGCCCAGN', 'GAAAATCGATCN',; ...; 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',; 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',; 'CTAAACGGCTGT', 'GAAATGAGGATG'],; dtype='object', length=500); >>> print(day02.var_names); Index(['AACCATCAGCGG', 'GTCCCACTACAT', 'CCCTTTCCGAGN', 'AGGGCACTTTGG',; 'CCTGAGAAGCGT', 'GGGGCTGTTGGG', 'ACTGACTTACCC', 'CAAGACTACTAT',; 'GCATTATGTCCC', 'TTCGGTGTCATG',; ...; 'AGCAGCGTTATA', 'A",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914#issuecomment-553986902
https://github.com/scverse/scanpy/issues/914#issuecomment-553986902:1425,Availability,error,errors,1425,"TCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',; 'CGCGCCTTGTCA', 'AACCTTTGATGG',; ...; 'TATCTGTAATCA', 'ATGGGTGAACAG', 'TCCGATAGTGGA', 'TTGTCAATCTCT',; 'CGGTGGCTGAGT', 'TCCATATCAGGG', 'TGGCGTTAGTAT', 'TTCTTCTGGTTT',; 'ACTATGGCTGGT', 'CGTGAACCCTGT'],; dtype='object', length=1500); >>> print(all((adata00a.var_names == day_x.var_names).all() for day_x in adata_list)); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<stdin>"", line 1, in <genexpr>; NameError: name 'adata00a' is not defined; >>> print(adata.var_names, adata); Index(['GTGCCATATTCN', 'TCGCAGCCTGCT', 'CTGTAGCCCCCA', 'TGTATGTTGCTT',; 'CAGTATCTCATN', 'AATCTATCACAN', 'CTCCCTTTTGCN', 'TTTGACACCGCC',; 'CGCGCCTTGTCA', 'AACCTTTGATGG',; ...; 'GCCCAGGTGCCA', 'GACGAAACCATG', 'CCCGCCCAAGTT', 'GTGCGTTAAGTG',; 'TCCTACCTGTAC', 'CACACTGATGAT', 'CAGCATACTCCN', 'AGAAACCTTGGG',; 'GACATAAATCAG', 'AGGGGTGACGAC'],; dtype='object', length=3971) AnnData object with n_obs × n_vars = 119446 × 3971 ; obs: 'batch', 'tech'; ```; The `adata00a` threw some errors as it hadn't been defined yet, and I wan't totally sure what you were trying to reference there, but I tried with both just `adata` and `day00a` and it still threw errors (not defined and ValueError respectively). Assuming that you just wanted the var_names for each day I just ran them separately and also got the cell barcodes:. ```; >>> print(day01.var_names); Index(['CCGTCATCTTCT', 'GATTCGATTTCC', 'TTTTGGCCGTTA', 'TGAGTTTTTATN',; 'CCGTCTCTACTN', 'ATAAGTTGCTTG', 'CCGTCGCAAGGT', 'ACACCTTGGAAA',; 'AGCCCGCCCAGN', 'GAAAATCGATCN',; ...; 'AACCCGCCCAGN', 'CCCATCGACTGA', 'GCGGAAGGCGCT', 'TACTTGGTTTGC',; 'GTCTTGGTTACC', 'CTCGCGGCCGTT', 'CCCAAATTTCGT', 'CCGGAGGTTTAG',; 'CTAAACGGCTGT', 'GAAATGAGGATG'],; dtype='object', length=500); >>> print(day02.var_names); Index(['AACCATCAGCGG', 'GTCCCACTACAT', 'CCCTTTCCGAGN', 'AGGGCACTTTGG',; 'CCTGAGAAGCGT', 'GGGGCTGTTGGG', 'ACTGACTTACCC', 'CAAGACTACTAT',; 'GCATTATGTCCC', 'TTCGGTGTCATG',; ...; 'AGCAGCGTTATA', 'A",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914#issuecomment-553986902
https://github.com/scverse/scanpy/issues/914#issuecomment-554597929:203,Modifiability,variab,variables,203,"Thanks for that. I did mean `adata00a`, whoops! I'm expecting the `ValueError` was due to the `.var_names` having different shapes?. I think the main issue here is that `AnnData.concatenate` expects the variables (`.var_names`) to be shared, and appends the objects along the observation axis. If your data is read in with observations in the `var` axis, you can use `AnnData.transpose` to fix that. It looks like something different is happening in the snippet I sent than the one you posted before. I'm pretty sure I didn't modify any variables when I added the print statements to your snippet. Any idea why the results would be different?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914#issuecomment-554597929
https://github.com/scverse/scanpy/issues/914#issuecomment-554597929:537,Modifiability,variab,variables,537,"Thanks for that. I did mean `adata00a`, whoops! I'm expecting the `ValueError` was due to the `.var_names` having different shapes?. I think the main issue here is that `AnnData.concatenate` expects the variables (`.var_names`) to be shared, and appends the objects along the observation axis. If your data is read in with observations in the `var` axis, you can use `AnnData.transpose` to fix that. It looks like something different is happening in the snippet I sent than the one you posted before. I'm pretty sure I didn't modify any variables when I added the print statements to your snippet. Any idea why the results would be different?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914#issuecomment-554597929
https://github.com/scverse/scanpy/issues/914#issuecomment-852691627:508,Availability,error,errors,508,"> @ivirshup @LuckyMD; > I fixed the problem - the issue was in the original h5ad file converted from a Seurat object using SeuratDisk::Convert(). It seems the var data wasn't ported over properly for the assay I was using. I rebuilt the h5ad file using reticulate instead and that solved the problem. I have encountered the same issue as @mosquitoCat .; Although adata.var_names still returns correct gene symbols, all my name IDs become numbers:; for example, sc.pl.umap(adata,color='GeneName') will return errors. but sc.pl.umap(adata,color='123') can be recognized.; SeuratDisk::Convert() seems to cause some trouble here. Is there a way to fix it? @ivirshup",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914#issuecomment-852691627
https://github.com/scverse/scanpy/pull/915#issuecomment-553322915:160,Testability,log,logcounts,160,"Nice! Can you please explain your rationale for why they shouldn’t a) be normal tools and b) saved into the AnnData object?. ```py; sc.tl.gearys_c(pbmc, layer=""logcounts""); to_plot = pbmc.var_names[np.argsort(pbmc.var.gearys_c)[:4]]; ```. Sure, adding more and more features is a good point to think about the API, I’d just like to hear why all current analysis tools belong into `tl` and these two don’t!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-553322915
https://github.com/scverse/scanpy/pull/915#issuecomment-559928610:334,Integrability,depend,depends,334,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed?. Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python; sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""); sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]); ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610
https://github.com/scverse/scanpy/pull/915#issuecomment-559928610:127,Modifiability,variab,variable,127,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed?. Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python; sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""); sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]); ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610
https://github.com/scverse/scanpy/pull/915#issuecomment-559928610:258,Modifiability,variab,variable,258,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed?. Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python; sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""); sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]); ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610
https://github.com/scverse/scanpy/pull/915#issuecomment-559928610:243,Security,access,access,243,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed?. Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python; sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""); sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]); ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610
https://github.com/scverse/scanpy/pull/915#issuecomment-559928610:389,Usability,intuit,intuitive,389,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` – the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed?. Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python; sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""); sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]); ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610
https://github.com/scverse/scanpy/pull/915#issuecomment-560100529:96,Energy Efficiency,reduce,reduce,96,Do you reckon it makes sense to make `sc.tl.marker_genes_overlap()` use this code internally to reduce code redundancy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-560100529
https://github.com/scverse/scanpy/pull/915#issuecomment-560100529:108,Safety,redund,redundancy,108,Do you reckon it makes sense to make `sc.tl.marker_genes_overlap()` use this code internally to reduce code redundancy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-560100529
https://github.com/scverse/scanpy/pull/915#issuecomment-763250072:4,Deployability,update,updates,4,any updates on this PR? @ivirshup I know some people in the lab really like your Geary's C implementation. Any thoughts on making it a small standalone package?. Also I do like the idea of the `sc.metrics` module if it makes more sense here.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763250072
https://github.com/scverse/scanpy/pull/915#issuecomment-763302767:245,Deployability,release,release,245,"@adamgayoso, I should definitely get around to merging this. I think I can pretty much do it as is, and open a second issue for getting the docs looking good. I'd like to target an initial `metrics` module for `1.8` (we're working on upping the release cadence as well). Question for your lab, are our implementations equivalent? I haven't actually gotten around to testing against the `VISION` R/C++ version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763302767
https://github.com/scverse/scanpy/pull/915#issuecomment-763302767:366,Testability,test,testing,366,"@adamgayoso, I should definitely get around to merging this. I think I can pretty much do it as is, and open a second issue for getting the docs looking good. I'd like to target an initial `metrics` module for `1.8` (we're working on upping the release cadence as well). Question for your lab, are our implementations equivalent? I haven't actually gotten around to testing against the `VISION` R/C++ version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763302767
https://github.com/scverse/scanpy/pull/915#issuecomment-763812897:75,Deployability,integrat,integration,75,"also as an aside, would it be appropriate to include some of @LuckyMD scIB integration metrics here? It would give people easier access and probably expand general use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763812897
https://github.com/scverse/scanpy/pull/915#issuecomment-763812897:75,Integrability,integrat,integration,75,"also as an aside, would it be appropriate to include some of @LuckyMD scIB integration metrics here? It would give people easier access and probably expand general use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763812897
https://github.com/scverse/scanpy/pull/915#issuecomment-763812897:129,Security,access,access,129,"also as an aside, would it be appropriate to include some of @LuckyMD scIB integration metrics here? It would give people easier access and probably expand general use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763812897
https://github.com/scverse/scanpy/pull/915#issuecomment-763835114:456,Availability,mainten,maintenance,456,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114
https://github.com/scverse/scanpy/pull/915#issuecomment-763835114:569,Deployability,integrat,integrate,569,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114
https://github.com/scverse/scanpy/pull/915#issuecomment-763835114:569,Integrability,integrat,integrate,569,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114
https://github.com/scverse/scanpy/pull/915#issuecomment-763835114:202,Usability,usab,usable,202,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114
https://github.com/scverse/scanpy/pull/915#issuecomment-763835114:329,Usability,usab,usability,329,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114
https://github.com/scverse/scanpy/pull/915#issuecomment-763835114:378,Usability,usab,usability,378,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114
https://github.com/scverse/scanpy/pull/915#issuecomment-764146920:343,Deployability,update,update,343,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764146920
https://github.com/scverse/scanpy/pull/915#issuecomment-764146920:260,Integrability,depend,dependencies,260,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764146920
https://github.com/scverse/scanpy/pull/915#issuecomment-764195420:35,Deployability,release,release,35,just want to say that even a first release with some of the easiest to implement metrics could help lead to greater widespread use and IMO would generally be appreciated by the community. Besides the fact that it seems like a perfect fit for this scanpy module as I understand it. Though I do understand the citation issue. Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! Maybe it could be accessed with `sc.citation_table` and displays which function calls used which paper's methods.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764195420
https://github.com/scverse/scanpy/pull/915#issuecomment-764195420:463,Security,access,accessed,463,just want to say that even a first release with some of the easiest to implement metrics could help lead to greater widespread use and IMO would generally be appreciated by the community. Besides the fact that it seems like a perfect fit for this scanpy module as I understand it. Though I do understand the citation issue. Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! Maybe it could be accessed with `sc.citation_table` and displays which function calls used which paper's methods.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764195420
https://github.com/scverse/scanpy/pull/915#issuecomment-764392892:464,Deployability,Integrat,Integration,464,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892
https://github.com/scverse/scanpy/pull/915#issuecomment-764392892:571,Deployability,integrat,integration,571,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892
https://github.com/scverse/scanpy/pull/915#issuecomment-764392892:155,Integrability,wrap,wrapping,155,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892
https://github.com/scverse/scanpy/pull/915#issuecomment-764392892:464,Integrability,Integrat,Integration,464,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892
https://github.com/scverse/scanpy/pull/915#issuecomment-764392892:571,Integrability,integrat,integration,571,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892
https://github.com/scverse/scanpy/pull/915#issuecomment-764392892:171,Usability,learn,learn,171,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892
https://github.com/scverse/scanpy/pull/915#issuecomment-764945553:548,Security,access,access,548,"> Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472. Yeah I suppose, though I could see how this gets complicated by the fact that I imagine more people than myself use multiple h5ads throughout their analysis of the same dataset. Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. But if there's any interest in it (a bit of a weird idea I understand) I can make an issue for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764945553
https://github.com/scverse/scanpy/pull/915#issuecomment-764945553:523,Usability,simpl,simpler,523,"> Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472. Yeah I suppose, though I could see how this gets complicated by the fact that I imagine more people than myself use multiple h5ads throughout their analysis of the same dataset. Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. But if there's any interest in it (a bit of a weird idea I understand) I can make an issue for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764945553
https://github.com/scverse/scanpy/pull/915#issuecomment-765107026:187,Availability,avail,available,187,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-765107026
https://github.com/scverse/scanpy/pull/915#issuecomment-765107026:50,Security,access,access,50,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-765107026
https://github.com/scverse/scanpy/pull/915#issuecomment-765107026:462,Security,access,accessible,462,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-765107026
https://github.com/scverse/scanpy/pull/915#issuecomment-765107026:25,Usability,simpl,simpler,25,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-765107026
https://github.com/scverse/scanpy/pull/915#issuecomment-765107026:792,Usability,learn,learn,792,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-765107026
https://github.com/scverse/scanpy/pull/915#issuecomment-765107026:840,Usability,learn,learn,840,"> Though maybe something simpler is to be able to access a global table of functions and citations, and it gives you the bibtex. @adamgayoso From my point of view, references are already available ([source](https://github.com/theislab/scanpy/blob/master/docs/references.rst), [rendered](https://scanpy.readthedocs.io/en/latest/references.html)) and linked to in the documentation. I'm not against the idea, I'm just not seeing how it makes this information more accessible/ prominent. A separate issue for the topic would be good for more discussion. ---------------------. > Would really welcome that. I can help where I can, although not so familiar with numba. @LuckyMD, meant to say, `numba` is super easy, it's really just python. Next best thing to Julia. Definitely worth some time to learn, but also it won't take that much time to learn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-765107026
https://github.com/scverse/scanpy/pull/915#issuecomment-788791783:193,Availability,error,errors,193,"I think this is currently bugged by: https://github.com/numba/numba/issues/6774. It's a weird bug: some code just doesn't execute, unless I swap out a `prange` with a `range`, in which case it errors. Unless I add an expression that does nothing. Then it can work, except it's doing the expensive computation again 🤯. It looks like this won't be solved by the next numba release, so working around it will be necessary for timely inclusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-788791783
https://github.com/scverse/scanpy/pull/915#issuecomment-788791783:371,Deployability,release,release,371,"I think this is currently bugged by: https://github.com/numba/numba/issues/6774. It's a weird bug: some code just doesn't execute, unless I swap out a `prange` with a `range`, in which case it errors. Unless I add an expression that does nothing. Then it can work, except it's doing the expensive computation again 🤯. It looks like this won't be solved by the next numba release, so working around it will be necessary for timely inclusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-788791783
https://github.com/scverse/scanpy/pull/917#issuecomment-563302687:88,Usability,user experience,user experience,88,"Although I understand technically the link between the seed and parallelization, from a user experience point of view, this is very unintuitive. How about another parameter about parallelization like `parallel` or `multicore` or so? Scanpy can then print a warning saying that results will not be reproducible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/917#issuecomment-563302687
https://github.com/scverse/scanpy/issues/918#issuecomment-554926531:130,Usability,guid,guides,130,Could you provide some more details? It'd be useful to see a script that can reproduce this behavior from scratch. There are some guides on how to write this up in the [contributing section](https://github.com/theislab/scanpy/blob/master/CONTRIBUTING.md#before-filing-an-issue). Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-554926531
https://github.com/scverse/scanpy/issues/918#issuecomment-555510263:483,Testability,log,logging,483,"@flying-sheep ; Because the paper have not been published, so i can't offer the data to reproduce this problem.; Here are the codes:; ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy; from IPython.core.display import display, HTML; display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###; sc.settings.verbosity = 1; sc.logging.print_versions(); results_file = './write/sp.h5ad'; sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.combat(sp,key='batch',covariates=['sample']); sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```; Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555510263
https://github.com/scverse/scanpy/issues/918#issuecomment-555510263:976,Testability,log,log,976,"@flying-sheep ; Because the paper have not been published, so i can't offer the data to reproduce this problem.; Here are the codes:; ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy; from IPython.core.display import display, HTML; display(HTML(""<style>.container { width:90% !important; }</style>"")). ###settings###; sc.settings.verbosity = 1; sc.logging.print_versions(); results_file = './write/sp.h5ad'; sc.settings.set_figure_params(dpi=150). sp=sc.read('sp_velo.loom'); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.combat(sp,key='batch',covariates=['sample']); sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```; Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555510263
https://github.com/scverse/scanpy/issues/918#issuecomment-555516223:121,Availability,error,error,121,"@ivirshup ; Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks!; ```; TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)); [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223
https://github.com/scverse/scanpy/issues/918#issuecomment-555516223:189,Availability,error,error,189,"@ivirshup ; Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks!; ```; TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)); [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223
https://github.com/scverse/scanpy/issues/918#issuecomment-555516223:1305,Availability,error,errors,1305,"@ivirshup ; Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks!; ```; TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)); [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223
https://github.com/scverse/scanpy/issues/918#issuecomment-555516223:1496,Availability,error,error,1496,"@ivirshup ; Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks!; ```; TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)); [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223
https://github.com/scverse/scanpy/issues/918#issuecomment-555516223:266,Deployability,pipeline,pipeline,266,"@ivirshup ; Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks!; ```; TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)); [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223
https://github.com/scverse/scanpy/issues/918#issuecomment-555516223:1100,Deployability,release,release,1100,"@ivirshup ; Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks!; ```; TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)); [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223
https://github.com/scverse/scanpy/issues/918#issuecomment-555516223:1502,Integrability,message,message,1502,"@ivirshup ; Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks!; ```; TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)); [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223
https://github.com/scverse/scanpy/issues/918#issuecomment-555516223:519,Modifiability,parameteriz,parameterized,519,"@ivirshup ; Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks!; ```; TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)); [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223
https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:2088,Availability,down,down,2088,"ython; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts?; * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868
https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:1930,Deployability,release,release,1930,"ython; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts?; * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868
https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:1949,Deployability,update,update,1949,"ython; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts?; * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868
https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:198,Modifiability,variab,variables,198,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868
https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:1766,Modifiability,variab,variables,1766,"ython; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts?; * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868
https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:2197,Modifiability,variab,variables,2197,"ython; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts?; * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868
https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:426,Testability,log,logging,426,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868
https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:973,Testability,log,log,973,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868
https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:562,Usability,learn,learn,562,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868
https://github.com/scverse/scanpy/issues/919#issuecomment-554271061:228,Testability,test,test,228,"Hi @arutik,. The reason the two DE gene sets are not the same is that `sc.tl.rank_genes_groups()` only reports genes that are upregulated in one cluster (the one in specified in the `group` parameter) compared to the other. The test itself should be symmetric if I'm not mistaken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919#issuecomment-554271061
https://github.com/scverse/scanpy/issues/919#issuecomment-554364259:208,Availability,down,downregulated,208,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919#issuecomment-554364259
https://github.com/scverse/scanpy/issues/919#issuecomment-554364259:96,Testability,test,test,96,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919#issuecomment-554364259
https://github.com/scverse/scanpy/issues/919#issuecomment-554364259:146,Testability,log,logFC,146,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919#issuecomment-554364259
https://github.com/scverse/scanpy/issues/919#issuecomment-554364259:174,Testability,test,test,174,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919#issuecomment-554364259
https://github.com/scverse/scanpy/issues/919#issuecomment-554364259:296,Testability,test,test,296,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919#issuecomment-554364259
https://github.com/scverse/scanpy/issues/919#issuecomment-554364259:537,Testability,test,tests,537,"Hi, . Actually that's not what I've experienced - if you compare with default rank_genes_groups test you get genes with positive **and** negative logFC, which means that the test reports both upregulated and downregulated genes in that comparison, but again, it's not symmetric - please try on a test dataset for yourself.. Also if I take lists produced by A vs B comparison and B vs A and filter the genes by a common adjusted p-value cutoff (0.05 let's say) I would get lists of different sizes, so all of that makes me think that the tests are not symmetric/two-sided.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919#issuecomment-554364259
https://github.com/scverse/scanpy/issues/919#issuecomment-555278819:467,Availability,down,downregulated,467,"Hi, sorry for the delay, sure,. I have my adata object and let's say a key 'group' in the annotation that can either be A or B for each cell, and then I do:; sc.tl.rank_genes_groups(adata, groupby='group', n_genes=1000). As a result, from the object attributes I can get the tables of DE genes, so I get a table for group A vs rest (which is only group B in this case) and for group B vs rest (which is only group A). Both of these lists contain both upregulated and downregulated DE genes, but are not symmetrical. Let me know if this is unclear. . Thank you. . Sincerely, ; Anna Arutyunyan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/919#issuecomment-555278819
https://github.com/scverse/scanpy/issues/920#issuecomment-553903320:55,Deployability,install,installation,55,"Old problem. If someone finds this: as seen in in the [installation instructions](https://scanpy.readthedocs.io/en/stable/installation.html#pypi-only) the package is called [python-igraph](https://pypi.org/project/python-igraph/), and you can e.g. do `pip install scanpy[louvain]`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/920#issuecomment-553903320
https://github.com/scverse/scanpy/issues/920#issuecomment-553903320:122,Deployability,install,installation,122,"Old problem. If someone finds this: as seen in in the [installation instructions](https://scanpy.readthedocs.io/en/stable/installation.html#pypi-only) the package is called [python-igraph](https://pypi.org/project/python-igraph/), and you can e.g. do `pip install scanpy[louvain]`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/920#issuecomment-553903320
https://github.com/scverse/scanpy/issues/920#issuecomment-553903320:256,Deployability,install,install,256,"Old problem. If someone finds this: as seen in in the [installation instructions](https://scanpy.readthedocs.io/en/stable/installation.html#pypi-only) the package is called [python-igraph](https://pypi.org/project/python-igraph/), and you can e.g. do `pip install scanpy[louvain]`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/920#issuecomment-553903320
https://github.com/scverse/scanpy/issues/921#issuecomment-554871161:447,Deployability,pipeline,pipeline,447,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases.; * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help.; * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161
https://github.com/scverse/scanpy/issues/921#issuecomment-554871161:948,Integrability,wrap,wrapping,948,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases.; * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help.; * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161
https://github.com/scverse/scanpy/issues/921#issuecomment-554871161:973,Integrability,wrap,wrapper,973,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases.; * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help.; * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161
https://github.com/scverse/scanpy/issues/921#issuecomment-554871161:1041,Integrability,protocol,protocol,1041,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases.; * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help.; * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161
https://github.com/scverse/scanpy/issues/921#issuecomment-554871161:417,Performance,perform,performance,417,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases.; * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help.; * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161
https://github.com/scverse/scanpy/issues/921#issuecomment-555940037:977,Deployability,pipeline,pipeline,977,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized?. You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-555940037
https://github.com/scverse/scanpy/issues/921#issuecomment-555940037:548,Performance,optimiz,optimized,548,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized?. You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-555940037
https://github.com/scverse/scanpy/issues/921#issuecomment-555940037:947,Performance,perform,performance,947,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized?. You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-555940037
https://github.com/scverse/scanpy/issues/921#issuecomment-555940037:1221,Performance,bottleneck,bottlenecks,1221,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized?. You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-555940037
https://github.com/scverse/scanpy/issues/921#issuecomment-555940037:1086,Testability,benchmark,benchmark,1086,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized?. You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-555940037
https://github.com/scverse/scanpy/issues/921#issuecomment-555940037:1156,Testability,benchmark,benchmark,1156,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized?. You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-555940037
https://github.com/scverse/scanpy/issues/921#issuecomment-557095212:83,Integrability,wrap,wrapper,83,"Hi @mrocklin, you might be interested in this work, especially the Dask-compatible wrapper around `scipy.sparse`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557095212
https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:62,Availability,ping,pinging,62,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880
https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:324,Availability,down,down,324,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880
https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2209,Availability,down,down,2209,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880
https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2450,Deployability,pipeline,pipeline,2450,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880
https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2827,Energy Efficiency,schedul,scheduler,2827,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880
https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:576,Integrability,wrap,wrapper,576,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880
https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:644,Integrability,protocol,protocol,644,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880
https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:924,Integrability,wrap,wrapper,924,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880
https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2420,Performance,perform,performance,2420,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880
https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2696,Performance,bottleneck,bottlenecks,2696,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880
https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:914,Safety,avoid,avoid,914,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880
https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2892,Safety,avoid,avoid,2892,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880
https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2561,Testability,benchmark,benchmark,2561,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880
https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2631,Testability,benchmark,benchmark,2631,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880
https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:471,Usability,responsiv,responsive,471,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880
https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:514,Usability,learn,learning,514,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880
https://github.com/scverse/scanpy/issues/921#issuecomment-557317682:17,Integrability,wrap,wrapper,17,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse.; > ; > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557317682
https://github.com/scverse/scanpy/issues/921#issuecomment-557317682:85,Integrability,protocol,protocol,85,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse.; > ; > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557317682
https://github.com/scverse/scanpy/issues/921#issuecomment-557317682:372,Integrability,wrap,wrapper,372,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse.; > ; > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557317682
https://github.com/scverse/scanpy/issues/921#issuecomment-557317682:362,Safety,avoid,avoid,362,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse.; > ; > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557317682
https://github.com/scverse/scanpy/issues/921#issuecomment-557319885:19,Integrability,wrap,wrapper,19,"The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. With some context about things like `@` vs `*` users can then make an informed decision to use it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557319885
https://github.com/scverse/scanpy/issues/921#issuecomment-557463310:85,Availability,down,down,85,"Thank you for the detailed response @mrocklin!. > It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. I will try to produce a test case and post it there. > Another option would be to see if you can swap out Anndata for Xarray. This has been discussed before (https://github.com/theislab/anndata/issues/32) but the sticking point was sparse support.; Perhaps with some of the techniques being discussed in this issue it might become an option again, with all the benefits you outlined. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. Thanks - I've opened issues for these features on the CuPy issue tracker.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557463310
https://github.com/scverse/scanpy/issues/921#issuecomment-557463310:224,Testability,test,test,224,"Thank you for the detailed response @mrocklin!. > It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. I will try to produce a test case and post it there. > Another option would be to see if you can swap out Anndata for Xarray. This has been discussed before (https://github.com/theislab/anndata/issues/32) but the sticking point was sparse support.; Perhaps with some of the techniques being discussed in this issue it might become an option again, with all the benefits you outlined. > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. Thanks - I've opened issues for these features on the CuPy issue tracker.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557463310
https://github.com/scverse/scanpy/issues/921#issuecomment-557464075:21,Integrability,wrap,wrapper,21,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557464075
https://github.com/scverse/scanpy/issues/921#issuecomment-557464075:259,Integrability,wrap,wrapper,259,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557464075
https://github.com/scverse/scanpy/issues/921#issuecomment-557464075:463,Integrability,wrap,wrapper,463,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557464075
https://github.com/scverse/scanpy/issues/921#issuecomment-557464075:292,Security,expose,exposes,292,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557464075
https://github.com/scverse/scanpy/issues/921#issuecomment-557644893:857,Performance,queue,queue,857,">> Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:; >; > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. > cc @jakirkham @pentschev. FWIW made sure to cc us on the issues that you opened. Though please cc us on other ones. Should add I think CuPy devs will want to keep their sparse implementation in pretty close alignment with SciPy's. So I don't think CuPy's sparse will solve any issues that SciPy's sparse does not also solve. However things that CuPy does not implement that SciPy does implement, are likely in scope. Though no idea where these sit in the priority queue ATM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557644893
https://github.com/scverse/scanpy/issues/921#issuecomment-557721953:684,Integrability,wrap,wrapper,684,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>; <summary><code>__array_function__</code> implementation</summary>. ```python; def __array_function__(self, func, types, args, kwargs):; result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs); if issparse(result):; result = SparseArray(result); elif isinstance(result, np.matrix):; result = np.asarray(result); return result; ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557721953
https://github.com/scverse/scanpy/issues/921#issuecomment-557721953:811,Usability,simpl,simple,811,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>; <summary><code>__array_function__</code> implementation</summary>. ```python; def __array_function__(self, func, types, args, kwargs):; result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs); if issparse(result):; result = SparseArray(result); elif isinstance(result, np.matrix):; result = np.asarray(result); return result; ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557721953
https://github.com/scverse/scanpy/issues/924#issuecomment-554854817:15,Deployability,update,update,15,Thanks for the update!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924#issuecomment-554854817
https://github.com/scverse/scanpy/issues/925#issuecomment-555069647:17,Usability,simpl,simply,17,"the clusters are simply annotations added in the `adata.obs` pandas dataframe. Thus, to merge the clusters you can create a new column containing your merged clusters. For example:. ```PYTHON; old_to_new = dict(; old_cluster1='new_cluster1',; old_cluster2='new_cluster1',; old_cluster3='new_cluster2',; ); adata.obs['new_clusters'] = (; adata.obs['old_clusters']; .map(old_to_new); .astype('category'); ); ````",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925#issuecomment-555069647
https://github.com/scverse/scanpy/issues/925#issuecomment-555083714:154,Usability,clear,clear,154,"For general help like this, please go to https://scanpy.discourse.group/. This is also what the issue template says. How could we have made the text more clear so that you’d have found your way there?. ![grafik](https://user-images.githubusercontent.com/291575/69068901-e0cfbd80-0a25-11ea-8095-52e567b86574.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925#issuecomment-555083714
https://github.com/scverse/scanpy/issues/925#issuecomment-941184403:503,Usability,simpl,simply,503,"Just to answer those that, like me, are beginners in python, the solution provided by @ivirshup works perfectly (of course for `louvain` and `leiden,` and any other `adata.obs` that you want to remap):; ```; adata.obs['new_clusters'] = (; adata.obs[""old_clusters""]; .map(lambda x: {""a"": ""b""}.get(x, x)); .astype(""category""); ); ```; Where ""a"" is the name of the category you want to change, and ""b"" is the new name of the category that you want to change. If you have more categories you want to change simply add more entries to the dictionary like:; ```; adata.obs['new_clusters'] = (; adata.obs[""old_clusters""]; .map(lambda x: {""a"": ""b"", ""c"": ""d""}.get(x, x)); .astype(""category""); ); ```; @fidelram answer does not work in this specific case because the `adata.obs` from the louvain (or leiden) algorithm are categories named 0, 1, 2, 3, 4 and you cannot construct a dictionary using '0':'X' because `SyntaxError: keyword can't be an expression`. Hope this helps,. Best,. A",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925#issuecomment-941184403
https://github.com/scverse/scanpy/issues/925#issuecomment-1153213330:98,Usability,simpl,simply,98,"I think anndata’s `rename_categories` should accept non-unique values as argument. Then one could simply do things like. ```py; cluster_markers = {; 'CD4 T': {'IL7R'},; 'CD14+\nMonocytes': {'CD14', 'LYZ'},; 'B': {'MS4A1'},; 'CD8 T': {'CD8A'},; 'NK': {'GNLY', 'NKG7'},; 'FCGR3A+\nMonocytes': {'FCGR3A', 'MS4A7'},; 'Dendritic': {'FCER1A', 'CST3'},; 'Mega-\nkaryocytes': {'PPBP'},; }; marker_matches = sc.tl.marker_gene_overlap(adata, cluster_markers); adata.rename_categories('leiden', marker_matches.idxmax()); ```. As it stands, things like the `pbmc3k` tutorial are super flaky because they hardcode things like this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/925#issuecomment-1153213330
https://github.com/scverse/scanpy/issues/926#issuecomment-555323780:180,Availability,error,error,180,"We could possibly add another parameter, (`handle_duplicates`, `duplicates_action`?), which could specify how to do this. I think the best default behavior for this is to throw an error. @fidelram, @VolkerBergen what do you think? I know we've been trying to reduce complexity in these methods, so is this worth it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926#issuecomment-555323780
https://github.com/scverse/scanpy/issues/926#issuecomment-555323780:259,Energy Efficiency,reduce,reduce,259,"We could possibly add another parameter, (`handle_duplicates`, `duplicates_action`?), which could specify how to do this. I think the best default behavior for this is to throw an error. @fidelram, @VolkerBergen what do you think? I know we've been trying to reduce complexity in these methods, so is this worth it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926#issuecomment-555323780
https://github.com/scverse/scanpy/issues/926#issuecomment-555494385:11,Availability,error,error,11,"I think an error should be thrown, thus the user can figure out what to do. ; @nh3 I don't understand the part about the `different summarization`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/926#issuecomment-555494385
https://github.com/scverse/scanpy/pull/927#issuecomment-556003335:2,Testability,test,tested,2,I tested this. 🙂,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/927#issuecomment-556003335
https://github.com/scverse/scanpy/issues/929#issuecomment-558069573:79,Performance,perform,performance,79,There are quite a few questions that need to be answered first:. 1. What's the performance difference here?; 2. Numpy only uses its internal code in the off chance that `#ifndef HAVE_LOG1P`: In which circumstances does accuracy suffer when using that naive implementation?; 3. Does numba.vectorize handle sparse matrices?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929#issuecomment-558069573
https://github.com/scverse/scanpy/pull/931#issuecomment-558143591:90,Performance,perform,performance,90,"My questions from #929 don’t apply since you don’t use numba here. Except for “What's the performance difference here”:. It’s not too bad, but we should use base 2 for everything that isn’t the natural logarithm: log2 can be calculated much faster on regular hardware due to binary storage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931#issuecomment-558143591
https://github.com/scverse/scanpy/pull/931#issuecomment-558143591:202,Testability,log,logarithm,202,"My questions from #929 don’t apply since you don’t use numba here. Except for “What's the performance difference here”:. It’s not too bad, but we should use base 2 for everything that isn’t the natural logarithm: log2 can be calculated much faster on regular hardware due to binary storage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931#issuecomment-558143591
https://github.com/scverse/scanpy/pull/931#issuecomment-558659047:13,Testability,test,test,13,"Let's please test this thoroughly, I'm not sure about how stable fold change estimates are in base 2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931#issuecomment-558659047
https://github.com/scverse/scanpy/issues/935#issuecomment-559305432:74,Deployability,release,released,74,@LuckyMD is your fix (https://github.com/theislab/scanpy/pull/824) in the released scanpy or still only on master?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935#issuecomment-559305432
https://github.com/scverse/scanpy/issues/935#issuecomment-559392108:143,Availability,error,error,143,"It's only on master, but I don't think that would change anything here. The fix was for cases where `sc.pp.highly_variable_genes()` outputs an error due to bin boundaries being duplicated as genes were unexpressed. . I reckon this is not actually a bug. It's a possible scenario that no genes are highly variable in all batches, no? Is `highly_variable_intersection` `False` everywhere, or is `highly_variable_nbatches` somehow false? The former can be `False` if your batches are heterogeneous.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935#issuecomment-559392108
https://github.com/scverse/scanpy/issues/935#issuecomment-559392108:304,Modifiability,variab,variable,304,"It's only on master, but I don't think that would change anything here. The fix was for cases where `sc.pp.highly_variable_genes()` outputs an error due to bin boundaries being duplicated as genes were unexpressed. . I reckon this is not actually a bug. It's a possible scenario that no genes are highly variable in all batches, no? Is `highly_variable_intersection` `False` everywhere, or is `highly_variable_nbatches` somehow false? The former can be `False` if your batches are heterogeneous.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935#issuecomment-559392108
https://github.com/scverse/scanpy/issues/935#issuecomment-559552890:198,Availability,down,down,198,"Yeah, so this is not a bug. It's just that there is no HVG that is shared between all of your batches. I would suggest selecting the number of HVGs that are shared by all batches but 1, and then go down to all batch but 2 if you want more HVGs. For example:; `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == adata.obs.batch.nunique()-1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935#issuecomment-559552890
https://github.com/scverse/scanpy/issues/935#issuecomment-559558621:19,Integrability,depend,depends,19,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935#issuecomment-559558621
https://github.com/scverse/scanpy/issues/935#issuecomment-559558621:615,Integrability,depend,depending,615,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935#issuecomment-559558621
https://github.com/scverse/scanpy/issues/935#issuecomment-559558621:52,Modifiability,variab,variable,52,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935#issuecomment-559558621
https://github.com/scverse/scanpy/issues/935#issuecomment-559568443:385,Safety,safe,safe,385,"I just checked again... and it's not exactly the same... if you select `n_top_genes` then, you will get the top genes shared by the most batches. If you select thresholds for mean and dispersion, you will use these thresholds against the mean dispersion and mean mean across all batches. And those can be lower than the thresholds if HVGs are not shared between many batches. So to be safe, you can go with selecting `n_top_genes`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935#issuecomment-559568443
https://github.com/scverse/scanpy/issues/936#issuecomment-560232016:77,Availability,error,error,77,"Ah, I think this was reported before in #769. Would you mind checking if the error still occurs on master?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936#issuecomment-560232016
https://github.com/scverse/scanpy/pull/941#issuecomment-560059018:186,Modifiability,variab,variable,186,"I'll give a brief hand-wavy explanation now, before checking with someone who knows more about it whether my in depth understanding is correct. PCA is finding a set linearly independent variable which form a new basis for the data. ICA is finding N (user defined) discrete maximally independent signals from the data. They won't form a basis for the input data, and results can vary a lot based on the number of components you try and find. However, each of the signals is discrete and made of a sparser set of variables, which I think makes them more interpretable. I'd relate this to how the PCA components become a single blob while the ICA components keep separating clusters. For example, in the components that you point out, I would agree 1, 5, and 7 look to be the same (note: I may have underspecified N here). However, component 3 is picking up a signal which is largely colinear with those, except for one cluster. To me, that says the difference in variable loadings between component 3 and the others is worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560059018
https://github.com/scverse/scanpy/pull/941#issuecomment-560059018:511,Modifiability,variab,variables,511,"I'll give a brief hand-wavy explanation now, before checking with someone who knows more about it whether my in depth understanding is correct. PCA is finding a set linearly independent variable which form a new basis for the data. ICA is finding N (user defined) discrete maximally independent signals from the data. They won't form a basis for the input data, and results can vary a lot based on the number of components you try and find. However, each of the signals is discrete and made of a sparser set of variables, which I think makes them more interpretable. I'd relate this to how the PCA components become a single blob while the ICA components keep separating clusters. For example, in the components that you point out, I would agree 1, 5, and 7 look to be the same (note: I may have underspecified N here). However, component 3 is picking up a signal which is largely colinear with those, except for one cluster. To me, that says the difference in variable loadings between component 3 and the others is worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560059018
https://github.com/scverse/scanpy/pull/941#issuecomment-560059018:961,Modifiability,variab,variable,961,"I'll give a brief hand-wavy explanation now, before checking with someone who knows more about it whether my in depth understanding is correct. PCA is finding a set linearly independent variable which form a new basis for the data. ICA is finding N (user defined) discrete maximally independent signals from the data. They won't form a basis for the input data, and results can vary a lot based on the number of components you try and find. However, each of the signals is discrete and made of a sparser set of variables, which I think makes them more interpretable. I'd relate this to how the PCA components become a single blob while the ICA components keep separating clusters. For example, in the components that you point out, I would agree 1, 5, and 7 look to be the same (note: I may have underspecified N here). However, component 3 is picking up a signal which is largely colinear with those, except for one cluster. To me, that says the difference in variable loadings between component 3 and the others is worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560059018
https://github.com/scverse/scanpy/pull/941#issuecomment-560059018:970,Performance,load,loadings,970,"I'll give a brief hand-wavy explanation now, before checking with someone who knows more about it whether my in depth understanding is correct. PCA is finding a set linearly independent variable which form a new basis for the data. ICA is finding N (user defined) discrete maximally independent signals from the data. They won't form a basis for the input data, and results can vary a lot based on the number of components you try and find. However, each of the signals is discrete and made of a sparser set of variables, which I think makes them more interpretable. I'd relate this to how the PCA components become a single blob while the ICA components keep separating clusters. For example, in the components that you point out, I would agree 1, 5, and 7 look to be the same (note: I may have underspecified N here). However, component 3 is picking up a signal which is largely colinear with those, except for one cluster. To me, that says the difference in variable loadings between component 3 and the others is worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560059018
https://github.com/scverse/scanpy/pull/941#issuecomment-560100063:338,Availability,redundant,redundant,338,"Thanks for the explanation. But what do you mean by ""discrete"" here?. And so you're saying 1, 5, and 7 being given as solutions to ICA is non-optimal. I guess that's just local optima that are found. It feels strange to generally say that ICA is better as higher dimensions still separate out clusters, while at lower dimensions there is redundant information compared to PCA.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560100063
https://github.com/scverse/scanpy/pull/941#issuecomment-560100063:338,Safety,redund,redundant,338,"Thanks for the explanation. But what do you mean by ""discrete"" here?. And so you're saying 1, 5, and 7 being given as solutions to ICA is non-optimal. I guess that's just local optima that are found. It feels strange to generally say that ICA is better as higher dimensions still separate out clusters, while at lower dimensions there is redundant information compared to PCA.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560100063
https://github.com/scverse/scanpy/pull/941#issuecomment-560219393:28,Availability,redundant,redundant,28,"I think saying discrete was redundant with independent, in that each component should correspond to a signal in the data. > And so you're saying 1, 5, and 7 being given as solutions to ICA is non-optimal. I'm not sure how to interpret it. I know that if I run an analysis on the same dataset with 20 components I get more independent ones. My impression is the ""failure modes"" of linear decompositions like this are not well characterized. > It feels strange to generally say that ICA is better as higher dimensions. I probably wouldn't say this. I think there are different use cases, and ICA components may be easier to interpret than PCA components. I was also just at a talk by Elana Fertig (who knows much more about this kind of thing than I do) where one of the take aways was ""different decompositions for different use-cases"". I think I'll still use PCA for clustering and generating UMAPs. > while at lower dimensions there is redundant information compared to PCA. I'd note that there is no order to ICA components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560219393
https://github.com/scverse/scanpy/pull/941#issuecomment-560219393:362,Availability,failure,failure,362,"I think saying discrete was redundant with independent, in that each component should correspond to a signal in the data. > And so you're saying 1, 5, and 7 being given as solutions to ICA is non-optimal. I'm not sure how to interpret it. I know that if I run an analysis on the same dataset with 20 components I get more independent ones. My impression is the ""failure modes"" of linear decompositions like this are not well characterized. > It feels strange to generally say that ICA is better as higher dimensions. I probably wouldn't say this. I think there are different use cases, and ICA components may be easier to interpret than PCA components. I was also just at a talk by Elana Fertig (who knows much more about this kind of thing than I do) where one of the take aways was ""different decompositions for different use-cases"". I think I'll still use PCA for clustering and generating UMAPs. > while at lower dimensions there is redundant information compared to PCA. I'd note that there is no order to ICA components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560219393
https://github.com/scverse/scanpy/pull/941#issuecomment-560219393:937,Availability,redundant,redundant,937,"I think saying discrete was redundant with independent, in that each component should correspond to a signal in the data. > And so you're saying 1, 5, and 7 being given as solutions to ICA is non-optimal. I'm not sure how to interpret it. I know that if I run an analysis on the same dataset with 20 components I get more independent ones. My impression is the ""failure modes"" of linear decompositions like this are not well characterized. > It feels strange to generally say that ICA is better as higher dimensions. I probably wouldn't say this. I think there are different use cases, and ICA components may be easier to interpret than PCA components. I was also just at a talk by Elana Fertig (who knows much more about this kind of thing than I do) where one of the take aways was ""different decompositions for different use-cases"". I think I'll still use PCA for clustering and generating UMAPs. > while at lower dimensions there is redundant information compared to PCA. I'd note that there is no order to ICA components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560219393
https://github.com/scverse/scanpy/pull/941#issuecomment-560219393:28,Safety,redund,redundant,28,"I think saying discrete was redundant with independent, in that each component should correspond to a signal in the data. > And so you're saying 1, 5, and 7 being given as solutions to ICA is non-optimal. I'm not sure how to interpret it. I know that if I run an analysis on the same dataset with 20 components I get more independent ones. My impression is the ""failure modes"" of linear decompositions like this are not well characterized. > It feels strange to generally say that ICA is better as higher dimensions. I probably wouldn't say this. I think there are different use cases, and ICA components may be easier to interpret than PCA components. I was also just at a talk by Elana Fertig (who knows much more about this kind of thing than I do) where one of the take aways was ""different decompositions for different use-cases"". I think I'll still use PCA for clustering and generating UMAPs. > while at lower dimensions there is redundant information compared to PCA. I'd note that there is no order to ICA components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560219393
https://github.com/scverse/scanpy/pull/941#issuecomment-560219393:937,Safety,redund,redundant,937,"I think saying discrete was redundant with independent, in that each component should correspond to a signal in the data. > And so you're saying 1, 5, and 7 being given as solutions to ICA is non-optimal. I'm not sure how to interpret it. I know that if I run an analysis on the same dataset with 20 components I get more independent ones. My impression is the ""failure modes"" of linear decompositions like this are not well characterized. > It feels strange to generally say that ICA is better as higher dimensions. I probably wouldn't say this. I think there are different use cases, and ICA components may be easier to interpret than PCA components. I was also just at a talk by Elana Fertig (who knows much more about this kind of thing than I do) where one of the take aways was ""different decompositions for different use-cases"". I think I'll still use PCA for clustering and generating UMAPs. > while at lower dimensions there is redundant information compared to PCA. I'd note that there is no order to ICA components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560219393
https://github.com/scverse/scanpy/pull/941#issuecomment-560313033:627,Availability,robust,robust,627,"> I know exactly that in PCA I can interpret a component based on its rank (and/or variance contribution). Ah, I meant more specifically that it may be easier to biologically interpret an ICA. > That would say I should try as many decompositions as possible to see when I get a good result. I'm a little unsure of your meaning here. Do you mean decompositions like decomposition techniques? If so, I don't think this is the right conclusion. I think it means: probably PCA for clustering, probably NMF for finding gene modules. I would also suspect something which finds sparser variable loadings like ICA or NMF could be more robust for cross dataset classification. If you mean, if the results are unstable how do we know which to trust – I did ask that question. I think it's the usual: have a validation dataset, maybe some ensemble/ robustness method, or do some sort of enrichment. It's an open question, but a lot of our analysis pipeline is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560313033
https://github.com/scverse/scanpy/pull/941#issuecomment-560313033:838,Availability,robust,robustness,838,"> I know exactly that in PCA I can interpret a component based on its rank (and/or variance contribution). Ah, I meant more specifically that it may be easier to biologically interpret an ICA. > That would say I should try as many decompositions as possible to see when I get a good result. I'm a little unsure of your meaning here. Do you mean decompositions like decomposition techniques? If so, I don't think this is the right conclusion. I think it means: probably PCA for clustering, probably NMF for finding gene modules. I would also suspect something which finds sparser variable loadings like ICA or NMF could be more robust for cross dataset classification. If you mean, if the results are unstable how do we know which to trust – I did ask that question. I think it's the usual: have a validation dataset, maybe some ensemble/ robustness method, or do some sort of enrichment. It's an open question, but a lot of our analysis pipeline is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560313033
https://github.com/scverse/scanpy/pull/941#issuecomment-560313033:937,Deployability,pipeline,pipeline,937,"> I know exactly that in PCA I can interpret a component based on its rank (and/or variance contribution). Ah, I meant more specifically that it may be easier to biologically interpret an ICA. > That would say I should try as many decompositions as possible to see when I get a good result. I'm a little unsure of your meaning here. Do you mean decompositions like decomposition techniques? If so, I don't think this is the right conclusion. I think it means: probably PCA for clustering, probably NMF for finding gene modules. I would also suspect something which finds sparser variable loadings like ICA or NMF could be more robust for cross dataset classification. If you mean, if the results are unstable how do we know which to trust – I did ask that question. I think it's the usual: have a validation dataset, maybe some ensemble/ robustness method, or do some sort of enrichment. It's an open question, but a lot of our analysis pipeline is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560313033
https://github.com/scverse/scanpy/pull/941#issuecomment-560313033:579,Modifiability,variab,variable,579,"> I know exactly that in PCA I can interpret a component based on its rank (and/or variance contribution). Ah, I meant more specifically that it may be easier to biologically interpret an ICA. > That would say I should try as many decompositions as possible to see when I get a good result. I'm a little unsure of your meaning here. Do you mean decompositions like decomposition techniques? If so, I don't think this is the right conclusion. I think it means: probably PCA for clustering, probably NMF for finding gene modules. I would also suspect something which finds sparser variable loadings like ICA or NMF could be more robust for cross dataset classification. If you mean, if the results are unstable how do we know which to trust – I did ask that question. I think it's the usual: have a validation dataset, maybe some ensemble/ robustness method, or do some sort of enrichment. It's an open question, but a lot of our analysis pipeline is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560313033
https://github.com/scverse/scanpy/pull/941#issuecomment-560313033:588,Performance,load,loadings,588,"> I know exactly that in PCA I can interpret a component based on its rank (and/or variance contribution). Ah, I meant more specifically that it may be easier to biologically interpret an ICA. > That would say I should try as many decompositions as possible to see when I get a good result. I'm a little unsure of your meaning here. Do you mean decompositions like decomposition techniques? If so, I don't think this is the right conclusion. I think it means: probably PCA for clustering, probably NMF for finding gene modules. I would also suspect something which finds sparser variable loadings like ICA or NMF could be more robust for cross dataset classification. If you mean, if the results are unstable how do we know which to trust – I did ask that question. I think it's the usual: have a validation dataset, maybe some ensemble/ robustness method, or do some sort of enrichment. It's an open question, but a lot of our analysis pipeline is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560313033
https://github.com/scverse/scanpy/pull/941#issuecomment-560313033:797,Security,validat,validation,797,"> I know exactly that in PCA I can interpret a component based on its rank (and/or variance contribution). Ah, I meant more specifically that it may be easier to biologically interpret an ICA. > That would say I should try as many decompositions as possible to see when I get a good result. I'm a little unsure of your meaning here. Do you mean decompositions like decomposition techniques? If so, I don't think this is the right conclusion. I think it means: probably PCA for clustering, probably NMF for finding gene modules. I would also suspect something which finds sparser variable loadings like ICA or NMF could be more robust for cross dataset classification. If you mean, if the results are unstable how do we know which to trust – I did ask that question. I think it's the usual: have a validation dataset, maybe some ensemble/ robustness method, or do some sort of enrichment. It's an open question, but a lot of our analysis pipeline is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560313033
https://github.com/scverse/scanpy/pull/941#issuecomment-560323107:74,Availability,robust,robustness,74,"I was referring to both the instability and what i understood to mean non-robustness to different datasets. But it seems a ""use case"" is an analytical step here, rather than a particular dataset to be analysed. That makes it a lot better, and it means there is work to be done but a general best practice conclusion would be reachable. . In that case it's only the instability of the algorithm that is the issue per dataset. And in the case where you're doing exploratory analysis for a new dataset, you don't typically have a validation dataset, which makes this pretty challenging for end users of the method. Enrichment could be a way forward I guess... I'm not the biggest fan of using enrichment results as a measure for success though. Enrichment results still require quite a bit of interpretation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560323107
https://github.com/scverse/scanpy/pull/941#issuecomment-560323107:527,Security,validat,validation,527,"I was referring to both the instability and what i understood to mean non-robustness to different datasets. But it seems a ""use case"" is an analytical step here, rather than a particular dataset to be analysed. That makes it a lot better, and it means there is work to be done but a general best practice conclusion would be reachable. . In that case it's only the instability of the algorithm that is the issue per dataset. And in the case where you're doing exploratory analysis for a new dataset, you don't typically have a validation dataset, which makes this pretty challenging for end users of the method. Enrichment could be a way forward I guess... I'm not the biggest fan of using enrichment results as a measure for success though. Enrichment results still require quite a bit of interpretation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560323107
https://github.com/scverse/scanpy/issues/942#issuecomment-560068082:39,Deployability,update,updated,39,"@ivirshup Thanks for pointing it up. I updated all to newest version, and it worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942#issuecomment-560068082
https://github.com/scverse/scanpy/issues/942#issuecomment-577681828:67,Availability,avail,available,67,@MichaelPeibo Could you install the version 1.4.5.post1? It is not available in conda and with 1.4.4.post1 I'm getting the same error. Thanks!. ```; conda search -c bioconda scanpy; Loading channels: done; # Name Version Build Channel ; scanpy 1.3.1 py36_0 bioconda ; scanpy 1.3.2 py36_0 bioconda ; scanpy 1.3.3 py36_0 bioconda ; scanpy 1.3.4 py36_0 bioconda ; scanpy 1.3.5 py36_0 bioconda ; scanpy 1.3.6 py36_0 bioconda ; scanpy 1.3.7 py36_0 bioconda ; scanpy 1.4 py_0 bioconda ; scanpy 1.4.1 py_0 bioconda ; scanpy 1.4.2 py_0 bioconda ; scanpy 1.4.3 py_0 bioconda ; scanpy 1.4.4 py_0 bioconda ; scanpy 1.4.4 py_1 bioconda ; scanpy 1.4.4.post1 py_0 bioconda ; scanpy 1.4.4.post1 py_1 bioconda ; scanpy 1.4.4.post1 py_2 bioconda ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942#issuecomment-577681828
https://github.com/scverse/scanpy/issues/942#issuecomment-577681828:128,Availability,error,error,128,@MichaelPeibo Could you install the version 1.4.5.post1? It is not available in conda and with 1.4.4.post1 I'm getting the same error. Thanks!. ```; conda search -c bioconda scanpy; Loading channels: done; # Name Version Build Channel ; scanpy 1.3.1 py36_0 bioconda ; scanpy 1.3.2 py36_0 bioconda ; scanpy 1.3.3 py36_0 bioconda ; scanpy 1.3.4 py36_0 bioconda ; scanpy 1.3.5 py36_0 bioconda ; scanpy 1.3.6 py36_0 bioconda ; scanpy 1.3.7 py36_0 bioconda ; scanpy 1.4 py_0 bioconda ; scanpy 1.4.1 py_0 bioconda ; scanpy 1.4.2 py_0 bioconda ; scanpy 1.4.3 py_0 bioconda ; scanpy 1.4.4 py_0 bioconda ; scanpy 1.4.4 py_1 bioconda ; scanpy 1.4.4.post1 py_0 bioconda ; scanpy 1.4.4.post1 py_1 bioconda ; scanpy 1.4.4.post1 py_2 bioconda ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942#issuecomment-577681828
https://github.com/scverse/scanpy/issues/942#issuecomment-577681828:24,Deployability,install,install,24,@MichaelPeibo Could you install the version 1.4.5.post1? It is not available in conda and with 1.4.4.post1 I'm getting the same error. Thanks!. ```; conda search -c bioconda scanpy; Loading channels: done; # Name Version Build Channel ; scanpy 1.3.1 py36_0 bioconda ; scanpy 1.3.2 py36_0 bioconda ; scanpy 1.3.3 py36_0 bioconda ; scanpy 1.3.4 py36_0 bioconda ; scanpy 1.3.5 py36_0 bioconda ; scanpy 1.3.6 py36_0 bioconda ; scanpy 1.3.7 py36_0 bioconda ; scanpy 1.4 py_0 bioconda ; scanpy 1.4.1 py_0 bioconda ; scanpy 1.4.2 py_0 bioconda ; scanpy 1.4.3 py_0 bioconda ; scanpy 1.4.4 py_0 bioconda ; scanpy 1.4.4 py_1 bioconda ; scanpy 1.4.4.post1 py_0 bioconda ; scanpy 1.4.4.post1 py_1 bioconda ; scanpy 1.4.4.post1 py_2 bioconda ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942#issuecomment-577681828
https://github.com/scverse/scanpy/issues/942#issuecomment-577681828:182,Performance,Load,Loading,182,@MichaelPeibo Could you install the version 1.4.5.post1? It is not available in conda and with 1.4.4.post1 I'm getting the same error. Thanks!. ```; conda search -c bioconda scanpy; Loading channels: done; # Name Version Build Channel ; scanpy 1.3.1 py36_0 bioconda ; scanpy 1.3.2 py36_0 bioconda ; scanpy 1.3.3 py36_0 bioconda ; scanpy 1.3.4 py36_0 bioconda ; scanpy 1.3.5 py36_0 bioconda ; scanpy 1.3.6 py36_0 bioconda ; scanpy 1.3.7 py36_0 bioconda ; scanpy 1.4 py_0 bioconda ; scanpy 1.4.1 py_0 bioconda ; scanpy 1.4.2 py_0 bioconda ; scanpy 1.4.3 py_0 bioconda ; scanpy 1.4.4 py_0 bioconda ; scanpy 1.4.4 py_1 bioconda ; scanpy 1.4.4.post1 py_0 bioconda ; scanpy 1.4.4.post1 py_1 bioconda ; scanpy 1.4.4.post1 py_2 bioconda ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942#issuecomment-577681828
https://github.com/scverse/scanpy/issues/942#issuecomment-577689480:28,Availability,avail,available,28,"@massonix Latest version is available on PyPI, so you can try installing via pip install.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942#issuecomment-577689480
https://github.com/scverse/scanpy/issues/942#issuecomment-577689480:62,Deployability,install,installing,62,"@massonix Latest version is available on PyPI, so you can try installing via pip install.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942#issuecomment-577689480
https://github.com/scverse/scanpy/issues/942#issuecomment-577689480:81,Deployability,install,install,81,"@massonix Latest version is available on PyPI, so you can try installing via pip install.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942#issuecomment-577689480
https://github.com/scverse/scanpy/issues/942#issuecomment-580322693:21,Availability,error,error,21,"I still had the same error with `scanpy` 1.4.5, in my case updating `anndata` solved the issue, but now I've hit the slow concatenation problem https://github.com/theislab/anndata/issues/303",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942#issuecomment-580322693
https://github.com/scverse/scanpy/pull/943#issuecomment-577981681:207,Modifiability,variab,variable,207,"I'd like to merge this. I think we could just rename the function, and deprecate `subsample`. I'd like the name `sample` more for this function if we could add a `dim` argument so users can subsample on the variable as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/943#issuecomment-577981681
https://github.com/scverse/scanpy/pull/945#issuecomment-561423626:18,Availability,error,error,18,No idea about the error in the performance test. @flying-sheep ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/945#issuecomment-561423626
https://github.com/scverse/scanpy/pull/945#issuecomment-561423626:31,Performance,perform,performance,31,No idea about the error in the performance test. @flying-sheep ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/945#issuecomment-561423626
https://github.com/scverse/scanpy/pull/945#issuecomment-561423626:43,Testability,test,test,43,No idea about the error in the performance test. @flying-sheep ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/945#issuecomment-561423626
https://github.com/scverse/scanpy/issues/947#issuecomment-562544523:855,Integrability,depend,depends,855,"> I'm not sure i fully understand the point of caching. So you store the exact output of all the computations of a function so that it can be rerun exactly? How big do those objects become?. We've had problems in the past when running notebooks on different computers (by having different distros or just using the server) or just updating a library produced different results in terms of embedding/clustering... The other benefit is that if analyzing the data in multiple stages (or multiple times), you'd have to either store the adata object after each stage and then load it for the next one. Or just run it from scratch, which can take some time. Not to mention a forgotten parameter which affects reproducibility. The caching makes this convenient - just run the notebook. We only store the attributes generated by each function, therefore the size depends on what you cache and the dimensionality of the data. For ~8k cells, PCA takes upto 8MB (if I remember correctly).; Currently, there's no compression scheme in place, but I have it on my todo list.; The other thing would be to add more control to user during runtime about what needs to be cached.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947#issuecomment-562544523
https://github.com/scverse/scanpy/issues/947#issuecomment-562544523:571,Performance,load,load,571,"> I'm not sure i fully understand the point of caching. So you store the exact output of all the computations of a function so that it can be rerun exactly? How big do those objects become?. We've had problems in the past when running notebooks on different computers (by having different distros or just using the server) or just updating a library produced different results in terms of embedding/clustering... The other benefit is that if analyzing the data in multiple stages (or multiple times), you'd have to either store the adata object after each stage and then load it for the next one. Or just run it from scratch, which can take some time. Not to mention a forgotten parameter which affects reproducibility. The caching makes this convenient - just run the notebook. We only store the attributes generated by each function, therefore the size depends on what you cache and the dimensionality of the data. For ~8k cells, PCA takes upto 8MB (if I remember correctly).; Currently, there's no compression scheme in place, but I have it on my todo list.; The other thing would be to add more control to user during runtime about what needs to be cached.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947#issuecomment-562544523
https://github.com/scverse/scanpy/issues/947#issuecomment-562544523:875,Performance,cache,cache,875,"> I'm not sure i fully understand the point of caching. So you store the exact output of all the computations of a function so that it can be rerun exactly? How big do those objects become?. We've had problems in the past when running notebooks on different computers (by having different distros or just using the server) or just updating a library produced different results in terms of embedding/clustering... The other benefit is that if analyzing the data in multiple stages (or multiple times), you'd have to either store the adata object after each stage and then load it for the next one. Or just run it from scratch, which can take some time. Not to mention a forgotten parameter which affects reproducibility. The caching makes this convenient - just run the notebook. We only store the attributes generated by each function, therefore the size depends on what you cache and the dimensionality of the data. For ~8k cells, PCA takes upto 8MB (if I remember correctly).; Currently, there's no compression scheme in place, but I have it on my todo list.; The other thing would be to add more control to user during runtime about what needs to be cached.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947#issuecomment-562544523
https://github.com/scverse/scanpy/issues/947#issuecomment-562544523:1153,Performance,cache,cached,1153,"> I'm not sure i fully understand the point of caching. So you store the exact output of all the computations of a function so that it can be rerun exactly? How big do those objects become?. We've had problems in the past when running notebooks on different computers (by having different distros or just using the server) or just updating a library produced different results in terms of embedding/clustering... The other benefit is that if analyzing the data in multiple stages (or multiple times), you'd have to either store the adata object after each stage and then load it for the next one. Or just run it from scratch, which can take some time. Not to mention a forgotten parameter which affects reproducibility. The caching makes this convenient - just run the notebook. We only store the attributes generated by each function, therefore the size depends on what you cache and the dimensionality of the data. For ~8k cells, PCA takes upto 8MB (if I remember correctly).; Currently, there's no compression scheme in place, but I have it on my todo list.; The other thing would be to add more control to user during runtime about what needs to be cached.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947#issuecomment-562544523
https://github.com/scverse/scanpy/issues/947#issuecomment-562563637:146,Security,hash,hash,146,"So basically this would be a transaction layer, right? Like subsequent lines in a Dockerfile:. 1. AnnDatas with certain initial data start with a hash computed from it; 2. Each interaction creates a new state with an associated hash. The difference (and only thing that has to be stored) between two states is all properties that changed.; 3. If you rerun a script with modifications, all steps that didn’t change just forward to the next state, all states after a change are deleted and the steps executed. Did I get this right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947#issuecomment-562563637
https://github.com/scverse/scanpy/issues/947#issuecomment-562563637:228,Security,hash,hash,228,"So basically this would be a transaction layer, right? Like subsequent lines in a Dockerfile:. 1. AnnDatas with certain initial data start with a hash computed from it; 2. Each interaction creates a new state with an associated hash. The difference (and only thing that has to be stored) between two states is all properties that changed.; 3. If you rerun a script with modifications, all steps that didn’t change just forward to the next state, all states after a change are deleted and the steps executed. Did I get this right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947#issuecomment-562563637
https://github.com/scverse/scanpy/issues/948#issuecomment-571371654:72,Testability,log,logging,72,"Solved same problem for me as well. . For the record, the output of `sc.logging.print_versions()` in my conda environment is: . `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948#issuecomment-571371654
https://github.com/scverse/scanpy/issues/948#issuecomment-571371654:233,Usability,learn,learn,233,"Solved same problem for me as well. . For the record, the output of `sc.logging.print_versions()` in my conda environment is: . `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948#issuecomment-571371654
https://github.com/scverse/scanpy/issues/948#issuecomment-595355427:838,Deployability,release,released,838,"> Well, as that issue says, it’s fixed in [lmcinnes/umap#261](https://github.com/lmcinnes/umap/pull/261), which means it’s in umap 0.3.10. @flying-sheep unfortunately `umap==3.10` does not fix this relative to the latest scanpy version on Bioconda (`1.4.4.post1`). The issue is that the UMAP fix does not address the branch of code that scanpy depends on (specifically the call follows [this branch in the UMAP code](https://github.com/lmcinnes/umap/blob/41205248fb48391d1f6e4effcb974307b7c229ce/umap/umap_.py#L1059)), which still just passes the `init_coords` in as is. . Of course, there has been [a workaround in scanpy since 1.4.5.post1](https://github.com/theislab/scanpy/commit/1400d1e35f908d6f5ab8a8681970ac4aba673565). However, I would caution against the advice in that commit's message, which assumed that once `umap==3.10` was released the workaround could be removed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948#issuecomment-595355427
https://github.com/scverse/scanpy/issues/948#issuecomment-595355427:344,Integrability,depend,depends,344,"> Well, as that issue says, it’s fixed in [lmcinnes/umap#261](https://github.com/lmcinnes/umap/pull/261), which means it’s in umap 0.3.10. @flying-sheep unfortunately `umap==3.10` does not fix this relative to the latest scanpy version on Bioconda (`1.4.4.post1`). The issue is that the UMAP fix does not address the branch of code that scanpy depends on (specifically the call follows [this branch in the UMAP code](https://github.com/lmcinnes/umap/blob/41205248fb48391d1f6e4effcb974307b7c229ce/umap/umap_.py#L1059)), which still just passes the `init_coords` in as is. . Of course, there has been [a workaround in scanpy since 1.4.5.post1](https://github.com/theislab/scanpy/commit/1400d1e35f908d6f5ab8a8681970ac4aba673565). However, I would caution against the advice in that commit's message, which assumed that once `umap==3.10` was released the workaround could be removed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948#issuecomment-595355427
https://github.com/scverse/scanpy/issues/948#issuecomment-595355427:788,Integrability,message,message,788,"> Well, as that issue says, it’s fixed in [lmcinnes/umap#261](https://github.com/lmcinnes/umap/pull/261), which means it’s in umap 0.3.10. @flying-sheep unfortunately `umap==3.10` does not fix this relative to the latest scanpy version on Bioconda (`1.4.4.post1`). The issue is that the UMAP fix does not address the branch of code that scanpy depends on (specifically the call follows [this branch in the UMAP code](https://github.com/lmcinnes/umap/blob/41205248fb48391d1f6e4effcb974307b7c229ce/umap/umap_.py#L1059)), which still just passes the `init_coords` in as is. . Of course, there has been [a workaround in scanpy since 1.4.5.post1](https://github.com/theislab/scanpy/commit/1400d1e35f908d6f5ab8a8681970ac4aba673565). However, I would caution against the advice in that commit's message, which assumed that once `umap==3.10` was released the workaround could be removed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948#issuecomment-595355427
https://github.com/scverse/scanpy/issues/950#issuecomment-1117994615:76,Deployability,integrat,integrate,76,@sjfleming Is there a GIST or repo url to use this code? Might take time to integrate into scanpy/anndata but people can benefit from the code if it already lives somewhere...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/950#issuecomment-1117994615
https://github.com/scverse/scanpy/issues/950#issuecomment-1117994615:76,Integrability,integrat,integrate,76,@sjfleming Is there a GIST or repo url to use this code? Might take time to integrate into scanpy/anndata but people can benefit from the code if it already lives somewhere...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/950#issuecomment-1117994615
https://github.com/scverse/scanpy/pull/952#issuecomment-567379722:187,Availability,error,error,187,"> ; > ; > #530 also related. Are you including the `log_transformed=True` kwarg in your call to `rank_genes_groups()`?. oh, I'm not. But when I included it, the ’ValueError: math domain‘ error still appeared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/952#issuecomment-567379722
https://github.com/scverse/scanpy/pull/952#issuecomment-567617566:231,Availability,error,error,231,"In #706 @LuckyMD gives a minimal working example in [this comment](https://github.com/theislab/scanpy/issues/706#issuecomment-505335006) that may give you a useful starting point. Their advice to create/post a way to reproduce the error is good too, as it may help you identify the source of the problem in the process (and make it easier for others to help troubleshoot)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/952#issuecomment-567617566
https://github.com/scverse/scanpy/pull/952#issuecomment-611280656:66,Safety,avoid,avoid,66,Unfortunately I am still facing this problem! ; Is there a way to avoid it?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/952#issuecomment-611280656
https://github.com/scverse/scanpy/issues/953#issuecomment-583346609:32,Availability,down,downgrade,32,"I've found a workaround, when I downgrade to `anndata=0.6.22.post1` (still with `scanpy==1.4.5.post2`), it generates an output with `paga_path` but also this warning:. ```; FutureWarning: In anndata v0.7+, arrays contained within an AnnData object will maintain their dimensionality. For example, prior to v0.7 `adata[0, 0].X` returned a scalar and `adata[0, :]` returned a 1d array, post v0.7 they will return two dimensional arrays. If you would like to get a one dimensional array from your AnnData object, consider using the `adata.obs_vector`, `adata.var_vector` methods or accessing the array directly.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-583346609
https://github.com/scverse/scanpy/issues/953#issuecomment-583346609:579,Security,access,accessing,579,"I've found a workaround, when I downgrade to `anndata=0.6.22.post1` (still with `scanpy==1.4.5.post2`), it generates an output with `paga_path` but also this warning:. ```; FutureWarning: In anndata v0.7+, arrays contained within an AnnData object will maintain their dimensionality. For example, prior to v0.7 `adata[0, 0].X` returned a scalar and `adata[0, :]` returned a 1d array, post v0.7 they will return two dimensional arrays. If you would like to get a one dimensional array from your AnnData object, consider using the `adata.obs_vector`, `adata.var_vector` methods or accessing the array directly.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-583346609
https://github.com/scverse/scanpy/issues/953#issuecomment-586333599:8,Testability,test,tested,8,"Hi, ; I tested your fix and it works! ; ```; scanpy==1.4.5.2.dev6+gfa408dc7 anndata==0.7.1 ; umap==0.3.10 numpy==1.17.4 scipy==1.3.1 ; pandas==0.25.2 scikit-learn==0.21.3 ; statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```; BTW:; `matplotlib==3.1.3`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-586333599
https://github.com/scverse/scanpy/issues/953#issuecomment-586333599:157,Usability,learn,learn,157,"Hi, ; I tested your fix and it works! ; ```; scanpy==1.4.5.2.dev6+gfa408dc7 anndata==0.7.1 ; umap==0.3.10 numpy==1.17.4 scipy==1.3.1 ; pandas==0.25.2 scikit-learn==0.21.3 ; statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```; BTW:; `matplotlib==3.1.3`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-586333599
https://github.com/scverse/scanpy/issues/953#issuecomment-586339424:34,Testability,test,test,34,"Cool! Can you also try to run the test that fails in the PR please?. Should be `PYTHONPATH=. pytest scanpy/tests/test_plotting.py::test_paga_path`. I have no idea why it does that, as it works for me. If it fails for you, please attach the failed-diff image and the corresponding plot image in a comment here (they’re in `scanpy/tests/figures`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-586339424
https://github.com/scverse/scanpy/issues/953#issuecomment-586339424:107,Testability,test,tests,107,"Cool! Can you also try to run the test that fails in the PR please?. Should be `PYTHONPATH=. pytest scanpy/tests/test_plotting.py::test_paga_path`. I have no idea why it does that, as it works for me. If it fails for you, please attach the failed-diff image and the corresponding plot image in a comment here (they’re in `scanpy/tests/figures`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-586339424
https://github.com/scverse/scanpy/issues/953#issuecomment-586339424:329,Testability,test,tests,329,"Cool! Can you also try to run the test that fails in the PR please?. Should be `PYTHONPATH=. pytest scanpy/tests/test_plotting.py::test_paga_path`. I have no idea why it does that, as it works for me. If it fails for you, please attach the failed-diff image and the corresponding plot image in a comment here (they’re in `scanpy/tests/figures`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-586339424
https://github.com/scverse/scanpy/issues/953#issuecomment-586343222:14,Testability,test,test,14,"Ok, I ran the test and was successful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-586343222
https://github.com/scverse/scanpy/issues/953#issuecomment-586596871:49,Testability,test,test,49,"Damn … it’s going to be hard to find out why the test fails on Travis then, and I don’t feel comfortable not adding a test. Do we have an Amazon web service (AWS) subscription at ICB that we could use to upload failed image tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-586596871
https://github.com/scverse/scanpy/issues/953#issuecomment-586596871:118,Testability,test,test,118,"Damn … it’s going to be hard to find out why the test fails on Travis then, and I don’t feel comfortable not adding a test. Do we have an Amazon web service (AWS) subscription at ICB that we could use to upload failed image tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-586596871
https://github.com/scverse/scanpy/issues/953#issuecomment-586596871:224,Testability,test,tests,224,"Damn … it’s going to be hard to find out why the test fails on Travis then, and I don’t feel comfortable not adding a test. Do we have an Amazon web service (AWS) subscription at ICB that we could use to upload failed image tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-586596871
https://github.com/scverse/scanpy/issues/953#issuecomment-618739346:67,Deployability,release,release,67,Getting the same bug. What's the status on merging this fix into a release? Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-618739346
https://github.com/scverse/scanpy/issues/953#issuecomment-622393426:61,Testability,log,login,61,Can someone at ICB make the S3 bucket happen and give me the login creds? Then I can merge this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-622393426
https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:1573,Deployability,patch,patch,1573,onda-forge; atomicwrites 1.4.0 pyh9f0ad1d_0 conda-forge; attrs 20.2.0 pyh9f0ad1d_0 conda-forge; autopep8 1.5.4 pyh9f0ad1d_0 conda-forge; babel 2.8.0 py_0 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 py_2 conda-forge; backports.functools_lru_cache 1.6.1 py_0 conda-forge; bleach 3.2.1 pyh9f0ad1d_0 conda-forge; blosc 1.20.1 hb1e8313_0 conda-forge; brotlipy 0.7.0 py38h94c058a_1001 conda-forge; bzip2 1.0.8 haf1e3a3_3 conda-forge; c-ares 1.16.1 haf1e3a3_3 conda-forge; ca-certificates 2020.10.14 0 ; cairo 1.16.0 h360c52f_1006 conda-forge; certifi 2020.6.20 py38h5347e94_2 conda-forge; cffi 1.14.3 py38h9edaa1b_1 conda-forge; chardet 3.0.4 py38h5347e94_1008 conda-forge; click 7.1.2 pyh9f0ad1d_0 conda-forge; cloudpickle 1.6.0 py_0 conda-forge; colorama 0.4.4 pyh9f0ad1d_0 conda-forge; cryptography 3.2 py38hf6767f5_0 conda-forge; cycler 0.10.0 py_2 conda-forge; dbus 1.13.18 h18a8e69_0 ; decorator 4.4.2 py_0 conda-forge; defusedxml 0.6.0 py_0 conda-forge; diff-match-patch 20200713 pyh9f0ad1d_0 conda-forge; docutils 0.16 py38h5347e94_2 conda-forge; entrypoints 0.3 py38h32f6830_1002 conda-forge; expat 2.2.10 hb1e8313_2 ; fa2 0.3.5 py38h4d0b108_0 conda-forge; flake8 3.8.4 py_0 conda-forge; fontconfig 2.13.1 h79c0d67_1002 conda-forge; freetype 2.10.4 ha233b18_0 conda-forge; future 0.18.2 py38h32f6830_2 conda-forge; get_version 2.1 py_1 conda-forge; gettext 0.19.8.1 haf92f58_1004 conda-forge; glib 2.66.2 hb1e8313_0 conda-forge; gmp 6.2.0 hb1e8313_3 conda-forge; h5py 2.10.0 nompi_py38hf6831e1_105 conda-forge; hdf5 1.10.6 nompi_hc457bb4_1110 conda-forge; icu 67.1 hb1e8313_0 conda-forge; idna 2.10 pyh9f0ad1d_0 conda-forge; imagesize 1.2.0 py_0 conda-forge; importlib-metadata 2.0.0 py38h32f6830_0 conda-forge; importlib_metadata 2.0.0 1 conda-forge; intervaltree 3.1.0 py_0 ; ipykernel 5.3.4 py38h1cdfbd6_1 conda-forge; ipython 7.18.1 py38h1cdfbd6_1 conda-forge; ipython_genutils 0.2.0 py_1 conda-forge; isort 5.6.4 py_0 conda-forge; jedi 0.17.1 py38h32f6830_0 conda-forge,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684
https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:3095,Integrability,wrap,wrap,3095,.10.0 nompi_py38hf6831e1_105 conda-forge; hdf5 1.10.6 nompi_hc457bb4_1110 conda-forge; icu 67.1 hb1e8313_0 conda-forge; idna 2.10 pyh9f0ad1d_0 conda-forge; imagesize 1.2.0 py_0 conda-forge; importlib-metadata 2.0.0 py38h32f6830_0 conda-forge; importlib_metadata 2.0.0 1 conda-forge; intervaltree 3.1.0 py_0 ; ipykernel 5.3.4 py38h1cdfbd6_1 conda-forge; ipython 7.18.1 py38h1cdfbd6_1 conda-forge; ipython_genutils 0.2.0 py_1 conda-forge; isort 5.6.4 py_0 conda-forge; jedi 0.17.1 py38h32f6830_0 conda-forge; jinja2 2.11.2 pyh9f0ad1d_0 conda-forge; joblib 0.17.0 py_0 conda-forge; jpeg 9d h0b31af3_0 conda-forge; jsonschema 3.2.0 py38h32f6830_1 conda-forge; jupyter_client 6.1.7 py_0 conda-forge; jupyter_core 4.6.3 py38h32f6830_2 conda-forge; jupyterlab_pygments 0.1.2 pyh9f0ad1d_0 conda-forge; keyring 21.4.0 py38h32f6830_2 conda-forge; kiwisolver 1.3.0 py38h02bb52f_0 conda-forge; krb5 1.17.1 h75d18d8_3 conda-forge; lazy-object-proxy 1.4.3 py38h64e0658_2 conda-forge; lcms2 2.11 h174193d_0 conda-forge; legacy-api-wrap 1.2 py_0 conda-forge; libblas 3.9.0 2_openblas conda-forge; libcblas 3.9.0 2_openblas conda-forge; libclang 10.0.1 default_hf57f61e_1 conda-forge; libcurl 7.71.1 h9bf37e3_8 conda-forge; libcxx 11.0.0 h439d374_0 conda-forge; libedit 3.1.20191231 hed1e85f_2 conda-forge; libev 4.33 haf1e3a3_1 conda-forge; libffi 3.2.1 1 bioconda; libgfortran 4.0.0 h50e675f_13 conda-forge; libgfortran4 7.5.0 h50e675f_13 conda-forge; libglib 2.66.2 hdb5fb44_0 conda-forge; libiconv 1.16 haf1e3a3_0 conda-forge; liblapack 3.9.0 2_openblas conda-forge; libllvm10 10.0.1 h009f743_3 conda-forge; libnghttp2 1.41.0 h7580e61_2 conda-forge; libopenblas 0.3.12 openmp_h63d9170_1 conda-forge; libpng 1.6.37 hb0a8c7a_2 conda-forge; libpq 12.3 haa216e0_2 conda-forge; libsodium 1.0.18 haf1e3a3_1 conda-forge; libspatialindex 1.9.3 h4a8c4bd_3 conda-forge; libssh2 1.9.0 h39bdce6_5 conda-forge; libtiff 4.1.0 h2ae36a8_6 conda-forge; libwebp-base 1.1.0 h0b31af3_3 conda-forge; libxml2 2.9.10 h7fdee97_2 conda-for,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684
https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:8765,Integrability,wrap,wrapt,8765,; rope 0.18.0 pyh9f0ad1d_0 conda-forge; rtree 0.9.4 py38h08f867b_1 conda-forge; scanpy 1.6.0 py_0 bioconda; scikit-learn 0.23.2 py38hc63f23e_1 conda-forge; scipy 1.5.2 py38hf17e0cf_2 conda-forge; seaborn 0.11.0 0 conda-forge; seaborn-base 0.11.0 py_0 conda-forge; setuptools 50.3.0 py38h0dc7051_1 ; setuptools-scm 4.1.2 pyh9f0ad1d_0 conda-forge; setuptools_scm 4.1.2 0 conda-forge; sinfo 0.3.1 py_0 conda-forge; six 1.15.0 pyh9f0ad1d_0 conda-forge; snowballstemmer 2.0.0 py_0 conda-forge; sortedcontainers 2.2.2 pyh9f0ad1d_0 conda-forge; sphinx 3.2.1 py_0 conda-forge; sphinxcontrib-applehelp 1.0.2 py_0 conda-forge; sphinxcontrib-devhelp 1.0.2 py_0 conda-forge; sphinxcontrib-htmlhelp 1.0.3 py_0 conda-forge; sphinxcontrib-jsmath 1.0.1 py_0 conda-forge; sphinxcontrib-qthelp 1.0.3 py_0 conda-forge; sphinxcontrib-serializinghtml 1.1.4 py_0 conda-forge; spyder 4.1.5 py38h32f6830_0 conda-forge; spyder-kernels 1.9.4 py38h32f6830_0 conda-forge; sqlite 3.33.0 h960bd1c_1 conda-forge; statsmodels 0.12.0 py38h174b24a_1 conda-forge; stdlib-list 0.7.0 py38h32f6830_1 conda-forge; tbb 2020.3 h879752b_0 ; testpath 0.4.4 py_0 conda-forge; texttable 1.6.3 pyh9f0ad1d_0 conda-forge; threadpoolctl 2.1.0 pyh5ca1d4c_0 conda-forge; tk 8.6.10 hb0a8c7a_1 conda-forge; toml 0.10.1 pyh9f0ad1d_0 conda-forge; tornado 6.0.4 py38h4d0b108_2 conda-forge; tqdm 4.51.0 pyh9f0ad1d_0 conda-forge; traitlets 5.0.5 py_0 conda-forge; ujson 4.0.1 py38h11c0d25_1 conda-forge; umap-learn 0.4.6 py38h32f6830_0 conda-forge; urllib3 1.25.11 py_0 conda-forge; watchdog 0.10.3 py38h4d0b108_2 conda-forge; wcwidth 0.2.5 pyh9f0ad1d_2 conda-forge; webencodings 0.5.1 py_1 conda-forge; wheel 0.35.1 pyh9f0ad1d_0 conda-forge; wrapt 1.11.2 py38h4d0b108_1 conda-forge; wurlitzer 2.0.1 py38_0 ; xz 5.2.5 haf1e3a3_1 conda-forge; yaml 0.2.5 haf1e3a3_0 conda-forge; yapf 0.30.0 pyh9f0ad1d_0 conda-forge; zeromq 4.3.3 hb1e8313_2 conda-forge; zipp 3.4.0 py_0 conda-forge; zlib 1.2.11 h7795811_1010 conda-forge; zstd 1.4.5 h0384e3a_2 conda-forge; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684
https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:1076,Security,certificate,certificates,1076,"aga_path(adata, nodes = [0, 5, 1], keys = ['IFNG', 'GZMB'], show = True)`. > TypeError: Image data of dtype object cannot be converted to float. What is the current solution to this issue?. Many thanks,; Lucy. ```; # Name Version Build Channel; alabaster 0.7.12 py_0 conda-forge; anndata 0.7.4 py38h32f6830_0 conda-forge; applaunchservices 0.2.1 py_0 conda-forge; appnope 0.1.0 py38h32f6830_1002 conda-forge; argh 0.26.2 py38_1001 conda-forge; astroid 2.4.2 py38h32f6830_1 conda-forge; async_generator 1.10 py_0 conda-forge; atomicwrites 1.4.0 pyh9f0ad1d_0 conda-forge; attrs 20.2.0 pyh9f0ad1d_0 conda-forge; autopep8 1.5.4 pyh9f0ad1d_0 conda-forge; babel 2.8.0 py_0 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 py_2 conda-forge; backports.functools_lru_cache 1.6.1 py_0 conda-forge; bleach 3.2.1 pyh9f0ad1d_0 conda-forge; blosc 1.20.1 hb1e8313_0 conda-forge; brotlipy 0.7.0 py38h94c058a_1001 conda-forge; bzip2 1.0.8 haf1e3a3_3 conda-forge; c-ares 1.16.1 haf1e3a3_3 conda-forge; ca-certificates 2020.10.14 0 ; cairo 1.16.0 h360c52f_1006 conda-forge; certifi 2020.6.20 py38h5347e94_2 conda-forge; cffi 1.14.3 py38h9edaa1b_1 conda-forge; chardet 3.0.4 py38h5347e94_1008 conda-forge; click 7.1.2 pyh9f0ad1d_0 conda-forge; cloudpickle 1.6.0 py_0 conda-forge; colorama 0.4.4 pyh9f0ad1d_0 conda-forge; cryptography 3.2 py38hf6767f5_0 conda-forge; cycler 0.10.0 py_2 conda-forge; dbus 1.13.18 h18a8e69_0 ; decorator 4.4.2 py_0 conda-forge; defusedxml 0.6.0 py_0 conda-forge; diff-match-patch 20200713 pyh9f0ad1d_0 conda-forge; docutils 0.16 py38h5347e94_2 conda-forge; entrypoints 0.3 py38h32f6830_1002 conda-forge; expat 2.2.10 hb1e8313_2 ; fa2 0.3.5 py38h4d0b108_0 conda-forge; flake8 3.8.4 py_0 conda-forge; fontconfig 2.13.1 h79c0d67_1002 conda-forge; freetype 2.10.4 ha233b18_0 conda-forge; future 0.18.2 py38h32f6830_2 conda-forge; get_version 2.1 py_1 conda-forge; gettext 0.19.8.1 haf92f58_1004 conda-forge; glib 2.66.2 hb1e8313_0 conda-forge; gmp 6.2.0 hb1e8313_3 conda-forge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684
https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:4409,Testability,mock,mock,4409,-forge; libffi 3.2.1 1 bioconda; libgfortran 4.0.0 h50e675f_13 conda-forge; libgfortran4 7.5.0 h50e675f_13 conda-forge; libglib 2.66.2 hdb5fb44_0 conda-forge; libiconv 1.16 haf1e3a3_0 conda-forge; liblapack 3.9.0 2_openblas conda-forge; libllvm10 10.0.1 h009f743_3 conda-forge; libnghttp2 1.41.0 h7580e61_2 conda-forge; libopenblas 0.3.12 openmp_h63d9170_1 conda-forge; libpng 1.6.37 hb0a8c7a_2 conda-forge; libpq 12.3 haa216e0_2 conda-forge; libsodium 1.0.18 haf1e3a3_1 conda-forge; libspatialindex 1.9.3 h4a8c4bd_3 conda-forge; libssh2 1.9.0 h39bdce6_5 conda-forge; libtiff 4.1.0 h2ae36a8_6 conda-forge; libwebp-base 1.1.0 h0b31af3_3 conda-forge; libxml2 2.9.10 h7fdee97_2 conda-forge; llvm-openmp 11.0.0 h73239a0_1 conda-forge; llvmlite 0.34.0 py38h3707e27_2 conda-forge; loompy 3.0.6 py_0 conda-forge; lz4-c 1.9.2 hb1e8313_3 conda-forge; markupsafe 1.1.1 py38h94c058a_2 conda-forge; matplotlib-base 3.3.2 py38had0acaf_1 conda-forge; mccabe 0.6.1 py_1 conda-forge; mistune 0.8.4 py38h4d0b108_1002 conda-forge; mock 4.0.2 py38h32f6830_1 conda-forge; mysql-common 8.0.21 2 conda-forge; mysql-libs 8.0.21 hfb8f7af_2 conda-forge; natsort 7.0.1 py_0 conda-forge; nbclient 0.5.1 py_0 conda-forge; nbconvert 6.0.7 py38h32f6830_2 conda-forge; nbformat 5.0.8 py_0 conda-forge; ncurses 6.2 hb1e8313_2 conda-forge; nest-asyncio 1.4.1 py_0 conda-forge; networkx 2.5 py_0 conda-forge; nspr 4.29 hb1e8313_1 conda-forge; nss 3.47 hcec2283_0 conda-forge; numba 0.51.2 py38h6be0db6_0 conda-forge; numexpr 2.7.1 py38h6be0db6_3 conda-forge; numpy 1.19.2 py38ha98150c_1 conda-forge; numpy_groupies 0.9.13 pyh9f0ad1d_1 conda-forge; numpydoc 1.1.0 py_1 conda-forge; olefile 0.46 pyh9f0ad1d_1 conda-forge; openssl 1.1.1h haf1e3a3_0 conda-forge; packaging 20.4 pyh9f0ad1d_0 conda-forge; pandas 1.1.3 py38h48ddb8e_2 conda-forge; pandoc 2.11.0.4 h22f3db7_0 conda-forge; pandocfilters 1.4.2 py_1 conda-forge; parso 0.7.0 pyh9f0ad1d_0 conda-forge; pathtools 0.1.2 py_1 conda-forge; patsy 0.5.1 py_0 conda-forge; pcre 8.44 hb1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684
https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:8179,Testability,test,testpath,8179,; rope 0.18.0 pyh9f0ad1d_0 conda-forge; rtree 0.9.4 py38h08f867b_1 conda-forge; scanpy 1.6.0 py_0 bioconda; scikit-learn 0.23.2 py38hc63f23e_1 conda-forge; scipy 1.5.2 py38hf17e0cf_2 conda-forge; seaborn 0.11.0 0 conda-forge; seaborn-base 0.11.0 py_0 conda-forge; setuptools 50.3.0 py38h0dc7051_1 ; setuptools-scm 4.1.2 pyh9f0ad1d_0 conda-forge; setuptools_scm 4.1.2 0 conda-forge; sinfo 0.3.1 py_0 conda-forge; six 1.15.0 pyh9f0ad1d_0 conda-forge; snowballstemmer 2.0.0 py_0 conda-forge; sortedcontainers 2.2.2 pyh9f0ad1d_0 conda-forge; sphinx 3.2.1 py_0 conda-forge; sphinxcontrib-applehelp 1.0.2 py_0 conda-forge; sphinxcontrib-devhelp 1.0.2 py_0 conda-forge; sphinxcontrib-htmlhelp 1.0.3 py_0 conda-forge; sphinxcontrib-jsmath 1.0.1 py_0 conda-forge; sphinxcontrib-qthelp 1.0.3 py_0 conda-forge; sphinxcontrib-serializinghtml 1.1.4 py_0 conda-forge; spyder 4.1.5 py38h32f6830_0 conda-forge; spyder-kernels 1.9.4 py38h32f6830_0 conda-forge; sqlite 3.33.0 h960bd1c_1 conda-forge; statsmodels 0.12.0 py38h174b24a_1 conda-forge; stdlib-list 0.7.0 py38h32f6830_1 conda-forge; tbb 2020.3 h879752b_0 ; testpath 0.4.4 py_0 conda-forge; texttable 1.6.3 pyh9f0ad1d_0 conda-forge; threadpoolctl 2.1.0 pyh5ca1d4c_0 conda-forge; tk 8.6.10 hb0a8c7a_1 conda-forge; toml 0.10.1 pyh9f0ad1d_0 conda-forge; tornado 6.0.4 py38h4d0b108_2 conda-forge; tqdm 4.51.0 pyh9f0ad1d_0 conda-forge; traitlets 5.0.5 py_0 conda-forge; ujson 4.0.1 py38h11c0d25_1 conda-forge; umap-learn 0.4.6 py38h32f6830_0 conda-forge; urllib3 1.25.11 py_0 conda-forge; watchdog 0.10.3 py38h4d0b108_2 conda-forge; wcwidth 0.2.5 pyh9f0ad1d_2 conda-forge; webencodings 0.5.1 py_1 conda-forge; wheel 0.35.1 pyh9f0ad1d_0 conda-forge; wrapt 1.11.2 py38h4d0b108_1 conda-forge; wurlitzer 2.0.1 py38_0 ; xz 5.2.5 haf1e3a3_1 conda-forge; yaml 0.2.5 haf1e3a3_0 conda-forge; yapf 0.30.0 pyh9f0ad1d_0 conda-forge; zeromq 4.3.3 hb1e8313_2 conda-forge; zipp 3.4.0 py_0 conda-forge; zlib 1.2.11 h7795811_1010 conda-forge; zstd 1.4.5 h0384e3a_2 conda-forge; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684
https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:7195,Usability,learn,learn,7195,.19.18 pypi_0 pypi; pyqtchart 5.12 pypi_0 pypi; pyqtwebengine 5.12.1 pypi_0 pypi; pyrsistent 0.17.3 py38h4d0b108_1 conda-forge; pysocks 1.7.1 py38h5347e94_2 conda-forge; pytables 3.6.1 py38h4e4ac5c_3 conda-forge; python 3.8.6 hcfdab8c_0_cpython conda-forge; python-dateutil 2.8.1 py_0 conda-forge; python-igraph 0.8.3 py38hcde0000_2 conda-forge; python-jsonrpc-server 0.4.0 pyh9f0ad1d_0 conda-forge; python-language-server 0.35.1 py_0 conda-forge; python.app 2 py38_10 ; python_abi 3.8 1_cp38 conda-forge; pytz 2020.1 pyh9f0ad1d_0 conda-forge; pyyaml 5.3.1 py38h94c058a_1 conda-forge; pyzmq 19.0.2 py38h2c785a9_2 conda-forge; qdarkstyle 2.8.1 pyh9f0ad1d_1 conda-forge; qt 5.12.9 h717870c_0 conda-forge; qtawesome 1.0.1 pyh9f0ad1d_0 conda-forge; qtconsole 4.7.7 pyh9f0ad1d_0 conda-forge; qtpy 1.9.0 py_0 conda-forge; readline 8.0 h0678c8f_2 conda-forge; requests 2.24.0 pyh9f0ad1d_0 conda-forge; rope 0.18.0 pyh9f0ad1d_0 conda-forge; rtree 0.9.4 py38h08f867b_1 conda-forge; scanpy 1.6.0 py_0 bioconda; scikit-learn 0.23.2 py38hc63f23e_1 conda-forge; scipy 1.5.2 py38hf17e0cf_2 conda-forge; seaborn 0.11.0 0 conda-forge; seaborn-base 0.11.0 py_0 conda-forge; setuptools 50.3.0 py38h0dc7051_1 ; setuptools-scm 4.1.2 pyh9f0ad1d_0 conda-forge; setuptools_scm 4.1.2 0 conda-forge; sinfo 0.3.1 py_0 conda-forge; six 1.15.0 pyh9f0ad1d_0 conda-forge; snowballstemmer 2.0.0 py_0 conda-forge; sortedcontainers 2.2.2 pyh9f0ad1d_0 conda-forge; sphinx 3.2.1 py_0 conda-forge; sphinxcontrib-applehelp 1.0.2 py_0 conda-forge; sphinxcontrib-devhelp 1.0.2 py_0 conda-forge; sphinxcontrib-htmlhelp 1.0.3 py_0 conda-forge; sphinxcontrib-jsmath 1.0.1 py_0 conda-forge; sphinxcontrib-qthelp 1.0.3 py_0 conda-forge; sphinxcontrib-serializinghtml 1.1.4 py_0 conda-forge; spyder 4.1.5 py38h32f6830_0 conda-forge; spyder-kernels 1.9.4 py38h32f6830_0 conda-forge; sqlite 3.33.0 h960bd1c_1 conda-forge; statsmodels 0.12.0 py38h174b24a_1 conda-forge; stdlib-list 0.7.0 py38h32f6830_1 conda-forge; tbb 2020.3 h879752b_0 ; testpath,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684
https://github.com/scverse/scanpy/issues/953#issuecomment-719504684:8531,Usability,learn,learn,8531,; rope 0.18.0 pyh9f0ad1d_0 conda-forge; rtree 0.9.4 py38h08f867b_1 conda-forge; scanpy 1.6.0 py_0 bioconda; scikit-learn 0.23.2 py38hc63f23e_1 conda-forge; scipy 1.5.2 py38hf17e0cf_2 conda-forge; seaborn 0.11.0 0 conda-forge; seaborn-base 0.11.0 py_0 conda-forge; setuptools 50.3.0 py38h0dc7051_1 ; setuptools-scm 4.1.2 pyh9f0ad1d_0 conda-forge; setuptools_scm 4.1.2 0 conda-forge; sinfo 0.3.1 py_0 conda-forge; six 1.15.0 pyh9f0ad1d_0 conda-forge; snowballstemmer 2.0.0 py_0 conda-forge; sortedcontainers 2.2.2 pyh9f0ad1d_0 conda-forge; sphinx 3.2.1 py_0 conda-forge; sphinxcontrib-applehelp 1.0.2 py_0 conda-forge; sphinxcontrib-devhelp 1.0.2 py_0 conda-forge; sphinxcontrib-htmlhelp 1.0.3 py_0 conda-forge; sphinxcontrib-jsmath 1.0.1 py_0 conda-forge; sphinxcontrib-qthelp 1.0.3 py_0 conda-forge; sphinxcontrib-serializinghtml 1.1.4 py_0 conda-forge; spyder 4.1.5 py38h32f6830_0 conda-forge; spyder-kernels 1.9.4 py38h32f6830_0 conda-forge; sqlite 3.33.0 h960bd1c_1 conda-forge; statsmodels 0.12.0 py38h174b24a_1 conda-forge; stdlib-list 0.7.0 py38h32f6830_1 conda-forge; tbb 2020.3 h879752b_0 ; testpath 0.4.4 py_0 conda-forge; texttable 1.6.3 pyh9f0ad1d_0 conda-forge; threadpoolctl 2.1.0 pyh5ca1d4c_0 conda-forge; tk 8.6.10 hb0a8c7a_1 conda-forge; toml 0.10.1 pyh9f0ad1d_0 conda-forge; tornado 6.0.4 py38h4d0b108_2 conda-forge; tqdm 4.51.0 pyh9f0ad1d_0 conda-forge; traitlets 5.0.5 py_0 conda-forge; ujson 4.0.1 py38h11c0d25_1 conda-forge; umap-learn 0.4.6 py38h32f6830_0 conda-forge; urllib3 1.25.11 py_0 conda-forge; watchdog 0.10.3 py38h4d0b108_2 conda-forge; wcwidth 0.2.5 pyh9f0ad1d_2 conda-forge; webencodings 0.5.1 py_1 conda-forge; wheel 0.35.1 pyh9f0ad1d_0 conda-forge; wrapt 1.11.2 py38h4d0b108_1 conda-forge; wurlitzer 2.0.1 py38_0 ; xz 5.2.5 haf1e3a3_1 conda-forge; yaml 0.2.5 haf1e3a3_0 conda-forge; yapf 0.30.0 pyh9f0ad1d_0 conda-forge; zeromq 4.3.3 hb1e8313_2 conda-forge; zipp 3.4.0 py_0 conda-forge; zlib 1.2.11 h7795811_1010 conda-forge; zstd 1.4.5 h0384e3a_2 conda-forge; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-719504684
https://github.com/scverse/scanpy/issues/953#issuecomment-778184917:108,Availability,error,error,108,"I hve to clarify: I tried out the fix provided above from @flying-sheep and it does the job for me, but the error persists on the `master` branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-778184917
https://github.com/scverse/scanpy/issues/953#issuecomment-778186427:604,Usability,learn,learn,604,"I just tried again with the reprex above and it works for me; ```python; import scanpy as sc; adata = sc.datasets.paul15(); sc.pp.pca(adata); sc.pp.neighbors(adata); sc.tl.dpt(adata); sc.tl.paga(adata, groups='paul15_clusters'); sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']); ```; ![image](https://user-images.githubusercontent.com/25887487/107772027-f1133d00-6d3b-11eb-8866-f514acea297a.png). I'm on a separate branch but it's on par with current master; ```; scanpy==1.7.0rc2.dev25+g56303580.d20210212 anndata==0.7.4 umap==0.4.6 numpy==1.19.4 scipy==1.5.2 pandas==1.1.4 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 leidenalg==0.8.2. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-778186427
https://github.com/scverse/scanpy/issues/953#issuecomment-778212671:67,Availability,Error,Error,67,"I copied your code to a google colabs instance and ran into a Type Error similar to the one above:; https://colab.research.google.com/drive/1LYxOAuNqaJHGfRjNjyluUHk9BFsmkWa4?usp=sharing . Error message:; ```; TypeError Traceback (most recent call last); <ipython-input-3-9abce68d1753> in <module>(); 4 sc.tl.dpt(adata); 5 sc.tl.paga(adata, groups='paul15_clusters'); ----> 6 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). 5 frames; /usr/local/lib/python3.6/dist-packages/matplotlib/image.py in set_data(self, A); 697 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 698 raise TypeError(""Invalid shape {} for image data""; --> 699 .format(self._A.shape)); 700 ; 701 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data; ```. Versions: ; ```; scanpy==1.7.0 ; anndata==0.7.5 ; umap==0.5.0 ; numpy==1.19.5 ; scipy==1.4.1 ; pandas==1.1.5 ; scikit-learn==0.22.2.post1 ; statsmodels==0.10.2 ; python-igraph==0.8.3 ; leidenalg==0.8.3; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-778212671
https://github.com/scverse/scanpy/issues/953#issuecomment-778212671:188,Availability,Error,Error,188,"I copied your code to a google colabs instance and ran into a Type Error similar to the one above:; https://colab.research.google.com/drive/1LYxOAuNqaJHGfRjNjyluUHk9BFsmkWa4?usp=sharing . Error message:; ```; TypeError Traceback (most recent call last); <ipython-input-3-9abce68d1753> in <module>(); 4 sc.tl.dpt(adata); 5 sc.tl.paga(adata, groups='paul15_clusters'); ----> 6 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). 5 frames; /usr/local/lib/python3.6/dist-packages/matplotlib/image.py in set_data(self, A); 697 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 698 raise TypeError(""Invalid shape {} for image data""; --> 699 .format(self._A.shape)); 700 ; 701 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data; ```. Versions: ; ```; scanpy==1.7.0 ; anndata==0.7.5 ; umap==0.5.0 ; numpy==1.19.5 ; scipy==1.4.1 ; pandas==1.1.5 ; scikit-learn==0.22.2.post1 ; statsmodels==0.10.2 ; python-igraph==0.8.3 ; leidenalg==0.8.3; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-778212671
https://github.com/scverse/scanpy/issues/953#issuecomment-778212671:194,Integrability,message,message,194,"I copied your code to a google colabs instance and ran into a Type Error similar to the one above:; https://colab.research.google.com/drive/1LYxOAuNqaJHGfRjNjyluUHk9BFsmkWa4?usp=sharing . Error message:; ```; TypeError Traceback (most recent call last); <ipython-input-3-9abce68d1753> in <module>(); 4 sc.tl.dpt(adata); 5 sc.tl.paga(adata, groups='paul15_clusters'); ----> 6 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). 5 frames; /usr/local/lib/python3.6/dist-packages/matplotlib/image.py in set_data(self, A); 697 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 698 raise TypeError(""Invalid shape {} for image data""; --> 699 .format(self._A.shape)); 700 ; 701 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data; ```. Versions: ; ```; scanpy==1.7.0 ; anndata==0.7.5 ; umap==0.5.0 ; numpy==1.19.5 ; scipy==1.4.1 ; pandas==1.1.5 ; scikit-learn==0.22.2.post1 ; statsmodels==0.10.2 ; python-igraph==0.8.3 ; leidenalg==0.8.3; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-778212671
https://github.com/scverse/scanpy/issues/953#issuecomment-778212671:894,Usability,learn,learn,894,"I copied your code to a google colabs instance and ran into a Type Error similar to the one above:; https://colab.research.google.com/drive/1LYxOAuNqaJHGfRjNjyluUHk9BFsmkWa4?usp=sharing . Error message:; ```; TypeError Traceback (most recent call last); <ipython-input-3-9abce68d1753> in <module>(); 4 sc.tl.dpt(adata); 5 sc.tl.paga(adata, groups='paul15_clusters'); ----> 6 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). 5 frames; /usr/local/lib/python3.6/dist-packages/matplotlib/image.py in set_data(self, A); 697 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 698 raise TypeError(""Invalid shape {} for image data""; --> 699 .format(self._A.shape)); 700 ; 701 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data; ```. Versions: ; ```; scanpy==1.7.0 ; anndata==0.7.5 ; umap==0.5.0 ; numpy==1.19.5 ; scipy==1.4.1 ; pandas==1.1.5 ; scikit-learn==0.22.2.post1 ; statsmodels==0.10.2 ; python-igraph==0.8.3 ; leidenalg==0.8.3; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-778212671
https://github.com/scverse/scanpy/issues/955#issuecomment-566487331:192,Availability,error,error,192,"Tbh, I found out about `groups` after writing the function and looking for a way to put the dots in front. Maybe there is a simpler way to do this... But then the command you suggest gives an error on my own data if I don't also specify `color='bulk_labels'` (works for the pbmc68k, but doesn't colour anything in), and then it just puts all the labels on the same plot and doesn't create small multiples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955#issuecomment-566487331
https://github.com/scverse/scanpy/issues/955#issuecomment-566487331:124,Usability,simpl,simpler,124,"Tbh, I found out about `groups` after writing the function and looking for a way to put the dots in front. Maybe there is a simpler way to do this... But then the command you suggest gives an error on my own data if I don't also specify `color='bulk_labels'` (works for the pbmc68k, but doesn't colour anything in), and then it just puts all the labels on the same plot and doesn't create small multiples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955#issuecomment-566487331
https://github.com/scverse/scanpy/issues/955#issuecomment-566661312:103,Usability,simpl,simply,103,"Just saw this by chance. As we're planning to merge with scvelo's plotting modules soonish, that would simply become; `scv.pl.scatter(adata, groups=[[c] for c in adata.obs['clusters'].cat.categories], color='clusters', ncols=4)`, simply passing a list of lists to `groups` (without copying a whole anndata object). Will that be sufficient?. ![image](https://user-images.githubusercontent.com/31883718/71019675-10dcb000-20fb-11ea-80da-78301d37b958.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955#issuecomment-566661312
https://github.com/scverse/scanpy/issues/955#issuecomment-566661312:230,Usability,simpl,simply,230,"Just saw this by chance. As we're planning to merge with scvelo's plotting modules soonish, that would simply become; `scv.pl.scatter(adata, groups=[[c] for c in adata.obs['clusters'].cat.categories], color='clusters', ncols=4)`, simply passing a list of lists to `groups` (without copying a whole anndata object). Will that be sufficient?. ![image](https://user-images.githubusercontent.com/31883718/71019675-10dcb000-20fb-11ea-80da-78301d37b958.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955#issuecomment-566661312
https://github.com/scverse/scanpy/issues/955#issuecomment-897602161:46,Availability,error,error,46,Tried with the group option but got an `Value error: The truth value of a Index is ambiguous.` As I didn't know how to deal with it I just applied the function @LuckyMD posted above and it worked perfectly alright.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955#issuecomment-897602161
https://github.com/scverse/scanpy/issues/955#issuecomment-1862227440:121,Availability,error,error,121,"Hi there - random question, but for some reason, after applying this plotting solution, it specifically then leads to an error being thrown when the adata object is written (I double checked, and this doesn't happen if this `scv.pl.scatter` command isn't called). . `--> 103 write_elem(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs)`. `TypeError: Can't implicitly convert non-string objects to strings`. Is something stored in the object that is specific to this here, that can lead to an AnnData write error? The issue relates to the .obs column, and I can certainly save the adata object if not running this plotting command. I also checked the dtypes of the obs columns, and there doesn't seem to be anything out of the ordinary there either. Any help would be appreciated, it took me some time to figure out this was causing the issue! (and it's a bit frustrating to not be able to save an object just from running a plot command)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955#issuecomment-1862227440
https://github.com/scverse/scanpy/issues/955#issuecomment-1862227440:508,Availability,error,error,508,"Hi there - random question, but for some reason, after applying this plotting solution, it specifically then leads to an error being thrown when the adata object is written (I double checked, and this doesn't happen if this `scv.pl.scatter` command isn't called). . `--> 103 write_elem(f, ""obs"", adata.obs, dataset_kwargs=dataset_kwargs)`. `TypeError: Can't implicitly convert non-string objects to strings`. Is something stored in the object that is specific to this here, that can lead to an AnnData write error? The issue relates to the .obs column, and I can certainly save the adata object if not running this plotting command. I also checked the dtypes of the obs columns, and there doesn't seem to be anything out of the ordinary there either. Any help would be appreciated, it took me some time to figure out this was causing the issue! (and it's a bit frustrating to not be able to save an object just from running a plot command)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/955#issuecomment-1862227440
https://github.com/scverse/scanpy/issues/956#issuecomment-567321810:233,Testability,log,logically,233,"I support the idea of tidying up plotting arguments. I think there are mainly two problems. 1) the high number of plotting arguments 2) lack of reusability of plotting ""styles"". . Chaining looks really cool and improves 1). Also, it logically partitions the plotting arguments. However, it doesn't solve 2). In other words, if we plot two figures, we'll need to copy the entire thing, and it'll be very verbose:. ```python; sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1). sc.pl.umap(adata2, color='fluffy').scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1); ```. One thing that comes to mind for reusability is to store the result of the chain somewhere and, well, reuse it:. ```python; style = sc.pl.styles.scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1). # using simple arguments, similar to https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/lmeControl.html; sc.pl.umap(adata, color='clusters', style=style); sc.pl.umap(adata2, color='fluffy', style=style). # using context managers, similar to https://seaborn.pydata.org/tutorial/aesthetics.html#temporarily-setting-figure-style; with style:; sc.pl.umap(adata, color='clusters'); sc.pl.umap(adata2, color='fluffy'). # overriding an existing style object; with style.legend(fontsize=12):; sc.pl.umap(adata, color='clusters'); sc.pl.umap(adata2, color='fluffy'). # or use predefined styles (?); with sc.pl.style('malte'):; sc.pl.umap(adata, color='clusters'); sc.pl.umap(adata2, color='fluffy'). ```. WDYT?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956#issuecomment-567321810
https://github.com/scverse/scanpy/issues/956#issuecomment-567321810:956,Usability,simpl,simple,956,"I support the idea of tidying up plotting arguments. I think there are mainly two problems. 1) the high number of plotting arguments 2) lack of reusability of plotting ""styles"". . Chaining looks really cool and improves 1). Also, it logically partitions the plotting arguments. However, it doesn't solve 2). In other words, if we plot two figures, we'll need to copy the entire thing, and it'll be very verbose:. ```python; sc.pl.umap(adata, color='clusters').scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1). sc.pl.umap(adata2, color='fluffy').scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1); ```. One thing that comes to mind for reusability is to store the result of the chain somewhere and, well, reuse it:. ```python; style = sc.pl.styles.scatter_outline(width=0.1); .legend(loc='on data', outline=1); .add_edges(color='black', width=0.1). # using simple arguments, similar to https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/lmeControl.html; sc.pl.umap(adata, color='clusters', style=style); sc.pl.umap(adata2, color='fluffy', style=style). # using context managers, similar to https://seaborn.pydata.org/tutorial/aesthetics.html#temporarily-setting-figure-style; with style:; sc.pl.umap(adata, color='clusters'); sc.pl.umap(adata2, color='fluffy'). # overriding an existing style object; with style.legend(fontsize=12):; sc.pl.umap(adata, color='clusters'); sc.pl.umap(adata2, color='fluffy'). # or use predefined styles (?); with sc.pl.style('malte'):; sc.pl.umap(adata, color='clusters'); sc.pl.umap(adata2, color='fluffy'). ```. WDYT?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956#issuecomment-567321810
https://github.com/scverse/scanpy/issues/956#issuecomment-567412942:59,Usability,intuit,intuitive,59,"I like the idea and I see your point, however I wonder how intuitive the concept of a style is for beginners. Also you'd have to know exactly what you are looking to plot during your analysis to set up a style for all plots for the future (e.g. do you need arrows? Will you use a marker gene dotplot?). Context managers definitely look clean (I love that I have my own ^^), but it would make things a bit more difficult for beginners to get an intuitive feel for scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956#issuecomment-567412942
https://github.com/scverse/scanpy/issues/956#issuecomment-567412942:444,Usability,intuit,intuitive,444,"I like the idea and I see your point, however I wonder how intuitive the concept of a style is for beginners. Also you'd have to know exactly what you are looking to plot during your analysis to set up a style for all plots for the future (e.g. do you need arrows? Will you use a marker gene dotplot?). Context managers definitely look clean (I love that I have my own ^^), but it would make things a bit more difficult for beginners to get an intuitive feel for scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/956#issuecomment-567412942
https://github.com/scverse/scanpy/pull/957#issuecomment-567410803:67,Performance,load,loaded,67,Is this backward compatible for cases where old AnnData object are loaded?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/957#issuecomment-567410803
https://github.com/scverse/scanpy/pull/957#issuecomment-567549770:470,Availability,failure,failure,470,"> I see... but as it's always `adata.uns['paga']` what would happen if I have an old object and then run a new `sc.tl.paga()` with `key` set to something. Then you'll have it under both `adata.uns['paga']` and `adata.uns['paga'][key]`. That's not backwards compatibility though. BC is running the old code (so no key='key') with an old object (where paga is under `adata.uns['paga']`) with new scanpy and getting the old result, which is satisfied here. There are weird failure modes though, like using `key='groups'` or `key='connectivities'` might override some parts of an existing, old-style paga result. We can forbid keys like these. Actually this is related to the versioning of AnnData specification. We should keep some sort of version like `/attrs/LOOM_SPEC_VERSION` in loom. Then it would be easier to understand what to expect from an anndata object that is created at any point in time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/957#issuecomment-567549770
https://github.com/scverse/scanpy/pull/957#issuecomment-567585303:18,Availability,failure,failure,18,"> There are weird failure modes though, like using `key='groups'` or `key='connectivities'` might override some parts of an existing, old-style paga result. We can forbid keys like these. this is exactly what i was worried about. 'groups' is probably not such a rare case as a key. But good to take care of that. I guess in the old & new case, you just have a bit of a messy `adata.uns['paga']` dictionary, but it will still work (in most cases).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/957#issuecomment-567585303
https://github.com/scverse/scanpy/pull/957#issuecomment-627228911:118,Deployability,release,release,118,"We'll have 1.5.0 in a couple of days, and so it's fine to make a few behavior changes. We should put a warning in the release notes, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/957#issuecomment-627228911
https://github.com/scverse/scanpy/pull/959#issuecomment-617017283:188,Testability,test,test,188,@gokceneraslan I've been thinking we should have options like `layer` and `obsm` in many more places. I've started trying to implement this in a systemic way with an internal API and some test helpers in: #1173. What would you think of using that here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/959#issuecomment-617017283
https://github.com/scverse/scanpy/issues/961#issuecomment-620301726:96,Availability,error,error,96,"I meet this problem last days, I uninstall the python-igraph package and reinstall it. then the error disappear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961#issuecomment-620301726
https://github.com/scverse/scanpy/issues/961#issuecomment-1219089556:263,Deployability,install,install,263,The following fix worked for me (informed by this issue: https://github.com/HumanCellAtlas/data-consumer-vignettes/issues/78). ```bash; # uninstall packages (most important one is igraph); pip uninstall igraph python-igraph leiden scanpy. # reinstall scanpy; pip install 'scanpy[leiden]'. # check to see that you can import scanpy and print the version; python -c 'import scanpy as sc; print(sc.__version__)'; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/961#issuecomment-1219089556
https://github.com/scverse/scanpy/issues/967#issuecomment-795139196:53,Testability,log,logarithmized,53,"@giovp ; In the docs it is mentioned that it expects logarithmized data.; As for `raw=True` i am not sure it is a problem. For example, in [the tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) normalized and logarithmized data is saved to `raw` to be used further in `rank_genes_groups`. Do you think it is a problem?. I don't actually consider raw as a container only for raw counts, i think it is a matter of filtered genes vs unfiltered and so on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967#issuecomment-795139196
https://github.com/scverse/scanpy/issues/967#issuecomment-795139196:232,Testability,log,logarithmized,232,"@giovp ; In the docs it is mentioned that it expects logarithmized data.; As for `raw=True` i am not sure it is a problem. For example, in [the tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) normalized and logarithmized data is saved to `raw` to be used further in `rank_genes_groups`. Do you think it is a problem?. I don't actually consider raw as a container only for raw counts, i think it is a matter of filtered genes vs unfiltered and so on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/967#issuecomment-795139196
https://github.com/scverse/scanpy/issues/969#issuecomment-629667682:169,Testability,log,log,169,Just a follow up here. I found the code from the Zheng et al. paper:. It appears they do calculate dispersion as var/mean but on the library size normalized counts (not log). https://github.com/10XGenomics/single-cell-3prime-paper/blob/265433ebf858c7fdcab759ca9f36b5e0241ceece/pbmc68k_analysis/util.R#L122-L135,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/969#issuecomment-629667682
https://github.com/scverse/scanpy/issues/974#issuecomment-572765359:140,Deployability,update,updated,140,"Hi, please give me a reproducible example that uses only public data or better manually typed data. (`np.array([...])`). Also make sure you updated to numpy 0.47 (not 0.46) and llvmlite 0.31 before trying to reproduce the bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974#issuecomment-572765359
https://github.com/scverse/scanpy/issues/974#issuecomment-572849200:326,Performance,Perform,Performing,326,"Here's an example with the newest numba and llvmlite. . I noticed with fewer cells it works. . ```python; import scanpy as sc; import anndata; import numpy as np. a = anndata.AnnData(np.random.poisson(size=(4000, 5000))); b = anndata.AnnData(np.random.poisson(size=(10000, 5000))). sc.external.pp.mnn_correct(a, b); ```. ```; Performing cosine normalization...; Starting MNN correct iteration. Reference batch: 0; Step 1 of 1: processing batch 1; Looking for MNNs...; Computing correction vectors...; /data/yosef2/users/adamgayoso/.pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/mnnpy/utils.py:102: NumbaWarning: ; Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""compute_correction"" failed type inference due to: non-precise type pyobject; [1] During: typing of argument at /data/yosef2/users/adamgayoso/.pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/mnnpy/utils.py (107). File "".pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/mnnpy/utils.py"", line 107:; def compute_correction(data1, data2, mnn1, mnn2, data2_or_raw2, sigma):; <source elided>; vect_reduced = np.zeros((data2.shape[0], vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. @jit(float32[:, :](float32[:, :], float32[:, :], int32[:], int32[:], float32[:, :], float32)); /data/yosef2/users/adamgayoso/.pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function ""compute_correction"" was compiled in object mode without forceobj=True. File "".pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/mnnpy/utils.py"", line 107:; def compute_correction(data1, data2, mnn1, mnn2, data2_or_raw2, sigma):; <source elided>; vect_reduced = np.zeros((data2.shape[0], vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. state.func_ir.loc)); /data/yosef2/users/adamgayoso/.pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-pac",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974#issuecomment-572849200
https://github.com/scverse/scanpy/issues/974#issuecomment-572849200:2157,Safety,detect,detected,2157,"_raw2, sigma):; <source elided>; vect_reduced = np.zeros((data2.shape[0], vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. @jit(float32[:, :](float32[:, :], float32[:, :], int32[:], int32[:], float32[:, :], float32)); /data/yosef2/users/adamgayoso/.pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function ""compute_correction"" was compiled in object mode without forceobj=True. File "".pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/mnnpy/utils.py"", line 107:; def compute_correction(data1, data2, mnn1, mnn2, data2_or_raw2, sigma):; <source elided>; vect_reduced = np.zeros((data2.shape[0], vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. state.func_ir.loc)); /data/yosef2/users/adamgayoso/.pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: ; Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File "".pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/mnnpy/utils.py"", line 107:; def compute_correction(data1, data2, mnn1, mnn2, data2_or_raw2, sigma):; <source elided>; vect_reduced = np.zeros((data2.shape[0], vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. state.func_ir.loc)); ```. ```pytb; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-16-0bc362244a72> in <module>; ----> 1 sc.external.pp.mnn_correct(a, b). /data/yosef2/users/adamgayoso/.pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_catego",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974#issuecomment-572849200
https://github.com/scverse/scanpy/issues/974#issuecomment-573589167:62,Integrability,wrap,wrapper,62,"Since the bug happens in mnnpy and isn’t caused by the scanpy wrapper, this is not a scanpy bug: chriscainx/mnnpy#30",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974#issuecomment-573589167
https://github.com/scverse/scanpy/pull/976#issuecomment-572730320:24,Testability,test,tests,24,"Actually this broke our tests (you probably missed it because numba broke them before) and has no type annotations. I’d also like to see a few changes, please fix and resubmit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/976#issuecomment-572730320
https://github.com/scverse/scanpy/issues/977#issuecomment-572685233:303,Performance,load,load,303,"Hi,. I don't think the data is actually different. Only the jitter in the violin plot places the dots in different places (this is an inherent stochastic effect). The underlying value of `n_genes` (and others) is still the same. You can check if `adata.obs['n_counts']` is the same in the 4 objects you load.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/977#issuecomment-572685233
https://github.com/scverse/scanpy/issues/977#issuecomment-573112548:140,Testability,test,tests,140,"Hi,. Thank you for your prompt reply and suggestions. I checked the `adata.obs[""n_counts]` and `adata.X` comparing them among the different tests and they are actually identical.; I also identified why there was a stochastic effect in the jitter plots inside the violin plots.; It is due to the `numpy` random seed; indeed, if you call `numpy.random.seed(N)` before calling the `scanpy.pl.violin` function, you obtain the same violin plots.; Notice that you have to call it before every call of `scanpy.pl.violin`.; It seems that the seed is reset somewhere in the code. Best wishes,; Simone",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/977#issuecomment-573112548
https://github.com/scverse/scanpy/issues/978#issuecomment-572698263:378,Availability,error,errors,378,"Are you sure about numba 0.43? This very much looks like a bug in numba. > It seems that `top_segment_proportions_sparse_csr` is new for scanpy 1.4.5. What makes you think that? It’s been there since @ivirshup added `calculate_qc_metrics` in #316. A second way for this to fail is:. ```pytb; NotImplementedError: No definition for lowering UniTuple(int64 x 2).shape; ...; numba.errors.LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); No definition for lowering UniTuple(int64 x 2).shape. File ""_qc_.py"", line 390:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; prev = 0; for j, n in enumerate(ns):; ^. [1] During: lowering ""$phi382.1_shape.158 = getattr(value=$380for_iter.2, attr=shape)"" at _qc.py (408); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978#issuecomment-572698263
https://github.com/scverse/scanpy/issues/978#issuecomment-572698263:424,Deployability,pipeline,pipeline,424,"Are you sure about numba 0.43? This very much looks like a bug in numba. > It seems that `top_segment_proportions_sparse_csr` is new for scanpy 1.4.5. What makes you think that? It’s been there since @ivirshup added `calculate_qc_metrics` in #316. A second way for this to fail is:. ```pytb; NotImplementedError: No definition for lowering UniTuple(int64 x 2).shape; ...; numba.errors.LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); No definition for lowering UniTuple(int64 x 2).shape. File ""_qc_.py"", line 390:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; prev = 0; for j, n in enumerate(ns):; ^. [1] During: lowering ""$phi382.1_shape.158 = getattr(value=$380for_iter.2, attr=shape)"" at _qc.py (408); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978#issuecomment-572698263
https://github.com/scverse/scanpy/issues/978#issuecomment-572708303:1929,Availability,error,errors,1929,"ocessing/_qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, parallel); 281 percent_top=percent_top,; 282 inplace=inplace,; --> 283 X=X,; 284 ); 285 var_metrics = describe_var(. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, X, parallel); 107 if percent_top:; 108 percent_top = sorted(percent_top); --> 109 proportions = top_segment_proportions(X, percent_top); 110 for i, n in enumerate(percent_top):; 111 obs_metrics[f""pct_{expr_type}_in_top_{n}_{var_type}""] = (. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/scanpy/preprocessing/_qc.py in top_segment_proportions(mtx, ns); 364 mtx = csr_matrix(mtx); 365 return top_segment_proportions_sparse_csr(; --> 366 mtx.data, mtx.indptr, np.array(ns, dtype=np.int); 367 ); 368 else:. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in _compile_for_args(self, *args, **kws); 348 e.patch_message(msg); 349 ; --> 350 error_rewrite(e, 'typing'); 351 except errors.UnsupportedError as e:; 352 # Something unsupported is present in the user code, add help info. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/dispatcher.py in error_rewrite(e, issue_type); 315 raise e; 316 else:; --> 317 reraise(type(e), e, None); 318 ; 319 argtypes = []. ~/packages/anaconda3/envs/testscanpy145/lib/python3.6/site-packages/numba/six.py in reraise(tp, value, tb); 656 value = tp(); 657 if value.__traceback__ is not tb:; --> 658 raise value.with_traceback(tb); 659 raise value; 660 ; ```. I was also surprised since this should be the first few functions people run.; ```calculate_qc_metrics``` was there for a long time. But ```top_segment_proportions_sparse_csr``` seems to be a new version since 1.4.5- I checked the _qc.py in the tar.gz files. Sorry my previous description was not accurate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978#issuecomment-572708303
https://github.com/scverse/scanpy/issues/978#issuecomment-572718846:5,Availability,error,error,5,"same error in 1.4.4; ```; sc.pp.calculate_qc_metrics(adata, inplace=True, parallel=True); ```; Maybe because sc.pp.calculate_qc_metrics was running in non parallel by default in 1.4.4 and before. the parallel= option has been removed since 1.4.5 and calculate_qc_metrics is running in parallel by default. That's why the error wasn't reported. I don't know enough about numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978#issuecomment-572718846
https://github.com/scverse/scanpy/issues/978#issuecomment-572718846:321,Availability,error,error,321,"same error in 1.4.4; ```; sc.pp.calculate_qc_metrics(adata, inplace=True, parallel=True); ```; Maybe because sc.pp.calculate_qc_metrics was running in non parallel by default in 1.4.4 and before. the parallel= option has been removed since 1.4.5 and calculate_qc_metrics is running in parallel by default. That's why the error wasn't reported. I don't know enough about numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/978#issuecomment-572718846
https://github.com/scverse/scanpy/issues/979#issuecomment-589863225:281,Testability,test,test,281,"Just opened a PR to fix this. Quoting from the PR (#1069):. > Note that if you wish to modify the figure in the same jupyter notebook cell in which the plotting function is called, you should set show=False:. ```; fig,ax = sc.pl.dotplot(adata,var_names,show=False); ax.set_xlabel('test'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979#issuecomment-589863225
https://github.com/scverse/scanpy/issues/984#issuecomment-627067415:17,Deployability,update,update,17,Will there be an update on this issue?; It would be really helpful!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984#issuecomment-627067415
https://github.com/scverse/scanpy/issues/984#issuecomment-656111734:91,Performance,perform,perform,91,"Looking at #842 this is possible by subsetting the data on the groups of interest and then perform the analysis (one-vs-rest). If the reference group is an aggregate of other groups, then one has to define the new labels in the subset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984#issuecomment-656111734
https://github.com/scverse/scanpy/issues/987#issuecomment-574063629:51,Testability,test,tested,51,"Something like this should work. Note, this is not tested. ```pytb; target_cells = 5000. adatas = [adata[adata.obs[cluster_key].isin(clust)] for clust in adata.obs[cluster_key].cat.categories]. for dat in adatas:; if dat.n_obs > target_cells:; sc.pp.subsample(dat, n_obs=target_cells). adata_downsampled = adatas[0].concatenate(*adatas[1:]); ```. Hope that helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-574063629
https://github.com/scverse/scanpy/issues/987#issuecomment-972943098:143,Usability,learn,learn,143,"I'll reopen this cause I think it's quite relevant still and could be very straightforward to implement with [sklearn resample](https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html). also, there is an entire package for subsampling strategies which is probably quite relevant: https://github.com/scikit-learn-contrib/imbalanced-learn. line here for reference: https://github.com/theislab/scanpy/blob/48cc7b38f1f31a78902a892041902cc810ddfcd3/scanpy/preprocessing/_simple.py#L857",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-972943098
https://github.com/scverse/scanpy/issues/987#issuecomment-972943098:328,Usability,learn,learn-contrib,328,"I'll reopen this cause I think it's quite relevant still and could be very straightforward to implement with [sklearn resample](https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html). also, there is an entire package for subsampling strategies which is probably quite relevant: https://github.com/scikit-learn-contrib/imbalanced-learn. line here for reference: https://github.com/theislab/scanpy/blob/48cc7b38f1f31a78902a892041902cc810ddfcd3/scanpy/preprocessing/_simple.py#L857",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-972943098
https://github.com/scverse/scanpy/issues/987#issuecomment-972943098:353,Usability,learn,learn,353,"I'll reopen this cause I think it's quite relevant still and could be very straightforward to implement with [sklearn resample](https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html). also, there is an entire package for subsampling strategies which is probably quite relevant: https://github.com/scikit-learn-contrib/imbalanced-learn. line here for reference: https://github.com/theislab/scanpy/blob/48cc7b38f1f31a78902a892041902cc810ddfcd3/scanpy/preprocessing/_simple.py#L857",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-972943098
https://github.com/scverse/scanpy/issues/987#issuecomment-1043141030:43,Availability,down,downsampling,43,"So assuming that we are only interested in downsampling, then I'd say `NearMiss` and related are straightforward and scalable (just need to compute a kmeans whcih is really fast)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1043141030
https://github.com/scverse/scanpy/issues/987#issuecomment-1043141030:117,Performance,scalab,scalable,117,"So assuming that we are only interested in downsampling, then I'd say `NearMiss` and related are straightforward and scalable (just need to compute a kmeans whcih is really fast)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1043141030
https://github.com/scverse/scanpy/issues/987#issuecomment-1054226637:35,Performance,perform,performed,35,"also, the fact that reshuflling is performed is not in docs and should be documented. @bio-la do you plan to work on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1054226637
https://github.com/scverse/scanpy/issues/987#issuecomment-1054247364:299,Availability,down,downsample,299,"> then I'd say NearMiss and related are straightforward and scalable (just need to compute a kmeans whcih is really fast). For sampling from datasets, I would want to go with either extremely straightforward or something that has been shown to work. Maybe we could start with use provided labels to downsample by?. > reshuflling is performed. Reshuffling meaning that the order is changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1054247364
https://github.com/scverse/scanpy/issues/987#issuecomment-1054247364:60,Performance,scalab,scalable,60,"> then I'd say NearMiss and related are straightforward and scalable (just need to compute a kmeans whcih is really fast). For sampling from datasets, I would want to go with either extremely straightforward or something that has been shown to work. Maybe we could start with use provided labels to downsample by?. > reshuflling is performed. Reshuffling meaning that the order is changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1054247364
https://github.com/scverse/scanpy/issues/987#issuecomment-1054247364:332,Performance,perform,performed,332,"> then I'd say NearMiss and related are straightforward and scalable (just need to compute a kmeans whcih is really fast). For sampling from datasets, I would want to go with either extremely straightforward or something that has been shown to work. Maybe we could start with use provided labels to downsample by?. > reshuflling is performed. Reshuffling meaning that the order is changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1054247364
https://github.com/scverse/scanpy/issues/987#issuecomment-1397060295:32,Availability,error,error,32,"@stefanpeidli's code gives this error. `ValueError: Cannot take a larger sample than population when 'replace=False'`. If a group has less than required number observations, it shouldn't subsample. ```python; target_cells = 1000; cluster_key = ""cell_type"". grouped = adata.obs.groupby(cluster_key); downsampled_indices = []. for _, group in grouped:; if len(group) > target_cells:; downsampled_indices.extend(group.sample(target_cells).index); else:; downsampled_indices.extend(group.index). adata_downsampled = adata[downsampled_indices]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1397060295
https://github.com/scverse/scanpy/issues/987#issuecomment-1397060295:402,Modifiability,extend,extend,402,"@stefanpeidli's code gives this error. `ValueError: Cannot take a larger sample than population when 'replace=False'`. If a group has less than required number observations, it shouldn't subsample. ```python; target_cells = 1000; cluster_key = ""cell_type"". grouped = adata.obs.groupby(cluster_key); downsampled_indices = []. for _, group in grouped:; if len(group) > target_cells:; downsampled_indices.extend(group.sample(target_cells).index); else:; downsampled_indices.extend(group.index). adata_downsampled = adata[downsampled_indices]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1397060295
https://github.com/scverse/scanpy/issues/987#issuecomment-1397060295:471,Modifiability,extend,extend,471,"@stefanpeidli's code gives this error. `ValueError: Cannot take a larger sample than population when 'replace=False'`. If a group has less than required number observations, it shouldn't subsample. ```python; target_cells = 1000; cluster_key = ""cell_type"". grouped = adata.obs.groupby(cluster_key); downsampled_indices = []. for _, group in grouped:; if len(group) > target_cells:; downsampled_indices.extend(group.sample(target_cells).index); else:; downsampled_indices.extend(group.index). adata_downsampled = adata[downsampled_indices]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1397060295
https://github.com/scverse/scanpy/pull/988#issuecomment-573589189:242,Availability,error,error,242,"Re: testing externals, I've tried my best to just test the way it interfaces with scanpy. i.e., if MAGIC silently fails to return the correct output, scanpy tests would pass so long as the output is the right type / shape. If MAGIC throws an error when run from scanpy, this might be something you would like to address (i.e. by contacting the relevant external developer) regardless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988#issuecomment-573589189
https://github.com/scverse/scanpy/pull/988#issuecomment-573589189:66,Integrability,interface,interfaces,66,"Re: testing externals, I've tried my best to just test the way it interfaces with scanpy. i.e., if MAGIC silently fails to return the correct output, scanpy tests would pass so long as the output is the right type / shape. If MAGIC throws an error when run from scanpy, this might be something you would like to address (i.e. by contacting the relevant external developer) regardless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988#issuecomment-573589189
https://github.com/scverse/scanpy/pull/988#issuecomment-573589189:4,Testability,test,testing,4,"Re: testing externals, I've tried my best to just test the way it interfaces with scanpy. i.e., if MAGIC silently fails to return the correct output, scanpy tests would pass so long as the output is the right type / shape. If MAGIC throws an error when run from scanpy, this might be something you would like to address (i.e. by contacting the relevant external developer) regardless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988#issuecomment-573589189
https://github.com/scverse/scanpy/pull/988#issuecomment-573589189:50,Testability,test,test,50,"Re: testing externals, I've tried my best to just test the way it interfaces with scanpy. i.e., if MAGIC silently fails to return the correct output, scanpy tests would pass so long as the output is the right type / shape. If MAGIC throws an error when run from scanpy, this might be something you would like to address (i.e. by contacting the relevant external developer) regardless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988#issuecomment-573589189
https://github.com/scverse/scanpy/pull/988#issuecomment-573589189:157,Testability,test,tests,157,"Re: testing externals, I've tried my best to just test the way it interfaces with scanpy. i.e., if MAGIC silently fails to return the correct output, scanpy tests would pass so long as the output is the right type / shape. If MAGIC throws an error when run from scanpy, this might be something you would like to address (i.e. by contacting the relevant external developer) regardless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988#issuecomment-573589189
https://github.com/scverse/scanpy/pull/989#issuecomment-577146797:49,Availability,error,error,49,"@flying-sheep, any idea what's up with the black error here? Am I at fault, are all the other listed files at fault, some combination of the two?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/989#issuecomment-577146797
https://github.com/scverse/scanpy/pull/989#issuecomment-577146797:69,Availability,fault,fault,69,"@flying-sheep, any idea what's up with the black error here? Am I at fault, are all the other listed files at fault, some combination of the two?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/989#issuecomment-577146797
https://github.com/scverse/scanpy/pull/989#issuecomment-577146797:110,Availability,fault,fault,110,"@flying-sheep, any idea what's up with the black error here? Am I at fault, are all the other listed files at fault, some combination of the two?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/989#issuecomment-577146797
https://github.com/scverse/scanpy/pull/989#issuecomment-577155563:24,Integrability,message,message,24,"Didn’t you rephrase the message?. > scanpy/tests/test_read_10x.py: +3 -1; > ; > This above file has < {thresh} changes to black formatting. Please black format it and afterwards remove it from “tool.black.exclude"" in pyproject.toml. Anyway, it should be “remove it from ‘tool.black.exclude’ *and then* black-format it”, as black won’t run on it if it’s excluded.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/989#issuecomment-577155563
https://github.com/scverse/scanpy/pull/989#issuecomment-577155563:43,Testability,test,tests,43,"Didn’t you rephrase the message?. > scanpy/tests/test_read_10x.py: +3 -1; > ; > This above file has < {thresh} changes to black formatting. Please black format it and afterwards remove it from “tool.black.exclude"" in pyproject.toml. Anyway, it should be “remove it from ‘tool.black.exclude’ *and then* black-format it”, as black won’t run on it if it’s excluded.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/989#issuecomment-577155563
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:21,Availability,error,error,21,"I'm getting the same error from RStudio with reticulate:. From the console:. ```; py_install('scanpy'); Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anacond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:653,Availability,avail,available,653,"I'm getting the same error from RStudio with reticulate:. From the console:. ```; py_install('scanpy'); Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anacond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:1169,Availability,Error,Error,1169,"ing... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement alre",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:1223,Availability,error,error,1223,"ing... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement alre",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:251,Deployability,install,installed,251,"I'm getting the same error from RStudio with reticulate:. From the console:. ```; py_install('scanpy'); Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anacond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:1214,Deployability,install,install,1214,"ing... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement alre",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:1313,Deployability,install,install,1313,"working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:6079,Deployability,install,install,6079,"equirement already satisfied: zipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: decorator>=4.3.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from networkx->scanpy) (4.4.1); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); tsundoku@tsundoku-OptiPlex-7070:~/networkanalyst/src/main/webapp/resources/data$ pip install scanpy; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: patsy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.5.1); Requirement already satisfied: numba>=0.41.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.46.0); Requirement already satisfied: networkx in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.4); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: h5py!=2.10.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.9.0); Requirement already satisfied: seaborn in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.9.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:10787,Deployability,install,install,10787,"naconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.4.5); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:15253,Deployability,install,installed,15253,"; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package natsort conflicts for:; scanpy -> natsort; Package openssl conflicts for:; python=3.7 -> openssl[version='>=1.0.2o,<1.0.3a|>=1.0.2p,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1b,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; ```. I can import `scanpy` by opening Python 3 interpreter from the terminal by running `python`. ```; Python 3.7.5 (default, Oct 25 2019, 15:51:11); [GCC 7.3.0] :: Anaconda, Inc. on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import scanpy as sc # this works; ```. Check the `PATH`:. ```; ['', '/home/tsundoku/anaconda3/lib/python37.zip', '/home/tsundoku/anaconda3/lib/python3.7', '/home/tsundoku/anaconda3/lib/python3.7/lib-dynload', '/home/tsundoku/.local/lib/python3.7/site-packages', '/home/tsundoku/anaconda3/lib/python3.7/site-packages']; ```. But it fails to load from `reticulate`. ```; library(reticulate); repl_python(); ```. ```; import pandas as pd; import scanpy as sc; ```. ```; ModuleNotFoundError: No module named 'scanpy'; ```. Check the `PATH`:. ```; import sys; sys.path; ```. ```; ['', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/bin', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python36.zip', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6/lib-dynload', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6/site-packages', '/home/tsundoku/R/x86_64-pc-linux-gnu-library/3.6/reticulate/python']; ```. Okay so `scanpy` is installed but the `PATH` are different. Not sure why `py_install()` doesn't work. I guess the alternative is including the those paths for reticulate but not sure at the moment how to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:2231,Integrability,wrap,wrap,2231,"ode 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requirement already satisfied: anndata>=0.6.22.post1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.6.22.post1); Requirement already satisfied: tqdm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (4.40.0); Requirement already satisfied: numba>=0.41.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.46.0); Requirement already satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: patsy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.5.1); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:4752,Integrability,wrap,wrap,4752,"ome/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.21.3); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: numpy>=1.13.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (1.17.4); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: python-dateutil>=2.6.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2.8.1); Requirement already satisfied: pyparsing>=2.0.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (2.4.5); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (1.13.0); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: llvmlite>=0.30.0dev0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from numba>=0.41.0->scanpy) (0.30.0); Requirement already satisfied: zipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: decorator>=4.3.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from networkx->scanpy) (4.4.1); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:4912,Integrability,wrap,wrap,4912,"hon3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: numpy>=1.13.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (1.17.4); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: python-dateutil>=2.6.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2.8.1); Requirement already satisfied: pyparsing>=2.0.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (2.4.5); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (1.13.0); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: llvmlite>=0.30.0dev0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from numba>=0.41.0->scanpy) (0.30.0); Requirement already satisfied: zipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: decorator>=4.3.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from networkx->scanpy) (4.4.1); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-pa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:7652,Integrability,wrap,wrap,7652,"ite-packages (from scanpy) (3.3.3); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: h5py!=2.10.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.9.0); Requirement already satisfied: seaborn in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.9.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement already satisfied: anndata>=0.6.22.post1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.6.22.post1); Requirement already satisfied: matplotlib==3.0.* in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.0.3); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: tqdm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (4.40.0); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: scikit-learn>=0.21.2 in /home/tsundoku/anaconda3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:10280,Integrability,wrap,wrap,10280,"ipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: python-dateutil>=2.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.8.1); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.4.5); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:10416,Integrability,wrap,wrap,10416," Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: python-dateutil>=2.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.8.1); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.4.5); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:418,Modifiability,flexible,flexible,418,"I'm getting the same error from RStudio with reticulate:. From the console:. ```; py_install('scanpy'); Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anacond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:583,Modifiability,flexible,flexible,583,"I'm getting the same error from RStudio with reticulate:. From the console:. ```; py_install('scanpy'); Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anacond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:10953,Modifiability,flexible,flexible,10953,"ython3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package networkx conflicts for:; scanpy -> networkx; Package readline conflicts for:; python=3.7 -> r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:11199,Modifiability,flexible,flexible,11199,">=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package networkx conflicts for:; scanpy -> networkx; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package joblib conflicts for:; scanpy -> joblib; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package tk conflicts for:; python=3.7 -> tk[version='>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:14523,Performance,load,load,14523,"; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package natsort conflicts for:; scanpy -> natsort; Package openssl conflicts for:; python=3.7 -> openssl[version='>=1.0.2o,<1.0.3a|>=1.0.2p,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1b,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; ```. I can import `scanpy` by opening Python 3 interpreter from the terminal by running `python`. ```; Python 3.7.5 (default, Oct 25 2019, 15:51:11); [GCC 7.3.0] :: Anaconda, Inc. on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import scanpy as sc # this works; ```. Check the `PATH`:. ```; ['', '/home/tsundoku/anaconda3/lib/python37.zip', '/home/tsundoku/anaconda3/lib/python3.7', '/home/tsundoku/anaconda3/lib/python3.7/lib-dynload', '/home/tsundoku/.local/lib/python3.7/site-packages', '/home/tsundoku/anaconda3/lib/python3.7/site-packages']; ```. But it fails to load from `reticulate`. ```; library(reticulate); repl_python(); ```. ```; import pandas as pd; import scanpy as sc; ```. ```; ModuleNotFoundError: No module named 'scanpy'; ```. Check the `PATH`:. ```; import sys; sys.path; ```. ```; ['', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/bin', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python36.zip', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6/lib-dynload', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6/site-packages', '/home/tsundoku/R/x86_64-pc-linux-gnu-library/3.6/reticulate/python']; ```. Okay so `scanpy` is installed but the `PATH` are different. Not sure why `py_install()` doesn't work. I guess the alternative is including the those paths for reticulate but not sure at the moment how to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:11343,Safety,abort,abort,11343,"tools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package networkx conflicts for:; scanpy -> networkx; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package joblib conflicts for:; scanpy -> joblib; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package tk conflicts for:; python=3.7 -> tk[version='>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package python-igraph conflicts for:; scanpy -> python-igraph; Package li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:3725,Usability,learn,learn,3725,"eady satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: patsy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.5.1); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied: seaborn in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.9.0); Requirement already satisfied: networkx in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.4); Requirement already satisfied: matplotlib==3.0.* in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.0.3); Requirement already satisfied: h5py!=2.10.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.9.0); Requirement already satisfied: scikit-learn>=0.21.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.21.3); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: numpy>=1.13.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (1.17.4); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: python-dateutil>=2.6.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2.8.1); Requirement already satisfied: pyparsing>=2.0.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (2.4.5); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (1.13.0); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:3855,Usability,learn,learn,3855,"d: patsy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.5.1); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied: seaborn in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.9.0); Requirement already satisfied: networkx in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.4); Requirement already satisfied: matplotlib==3.0.* in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.0.3); Requirement already satisfied: h5py!=2.10.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.9.0); Requirement already satisfied: scikit-learn>=0.21.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.21.3); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: numpy>=1.13.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (1.17.4); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: python-dateutil>=2.6.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2.8.1); Requirement already satisfied: pyparsing>=2.0.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (2.4.5); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (1.13.0); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: get-version>=2.0.4 i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:8123,Usability,learn,learn,8123,tisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement already satisfied: anndata>=0.6.22.post1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.6.22.post1); Requirement already satisfied: matplotlib==3.0.* in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.0.3); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: tqdm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (4.40.0); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: scikit-learn>=0.21.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.21.3); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from patsy->scanpy) (1.13.0); Requirement already satisfied: numpy>=1.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from patsy->scanpy) (1.17.4); Requirement already satisfied: llvmlite>=0.30.0dev0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from numba>=0.41.0->scanpy) (0.30.0); Requi,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:8611,Usability,learn,learn,8611,"anpy) (0.25.3); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: tqdm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (4.40.0); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: scikit-learn>=0.21.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.21.3); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from patsy->scanpy) (1.13.0); Requirement already satisfied: numpy>=1.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from patsy->scanpy) (1.17.4); Requirement already satisfied: llvmlite>=0.30.0dev0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from numba>=0.41.0->scanpy) (0.30.0); Requirement already satisfied: decorator>=4.3.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from networkx->scanpy) (4.4.1); Requirement already satisfied: zipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: python-d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:11789,Usability,learn,learn,11789,"ckages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package networkx conflicts for:; scanpy -> networkx; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package joblib conflicts for:; scanpy -> joblib; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package tk conflicts for:; python=3.7 -> tk[version='>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package python-igraph conflicts for:; scanpy -> python-igraph; Package libstdcxx-ng conflicts for:; python=3.7 -> libstdcxx-ng[version='>=7.2.0|>=7.3.0']; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package ncurses conflicts for:; python=3.7 -> ncurses[version='>=6.1,<7.0a0']; Package sqlite conflicts for:; python=3.7 -> sqlite[version='>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.25.3,<",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:11826,Usability,learn,learn,11826,"ckages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package networkx conflicts for:; scanpy -> networkx; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package joblib conflicts for:; scanpy -> joblib; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package tk conflicts for:; python=3.7 -> tk[version='>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package python-igraph conflicts for:; scanpy -> python-igraph; Package libstdcxx-ng conflicts for:; python=3.7 -> libstdcxx-ng[version='>=7.2.0|>=7.3.0']; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package ncurses conflicts for:; python=3.7 -> ncurses[version='>=6.1,<7.0a0']; Package sqlite conflicts for:; python=3.7 -> sqlite[version='>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.25.3,<",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:13542,Usability,learn,learn,13542,"'>=6.1,<7.0a0']; Package sqlite conflicts for:; python=3.7 -> sqlite[version='>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.25.3,<4.0a0|>=3.26.0,<4.0a0|>=3.27.2,<4.0a0|>=3.29.0,<4.0a0|>=3.30.1,<4.0a0']; Package ld_impl_linux-64 conflicts for:; python=3.7 -> ld_impl_linux-64; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package louvain conflicts for:; scanpy -> louvain; Package pip conflicts for:; python=3.7 -> pip; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package xz conflicts for:; python=3.7 -> xz[version='>=5.2.4,<6.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package libgcc-ng conflicts for:; python=3.7 -> libgcc-ng[version='>=7.2.0|>=7.3.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package natsort conflicts for:; scanpy -> natsort; Package openssl conflicts for:; python=3.7 -> openssl[version='>=1.0.2o,<1.0.3a|>=1.0.2p,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1b,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; ```. I can import `scanpy` by opening Python 3 interpreter from the terminal by running `python`. ```; Python 3.7.5 (default, Oct 25 2019, 15:51:11); [GCC 7.3.0] :: Anaconda, Inc. on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import scanpy as sc # this works; ```. Check the `PATH`:. ```; ['', '/home/tsundoku/anaconda3/lib/python37.zip', '/home/tsundoku/anaconda3/lib/python3.7', '/home/tsundoku/anaconda3/lib/python3.7/lib-dynload', '/home/tsundoku/.local/lib/python3.7/site-packages', '/home/tsundoku/anaconda3/lib/python3.7/site-packages']; ```. But it fails to load from `reticulate`. ```; library(r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:13581,Usability,learn,learn,13581,"'>=6.1,<7.0a0']; Package sqlite conflicts for:; python=3.7 -> sqlite[version='>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.25.3,<4.0a0|>=3.26.0,<4.0a0|>=3.27.2,<4.0a0|>=3.29.0,<4.0a0|>=3.30.1,<4.0a0']; Package ld_impl_linux-64 conflicts for:; python=3.7 -> ld_impl_linux-64; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package louvain conflicts for:; scanpy -> louvain; Package pip conflicts for:; python=3.7 -> pip; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package xz conflicts for:; python=3.7 -> xz[version='>=5.2.4,<6.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package libgcc-ng conflicts for:; python=3.7 -> libgcc-ng[version='>=7.2.0|>=7.3.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package natsort conflicts for:; scanpy -> natsort; Package openssl conflicts for:; python=3.7 -> openssl[version='>=1.0.2o,<1.0.3a|>=1.0.2p,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1b,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; ```. I can import `scanpy` by opening Python 3 interpreter from the terminal by running `python`. ```; Python 3.7.5 (default, Oct 25 2019, 15:51:11); [GCC 7.3.0] :: Anaconda, Inc. on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import scanpy as sc # this works; ```. Check the `PATH`:. ```; ['', '/home/tsundoku/anaconda3/lib/python37.zip', '/home/tsundoku/anaconda3/lib/python3.7', '/home/tsundoku/anaconda3/lib/python3.7/lib-dynload', '/home/tsundoku/.local/lib/python3.7/site-packages', '/home/tsundoku/anaconda3/lib/python3.7/site-packages']; ```. But it fails to load from `reticulate`. ```; library(r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452
https://github.com/scverse/scanpy/issues/990#issuecomment-575292722:282,Performance,load,load,282,"There is a function called `reticulate::install_miniconda()` which [isn't mentioned in the documentation ](https://rstudio.github.io/reticulate/articles/python_packages.html) that solves this problem. ```; install_miniconda(""scanpy""); repl_python(); ```. ```; import scanpy as sc # load successfully now; exit # close the Python interpreter ; ```. Here's my `sessionInfo()` in-case it helps anyone:. ```; R version 3.6.2 (2019-12-12); Platform: x86_64-pc-linux-gnu (64-bit); Running under: Ubuntu 18.04.3 LTS. Matrix products: default; BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1; LAPACK: /home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/libmkl_rt.so. locale:; [1] LC_CTYPE=en_CA.UTF-8 LC_NUMERIC=C LC_TIME=en_CA.UTF-8 LC_COLLATE=en_CA.UTF-8 LC_MONETARY=en_CA.UTF-8 ; [6] LC_MESSAGES=en_CA.UTF-8 LC_PAPER=en_CA.UTF-8 LC_NAME=C LC_ADDRESS=C LC_TELEPHONE=C ; [11] LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C . attached base packages:; [1] stats graphics grDevices utils datasets methods base . other attached packages:; [1] reticulate_1.14. loaded via a namespace (and not attached):; [1] BiocManager_1.30.10 compiler_3.6.2 tools_3.6.2 rappdirs_0.3.1 Rcpp_1.0.3 jsonlite_1.6 packrat_0.5.0 ; [8] png_0.1-7 ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575292722
https://github.com/scverse/scanpy/issues/990#issuecomment-575292722:1065,Performance,load,loaded,1065,"There is a function called `reticulate::install_miniconda()` which [isn't mentioned in the documentation ](https://rstudio.github.io/reticulate/articles/python_packages.html) that solves this problem. ```; install_miniconda(""scanpy""); repl_python(); ```. ```; import scanpy as sc # load successfully now; exit # close the Python interpreter ; ```. Here's my `sessionInfo()` in-case it helps anyone:. ```; R version 3.6.2 (2019-12-12); Platform: x86_64-pc-linux-gnu (64-bit); Running under: Ubuntu 18.04.3 LTS. Matrix products: default; BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1; LAPACK: /home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/libmkl_rt.so. locale:; [1] LC_CTYPE=en_CA.UTF-8 LC_NUMERIC=C LC_TIME=en_CA.UTF-8 LC_COLLATE=en_CA.UTF-8 LC_MONETARY=en_CA.UTF-8 ; [6] LC_MESSAGES=en_CA.UTF-8 LC_PAPER=en_CA.UTF-8 LC_NAME=C LC_ADDRESS=C LC_TELEPHONE=C ; [11] LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C . attached base packages:; [1] stats graphics grDevices utils datasets methods base . other attached packages:; [1] reticulate_1.14. loaded via a namespace (and not attached):; [1] BiocManager_1.30.10 compiler_3.6.2 tools_3.6.2 rappdirs_0.3.1 Rcpp_1.0.3 jsonlite_1.6 packrat_0.5.0 ; [8] png_0.1-7 ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575292722
https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:73,Deployability,install,install,73,"Having the exact same problem. Windows machine, win10, 64 bit. Trying to install from miniconda. FWIW, I have installed scanpy successfully on two other windows machines (my home computer and my work computer) in the last three weeks. Now following identical steps on my laptop and having this tissue. . ```; conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824
https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:110,Deployability,install,installed,110,"Having the exact same problem. Windows machine, win10, 64 bit. Trying to install from miniconda. FWIW, I have installed scanpy successfully on two other windows machines (my home computer and my work computer) in the last three weeks. Now following identical steps on my laptop and having this tissue. . ```; conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824
https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:315,Deployability,install,install,315,"Having the exact same problem. Windows machine, win10, 64 bit. Trying to install from miniconda. FWIW, I have installed scanpy successfully on two other windows machines (my home computer and my work computer) in the last three weeks. Now following identical steps on my laptop and having this tissue. . ```; conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824
https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:471,Modifiability,flexible,flexible,471,"Having the exact same problem. Windows machine, win10, 64 bit. Trying to install from miniconda. FWIW, I have installed scanpy successfully on two other windows machines (my home computer and my work computer) in the last three weeks. Now following identical steps on my laptop and having this tissue. . ```; conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824
https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:717,Modifiability,flexible,flexible,717,"Having the exact same problem. Windows machine, win10, 64 bit. Trying to install from miniconda. FWIW, I have installed scanpy successfully on two other windows machines (my home computer and my work computer) in the last three weeks. Now following identical steps on my laptop and having this tissue. . ```; conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824
https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:858,Safety,abort,abort,858,"Having the exact same problem. Windows machine, win10, 64 bit. Trying to install from miniconda. FWIW, I have installed scanpy successfully on two other windows machines (my home computer and my work computer) in the last three weeks. Now following identical steps on my laptop and having this tissue. . ```; conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824
https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:1104,Usability,learn,learn,1104,"anpy successfully on two other windows machines (my home computer and my work computer) in the last three weeks. Now following identical steps on my laptop and having this tissue. . ```; conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain; Package tqdm conflicts for:; scanpy -> tqdm; Package joblib conflicts for:; scanpy -> joblib; Package natsort conflicts f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824
https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:1141,Usability,learn,learn,1141,"anpy successfully on two other windows machines (my home computer and my work computer) in the last three weeks. Now following identical steps on my laptop and having this tissue. . ```; conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain; Package tqdm conflicts for:; scanpy -> tqdm; Package joblib conflicts for:; scanpy -> joblib; Package natsort conflicts f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824
https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:1769,Usability,learn,learn,1769,"epodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain; Package tqdm conflicts for:; scanpy -> tqdm; Package joblib conflicts for:; scanpy -> joblib; Package natsort conflicts for:; scanpy -> natsort; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824
https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:1808,Usability,learn,learn,1808,"epodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain; Package tqdm conflicts for:; scanpy -> tqdm; Package joblib conflicts for:; scanpy -> joblib; Package natsort conflicts for:; scanpy -> natsort; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824
https://github.com/scverse/scanpy/issues/990#issuecomment-578529012:280,Usability,learn,learn,280,"Same issue: . ```; UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package pip conflicts for:; python=3.6 -> pip; Package python-igraph conflicts for:; scanpy -> python-igraph; Package sqlite conflicts for:; python=3.6 -> sqlite[version='>=3.20.1,<4.0a0|>=3.22.0,<4.0a0|>=3.23.1,<4.0a0|>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.26.0,<4.0a0|>=3.29.0,<4.0a0|>=3.30.1,<4.0a0']; Package openssl conflicts for:; python=3.6 -> openssl[version='1.0.*|1.0.*,>=1.0.2l,<1.0.3a|>=1.0.2m,<1.0.3a|>=1.0.2n,<1.0.3a|>=1.0.2o,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package zlib conflicts for:; python=3.6 -> zlib[version='>=1.2.11,<1.3.0a0']; Package tk conflicts for:; python=3.6 -> tk[version='8.6.*|>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package pytables conflicts for:; scanpy -> pytables; Package tqdm conflicts for:; scanpy -> tqdm; Package patsy conflicts for:; scanpy -> patsy; Package readline conflicts for:; python=3.6 -> readline[version='7.*|>=7.0,<8.0a0']; Package setuptools conflicts for:; scanpy -> setuptools; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package libcxx conflicts for:; python=3.6 -> libcxx[version='>=4.0.1']; Package libffi conflicts for:; python=3.6 -> libffi[version='3.2.*|>=3.2.1,<4.0a0']; Package seaborn conflicts for:; scanpy -> seaborn; Package ncurses conflicts for:; python=3.6 -> ncurses[version='>=6.0,<7.0a0|>=6.1,<7.0a0']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package joblib conflicts for:; scanpy -> joblib; Package networkx conflicts for:; scanpy ->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-578529012
https://github.com/scverse/scanpy/issues/990#issuecomment-578529012:317,Usability,learn,learn,317,"Same issue: . ```; UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package pip conflicts for:; python=3.6 -> pip; Package python-igraph conflicts for:; scanpy -> python-igraph; Package sqlite conflicts for:; python=3.6 -> sqlite[version='>=3.20.1,<4.0a0|>=3.22.0,<4.0a0|>=3.23.1,<4.0a0|>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.26.0,<4.0a0|>=3.29.0,<4.0a0|>=3.30.1,<4.0a0']; Package openssl conflicts for:; python=3.6 -> openssl[version='1.0.*|1.0.*,>=1.0.2l,<1.0.3a|>=1.0.2m,<1.0.3a|>=1.0.2n,<1.0.3a|>=1.0.2o,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package zlib conflicts for:; python=3.6 -> zlib[version='>=1.2.11,<1.3.0a0']; Package tk conflicts for:; python=3.6 -> tk[version='8.6.*|>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package pytables conflicts for:; scanpy -> pytables; Package tqdm conflicts for:; scanpy -> tqdm; Package patsy conflicts for:; scanpy -> patsy; Package readline conflicts for:; python=3.6 -> readline[version='7.*|>=7.0,<8.0a0']; Package setuptools conflicts for:; scanpy -> setuptools; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package libcxx conflicts for:; python=3.6 -> libcxx[version='>=4.0.1']; Package libffi conflicts for:; python=3.6 -> libffi[version='3.2.*|>=3.2.1,<4.0a0']; Package seaborn conflicts for:; scanpy -> seaborn; Package ncurses conflicts for:; python=3.6 -> ncurses[version='>=6.0,<7.0a0|>=6.1,<7.0a0']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package joblib conflicts for:; scanpy -> joblib; Package networkx conflicts for:; scanpy ->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-578529012
https://github.com/scverse/scanpy/issues/990#issuecomment-578529012:1844,Usability,learn,learn,1844,"age pip conflicts for:; python=3.6 -> pip; Package python-igraph conflicts for:; scanpy -> python-igraph; Package sqlite conflicts for:; python=3.6 -> sqlite[version='>=3.20.1,<4.0a0|>=3.22.0,<4.0a0|>=3.23.1,<4.0a0|>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.26.0,<4.0a0|>=3.29.0,<4.0a0|>=3.30.1,<4.0a0']; Package openssl conflicts for:; python=3.6 -> openssl[version='1.0.*|1.0.*,>=1.0.2l,<1.0.3a|>=1.0.2m,<1.0.3a|>=1.0.2n,<1.0.3a|>=1.0.2o,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package zlib conflicts for:; python=3.6 -> zlib[version='>=1.2.11,<1.3.0a0']; Package tk conflicts for:; python=3.6 -> tk[version='8.6.*|>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package pytables conflicts for:; scanpy -> pytables; Package tqdm conflicts for:; scanpy -> tqdm; Package patsy conflicts for:; scanpy -> patsy; Package readline conflicts for:; python=3.6 -> readline[version='7.*|>=7.0,<8.0a0']; Package setuptools conflicts for:; scanpy -> setuptools; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package libcxx conflicts for:; python=3.6 -> libcxx[version='>=4.0.1']; Package libffi conflicts for:; python=3.6 -> libffi[version='3.2.*|>=3.2.1,<4.0a0']; Package seaborn conflicts for:; scanpy -> seaborn; Package ncurses conflicts for:; python=3.6 -> ncurses[version='>=6.0,<7.0a0|>=6.1,<7.0a0']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package joblib conflicts for:; scanpy -> joblib; Package networkx conflicts for:; scanpy -> networkx; Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package xz conflicts for:; python=3.6 -> xz[version='>=5.2.3,<6.0a0|>=5.2.4,<6.0a0']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-578529012
https://github.com/scverse/scanpy/issues/990#issuecomment-578529012:1883,Usability,learn,learn,1883,"age pip conflicts for:; python=3.6 -> pip; Package python-igraph conflicts for:; scanpy -> python-igraph; Package sqlite conflicts for:; python=3.6 -> sqlite[version='>=3.20.1,<4.0a0|>=3.22.0,<4.0a0|>=3.23.1,<4.0a0|>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.26.0,<4.0a0|>=3.29.0,<4.0a0|>=3.30.1,<4.0a0']; Package openssl conflicts for:; python=3.6 -> openssl[version='1.0.*|1.0.*,>=1.0.2l,<1.0.3a|>=1.0.2m,<1.0.3a|>=1.0.2n,<1.0.3a|>=1.0.2o,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package zlib conflicts for:; python=3.6 -> zlib[version='>=1.2.11,<1.3.0a0']; Package tk conflicts for:; python=3.6 -> tk[version='8.6.*|>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package pytables conflicts for:; scanpy -> pytables; Package tqdm conflicts for:; scanpy -> tqdm; Package patsy conflicts for:; scanpy -> patsy; Package readline conflicts for:; python=3.6 -> readline[version='7.*|>=7.0,<8.0a0']; Package setuptools conflicts for:; scanpy -> setuptools; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package libcxx conflicts for:; python=3.6 -> libcxx[version='>=4.0.1']; Package libffi conflicts for:; python=3.6 -> libffi[version='3.2.*|>=3.2.1,<4.0a0']; Package seaborn conflicts for:; scanpy -> seaborn; Package ncurses conflicts for:; python=3.6 -> ncurses[version='>=6.0,<7.0a0|>=6.1,<7.0a0']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package joblib conflicts for:; scanpy -> joblib; Package networkx conflicts for:; scanpy -> networkx; Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package xz conflicts for:; python=3.6 -> xz[version='>=5.2.3,<6.0a0|>=5.2.4,<6.0a0']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-578529012
https://github.com/scverse/scanpy/issues/990#issuecomment-578560517:216,Deployability,install,install,216,"I was recently directed to the [RStudio tutorial for setting up Python/R with {reticulate}](https://t.co/DjbnfZmjQn?amp=1). After this it worked successfully for me; hope it helps. First, source the `virtualenv` and install the package only to the directory/project your working on ([similar concept to `{packrat}`](https://rstudio.github.io/packrat/)); ![image](https://user-images.githubusercontent.com/5749465/73144449-d0663f80-4073-11ea-85d2-2277fb049342.png). *Voila!*. ![image](https://user-images.githubusercontent.com/5749465/73144482-20450680-4074-11ea-9374-6dac81968f89.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-578560517
https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:68,Deployability,install,install,68,"I have the same problem. I am using macOS catalina 10.15.2. $ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package patsy conflicts for:; scanpy -> patsy; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241
https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:224,Modifiability,flexible,flexible,224,"I have the same problem. I am using macOS catalina 10.15.2. $ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package patsy conflicts for:; scanpy -> patsy; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241
https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:470,Modifiability,flexible,flexible,470,"I have the same problem. I am using macOS catalina 10.15.2. $ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package patsy conflicts for:; scanpy -> patsy; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241
https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:612,Safety,abort,abort,612,"I have the same problem. I am using macOS catalina 10.15.2. $ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package patsy conflicts for:; scanpy -> patsy; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241
https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:1130,Usability,learn,learn,1130,"done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package patsy conflicts for:; scanpy -> patsy; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> seaborn; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package python-igraph conflicts for:; scanpy -> python-igraph; ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241
https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:1169,Usability,learn,learn,1169,"done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package patsy conflicts for:; scanpy -> patsy; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> seaborn; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package python-igraph conflicts for:; scanpy -> python-igraph; ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241
https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:2023,Usability,learn,learn,2023,"or:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> seaborn; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package python-igraph conflicts for:; scanpy -> python-igraph; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package joblib conflicts for:; scanpy -> joblib; Package networkx conflicts for:; scanpy -> networkx; Package openssl conflicts for:; python=3.7 -> openssl[version='>=1.0.2o,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1b,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package tk conflicts for:; python=3.7 -> tk[version='>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package xz conflicts for:; python=3.7 -> xz[version='>=5.2.4,<6.0a0']; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package sqlite conflicts for:; python=3.7 -> sqlite[version='>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.25.3,<4.0a0|>=3.26.0,<4.0a0|>=3.27.2,<4.0a0|>=3.29.0,<4.0a0|>=3.30.1,<4.0a0']; Package ncurses conflicts for:; python=3.7 -> ncurses[version='>=6.1,<7.0a0']; Package pytables conflicts for:; scanpy -> pytables",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241
https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:2060,Usability,learn,learn,2060,"or:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> seaborn; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package python-igraph conflicts for:; scanpy -> python-igraph; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package joblib conflicts for:; scanpy -> joblib; Package networkx conflicts for:; scanpy -> networkx; Package openssl conflicts for:; python=3.7 -> openssl[version='>=1.0.2o,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1b,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package tk conflicts for:; python=3.7 -> tk[version='>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; Package xz conflicts for:; python=3.7 -> xz[version='>=5.2.4,<6.0a0']; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package sqlite conflicts for:; python=3.7 -> sqlite[version='>=3.24.0,<4.0a0|>=3.25.2,<4.0a0|>=3.25.3,<4.0a0|>=3.26.0,<4.0a0|>=3.27.2,<4.0a0|>=3.29.0,<4.0a0|>=3.30.1,<4.0a0']; Package ncurses conflicts for:; python=3.7 -> ncurses[version='>=6.1,<7.0a0']; Package pytables conflicts for:; scanpy -> pytables",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241
https://github.com/scverse/scanpy/issues/990#issuecomment-581828751:72,Deployability,install,install,72,Facing the same issue! Any guidance would be appreciated. Was trying to install using Anaconda Navigator for Windows but i guess I will try the Miniconda route,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751
https://github.com/scverse/scanpy/issues/990#issuecomment-581828751:154,Integrability,rout,route,154,Facing the same issue! Any guidance would be appreciated. Was trying to install using Anaconda Navigator for Windows but i guess I will try the Miniconda route,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751
https://github.com/scverse/scanpy/issues/990#issuecomment-581828751:27,Usability,guid,guidance,27,Facing the same issue! Any guidance would be appreciated. Was trying to install using Anaconda Navigator for Windows but i guess I will try the Miniconda route,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751
https://github.com/scverse/scanpy/issues/990#issuecomment-582183368:66,Deployability,install,install,66,"Have the same issue. Windows, Ubuntu for WSL, miniconda:. > conda install -c bioconda/label/cf201901 scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. > UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. > Specifications:. > - scanpy -> python[version='>=3.6,<3.7.0a0']. > Your python: python=3.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-582183368
https://github.com/scverse/scanpy/issues/990#issuecomment-582183368:747,Deployability,install,installation,747,"Have the same issue. Windows, Ubuntu for WSL, miniconda:. > conda install -c bioconda/label/cf201901 scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. > UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. > Specifications:. > - scanpy -> python[version='>=3.6,<3.7.0a0']. > Your python: python=3.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-582183368
https://github.com/scverse/scanpy/issues/990#issuecomment-582183368:237,Modifiability,flexible,flexible,237,"Have the same issue. Windows, Ubuntu for WSL, miniconda:. > conda install -c bioconda/label/cf201901 scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. > UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. > Specifications:. > - scanpy -> python[version='>=3.6,<3.7.0a0']. > Your python: python=3.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-582183368
https://github.com/scverse/scanpy/issues/990#issuecomment-582183368:483,Modifiability,flexible,flexible,483,"Have the same issue. Windows, Ubuntu for WSL, miniconda:. > conda install -c bioconda/label/cf201901 scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. > UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. > Specifications:. > - scanpy -> python[version='>=3.6,<3.7.0a0']. > Your python: python=3.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-582183368
https://github.com/scverse/scanpy/issues/990#issuecomment-582183368:624,Safety,abort,abort,624,"Have the same issue. Windows, Ubuntu for WSL, miniconda:. > conda install -c bioconda/label/cf201901 scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. > UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. > Specifications:. > - scanpy -> python[version='>=3.6,<3.7.0a0']. > Your python: python=3.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-582183368
https://github.com/scverse/scanpy/issues/990#issuecomment-583508242:312,Deployability,install,install,312,"I had the same issue, and it turns out setting up channels solves the problem as follows:; ```; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; ```; Ref: ; https://bioconda.github.io/recipes/scanpy/README.html; https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-583508242
https://github.com/scverse/scanpy/issues/990#issuecomment-583508242:102,Modifiability,config,config,102,"I had the same issue, and it turns out setting up channels solves the problem as follows:; ```; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; ```; Ref: ; https://bioconda.github.io/recipes/scanpy/README.html; https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-583508242
https://github.com/scverse/scanpy/issues/990#issuecomment-583508242:140,Modifiability,config,config,140,"I had the same issue, and it turns out setting up channels solves the problem as follows:; ```; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; ```; Ref: ; https://bioconda.github.io/recipes/scanpy/README.html; https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-583508242
https://github.com/scverse/scanpy/issues/990#issuecomment-583508242:178,Modifiability,config,config,178,"I had the same issue, and it turns out setting up channels solves the problem as follows:; ```; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; ```; Ref: ; https://bioconda.github.io/recipes/scanpy/README.html; https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-583508242
https://github.com/scverse/scanpy/issues/990#issuecomment-583628244:371,Deployability,install,install,371,"Thank you! worked for me; > ; > ; > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-583628244
https://github.com/scverse/scanpy/issues/990#issuecomment-583628244:146,Modifiability,config,config,146,"Thank you! worked for me; > ; > ; > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-583628244
https://github.com/scverse/scanpy/issues/990#issuecomment-583628244:186,Modifiability,config,config,186,"Thank you! worked for me; > ; > ; > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-583628244
https://github.com/scverse/scanpy/issues/990#issuecomment-583628244:226,Modifiability,config,config,226,"Thank you! worked for me; > ; > ; > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-583628244
https://github.com/scverse/scanpy/issues/990#issuecomment-584071023:360,Deployability,install,install,360,"Thanks, worked for me. > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584071023
https://github.com/scverse/scanpy/issues/990#issuecomment-584071023:135,Modifiability,config,config,135,"Thanks, worked for me. > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584071023
https://github.com/scverse/scanpy/issues/990#issuecomment-584071023:175,Modifiability,config,config,175,"Thanks, worked for me. > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584071023
https://github.com/scverse/scanpy/issues/990#issuecomment-584071023:215,Modifiability,config,config,215,"Thanks, worked for me. > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584071023
https://github.com/scverse/scanpy/issues/990#issuecomment-584658003:87,Deployability,install,installed,87,"I experienced the same issue, but none of fixes proposed here worked.; Eventually I re-installed Anaconda, immediately set up the channels, and made a new environment:. ```; conda config --add channels default; conda config --add channels bioconda; conda config --add channels bioconda. #create a new environment; conda create --name <environment name>; #activate your environment ; conda activate <environment name>; ```. Now that I had a new environment (which is easier to work with if you're working on multiple projects; easy switch between environments!), I tried to install scanpy again. Did not work, but then I tried it again, this time with the version number of Python, and that did the trick for me!. `conda install -c bioconda scanpy python=3.7`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584658003
https://github.com/scverse/scanpy/issues/990#issuecomment-584658003:573,Deployability,install,install,573,"I experienced the same issue, but none of fixes proposed here worked.; Eventually I re-installed Anaconda, immediately set up the channels, and made a new environment:. ```; conda config --add channels default; conda config --add channels bioconda; conda config --add channels bioconda. #create a new environment; conda create --name <environment name>; #activate your environment ; conda activate <environment name>; ```. Now that I had a new environment (which is easier to work with if you're working on multiple projects; easy switch between environments!), I tried to install scanpy again. Did not work, but then I tried it again, this time with the version number of Python, and that did the trick for me!. `conda install -c bioconda scanpy python=3.7`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584658003
https://github.com/scverse/scanpy/issues/990#issuecomment-584658003:720,Deployability,install,install,720,"I experienced the same issue, but none of fixes proposed here worked.; Eventually I re-installed Anaconda, immediately set up the channels, and made a new environment:. ```; conda config --add channels default; conda config --add channels bioconda; conda config --add channels bioconda. #create a new environment; conda create --name <environment name>; #activate your environment ; conda activate <environment name>; ```. Now that I had a new environment (which is easier to work with if you're working on multiple projects; easy switch between environments!), I tried to install scanpy again. Did not work, but then I tried it again, this time with the version number of Python, and that did the trick for me!. `conda install -c bioconda scanpy python=3.7`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584658003
https://github.com/scverse/scanpy/issues/990#issuecomment-584658003:180,Modifiability,config,config,180,"I experienced the same issue, but none of fixes proposed here worked.; Eventually I re-installed Anaconda, immediately set up the channels, and made a new environment:. ```; conda config --add channels default; conda config --add channels bioconda; conda config --add channels bioconda. #create a new environment; conda create --name <environment name>; #activate your environment ; conda activate <environment name>; ```. Now that I had a new environment (which is easier to work with if you're working on multiple projects; easy switch between environments!), I tried to install scanpy again. Did not work, but then I tried it again, this time with the version number of Python, and that did the trick for me!. `conda install -c bioconda scanpy python=3.7`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584658003
https://github.com/scverse/scanpy/issues/990#issuecomment-584658003:217,Modifiability,config,config,217,"I experienced the same issue, but none of fixes proposed here worked.; Eventually I re-installed Anaconda, immediately set up the channels, and made a new environment:. ```; conda config --add channels default; conda config --add channels bioconda; conda config --add channels bioconda. #create a new environment; conda create --name <environment name>; #activate your environment ; conda activate <environment name>; ```. Now that I had a new environment (which is easier to work with if you're working on multiple projects; easy switch between environments!), I tried to install scanpy again. Did not work, but then I tried it again, this time with the version number of Python, and that did the trick for me!. `conda install -c bioconda scanpy python=3.7`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584658003
https://github.com/scverse/scanpy/issues/990#issuecomment-584658003:255,Modifiability,config,config,255,"I experienced the same issue, but none of fixes proposed here worked.; Eventually I re-installed Anaconda, immediately set up the channels, and made a new environment:. ```; conda config --add channels default; conda config --add channels bioconda; conda config --add channels bioconda. #create a new environment; conda create --name <environment name>; #activate your environment ; conda activate <environment name>; ```. Now that I had a new environment (which is easier to work with if you're working on multiple projects; easy switch between environments!), I tried to install scanpy again. Did not work, but then I tried it again, this time with the version number of Python, and that did the trick for me!. `conda install -c bioconda scanpy python=3.7`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584658003
https://github.com/scverse/scanpy/issues/990#issuecomment-609560683:81,Deployability,install,install,81,I am not sure what --add does or -c . The following is probably a cleaner way to install. It should not have any unforeseen 'channels' related side effects. . This worked for me on MacOS Catalina; ```; $ conda create --name SCA python=3.8.2; (base) $ conda activate SCA; (SCA) $ conda install scanpy --channel conda-forge --channel bioconda; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-609560683
https://github.com/scverse/scanpy/issues/990#issuecomment-609560683:285,Deployability,install,install,285,I am not sure what --add does or -c . The following is probably a cleaner way to install. It should not have any unforeseen 'channels' related side effects. . This worked for me on MacOS Catalina; ```; $ conda create --name SCA python=3.8.2; (base) $ conda activate SCA; (SCA) $ conda install scanpy --channel conda-forge --channel bioconda; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-609560683
https://github.com/scverse/scanpy/issues/990#issuecomment-613099267:383,Deployability,install,install,383,"This worked for me as well.; Amazing thanks!. > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-613099267
https://github.com/scverse/scanpy/issues/990#issuecomment-613099267:158,Modifiability,config,config,158,"This worked for me as well.; Amazing thanks!. > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-613099267
https://github.com/scverse/scanpy/issues/990#issuecomment-613099267:198,Modifiability,config,config,198,"This worked for me as well.; Amazing thanks!. > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-613099267
https://github.com/scverse/scanpy/issues/990#issuecomment-613099267:238,Modifiability,config,config,238,"This worked for me as well.; Amazing thanks!. > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-613099267
https://github.com/scverse/scanpy/issues/990#issuecomment-614315370:83,Deployability,install,install,83,> I am not sure what --add does or -c . The following is probably a cleaner way to install. It should not have any unforeseen 'channels' related side effects.; > ; > This worked for me on MacOS Catalina; > ; > ```; > $ conda create --name SCA python=3.8.2; > (base) $ conda activate SCA; > (SCA) $ conda install scanpy --channel conda-forge --channel bioconda; > ```. That's the only way it worked for me. Thanks! Without creating a virtualenv it just did not move forward.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-614315370
https://github.com/scverse/scanpy/issues/990#issuecomment-614315370:304,Deployability,install,install,304,> I am not sure what --add does or -c . The following is probably a cleaner way to install. It should not have any unforeseen 'channels' related side effects.; > ; > This worked for me on MacOS Catalina; > ; > ```; > $ conda create --name SCA python=3.8.2; > (base) $ conda activate SCA; > (SCA) $ conda install scanpy --channel conda-forge --channel bioconda; > ```. That's the only way it worked for me. Thanks! Without creating a virtualenv it just did not move forward.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-614315370
https://github.com/scverse/scanpy/issues/990#issuecomment-736279460:0,Deployability,upgrade,upgrade,0,upgrade your kali linux with the command ; sudo apt upgrade; pip3 install scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-736279460
https://github.com/scverse/scanpy/issues/990#issuecomment-736279460:52,Deployability,upgrade,upgrade,52,upgrade your kali linux with the command ; sudo apt upgrade; pip3 install scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-736279460
https://github.com/scverse/scanpy/issues/990#issuecomment-736279460:66,Deployability,install,install,66,upgrade your kali linux with the command ; sudo apt upgrade; pip3 install scanpy,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-736279460
https://github.com/scverse/scanpy/issues/990#issuecomment-762368917:337,Deployability,install,install,337,"> I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels. Thanks, this also worked for me!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-762368917
https://github.com/scverse/scanpy/issues/990#issuecomment-762368917:112,Modifiability,config,config,112,"> I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels. Thanks, this also worked for me!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-762368917
https://github.com/scverse/scanpy/issues/990#issuecomment-762368917:152,Modifiability,config,config,152,"> I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels. Thanks, this also worked for me!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-762368917
https://github.com/scverse/scanpy/issues/990#issuecomment-762368917:192,Modifiability,config,config,192,"> I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels. Thanks, this also worked for me!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-762368917
https://github.com/scverse/scanpy/issues/991#issuecomment-573925992:95,Deployability,release,release,95,"Just to clarify, are you referring to 3 plots in the middle (PCA loading plots)? In new scanpy release, we render both positive and negative genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/991#issuecomment-573925992
https://github.com/scverse/scanpy/issues/991#issuecomment-573925992:65,Performance,load,loading,65,"Just to clarify, are you referring to 3 plots in the middle (PCA loading plots)? In new scanpy release, we render both positive and negative genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/991#issuecomment-573925992
https://github.com/scverse/scanpy/issues/993#issuecomment-615137108:119,Modifiability,rewrite,rewrite,119,"Agreed! . On another note, we currently lack a method for HVG selection that works on scaled/regressed out data. Could rewrite `regress_out` to not just output the residuals, but also add the intercept again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615137108
https://github.com/scverse/scanpy/issues/993#issuecomment-615304326:48,Deployability,update,updated,48,"I'm glad you all are considering adding this. I updated the implementation to work with sparse counts. . ```python; def seurat_v3_highly_variable_genes(; adata, n_top_genes: int = 4000, batch_key: str = ""batch""; ):; """""" An adapted implementation of the ""vst"" feature selection in Seurat v3. The major differences are that we use lowess insted of loess. For further details of the sparse arithmetic see https://www.overleaf.com/read/ckptrbgzzzpg. :param n_top_genes: How many variable genes to return; :param batch_key: key in adata.obs that contains batch info. If None, do not use batch info. """""". from scanpy.preprocessing._utils import _get_mean_var; from scanpy.preprocessing._distributed import materialize_as_ndarray. lowess = sm.nonparametric.lowess. if batch_key is None:; batch_correction = False; batch_key = ""batch""; adata.obs[batch_key] = pd.Categorical(np.zeros((adata.X.shape[0])).astype(int)); else:; batch_correction = True. norm_gene_vars = []; for b in np.unique(adata.obs[batch_key]):. mean, var = materialize_as_ndarray(; _get_mean_var(adata[adata.obs[batch_key] == b].X); ); not_const = var > 0; estimat_var = np.zeros((adata.X.shape[1])). y = np.log10(var[not_const]); x = np.log10(mean[not_const]); # output is sorted by x; v = lowess(y, x, frac=0.15); estimat_var[not_const][np.argsort(x)] = v[:, 1]. # get normalized variance; reg_std = np.sqrt(10 ** estimat_var); batch_counts = adata[adata.obs[batch_key] == b].X.copy(); # clip large values as in Seurat; N = np.sum(adata.obs[""batch""] == b); vmax = np.sqrt(N); clip_val = reg_std * vmax + mean; # could be something faster here; for g in range(batch_counts.shape[1]):; batch_counts[:, g][batch_counts[:, g] > vmax] = clip_val[g]. if sp_sparse.issparse(batch_counts):; squared_batch_counts_sum = np.array(batch_counts.power(2).sum(axis=0)); batch_counts_sum = np.array(batch_counts.sum(axis=0)); else:; squared_batch_counts_sum = np.square(batch_counts).sum(axis=0); batch_counts_sum = batch_counts.sum(axis=0). norm_gene_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326
https://github.com/scverse/scanpy/issues/993#issuecomment-615304326:223,Energy Efficiency,adapt,adapted,223,"I'm glad you all are considering adding this. I updated the implementation to work with sparse counts. . ```python; def seurat_v3_highly_variable_genes(; adata, n_top_genes: int = 4000, batch_key: str = ""batch""; ):; """""" An adapted implementation of the ""vst"" feature selection in Seurat v3. The major differences are that we use lowess insted of loess. For further details of the sparse arithmetic see https://www.overleaf.com/read/ckptrbgzzzpg. :param n_top_genes: How many variable genes to return; :param batch_key: key in adata.obs that contains batch info. If None, do not use batch info. """""". from scanpy.preprocessing._utils import _get_mean_var; from scanpy.preprocessing._distributed import materialize_as_ndarray. lowess = sm.nonparametric.lowess. if batch_key is None:; batch_correction = False; batch_key = ""batch""; adata.obs[batch_key] = pd.Categorical(np.zeros((adata.X.shape[0])).astype(int)); else:; batch_correction = True. norm_gene_vars = []; for b in np.unique(adata.obs[batch_key]):. mean, var = materialize_as_ndarray(; _get_mean_var(adata[adata.obs[batch_key] == b].X); ); not_const = var > 0; estimat_var = np.zeros((adata.X.shape[1])). y = np.log10(var[not_const]); x = np.log10(mean[not_const]); # output is sorted by x; v = lowess(y, x, frac=0.15); estimat_var[not_const][np.argsort(x)] = v[:, 1]. # get normalized variance; reg_std = np.sqrt(10 ** estimat_var); batch_counts = adata[adata.obs[batch_key] == b].X.copy(); # clip large values as in Seurat; N = np.sum(adata.obs[""batch""] == b); vmax = np.sqrt(N); clip_val = reg_std * vmax + mean; # could be something faster here; for g in range(batch_counts.shape[1]):; batch_counts[:, g][batch_counts[:, g] > vmax] = clip_val[g]. if sp_sparse.issparse(batch_counts):; squared_batch_counts_sum = np.array(batch_counts.power(2).sum(axis=0)); batch_counts_sum = np.array(batch_counts.sum(axis=0)); else:; squared_batch_counts_sum = np.square(batch_counts).sum(axis=0); batch_counts_sum = batch_counts.sum(axis=0). norm_gene_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326
https://github.com/scverse/scanpy/issues/993#issuecomment-615304326:1794,Energy Efficiency,power,power,1794,"= False; batch_key = ""batch""; adata.obs[batch_key] = pd.Categorical(np.zeros((adata.X.shape[0])).astype(int)); else:; batch_correction = True. norm_gene_vars = []; for b in np.unique(adata.obs[batch_key]):. mean, var = materialize_as_ndarray(; _get_mean_var(adata[adata.obs[batch_key] == b].X); ); not_const = var > 0; estimat_var = np.zeros((adata.X.shape[1])). y = np.log10(var[not_const]); x = np.log10(mean[not_const]); # output is sorted by x; v = lowess(y, x, frac=0.15); estimat_var[not_const][np.argsort(x)] = v[:, 1]. # get normalized variance; reg_std = np.sqrt(10 ** estimat_var); batch_counts = adata[adata.obs[batch_key] == b].X.copy(); # clip large values as in Seurat; N = np.sum(adata.obs[""batch""] == b); vmax = np.sqrt(N); clip_val = reg_std * vmax + mean; # could be something faster here; for g in range(batch_counts.shape[1]):; batch_counts[:, g][batch_counts[:, g] > vmax] = clip_val[g]. if sp_sparse.issparse(batch_counts):; squared_batch_counts_sum = np.array(batch_counts.power(2).sum(axis=0)); batch_counts_sum = np.array(batch_counts.sum(axis=0)); else:; squared_batch_counts_sum = np.square(batch_counts).sum(axis=0); batch_counts_sum = batch_counts.sum(axis=0). norm_gene_var = (1 / ((N - 1) * np.square(reg_std))) * (; (N * np.square(mean)); + squared_batch_counts_sum; - 2 * batch_counts_sum * mean; ); norm_gene_vars.append(norm_gene_var.reshape(1, -1)). norm_gene_vars = np.concatenate(norm_gene_vars, axis=0); # argsort twice gives ranks; ranked_norm_gene_vars = np.argsort(np.argsort(norm_gene_vars, axis=1), axis=1); median_norm_gene_vars = np.median(norm_gene_vars, axis=0); median_ranked = np.median(ranked_norm_gene_vars, axis=0). num_batches_high_var = np.sum(; ranked_norm_gene_vars >= (adata.X.shape[1] - n_top_genes), axis=0; ); df = pd.DataFrame(index=np.array(adata.var_names)); df[""highly_variable_nbatches""] = num_batches_high_var; df[""highly_variable_median_rank""] = median_ranked. df[""highly_variable_median_variance""] = median_norm_gene_vars; df.sort_v",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326
https://github.com/scverse/scanpy/issues/993#issuecomment-615304326:223,Modifiability,adapt,adapted,223,"I'm glad you all are considering adding this. I updated the implementation to work with sparse counts. . ```python; def seurat_v3_highly_variable_genes(; adata, n_top_genes: int = 4000, batch_key: str = ""batch""; ):; """""" An adapted implementation of the ""vst"" feature selection in Seurat v3. The major differences are that we use lowess insted of loess. For further details of the sparse arithmetic see https://www.overleaf.com/read/ckptrbgzzzpg. :param n_top_genes: How many variable genes to return; :param batch_key: key in adata.obs that contains batch info. If None, do not use batch info. """""". from scanpy.preprocessing._utils import _get_mean_var; from scanpy.preprocessing._distributed import materialize_as_ndarray. lowess = sm.nonparametric.lowess. if batch_key is None:; batch_correction = False; batch_key = ""batch""; adata.obs[batch_key] = pd.Categorical(np.zeros((adata.X.shape[0])).astype(int)); else:; batch_correction = True. norm_gene_vars = []; for b in np.unique(adata.obs[batch_key]):. mean, var = materialize_as_ndarray(; _get_mean_var(adata[adata.obs[batch_key] == b].X); ); not_const = var > 0; estimat_var = np.zeros((adata.X.shape[1])). y = np.log10(var[not_const]); x = np.log10(mean[not_const]); # output is sorted by x; v = lowess(y, x, frac=0.15); estimat_var[not_const][np.argsort(x)] = v[:, 1]. # get normalized variance; reg_std = np.sqrt(10 ** estimat_var); batch_counts = adata[adata.obs[batch_key] == b].X.copy(); # clip large values as in Seurat; N = np.sum(adata.obs[""batch""] == b); vmax = np.sqrt(N); clip_val = reg_std * vmax + mean; # could be something faster here; for g in range(batch_counts.shape[1]):; batch_counts[:, g][batch_counts[:, g] > vmax] = clip_val[g]. if sp_sparse.issparse(batch_counts):; squared_batch_counts_sum = np.array(batch_counts.power(2).sum(axis=0)); batch_counts_sum = np.array(batch_counts.sum(axis=0)); else:; squared_batch_counts_sum = np.square(batch_counts).sum(axis=0); batch_counts_sum = batch_counts.sum(axis=0). norm_gene_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326
https://github.com/scverse/scanpy/issues/993#issuecomment-615304326:475,Modifiability,variab,variable,475,"I'm glad you all are considering adding this. I updated the implementation to work with sparse counts. . ```python; def seurat_v3_highly_variable_genes(; adata, n_top_genes: int = 4000, batch_key: str = ""batch""; ):; """""" An adapted implementation of the ""vst"" feature selection in Seurat v3. The major differences are that we use lowess insted of loess. For further details of the sparse arithmetic see https://www.overleaf.com/read/ckptrbgzzzpg. :param n_top_genes: How many variable genes to return; :param batch_key: key in adata.obs that contains batch info. If None, do not use batch info. """""". from scanpy.preprocessing._utils import _get_mean_var; from scanpy.preprocessing._distributed import materialize_as_ndarray. lowess = sm.nonparametric.lowess. if batch_key is None:; batch_correction = False; batch_key = ""batch""; adata.obs[batch_key] = pd.Categorical(np.zeros((adata.X.shape[0])).astype(int)); else:; batch_correction = True. norm_gene_vars = []; for b in np.unique(adata.obs[batch_key]):. mean, var = materialize_as_ndarray(; _get_mean_var(adata[adata.obs[batch_key] == b].X); ); not_const = var > 0; estimat_var = np.zeros((adata.X.shape[1])). y = np.log10(var[not_const]); x = np.log10(mean[not_const]); # output is sorted by x; v = lowess(y, x, frac=0.15); estimat_var[not_const][np.argsort(x)] = v[:, 1]. # get normalized variance; reg_std = np.sqrt(10 ** estimat_var); batch_counts = adata[adata.obs[batch_key] == b].X.copy(); # clip large values as in Seurat; N = np.sum(adata.obs[""batch""] == b); vmax = np.sqrt(N); clip_val = reg_std * vmax + mean; # could be something faster here; for g in range(batch_counts.shape[1]):; batch_counts[:, g][batch_counts[:, g] > vmax] = clip_val[g]. if sp_sparse.issparse(batch_counts):; squared_batch_counts_sum = np.array(batch_counts.power(2).sum(axis=0)); batch_counts_sum = np.array(batch_counts.sum(axis=0)); else:; squared_batch_counts_sum = np.square(batch_counts).sum(axis=0); batch_counts_sum = batch_counts.sum(axis=0). norm_gene_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326
https://github.com/scverse/scanpy/issues/993#issuecomment-615957573:81,Deployability,integrat,integrate,81,"yes, I'll get to it next week. It didn't seem there was a straightforward way to integrate with the existing implementation given the filtering criterion is different, but I'll try my best.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615957573
https://github.com/scverse/scanpy/issues/993#issuecomment-615957573:81,Integrability,integrat,integrate,81,"yes, I'll get to it next week. It didn't seem there was a straightforward way to integrate with the existing implementation given the filtering criterion is different, but I'll try my best.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615957573
https://github.com/scverse/scanpy/issues/993#issuecomment-1040462161:602,Performance,perform,perform,602,"@adamgayoso, I have a question regarding the implementation of Seurat v3 HVG and am not sure if this is the correct thread (it's probably not). My question is regarding the final step where the function reports, variances_norm or norm_gene_var. Based on the description here, https://www.overleaf.com/project/5e7e320564f7d4000175d082, the norm_gene_var function computes the variance of the transformed values assuming that the mean of the zscores is 0. I guess my question is, post clipping values to a maximum, I think the mean of the transformed values might not be 0 anymore so if you were just to perform, var(transformed values), it will not equal the same value as variances_norm equation for the sparse approach. Reading through the referenced paper provided (Stuart 2019) its not clear whether they perform the variance of zscores post clipping, or with the assumption that mean zscore is 0 preclipping. . If this is not relevant, please feel free to ask me to delete this comment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-1040462161
https://github.com/scverse/scanpy/issues/993#issuecomment-1040462161:808,Performance,perform,perform,808,"@adamgayoso, I have a question regarding the implementation of Seurat v3 HVG and am not sure if this is the correct thread (it's probably not). My question is regarding the final step where the function reports, variances_norm or norm_gene_var. Based on the description here, https://www.overleaf.com/project/5e7e320564f7d4000175d082, the norm_gene_var function computes the variance of the transformed values assuming that the mean of the zscores is 0. I guess my question is, post clipping values to a maximum, I think the mean of the transformed values might not be 0 anymore so if you were just to perform, var(transformed values), it will not equal the same value as variances_norm equation for the sparse approach. Reading through the referenced paper provided (Stuart 2019) its not clear whether they perform the variance of zscores post clipping, or with the assumption that mean zscore is 0 preclipping. . If this is not relevant, please feel free to ask me to delete this comment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-1040462161
https://github.com/scverse/scanpy/issues/993#issuecomment-1040462161:789,Usability,clear,clear,789,"@adamgayoso, I have a question regarding the implementation of Seurat v3 HVG and am not sure if this is the correct thread (it's probably not). My question is regarding the final step where the function reports, variances_norm or norm_gene_var. Based on the description here, https://www.overleaf.com/project/5e7e320564f7d4000175d082, the norm_gene_var function computes the variance of the transformed values assuming that the mean of the zscores is 0. I guess my question is, post clipping values to a maximum, I think the mean of the transformed values might not be 0 anymore so if you were just to perform, var(transformed values), it will not equal the same value as variances_norm equation for the sparse approach. Reading through the referenced paper provided (Stuart 2019) its not clear whether they perform the variance of zscores post clipping, or with the assumption that mean zscore is 0 preclipping. . If this is not relevant, please feel free to ask me to delete this comment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-1040462161
https://github.com/scverse/scanpy/issues/993#issuecomment-1040470890:128,Testability,test,tests,128,"@cchrysostomou Indeed the mean will no longer be zero, I was merely reimplementing exactly what was done in Seurat, and we have tests to show in the single batch case that we get the same exact genes. No need to delete this comment. . I suppose you can think of it as the second moment instead of the variance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-1040470890
https://github.com/scverse/scanpy/issues/995#issuecomment-574584113:23,Testability,test,tests,23,"On the other hand, the tests are in the source distribution, including test data, blowing up scanpy’s size to 6MB. I usually put tests next to the package and don’t deliver them to users. We should probably start doing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/995#issuecomment-574584113
https://github.com/scverse/scanpy/issues/995#issuecomment-574584113:71,Testability,test,test,71,"On the other hand, the tests are in the source distribution, including test data, blowing up scanpy’s size to 6MB. I usually put tests next to the package and don’t deliver them to users. We should probably start doing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/995#issuecomment-574584113
https://github.com/scverse/scanpy/issues/995#issuecomment-574584113:129,Testability,test,tests,129,"On the other hand, the tests are in the source distribution, including test data, blowing up scanpy’s size to 6MB. I usually put tests next to the package and don’t deliver them to users. We should probably start doing that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/995#issuecomment-574584113
https://github.com/scverse/scanpy/issues/995#issuecomment-574587706:72,Deployability,install,installed,72,"Hmm, it’s in the source package and the wheel, but maybe it doesn’t get installed? Maybe `include_package_data=True` needs to be restored. How did you install from the source package anyway? Pip always uses wheels if possible, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/995#issuecomment-574587706
https://github.com/scverse/scanpy/issues/995#issuecomment-574587706:151,Deployability,install,install,151,"Hmm, it’s in the source package and the wheel, but maybe it doesn’t get installed? Maybe `include_package_data=True` needs to be restored. How did you install from the source package anyway? Pip always uses wheels if possible, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/995#issuecomment-574587706
https://github.com/scverse/scanpy/issues/998#issuecomment-575066688:102,Availability,down,downgrade,102,"For me is working properly with an earlier version of scanpy and the same matplotlib version. Can you downgrade and test if the problem is only happening in the last version. Also, you can try to see if the problem is related to some matplotlib parameters by resetting them.; ```PYTHON; matplotlib.rcdefaults(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/998#issuecomment-575066688
https://github.com/scverse/scanpy/issues/998#issuecomment-575066688:116,Testability,test,test,116,"For me is working properly with an earlier version of scanpy and the same matplotlib version. Can you downgrade and test if the problem is only happening in the last version. Also, you can try to see if the problem is related to some matplotlib parameters by resetting them.; ```PYTHON; matplotlib.rcdefaults(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/998#issuecomment-575066688
https://github.com/scverse/scanpy/issues/998#issuecomment-913395778:87,Deployability,patch,patch,87,"Just want to leave a comment here that the relevant matplotlib parameter to change is ""patch.edgecolor"". The following should fix the missing grouping lines. ```python; matplotlib.rcParams['patch.edgecolor'] = 'black'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/998#issuecomment-913395778
https://github.com/scverse/scanpy/issues/998#issuecomment-913395778:190,Deployability,patch,patch,190,"Just want to leave a comment here that the relevant matplotlib parameter to change is ""patch.edgecolor"". The following should fix the missing grouping lines. ```python; matplotlib.rcParams['patch.edgecolor'] = 'black'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/998#issuecomment-913395778
https://github.com/scverse/scanpy/pull/1003#issuecomment-577253898:42,Testability,test,tests,42,It's not clear to me why these `test_10x` tests are failing here and not on master -- there shouldn't be anything in this diff that affects those tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1003#issuecomment-577253898
https://github.com/scverse/scanpy/pull/1003#issuecomment-577253898:146,Testability,test,tests,146,It's not clear to me why these `test_10x` tests are failing here and not on master -- there shouldn't be anything in this diff that affects those tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1003#issuecomment-577253898
https://github.com/scverse/scanpy/pull/1003#issuecomment-577253898:9,Usability,clear,clear,9,It's not clear to me why these `test_10x` tests are failing here and not on master -- there shouldn't be anything in this diff that affects those tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1003#issuecomment-577253898
https://github.com/scverse/scanpy/pull/1003#issuecomment-577272388:94,Deployability,release,released,94,"You’re right: They would be failing on master if master had been tested since anndata 0.7 was released earlier today. We have to merge #989 to get tests to pass again before we can merge this. /edit: done, should work now",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1003#issuecomment-577272388
https://github.com/scverse/scanpy/pull/1003#issuecomment-577272388:65,Testability,test,tested,65,"You’re right: They would be failing on master if master had been tested since anndata 0.7 was released earlier today. We have to merge #989 to get tests to pass again before we can merge this. /edit: done, should work now",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1003#issuecomment-577272388
https://github.com/scverse/scanpy/pull/1003#issuecomment-577272388:147,Testability,test,tests,147,"You’re right: They would be failing on master if master had been tested since anndata 0.7 was released earlier today. We have to merge #989 to get tests to pass again before we can merge this. /edit: done, should work now",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1003#issuecomment-577272388
https://github.com/scverse/scanpy/pull/1004#issuecomment-577771644:25,Testability,test,test,25,> @awnimo can you please test this? Does the plot look like you want it to?. @flying-sheep The plots look the same as expected. Compared plots using methods from Scanpy and Harmony,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1004#issuecomment-577771644
https://github.com/scverse/scanpy/issues/1009#issuecomment-578310404:653,Availability,fault,fault,653,"I ran your code snippet multiple times on my end and I got the same results each time. Is this true for you as well? If both you and your partner can generate the same results consistently each time, then it is strange that your results disagree with each other... Some followup questions I have:; 1) Are ALL packages the same version? (Packages like Numba, scipy, sklearn, etc. should also be the same version to remove that as a potential source of variability); 2) Are you guys using the same operating system? ; 3) Can you run UMAP directly on the randomly generated matrix to see if your embeddings are the same? If they are, UMAP is likely not at fault.; 4) If you perturb your nearest neighbor matrix by adding noise to the edges such that the total edge weight differs by ~0.001 between perturbations, can you recreate the big differences in the UMAP projection? Small differences in the edge weights of the nearest neighbor graph CAN lead to huge differences in the UMAP projection if the graph has no inherent structure (which should be the case for randomly generated data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1009#issuecomment-578310404
https://github.com/scverse/scanpy/issues/1009#issuecomment-578310404:451,Modifiability,variab,variability,451,"I ran your code snippet multiple times on my end and I got the same results each time. Is this true for you as well? If both you and your partner can generate the same results consistently each time, then it is strange that your results disagree with each other... Some followup questions I have:; 1) Are ALL packages the same version? (Packages like Numba, scipy, sklearn, etc. should also be the same version to remove that as a potential source of variability); 2) Are you guys using the same operating system? ; 3) Can you run UMAP directly on the randomly generated matrix to see if your embeddings are the same? If they are, UMAP is likely not at fault.; 4) If you perturb your nearest neighbor matrix by adding noise to the edges such that the total edge weight differs by ~0.001 between perturbations, can you recreate the big differences in the UMAP projection? Small differences in the edge weights of the nearest neighbor graph CAN lead to huge differences in the UMAP projection if the graph has no inherent structure (which should be the case for randomly generated data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1009#issuecomment-578310404
https://github.com/scverse/scanpy/issues/1009#issuecomment-578344362:434,Deployability,continuous,continuous,434,"> 4\. Small differences in the edge weights of the nearest neighbor graph CAN lead to huge differences in the UMAP projection if the graph has no inherent structure. Exactly this! We have come across the difficulty of exactly reproducing the umap and clustering results in our [single-cell tutorial/best practices](www.github.com/theislab/single-cell-tutorial), however it was always due to the difficulty of defining boundaries in a continuous phenotype. Essentially, that means that the biological interpretations should not rely on this moving boundary anyway. On another note, you may have more luck with reproducibility by setting `PYTHONHASHSEED` as well. Check out the discussion here: #313.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1009#issuecomment-578344362
https://github.com/scverse/scanpy/issues/1009#issuecomment-579254312:1410,Integrability,depend,depending,1410,"Hi! I've tried first the 4th point from @atarashansky and yeah, multiplying the distance, connectivities, or both with a normal distribution with std 0.0001 (which makes roughly a 0.008% of absolute difference in the matrix) and the results are different. . Python code:; ```python; c0 = adata.uns['neighbors']['connectivities']; connect = adata.uns['neighbors']['connectivities'].todense(); connect = np.multiply(connect, np.random.normal(1, 0.0001, connect.shape)); adata.uns['neighbors']['connectivities'] = spr.csr_matrix(connect); print(100*np.sum(np.abs(c0 - connect))/min(np.sum(c0), np.sum(connect))); ```. ```python; c0 = adata.uns['neighbors']['distances']; csum = adata.uns['neighbors']['distances'].todense().sum(); connect = adata.uns['neighbors']['distances'].todense(); connect = np.multiply(connect, np.random.normal(1, 0.0001, connect.shape)); adata.uns['neighbors']['distances'] = spr.csr_matrix(connect); print(100*np.sum(np.abs(c0 - connect))/min(np.sum(c0), np.sum(connect))); ```. Before; ![image](https://user-images.githubusercontent.com/35657291/73247960-52f51900-41b2-11ea-937c-c09e301c5e7a.png). After; ![image](https://user-images.githubusercontent.com/35657291/73247968-55f00980-41b2-11ea-9d44-8e4cef0149d5.png). I mean, the results (hopefully) are not drastically different, but there are minor rearrangements and clustering assignments that might make the analysis be rearranged depending on the computer. . I've realized that, for some reason, the differences between points are not associated to different random seeds, that is, after setting the seed, the PCA look equal but are different in less than 0.001 when doing a pairwise comparison. This happens as well to the neighbors graph, so I believe that randomness is not really an issue. However I am puzzled as to why the differences in PCA / neighbour graphs is so small. Shouldn't two different computers do calculations equally at least at 3 or 4 decimals?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1009#issuecomment-579254312
https://github.com/scverse/scanpy/issues/1010#issuecomment-578570558:31,Availability,error,error,31,"I had the exact same issue and error message at that step in the tutorial. I installed scanpy using pip, because installing with conda was not working.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1010#issuecomment-578570558
https://github.com/scverse/scanpy/issues/1010#issuecomment-578570558:77,Deployability,install,installed,77,"I had the exact same issue and error message at that step in the tutorial. I installed scanpy using pip, because installing with conda was not working.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1010#issuecomment-578570558
https://github.com/scverse/scanpy/issues/1010#issuecomment-578570558:113,Deployability,install,installing,113,"I had the exact same issue and error message at that step in the tutorial. I installed scanpy using pip, because installing with conda was not working.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1010#issuecomment-578570558
https://github.com/scverse/scanpy/issues/1010#issuecomment-578570558:37,Integrability,message,message,37,"I had the exact same issue and error message at that step in the tutorial. I installed scanpy using pip, because installing with conda was not working.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1010#issuecomment-578570558
https://github.com/scverse/scanpy/issues/1010#issuecomment-578596689:33,Availability,error,error,33,"> I had the exact same issue and error message at that step in the tutorial. I installed scanpy using pip, because installing with conda was not working. Same here. I assume there is some issue with the implementation of the setter of adata.X, which prevents `adata.X = adata.X.toarray()` from updating X to its densified version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1010#issuecomment-578596689
https://github.com/scverse/scanpy/issues/1010#issuecomment-578596689:79,Deployability,install,installed,79,"> I had the exact same issue and error message at that step in the tutorial. I installed scanpy using pip, because installing with conda was not working. Same here. I assume there is some issue with the implementation of the setter of adata.X, which prevents `adata.X = adata.X.toarray()` from updating X to its densified version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1010#issuecomment-578596689
https://github.com/scverse/scanpy/issues/1010#issuecomment-578596689:115,Deployability,install,installing,115,"> I had the exact same issue and error message at that step in the tutorial. I installed scanpy using pip, because installing with conda was not working. Same here. I assume there is some issue with the implementation of the setter of adata.X, which prevents `adata.X = adata.X.toarray()` from updating X to its densified version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1010#issuecomment-578596689
https://github.com/scverse/scanpy/issues/1010#issuecomment-578596689:39,Integrability,message,message,39,"> I had the exact same issue and error message at that step in the tutorial. I installed scanpy using pip, because installing with conda was not working. Same here. I assume there is some issue with the implementation of the setter of adata.X, which prevents `adata.X = adata.X.toarray()` from updating X to its densified version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1010#issuecomment-578596689
https://github.com/scverse/scanpy/issues/1011#issuecomment-1959463716:44,Deployability,update,update,44,"@flying-sheep, was this fixed by the recent update?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1011#issuecomment-1959463716
https://github.com/scverse/scanpy/issues/1011#issuecomment-2370978435:32,Testability,test,test,32,"The only thing missing from the test in #1413 is `ll_dirichlet`, which seems to be a metric that’s implemented in `umap` but not PyNNDescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1011#issuecomment-2370978435
https://github.com/scverse/scanpy/pull/1012#issuecomment-578688703:810,Modifiability,extend,extend,810,"Looks good! Remaining questions:. - The plan is to add the Visium reading function to `anndata`, right?; - You’re repeating yourself with the docs: `doc_scatter_basic` (and therefore `doc_scatter_embedding`) and the docstring of `pl.spatial` both contain similar text for the same parameters. If you want to reorder them, you could do something fancy (like slicing doc_scatter_embedding) or just mention the parameter names in the free text, something like:. ```restructuredtext; Scatter plot in spatial coordinates. Use the parameter `img_key` to see the microscopy image in the background.; Use `crop_coord`, `alpha_img`, and `bw` to control how it is displayed,; and `scale_spot` to control the size of the Visium spots plotted on top.; ```. - Is it possible to derive the amount of cropping? Then we could extend the `crop_coord` parameter to this:. ```py; Union[; Iterable[Literal['left', 'l', 'right', 'r', 'top', 't', 'bottom', 'b']],; Tuple[int, int, int, int], # l, r, t, b; ]; ```. - Maybe it makes sense to add some test data and a test plot? (very low res of course)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-578688703
https://github.com/scverse/scanpy/pull/1012#issuecomment-578688703:1027,Testability,test,test,1027,"Looks good! Remaining questions:. - The plan is to add the Visium reading function to `anndata`, right?; - You’re repeating yourself with the docs: `doc_scatter_basic` (and therefore `doc_scatter_embedding`) and the docstring of `pl.spatial` both contain similar text for the same parameters. If you want to reorder them, you could do something fancy (like slicing doc_scatter_embedding) or just mention the parameter names in the free text, something like:. ```restructuredtext; Scatter plot in spatial coordinates. Use the parameter `img_key` to see the microscopy image in the background.; Use `crop_coord`, `alpha_img`, and `bw` to control how it is displayed,; and `scale_spot` to control the size of the Visium spots plotted on top.; ```. - Is it possible to derive the amount of cropping? Then we could extend the `crop_coord` parameter to this:. ```py; Union[; Iterable[Literal['left', 'l', 'right', 'r', 'top', 't', 'bottom', 'b']],; Tuple[int, int, int, int], # l, r, t, b; ]; ```. - Maybe it makes sense to add some test data and a test plot? (very low res of course)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-578688703
https://github.com/scverse/scanpy/pull/1012#issuecomment-578688703:1043,Testability,test,test,1043,"Looks good! Remaining questions:. - The plan is to add the Visium reading function to `anndata`, right?; - You’re repeating yourself with the docs: `doc_scatter_basic` (and therefore `doc_scatter_embedding`) and the docstring of `pl.spatial` both contain similar text for the same parameters. If you want to reorder them, you could do something fancy (like slicing doc_scatter_embedding) or just mention the parameter names in the free text, something like:. ```restructuredtext; Scatter plot in spatial coordinates. Use the parameter `img_key` to see the microscopy image in the background.; Use `crop_coord`, `alpha_img`, and `bw` to control how it is displayed,; and `scale_spot` to control the size of the Visium spots plotted on top.; ```. - Is it possible to derive the amount of cropping? Then we could extend the `crop_coord` parameter to this:. ```py; Union[; Iterable[Literal['left', 'l', 'right', 'r', 'top', 't', 'bottom', 'b']],; Tuple[int, int, int, int], # l, r, t, b; ]; ```. - Maybe it makes sense to add some test data and a test plot? (very low res of course)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-578688703
https://github.com/scverse/scanpy/pull/1012#issuecomment-580139642:166,Usability,simpl,simply,166,"scale_spot is a float to scale the size of the spots from users. We have original radius dimension but it can be handy to modify it according to cropping/zooming, or simply for visualization purposes. Regarding examples, I've made a tutorial that I will push to scanpy-tutorial. I'm waiting for @Mirkazemi read function that I need to complete the notebook",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-580139642
https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894:675,Energy Efficiency,adapt,adapt,675,"> We have original radius dimension but it can be handy to modify it according to cropping/zooming, or simply for visualization purposes. Cropping/zooming won’t make a difference if you plot circles in data space. So there’s our problem: We have the original radius in data space, but you’re plotting markers, whose size is in figure space (i.e. their center position in the final figure is determined and then they’re plotted as circles right into the graphic). So you need to switch from `ax.scatter` to a `circles` function that does what we need: https://stackoverflow.com/questions/9081553/python-scatter-plot-size-and-style-of-the-marker/24567352#24567352. We can just adapt that one (throw out what we don’t need), make it so the `scatter(...)` calls in “embedding” work with it, and do `scatter = ax.scatter if img_key is None else partial(circles, ax=ax)`. This means that we don’t have to do difficult math when cropping/zooming, as the spots will always just be the correct size. We can also get rid of `spot_size` and make `size` a scale factor in the image case (1=normal size, 0.8=slightly smaller than in the data, 1.2=slightly larger than in the data)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894
https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894:675,Modifiability,adapt,adapt,675,"> We have original radius dimension but it can be handy to modify it according to cropping/zooming, or simply for visualization purposes. Cropping/zooming won’t make a difference if you plot circles in data space. So there’s our problem: We have the original radius in data space, but you’re plotting markers, whose size is in figure space (i.e. their center position in the final figure is determined and then they’re plotted as circles right into the graphic). So you need to switch from `ax.scatter` to a `circles` function that does what we need: https://stackoverflow.com/questions/9081553/python-scatter-plot-size-and-style-of-the-marker/24567352#24567352. We can just adapt that one (throw out what we don’t need), make it so the `scatter(...)` calls in “embedding” work with it, and do `scatter = ax.scatter if img_key is None else partial(circles, ax=ax)`. This means that we don’t have to do difficult math when cropping/zooming, as the spots will always just be the correct size. We can also get rid of `spot_size` and make `size` a scale factor in the image case (1=normal size, 0.8=slightly smaller than in the data, 1.2=slightly larger than in the data)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894
https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894:103,Usability,simpl,simply,103,"> We have original radius dimension but it can be handy to modify it according to cropping/zooming, or simply for visualization purposes. Cropping/zooming won’t make a difference if you plot circles in data space. So there’s our problem: We have the original radius in data space, but you’re plotting markers, whose size is in figure space (i.e. their center position in the final figure is determined and then they’re plotted as circles right into the graphic). So you need to switch from `ax.scatter` to a `circles` function that does what we need: https://stackoverflow.com/questions/9081553/python-scatter-plot-size-and-style-of-the-marker/24567352#24567352. We can just adapt that one (throw out what we don’t need), make it so the `scatter(...)` calls in “embedding” work with it, and do `scatter = ax.scatter if img_key is None else partial(circles, ax=ax)`. This means that we don’t have to do difficult math when cropping/zooming, as the spots will always just be the correct size. We can also get rid of `spot_size` and make `size` a scale factor in the image case (1=normal size, 0.8=slightly smaller than in the data, 1.2=slightly larger than in the data)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894
https://github.com/scverse/scanpy/pull/1013#issuecomment-580245419:98,Modifiability,extend,extend,98,"For me it's good, I would just merge so we have a prototype version for the vignette, then we can extend for other data type/modify it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1013#issuecomment-580245419
https://github.com/scverse/scanpy/pull/1014#issuecomment-580247580:55,Testability,test,tests,55,"Should probably figure out what's happening with these tests. @flying-sheep, has this been happening for other PRs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1014#issuecomment-580247580
https://github.com/scverse/scanpy/pull/1014#issuecomment-580253413:35,Testability,test,tests,35,"What the heck is going on with the tests? Sorry, didn't see this before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1014#issuecomment-580253413
https://github.com/scverse/scanpy/pull/1015#issuecomment-581011973:203,Testability,test,test,203,"@flying-sheep, I made two small changes:. * The 10x readers should no longer return views, fixing `test_read_10x`; * Slightly cleaner providing of categories for leiden/ louvain code. For the clustermap test, it's not clear to me that the problems are even related to pandas, though the cause might be: https://github.com/pandas-dev/pandas/issues/18720. There are two images which are compared in this test. I'll post the comparisons here:. # `master_clustermap.png`. I believe the difference is just the margin, so we should be good to just change the test image. ## Expected. ![master_clustermap](https://user-images.githubusercontent.com/8238804/73589759-d73af980-452e-11ea-9a77-89ecf9e752dc.png). ## Actual. ![master_clustermap](https://user-images.githubusercontent.com/8238804/73589766-e5891580-452e-11ea-9762-aa483399c8b3.png). # `master_clustermap_withcolor.png`. This one looks worse, but I'm not sure how to fix it. @fidelram might know better?. ## Expected. ![master_clustermap_withcolor](https://user-images.githubusercontent.com/8238804/73589782-123d2d00-452f-11ea-828c-5e6fdc0b6091.png). ## Actual. ![master_clustermap_withcolor](https://user-images.githubusercontent.com/8238804/73589788-21bc7600-452f-11ea-9661-ec55aeee07de.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-581011973
https://github.com/scverse/scanpy/pull/1015#issuecomment-581011973:402,Testability,test,test,402,"@flying-sheep, I made two small changes:. * The 10x readers should no longer return views, fixing `test_read_10x`; * Slightly cleaner providing of categories for leiden/ louvain code. For the clustermap test, it's not clear to me that the problems are even related to pandas, though the cause might be: https://github.com/pandas-dev/pandas/issues/18720. There are two images which are compared in this test. I'll post the comparisons here:. # `master_clustermap.png`. I believe the difference is just the margin, so we should be good to just change the test image. ## Expected. ![master_clustermap](https://user-images.githubusercontent.com/8238804/73589759-d73af980-452e-11ea-9a77-89ecf9e752dc.png). ## Actual. ![master_clustermap](https://user-images.githubusercontent.com/8238804/73589766-e5891580-452e-11ea-9762-aa483399c8b3.png). # `master_clustermap_withcolor.png`. This one looks worse, but I'm not sure how to fix it. @fidelram might know better?. ## Expected. ![master_clustermap_withcolor](https://user-images.githubusercontent.com/8238804/73589782-123d2d00-452f-11ea-828c-5e6fdc0b6091.png). ## Actual. ![master_clustermap_withcolor](https://user-images.githubusercontent.com/8238804/73589788-21bc7600-452f-11ea-9661-ec55aeee07de.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-581011973
https://github.com/scverse/scanpy/pull/1015#issuecomment-581011973:553,Testability,test,test,553,"@flying-sheep, I made two small changes:. * The 10x readers should no longer return views, fixing `test_read_10x`; * Slightly cleaner providing of categories for leiden/ louvain code. For the clustermap test, it's not clear to me that the problems are even related to pandas, though the cause might be: https://github.com/pandas-dev/pandas/issues/18720. There are two images which are compared in this test. I'll post the comparisons here:. # `master_clustermap.png`. I believe the difference is just the margin, so we should be good to just change the test image. ## Expected. ![master_clustermap](https://user-images.githubusercontent.com/8238804/73589759-d73af980-452e-11ea-9a77-89ecf9e752dc.png). ## Actual. ![master_clustermap](https://user-images.githubusercontent.com/8238804/73589766-e5891580-452e-11ea-9762-aa483399c8b3.png). # `master_clustermap_withcolor.png`. This one looks worse, but I'm not sure how to fix it. @fidelram might know better?. ## Expected. ![master_clustermap_withcolor](https://user-images.githubusercontent.com/8238804/73589782-123d2d00-452f-11ea-828c-5e6fdc0b6091.png). ## Actual. ![master_clustermap_withcolor](https://user-images.githubusercontent.com/8238804/73589788-21bc7600-452f-11ea-9661-ec55aeee07de.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-581011973
https://github.com/scverse/scanpy/pull/1015#issuecomment-581011973:218,Usability,clear,clear,218,"@flying-sheep, I made two small changes:. * The 10x readers should no longer return views, fixing `test_read_10x`; * Slightly cleaner providing of categories for leiden/ louvain code. For the clustermap test, it's not clear to me that the problems are even related to pandas, though the cause might be: https://github.com/pandas-dev/pandas/issues/18720. There are two images which are compared in this test. I'll post the comparisons here:. # `master_clustermap.png`. I believe the difference is just the margin, so we should be good to just change the test image. ## Expected. ![master_clustermap](https://user-images.githubusercontent.com/8238804/73589759-d73af980-452e-11ea-9a77-89ecf9e752dc.png). ## Actual. ![master_clustermap](https://user-images.githubusercontent.com/8238804/73589766-e5891580-452e-11ea-9762-aa483399c8b3.png). # `master_clustermap_withcolor.png`. This one looks worse, but I'm not sure how to fix it. @fidelram might know better?. ## Expected. ![master_clustermap_withcolor](https://user-images.githubusercontent.com/8238804/73589782-123d2d00-452f-11ea-828c-5e6fdc0b6091.png). ## Actual. ![master_clustermap_withcolor](https://user-images.githubusercontent.com/8238804/73589788-21bc7600-452f-11ea-9661-ec55aeee07de.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-581011973
https://github.com/scverse/scanpy/pull/1015#issuecomment-581052169:72,Integrability,depend,dependency,72,"Sorry yeah, the branch name might not be accurate when we’re done. What dependency causes those mismatches? Locally the image comparisons succeed unless I create a venv …. > master_clustermap.png; > ; > I believe the difference is just the margin, so we should be good to just change the test image. nah, it also has an ugly white gap between tree and heatmap, probably caused by the same reason as the other one …. /edit: the space seems exactly enough to hold the color bar’s x tick labels, that might be the cause. /edit2: also appears without scanpy’s style or anything: mwaskom/seaborn#1953",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-581052169
https://github.com/scverse/scanpy/pull/1015#issuecomment-581052169:288,Testability,test,test,288,"Sorry yeah, the branch name might not be accurate when we’re done. What dependency causes those mismatches? Locally the image comparisons succeed unless I create a venv …. > master_clustermap.png; > ; > I believe the difference is just the margin, so we should be good to just change the test image. nah, it also has an ugly white gap between tree and heatmap, probably caused by the same reason as the other one …. /edit: the space seems exactly enough to hold the color bar’s x tick labels, that might be the cause. /edit2: also appears without scanpy’s style or anything: mwaskom/seaborn#1953",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-581052169
https://github.com/scverse/scanpy/pull/1015#issuecomment-585115015:90,Availability,down,downgrade,90,Whats the timeline here? When will there be a release that includes this fix? Or should I downgrade pandas in the meantime?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-585115015
https://github.com/scverse/scanpy/pull/1015#issuecomment-585115015:46,Deployability,release,release,46,Whats the timeline here? When will there be a release that includes this fix? Or should I downgrade pandas in the meantime?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-585115015
https://github.com/scverse/scanpy/pull/1015#issuecomment-585121986:86,Deployability,release,release,86,You can just use scanpy master if you need this fix ASAP. Otherwise I think we should release a new version pretty soon after merging #1038.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-585121986
https://github.com/scverse/scanpy/issues/1016#issuecomment-583249005:109,Deployability,pipeline,pipeline,109,"Could you give examples of how you'd like scanpy to work with MetaCell? It looks like a pretty comprehensive pipeline, so I'm a little unsure of at what point in an analysis you'd want to use scanpy with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1016#issuecomment-583249005
https://github.com/scverse/scanpy/issues/1017#issuecomment-645824317:80,Deployability,update,update,80,@salwanbutrus This should be fixed from Scanpy version 1.4.5.1. You may need to update your Scanpy version.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1017#issuecomment-645824317
https://github.com/scverse/scanpy/pull/1020#issuecomment-581810484:23,Testability,test,test,23,"Yeah, but why does the test not fail on master then? The following works:. ```py; import scanpy as sc; pbmc = sc.datasets.pbmc68k_reduced(); assert pbmc.uns['louvain_colors'].tolist() == sc.pl.palettes.zeileis_28[:11]; ```. and the Zeileis palette is:; ![](https://i.stack.imgur.com/9d15A.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1020#issuecomment-581810484
https://github.com/scverse/scanpy/pull/1020#issuecomment-581810484:141,Testability,assert,assert,141,"Yeah, but why does the test not fail on master then? The following works:. ```py; import scanpy as sc; pbmc = sc.datasets.pbmc68k_reduced(); assert pbmc.uns['louvain_colors'].tolist() == sc.pl.palettes.zeileis_28[:11]; ```. and the Zeileis palette is:; ![](https://i.stack.imgur.com/9d15A.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1020#issuecomment-581810484
https://github.com/scverse/scanpy/issues/1021#issuecomment-582636654:83,Safety,risk,risky,83,I would tend to agree with @gokceneraslan on this. Concatenating obsm seems like a risky thing to do.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1021#issuecomment-582636654
https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183:1185,Energy Efficiency,reduce,reducedDims,1185,"> Concatenating obsm without touching uns puts the object in an unstable state somehow from diffmap point of view. Sure, but this should only ever effect `diffmap`. . Arguably it also puts the object in an unstable state from a PCA point of view since there's no promise that observation loadings correspond to the variable loadings. I don't think users should have the expectation that meaning is preserved by concatenation, but I'm not sure if this is something people would believe. > I'm not entirely sure. Less experienced users might concatenate things and plot a UMAP without running sc.tl.umap on the new concatenated object and see some super weird things. Have users reported that this is confusing?. > It'd be cool to print a warning in such cases somehow, that concatenated obsms are not compatible or so. I think a note in the docstring for concatenation should be sufficient. My expectation is that it's much more common for our users to be familiar with what similar methods (like `np.concatenate` and `pd.concat`) do, and to have the right expectations about this. Bioconductor's `SummarizedExperiment` classes also do not warn about this, and concatenate along their `reducedDims`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183
https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183:315,Modifiability,variab,variable,315,"> Concatenating obsm without touching uns puts the object in an unstable state somehow from diffmap point of view. Sure, but this should only ever effect `diffmap`. . Arguably it also puts the object in an unstable state from a PCA point of view since there's no promise that observation loadings correspond to the variable loadings. I don't think users should have the expectation that meaning is preserved by concatenation, but I'm not sure if this is something people would believe. > I'm not entirely sure. Less experienced users might concatenate things and plot a UMAP without running sc.tl.umap on the new concatenated object and see some super weird things. Have users reported that this is confusing?. > It'd be cool to print a warning in such cases somehow, that concatenated obsms are not compatible or so. I think a note in the docstring for concatenation should be sufficient. My expectation is that it's much more common for our users to be familiar with what similar methods (like `np.concatenate` and `pd.concat`) do, and to have the right expectations about this. Bioconductor's `SummarizedExperiment` classes also do not warn about this, and concatenate along their `reducedDims`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183
https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183:288,Performance,load,loadings,288,"> Concatenating obsm without touching uns puts the object in an unstable state somehow from diffmap point of view. Sure, but this should only ever effect `diffmap`. . Arguably it also puts the object in an unstable state from a PCA point of view since there's no promise that observation loadings correspond to the variable loadings. I don't think users should have the expectation that meaning is preserved by concatenation, but I'm not sure if this is something people would believe. > I'm not entirely sure. Less experienced users might concatenate things and plot a UMAP without running sc.tl.umap on the new concatenated object and see some super weird things. Have users reported that this is confusing?. > It'd be cool to print a warning in such cases somehow, that concatenated obsms are not compatible or so. I think a note in the docstring for concatenation should be sufficient. My expectation is that it's much more common for our users to be familiar with what similar methods (like `np.concatenate` and `pd.concat`) do, and to have the right expectations about this. Bioconductor's `SummarizedExperiment` classes also do not warn about this, and concatenate along their `reducedDims`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183
https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183:324,Performance,load,loadings,324,"> Concatenating obsm without touching uns puts the object in an unstable state somehow from diffmap point of view. Sure, but this should only ever effect `diffmap`. . Arguably it also puts the object in an unstable state from a PCA point of view since there's no promise that observation loadings correspond to the variable loadings. I don't think users should have the expectation that meaning is preserved by concatenation, but I'm not sure if this is something people would believe. > I'm not entirely sure. Less experienced users might concatenate things and plot a UMAP without running sc.tl.umap on the new concatenated object and see some super weird things. Have users reported that this is confusing?. > It'd be cool to print a warning in such cases somehow, that concatenated obsms are not compatible or so. I think a note in the docstring for concatenation should be sufficient. My expectation is that it's much more common for our users to be familiar with what similar methods (like `np.concatenate` and `pd.concat`) do, and to have the right expectations about this. Bioconductor's `SummarizedExperiment` classes also do not warn about this, and concatenate along their `reducedDims`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183
https://github.com/scverse/scanpy/issues/1021#issuecomment-739622198:138,Usability,simpl,simply,138,"Hi all! I've been using `scanpy` and ran into a similar problem. I was wondering if there's an easy / appropriate work-around, other than simply deleting `obsm[X_diffmap]`?. Thank you all for developing `scanpy`, it's a really wonderful piece of software.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1021#issuecomment-739622198
https://github.com/scverse/scanpy/pull/1024#issuecomment-584591351:32,Testability,test,test,32,"Hi @giovp still up for adding a test dataset and tests? If so, this would be a good moment in time, as we should merge this quickly before more changes to master cause conflicts",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1024#issuecomment-584591351
https://github.com/scverse/scanpy/pull/1024#issuecomment-584591351:49,Testability,test,tests,49,"Hi @giovp still up for adding a test dataset and tests? If so, this would be a good moment in time, as we should merge this quickly before more changes to master cause conflicts",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1024#issuecomment-584591351
https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661:638,Availability,down,download,638,"Hi @giovp! The test data is too large, it’ll take scanpy a long time to clone once this is in `master`. The way we fix it is that we replace the data and then merge our changes into commit bb70446 (creating a new commit from the two and eliminating any trace of the big dataset). For reference, the test data `filtered_feature_bc_matrix.h5` is <100kb. I’d say you find the smallest of the 10x example datasets, reduce it so the (non-image) data is <100kb all in all, and delete the hires pic. The code should work if there’s only the lores pic anyway, right?. An alternative would be to mark our tests as “internet” tests and dynamically download the data, but I think it’s better to always run the spatial tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661
https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661:411,Energy Efficiency,reduce,reduce,411,"Hi @giovp! The test data is too large, it’ll take scanpy a long time to clone once this is in `master`. The way we fix it is that we replace the data and then merge our changes into commit bb70446 (creating a new commit from the two and eliminating any trace of the big dataset). For reference, the test data `filtered_feature_bc_matrix.h5` is <100kb. I’d say you find the smallest of the 10x example datasets, reduce it so the (non-image) data is <100kb all in all, and delete the hires pic. The code should work if there’s only the lores pic anyway, right?. An alternative would be to mark our tests as “internet” tests and dynamically download the data, but I think it’s better to always run the spatial tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661
https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661:15,Testability,test,test,15,"Hi @giovp! The test data is too large, it’ll take scanpy a long time to clone once this is in `master`. The way we fix it is that we replace the data and then merge our changes into commit bb70446 (creating a new commit from the two and eliminating any trace of the big dataset). For reference, the test data `filtered_feature_bc_matrix.h5` is <100kb. I’d say you find the smallest of the 10x example datasets, reduce it so the (non-image) data is <100kb all in all, and delete the hires pic. The code should work if there’s only the lores pic anyway, right?. An alternative would be to mark our tests as “internet” tests and dynamically download the data, but I think it’s better to always run the spatial tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661
https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661:299,Testability,test,test,299,"Hi @giovp! The test data is too large, it’ll take scanpy a long time to clone once this is in `master`. The way we fix it is that we replace the data and then merge our changes into commit bb70446 (creating a new commit from the two and eliminating any trace of the big dataset). For reference, the test data `filtered_feature_bc_matrix.h5` is <100kb. I’d say you find the smallest of the 10x example datasets, reduce it so the (non-image) data is <100kb all in all, and delete the hires pic. The code should work if there’s only the lores pic anyway, right?. An alternative would be to mark our tests as “internet” tests and dynamically download the data, but I think it’s better to always run the spatial tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661
https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661:596,Testability,test,tests,596,"Hi @giovp! The test data is too large, it’ll take scanpy a long time to clone once this is in `master`. The way we fix it is that we replace the data and then merge our changes into commit bb70446 (creating a new commit from the two and eliminating any trace of the big dataset). For reference, the test data `filtered_feature_bc_matrix.h5` is <100kb. I’d say you find the smallest of the 10x example datasets, reduce it so the (non-image) data is <100kb all in all, and delete the hires pic. The code should work if there’s only the lores pic anyway, right?. An alternative would be to mark our tests as “internet” tests and dynamically download the data, but I think it’s better to always run the spatial tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661
https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661:616,Testability,test,tests,616,"Hi @giovp! The test data is too large, it’ll take scanpy a long time to clone once this is in `master`. The way we fix it is that we replace the data and then merge our changes into commit bb70446 (creating a new commit from the two and eliminating any trace of the big dataset). For reference, the test data `filtered_feature_bc_matrix.h5` is <100kb. I’d say you find the smallest of the 10x example datasets, reduce it so the (non-image) data is <100kb all in all, and delete the hires pic. The code should work if there’s only the lores pic anyway, right?. An alternative would be to mark our tests as “internet” tests and dynamically download the data, but I think it’s better to always run the spatial tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661
https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661:707,Testability,test,tests,707,"Hi @giovp! The test data is too large, it’ll take scanpy a long time to clone once this is in `master`. The way we fix it is that we replace the data and then merge our changes into commit bb70446 (creating a new commit from the two and eliminating any trace of the big dataset). For reference, the test data `filtered_feature_bc_matrix.h5` is <100kb. I’d say you find the smallest of the 10x example datasets, reduce it so the (non-image) data is <100kb all in all, and delete the hires pic. The code should work if there’s only the lores pic anyway, right?. An alternative would be to mark our tests as “internet” tests and dynamically download the data, but I think it’s better to always run the spatial tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1024#issuecomment-586185661
https://github.com/scverse/scanpy/pull/1024#issuecomment-586196692:29,Energy Efficiency,reduce,reduce,29,"I agree, I've been trying to reduce the dataset and save it as 10x_h5 format again but it seems to be more complicated than I thought. I think it would be best to have an 10x_h5 format so to also use it for the read function test. Also, yes will only keep the lowres image",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1024#issuecomment-586196692
https://github.com/scverse/scanpy/pull/1024#issuecomment-586196692:225,Testability,test,test,225,"I agree, I've been trying to reduce the dataset and save it as 10x_h5 format again but it seems to be more complicated than I thought. I think it would be best to have an 10x_h5 format so to also use it for the read function test. Also, yes will only keep the lowres image",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1024#issuecomment-586196692
https://github.com/scverse/scanpy/issues/1027#issuecomment-587178083:67,Availability,error,error,67,"We have a backwards compatibility wrapper, I have no idea how this error can be possible:. https://github.com/theislab/anndata/blob/41eadb2a76d91ae455faf01afd2382143b9af4b2/anndata/_core/anndata.py#L2137-L2140",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1027#issuecomment-587178083
https://github.com/scverse/scanpy/issues/1027#issuecomment-587178083:34,Integrability,wrap,wrapper,34,"We have a backwards compatibility wrapper, I have no idea how this error can be possible:. https://github.com/theislab/anndata/blob/41eadb2a76d91ae455faf01afd2382143b9af4b2/anndata/_core/anndata.py#L2137-L2140",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1027#issuecomment-587178083
https://github.com/scverse/scanpy/issues/1028#issuecomment-583439056:61,Integrability,depend,dependencies,61,"That being said, maybe this better be reflected in the conda dependencies, so pandas 1.0 is not pulled with scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1028#issuecomment-583439056
https://github.com/scverse/scanpy/pull/1029#issuecomment-584665061:45,Availability,error,error,45,"Thank you! I’d prefer a nicer, more explicit error that describes the problem. You could use sth like:. ```py; if not is_categorical_dtype(adata.obs[groupby].dtype):; raise ValueError(; f""The column `adata.obs[groupby]` needs to be categorical, ""; f""but the {groupby!r} column is of dtype {adata.obs[groupby].dtype}.""; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1029#issuecomment-584665061
https://github.com/scverse/scanpy/issues/1030#issuecomment-582648527:155,Deployability,continuous,continuous,155,"Hey!; So one reason I can think of why it's important that `.obs` covariates are strings is that matplotlib will assume that numerical covariates lie on a continuous scale and thus colour this with a continuous colour scale and provide the corresponding colour bar. Typically that is not what you want for louvain clusters. These are inherently categorical, so the conversion to string is used to further convert to `pd.Categorical` via `sanitize_anndata()`. From my point of view the `.loc` and `.iloc` convention isn't particularly intuitive for new users, so I wouldn't be in favour of that setup. I'm not sure I see the issue with converting numerical values to strings if what you are using these as are labels, and thus categories (e.g. `obs_names` or other). Integers are after all values which have an inherent ordering and a defined distance, which is not a characteristic you would assign to an index.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582648527
https://github.com/scverse/scanpy/issues/1030#issuecomment-582648527:200,Deployability,continuous,continuous,200,"Hey!; So one reason I can think of why it's important that `.obs` covariates are strings is that matplotlib will assume that numerical covariates lie on a continuous scale and thus colour this with a continuous colour scale and provide the corresponding colour bar. Typically that is not what you want for louvain clusters. These are inherently categorical, so the conversion to string is used to further convert to `pd.Categorical` via `sanitize_anndata()`. From my point of view the `.loc` and `.iloc` convention isn't particularly intuitive for new users, so I wouldn't be in favour of that setup. I'm not sure I see the issue with converting numerical values to strings if what you are using these as are labels, and thus categories (e.g. `obs_names` or other). Integers are after all values which have an inherent ordering and a defined distance, which is not a characteristic you would assign to an index.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582648527
https://github.com/scverse/scanpy/issues/1030#issuecomment-582648527:534,Usability,intuit,intuitive,534,"Hey!; So one reason I can think of why it's important that `.obs` covariates are strings is that matplotlib will assume that numerical covariates lie on a continuous scale and thus colour this with a continuous colour scale and provide the corresponding colour bar. Typically that is not what you want for louvain clusters. These are inherently categorical, so the conversion to string is used to further convert to `pd.Categorical` via `sanitize_anndata()`. From my point of view the `.loc` and `.iloc` convention isn't particularly intuitive for new users, so I wouldn't be in favour of that setup. I'm not sure I see the issue with converting numerical values to strings if what you are using these as are labels, and thus categories (e.g. `obs_names` or other). Integers are after all values which have an inherent ordering and a defined distance, which is not a characteristic you would assign to an index.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582648527
https://github.com/scverse/scanpy/issues/1030#issuecomment-582657247:192,Availability,error,error,192,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582657247
https://github.com/scverse/scanpy/issues/1030#issuecomment-582657247:604,Availability,error,errors,604,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582657247
https://github.com/scverse/scanpy/issues/1030#issuecomment-582657247:65,Deployability,continuous,continuous,65,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582657247
https://github.com/scverse/scanpy/issues/1030#issuecomment-582657247:804,Deployability,continuous,continuous,804,"So it appears to me that the difference between the discrete and continuous colours is purely an internal `scanpy` decision. Plotting with `matplotlib` and a `pd.Categorical` returns the same error as before. ![image](https://user-images.githubusercontent.com/8499679/73891118-81719480-4841-11ea-8752-b7490d89f4bd.png). An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of `str`, ensure that it returns a categorical where the categories are ints. Categorical (string) output: scanpy works, matplotlib errors:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891608-a1ee1e80-4842-11ea-97b8-16c4618a894f.png). </details>. Integer output: matplotlib works, scanpy mistakenly uses continuous colormap:. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891676-bd592980-4842-11ea-8043-5ed74693ee28.png). </details>. Cateogrical (integer) output: both work. <details>. ![image](https://user-images.githubusercontent.com/8499679/73891704-ccd87280-4842-11ea-91c1-445b1574d812.png). </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582657247
https://github.com/scverse/scanpy/issues/1030#issuecomment-582728678:1327,Performance,Perform,Performance,1327,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward?. Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this.; * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way.; * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582728678
https://github.com/scverse/scanpy/issues/1030#issuecomment-582728678:241,Safety,avoid,avoid,241,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward?. Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this.; * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way.; * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582728678
https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:2108,Deployability,pipeline,pipelines,2108," to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python; In [1]: '1' < 'a'; Out[1]: True; ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result in greater widespread adoption that I would love to see for scanpy and anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545
https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:2280,Deployability,continuous,continuous,2280," to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python; In [1]: '1' < 'a'; Out[1]: True; ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result in greater widespread adoption that I would love to see for scanpy and anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545
https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:1412,Security,access,access,1412,",0], dtype='category'); sizes = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes); ```; I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tool",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545
https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:2211,Security,access,accessible,2211," to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python; In [1]: '1' < 'a'; Out[1]: True; ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result in greater widespread adoption that I would love to see for scanpy and anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545
https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:2627,Testability,test,test,2627," to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python; In [1]: '1' < 'a'; Out[1]: True; ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / dpt for my research. I bring up these issues here because I think changing some of these conventions could result in greater widespread adoption that I would love to see for scanpy and anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545
https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:934,Usability,learn,learning,934,"Oh sorry, so actually in my original post, I added the wrong code that works. Matplotlib apparently has already added support for categoricals **as long as the categories are numerics**. For example, the following code works as intended:. ```python; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt. data = np.random.normal(size=(100,2)); colors = pd.Series(data[:,0], dtype='category'); sizes = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes); ```; I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545
https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:1505,Usability,learn,learning,1505,",0], dtype='category'); sizes = pd.Series(data[:,1], dtype='category'). plt.scatter(data[:,0], data[:,1], c=colors, s=sizes); ```; I made a note of this https://github.com/matplotlib/matplotlib/issues/6214 . Thanks @ivirshup for pointing me to https://github.com/theislab/anndata/issues/35 and https://github.com/theislab/anndata/issues/31. I'm not convinced that positional vs label indexing is so complicated to understand that people will find scanpy difficult to use if you start adopting an `iloc` vs `loc` syntax. I agree that it makes the learning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tool",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545
https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545:1852,Usability,learn,learning,1852,"ning curve a little steeper, but it enables greater comparability with the ecosystem of data science tools in python. It looks like there are some strong opinions here though, and I don't want to start a flame war. Scanpy is an excellent piece of software, and I greatly appreciate at the work that goes into it. Responding to @LuckyMD, I again would just point out that returning cluster labels as ints is the standard for sklearn, and I would urge that scanpy serve as an access point to single cell analysis both for biologists and also for data science / machine learning researchers. Biologists will likely stick to using scanpy's plotting functions where you can handle default color maps for things that appear to be labels. We do this kind of checking in scprep: https://github.com/KrishnaswamyLab/scprep/blob/09de1bf41c4b42d331b29a4493c436110b641e07/scprep/plot/scatter.py#L206-L253. However, for machine learning researchers who likely have their own preferred plotting tools in matplotib or seaborn, might be trying to use the results from clustering in scanpy to compare to results from `sklearn.cluster`, or otherwise want to fit scanpy into their analysis pipelines, turning arrays of numerics into arrays of strings causes headaches that make the tools less accessible. The argument about the default colormap in matplotlib is continuous seems less important than making scanpy compatible with the larger ecosystem of data science tools in Python. Finally, I will note that in Python, strings are also defined ordinally, even if you might not think of them that way. Although in some respects the question, ""Is `'1'` less than `'a'`?"" is nonsensical, this is a well defined test in Python. ```python; In [1]: '1' < 'a'; Out[1]: True; ```. Again, I want to emphasize that I really love what has been done with scanpy / anndata so far. We use it in various places in our single cell workshop (https://krishnaswamylab.org/workshop), and I rely on the implementations of louvain / paga / d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582988545
https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:2638,Performance,perform,perform,2638,"earn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work?. ```python; import numpy as np; import pandas as pd; import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))); adata = sc.AnnData(data); np.sqrt(adata); ```; Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715
https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:412,Security,access,accessible,412,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to res",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715
https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:596,Security,access,accessible,596,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to res",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715
https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:3107,Security,access,access,3107,"earn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work?. ```python; import numpy as np; import pandas as pd; import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))); adata = sc.AnnData(data); np.sqrt(adata); ```; Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715
https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:340,Usability,learn,learning,340,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to res",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715
https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:392,Usability,learn,learning,392,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to res",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715
https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:828,Usability,learn,learned,828,"Great! Glad to have the discussion. I think there's a lot to talk about here, and it seems like a lot of it circles around how scanpy / anndata should interact with the greater ecosystem of tools for data analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to res",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715
https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:1205,Usability,intuit,intuitive,1205,"ta analysis in Python. . I think there are conventions in `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems that result in a steep learning curve. I am very supportive of making that learning curve more accessible. I think it's great to provide helper functions that ""just work."" I think of the the filtering, normalization, and plotting functions especially. I also am very in favor of accessible tutorials and documentation and workshops that make using these tools approachable for a lay audience that may not understand the distinctions between various APIs. I've relied heavily of these kinds of resources as I've learned how to program within this ecosystem, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the fol",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715
https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:1941,Usability,clear,clear,1941,"em, and I've seen how helpful they can be for new users. What I find less desirable here is introducing incompatibilities or breaking conventions used in the broader `numpy` / `pandas` / `sklearn` / `matplotlib` ecosystems to lower the barrier to entry for scanpy. I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use `scanpy` and `sklearn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work?. ```python; import numpy as np; import pandas as pd; import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))); adata = sc.AnnData(data); np.sqrt(adata); ```; Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715
https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922:62,Deployability,integrat,integration,62,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922
https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922:173,Deployability,integrat,integration,173,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922
https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922:62,Integrability,integrat,integration,62,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922
https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922:173,Integrability,integrat,integration,173,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922
https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922:866,Usability,clear,clear,866,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922
https://github.com/scverse/scanpy/issues/1030#issuecomment-584144674:875,Availability,error,error,875,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584144674
https://github.com/scverse/scanpy/issues/1030#issuecomment-584144674:790,Usability,simpl,simply,790,"I think `pandas` provides a good template for the question of `np.min(adata)`. `np.min(df)` gives the minimum value stored in the dataframe, not the minimum value in the `Index` (aka `obs`) or `Columns` (aka `var`). Given `AnnData` is basically a way of storing data and metadata associated with both the rows and columns of that data, it goes without saying (in my opinion at least) that numerical methods applied to `adata` should be applied to `adata.X`. Re: slicing, I think it makes sense to have explicit slicing for one or the other (i.e. `loc` and `iloc`) and then a default slicing (i.e. `adata['Cell A',:]`) which takes both position-based and name-based slicing if the two are unambiguous. It wouldn't be hard to include a check that says if the names are a) integers and b) not simply a RangeIndex (ie names and positions are the same) then throw a warning or an error asking the user to specify which of name or position they want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584144674
https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033:114,Deployability,integrat,integrate,114,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033
https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033:114,Integrability,integrat,integrate,114,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033
https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033:429,Usability,simpl,simply,429,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033
https://github.com/scverse/scanpy/issues/1030#issuecomment-584222835:236,Usability,simpl,simpler,236,"I still fail to see where it is harder to work with `AnnData` than with `pandas`. But maybe I'm the wrong person to comment on this, as I don't work as much matplotlib plotting (more seaborn and scanpy). Also, `pandas` is an inherently simpler structure than `AnnData`, so not really a competing project from my point of view. We have to worry about scaling in several dimensions, which is quite different than `pandas`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584222835
https://github.com/scverse/scanpy/issues/1030#issuecomment-584238178:2009,Energy Efficiency,efficient,efficient,2009,"anpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:; 1. Return cluster labels as `ints`; 2. Support non-string indexes (and adopt `loc` vs `iloc`); 3. Support `ufuncs` with `AnnData`; 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more convenient and efficient than `MultiIndex` for slicing, and `AnnData` has a handy `uns` slot for other miscellany that's just helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584238178
https://github.com/scverse/scanpy/issues/1030#issuecomment-584238178:788,Usability,simpl,simple,788,"So I think the issue here isn't that AnnData is _harder_ to work with than pandas, it's that there are several API choices in scanpy / AnnData introduce incompatibility with other tools in the ecosystem, which is generally undesirable. I can understand if you just look at `scanpy` and `AnnData` as standalone packages for single cell analysis in Python, then this doesn't seem like a big deal. However, I think these tools, especially `AnnData`, have the potential to serve the broader Python data analysis community. `scanpy` might be limited to people who are exclusively looking at single cell data, but `AnnData` definitely has utility outside of single cell (which I thought was why the documentation doesn't discuss scRNA-seq much). The good news is with most of these, relatively simple changes would make these tools all inter-compatible in ways that ""just work."" Among these changes are:; 1. Return cluster labels as `ints`; 2. Support non-string indexes (and adopt `loc` vs `iloc`); 3. Support `ufuncs` with `AnnData`; 4. (maybe) Return copies of input for most `scanpy` functions. Now I'm not saying there aren't reasons for keeping the conventions that have been selected, but it's definitely true that these conventions are different from the conventions in `numpy`, `pandas`, and `sklearn`. I think where Scott and I are coming from is the perspective that unless it would be unbearably difficult to keep to those conventions, it's generally better to stick to conventions used in the larger data analysis ecosystem. I'm not sure I agree that `pd.DataFrame` and and `AnnData` don't compete when it comes to people who are doing single cell analysis in Python. What do you mean by ""have to worry about scaling in several dimensions""? . I think sparse `DataFrame`s with a `MultiIndex` are similar to `AnnData` objects. It's just that `AnnData` objects have a more consistent API for supporting sparse data structures, having the `obs` and `var` annotations be `DataFrames` is more conveni",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584238178
https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:1715,Deployability,update,update,1715,"ork"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python; adata.apply_ufunc(np.log1p, in=""X"", out=""X""); adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")); ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python; clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""); clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""); ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629
https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:2115,Modifiability,layers,layers,2115,"ork"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python; adata.apply_ufunc(np.log1p, in=""X"", out=""X""); adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")); ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python; clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""); clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""); ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629
https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:2141,Modifiability,layers,layers,2141,"ork"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python; adata.apply_ufunc(np.log1p, in=""X"", out=""X""); adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")); ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python; clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""); clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""); ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629
https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:2549,Performance,perform,performance,2549,"ork"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python; adata.apply_ufunc(np.log1p, in=""X"", out=""X""); adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")); ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python; clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""); clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""); ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629
https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:2333,Safety,predict,predict,2333,"ork"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python; adata.apply_ufunc(np.log1p, in=""X"", out=""X""); adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")); ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python; clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""); clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""); ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629
https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:321,Usability,intuit,intuitive,321,"It's cool to see people put so much thought into the discussion here! I think a lot of the ideas here are things that have been tossed around in the past for anndata. There's a lot here, so I'm just going to respond to @dburkhardt's points for now. > I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use scanpy and sklearn and I want this to ""just work"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629
https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:744,Usability,learn,learn,744,"It's cool to see people put so much thought into the discussion here! I think a lot of the ideas here are things that have been tossed around in the past for anndata. There's a lot here, so I'm just going to respond to @dburkhardt's points for now. > I agree with you that using numerics to represent clusters is counter-intuitive when these integers actually represent discrete labels. However, I don't find this to be a compelling reason to break the convention used in the broader data analysis ecosystem. If I want to compare louvain to spectral clustering in Python, I need to use scanpy and sklearn and I want this to ""just work"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629
https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458:311,Availability,error,errors,311,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)); adata = sc.AnnData(data). # All of the following raise errors; np.sqrt(adata); adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata); ```; To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. ; **2. Requirement to use .var_vector or .obs_vector for single columns**; ```python; # This works as expected; adata[:, adata.var_names[0:3]]. # I wish this did as well.; adata[:, adata.var_names[0]]; ```; **3. .var_vector doesn't return a Series**. ```python; pdata = pd.DataFrame(data); # Returns series; pdata[0]. # Returns ndarray; adata.var_vector[0]; ```. **4. Clusters as categories creates confusing scatterplots**; ```python; sc.pp.neighbors(adata); sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]); ```; Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458
https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458:736,Availability,error,errors,736,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)); adata = sc.AnnData(data). # All of the following raise errors; np.sqrt(adata); adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata); ```; To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. ; **2. Requirement to use .var_vector or .obs_vector for single columns**; ```python; # This works as expected; adata[:, adata.var_names[0:3]]. # I wish this did as well.; adata[:, adata.var_names[0]]; ```; **3. .var_vector doesn't return a Series**. ```python; pdata = pd.DataFrame(data); # Returns series; pdata[0]. # Returns ndarray; adata.var_vector[0]; ```. **4. Clusters as categories creates confusing scatterplots**; ```python; sc.pp.neighbors(adata); sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]); ```; Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458
https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458:2303,Availability,error,error,2303," we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. ; **2. Requirement to use .var_vector or .obs_vector for single columns**; ```python; # This works as expected; adata[:, adata.var_names[0:3]]. # I wish this did as well.; adata[:, adata.var_names[0]]; ```; **3. .var_vector doesn't return a Series**. ```python; pdata = pd.DataFrame(data); # Returns series; pdata[0]. # Returns ndarray; adata.var_vector[0]; ```. **4. Clusters as categories creates confusing scatterplots**; ```python; sc.pp.neighbors(adata); sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]); ```; Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com/8322751/78272834-8bd8a380-74fd-11ea-9034-6c8d0aefebe8.png"">. **5. Cannot pass clusters to `c` parameter in plt.scatter**; I would like this to just work. Instead it throws a huge error.; ```python; plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['leiden']); ```. **6. Clusters as categories frustrate subclustering**; I understand this is a niche application, but like 4 and 5, this would be fixed by matching the output of sklearn.cluster operators.; ```python; sc.pp.neighbors(adata); sc.tl.leiden(adata). cluster_zero = adata[adata.obs['leiden'] == '0']; sub_clusters = cluster.KMeans(n_clusters=2).fit_predict(adata.X). # Here I'm trying to break up cluster '0' into subclusters with ; # new names that don't clash with the existing clusters; # However, np.max() and the + operators aren't well defined for ; # cateogricals of strings; sub_clusters = sub_clusters + np.max(adata.obs['leiden']); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458
https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458:1051,Deployability,update,update,1051,"ecause I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)); adata = sc.AnnData(data). # All of the following raise errors; np.sqrt(adata); adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata); ```; To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. ; **2. Requirement to use .var_vector or .obs_vector for single columns**; ```python; # This works as expected; adata[:, adata.var_names[0:3]]. # I wish this did as well.; adata[:, adata.var_names[0]]; ```; **3. .var_vector doesn't return a Series**. ```python; pdata = pd.DataFrame(data); # Returns series; pdata[0]. # Returns ndarray; adata.var_vector[0]; ```. **4. Clusters as categories creates confusing scatterplots**; ```python; sc.pp.neighbors(adata); sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]); ```; Produces the following plot. I would like it to have order 0-5 by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458
https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458:1124,Performance,perform,performance,1124,"e at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)); adata = sc.AnnData(data). # All of the following raise errors; np.sqrt(adata); adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata); ```; To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. ; **2. Requirement to use .var_vector or .obs_vector for single columns**; ```python; # This works as expected; adata[:, adata.var_names[0:3]]. # I wish this did as well.; adata[:, adata.var_names[0]]; ```; **3. .var_vector doesn't return a Series**. ```python; pdata = pd.DataFrame(data); # Returns series; pdata[0]. # Returns ndarray; adata.var_vector[0]; ```. **4. Clusters as categories creates confusing scatterplots**; ```python; sc.pp.neighbors(adata); sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]); ```; Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458
https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458:64,Testability,log,logging,64,"Hi, I just wanted to bring this back up again because I've been logging some of the issue's I've encountered. It seems we're at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)); adata = sc.AnnData(data). # All of the following raise errors; np.sqrt(adata); adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata); ```; To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. ; **2. Requirement to use .var_vector or .obs_vector for single columns**; ```python; # This works as expected; adata[:, adata.var_names[0:3]]. # I wish this did as well.; adata[:, adata.var_names[0]]; ```; **3. .var_vector doesn't return a Series**. ```python; pdata = pd.DataFrame(data); # Returns series; pdata[0]. # Returns ndarray; adata.var_vector[0]; ```. **4. Clusters as categories creates confusing scatterplots**; ```python; sc.pp.neighbors(adata); sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]); ```; Produces the following plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458
https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:690,Availability,error,error,690,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python; adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]; ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python; adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X; ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease?. If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245
https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:1117,Deployability,update,update,1117," implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python; adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]; ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python; adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X; ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease?. If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245
https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:1259,Deployability,update,update,1259,"an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python; adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]; ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python; adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X; ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease?. If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245
https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:3448,Deployability,continuous,continuous,3448," remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`?. ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these?. ```python; sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns; sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]); ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do you know where this was @LuckyMD?. Basically, I'd say do something more like:. ```python; from collections.abc import Iterable; import numpy as np; import pandas as pd; from sklearn import cluster. def kmeans_subcluster(adata, orig_key, orig_clusters, key_added):; if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):; orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]; sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering; ; new_clustering = adata.obs[orig_key].copy(); # Make n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245
https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:354,Integrability,wrap,wrapping,354,"### 1. Passing anndata objects to numpy and sklearn operators. I think this would be great! This would be easy to implement if python had generic functions. This is kinda something that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python; adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]; ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python; adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X; ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease?. If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a differ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245
https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:1190,Performance,perform,performance,1190,"g that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python; adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]; ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python; adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X; ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease?. If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245
https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:5575,Testability,assert,assert,5575,"or not isinstance(orig_clusters, Iterable):; orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]; sub_clustering = cluster.KMeans(n_clusters=2).fit_predict(subset.X). # Make new clustering; ; new_clustering = adata.obs[orig_key].copy(); # Make new names; new_clusters = "","".join(orig_clusters) + ""-"" + pd.Series(np.unique(sub_clustering), dtype=str); new_clustering.cat.add_categories(new_clusters, inplace=True); new_clustering[subset.obs_names] = new_clusters[sub_clustering]. # Add back to adata; adata.obs[key_added] = new_clustering; ```. <details>; <summary> Or if you wanted something more generic: </summary>. ```python; from typing import Callable, Collection; from anndata import AnnData. def subcluster(; cluster_func: Callable[[AnnData], Collection],; adata,; orig_key,; orig_clusters,; key_added,; ):; """"""; Params; ------; cluster_func; Function that produces a clustering of observations from an anndata object.; adata; orig_key; Key in adata.obs for original clustering; orig_clusters; Set of clusters to subset to before clustering.; key_added; Key in obs to add resulting clustering to.; """"""; if isinstance(orig_clusters, str) or not isinstance(orig_clusters, Iterable):; orig_clusters = [orig_clusters]. subset = adata[adata.obs[orig_key].isin(orig_clusters)]; sub_clustering = (; pd.Categorical(cluster_func(subset)); .map(lambda x: "","".join(orig_clusters) + ""-"" + str(x)); ). assert not (adata.obs[orig_key].isin(sub_clustering.categories)).any(). # Create new cluster assignment; new_clustering = (; adata.obs[orig_key].astype(""category"", copy=True); .cat.add_categories(sub_clustering.categories); ); new_clustering[subset.obs_names] = sub_clustering.astype(str). # Add back to adata; adata.obs[key_added] = new_clustering; ; from functools import partial. subcluster_kmeans = partial(; subcluster,; lambda x: cluster.KMeans(n_clusters=2).fit_predict(x.X); ); ```. </details>. This will still work if you've already assigned some labels too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245
https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:2912,Usability,intuit,intuitive,2912,"t: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/anndata/issues/145. Another issue is that `scipy.sparse` has no such thing as a 1-dimensional sparse array. This is a long standing problem, which I'll write a bit more about in the context of xarray. ### 3. .var_vector doesn't return a Series. I remeber thinking about this as a possibility. IIRC I decided against this because I just as frequently wanted some other column, like `""gene_symbols""` as the index. I could see adding this as an option via a keyword argument now. But maybe you just want to use `sc.get.obs_df`?. ### 4. Clusters as categories creates confusing scatterplots. Well, there is no order to the categories, so I guess I see why `matplotlib` wouldn't plot those in sorted order, but agree it's a little counter intuitive. Seems like more of a matplotlib issue to me though. ### 5. Cannot pass clusters to c parameter in plt.scatter. Use one of these?. ```python; sc.pl.scatter(pbmc, x=pbmc.var_names[0], y=pbmc.var_names[1], color=""leiden"") . import seaborn as sns; sns.scatterplot(pbmc.X[:, 0], pbmc.X[:, 0], hue=pbmc.obs[""leiden""]); ```. Categorical values for scatter plots are a known issue for matplotlib, as I linked to above. Their current behaviour if you pass a numeric valued categorical (regardless of whether it's ordered) is to use a continuous color palette, which in my opinion is easily misleading. ### 6. Clusters as categories frustrate subclustering. We've used a different convention for subclustering, which is actually the reason we use strings. We're assuming you're breaking a cluster or set of clusters into smaller ones, so the new id is appended to the old one. I believe there's a tutorial with this somewhere. Do y",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245
https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154:1828,Availability,error,errors,1828,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do.; * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154
https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154:1922,Deployability,integrat,integrating,1922,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do.; * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154
https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154:640,Energy Efficiency,Efficient,Efficient,640,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do.; * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154
https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154:840,Integrability,interface,interface,840,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do.; * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154
https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154:1922,Integrability,integrat,integrating,1922,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do.; * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154
https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154:1330,Usability,usab,usable,1330,"## Xarray and anndata. Theoretically, `AnnData` objects are kind of like a special case of `xarray.Dataset`s. While `AnnData` objects have an `obs` and a `var` dimension `xarray.Dataset` can have any number of dimensions. `AnnData` objects are just specializing to the the 2d case. I think it would make a lot of sense to eventually have `anndata` and `scanpy` be based on `xarray`, or something like it. In practice there are a number of difficulties here. The biggest one is support for sparse data, and I'll briefly point out a couple others. ### Sparse arrays. I could probably rant about this for a while, since it's always a problem. Efficient processing of scRNA-seq data needs sparse matrices. The only fully featured sparse array library in python right now is `scipy.sparse`. All of it's sparse arrays only follow the `np.matrix` interface, which is deprecated. This means that they only kind-of work like arrays, and need to be special cased pretty frequently. `xarray` seems to work pretty well with a number of different array types, as long as they act like `np.ndarray`s. They have explicit support for `pydata/sparse`, but that library isn't well supported by the rest of the ecosystem, probably because it doesn't have CSR or CSC matrices yet. This leaves `xarray` with a level of sparse array support that isn't usable for us. ### Pairwise arrays and other weird behaviour. * Having an array where multiple axes have the same name doesn't work well with `xarray`. This is a problem if you're frequently using adjacency matrices like we do.; * `xarray.DataArray`s do not necessarily have the same behaviour as numpy arrays. For example, they have specific behaviour for matrix multiplication. Any transition would be much easier if `DataArrays` could be used as drop-in replacements for numpy arrays (plus some errors for misaligned data). We need to map this out more before we could make any attempt at integrating the libraries.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608238154
https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004:495,Availability,error,error,495,"Thanks for the long response @ivirshup!. For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004
https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004:1714,Availability,down,down,1714,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004
https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004:582,Security,access,access,582,"Thanks for the long response @ivirshup!. For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004
https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004:696,Security,access,access,696,"Thanks for the long response @ivirshup!. For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004
https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004:2350,Security,access,accessible,2350,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004
https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004:934,Usability,learn,learning,934,"Thanks for the long response @ivirshup!. For 1. I think a ufunc should always act on adata.X and I want it to return the adata object with the sqrt applied to adata.X. Adding support for the sklearn operators would be great. For the second part, my intention is for the result to be `adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X`, and it's fine with me in the varnames are lost so long as the obsnames are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004
https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004:2001,Usability,feedback,feedback,2001,"are kept. If they're not the same shape, then I would expect the same error as pandas throws. For 2. I think its okay if you return a dense 1-d array when I access a single column vector. I don't understand where the confusion is coming in with adata.X changing when you access a single column, but that's not been an issue for me. For the rest, I hope you can survey the community to figure out how rare my use-cases are. I would like scanpy / anndata to fit into my existing workflow that I picked up while learning matplotlib / pandas / numpy. I want slicing an AnnData to behave like slicing a DataFrame; I want clusters to be ints; I want to apply a transformation to a data-container and get the whole container returned with the transformation applied to the values. . I can come up with workarounds for all of the choices you've made here. That's not the issue. I raised this comment because these workarounds add overhead to getting my work done. I'm not going to change my work flow to match your design choices where they diverge from the apis for sklearn / numpy / pandas etc. I know I'm not the only one with these wants (e.g. @scottgigante has similar frustrations), but I don't know how prevalent these frustrations are. I think at the end of the day, my concern here boils down to what infrastructure you put in place to make sure the needs of the community are balanced with the intentions of the developers. I think the efforts be cellxgene are a great model for this, and I would happily get involved with figuring out the best way to incorporate community feedback into the development of scanpy / anndata. All this said, your tools do provide a bunch of amazing functionality that I rely on for my PhD. I really appreciate all the effort you've put in. I especially love how easy it is to run louvain / leiden, and how supportive you've been to people adding external tools to scanpy so they can be made accessible to the broader community of single cell users in Python. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-609066004
https://github.com/scverse/scanpy/issues/1032#issuecomment-616240702:547,Availability,down,downstream,547,"> Maybe a solution would be to set `highly_variable` equal to `highly_variable_intersection` when using the `batch_key`. I think `highly_variable` is a remnant of using `highly_variable_genes_single_batch()` (or whatever the function is called) to get the individual per-batch HVGs for intersection calculation. @gokceneraslan will be able to correct me here though. Encountered this exact issue today. In my example, `highly_variable_intersection` only contains 17 genes across 30 datasets, which I imagine might silently give unexpected results downstream. In addition to that option, another option might be to allow the user to define a minimum number of `highly_variable_nbatches` so `highly_variable` is derived from `highly_variable_nbatches > NUMBER`. This is an approach used here FWIW: https://nbisweden.github.io/workshop-scRNAseq/labs/compiled/scanpy/scanpy_03_integration.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616240702
https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607:1366,Availability,error,error,1366,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata.var.highly_variable = adata.var.highly_variable_intersection; sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607
https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607:2000,Availability,error,error,2000,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata.var.highly_variable = adata.var.highly_variable_intersection; sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607
https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607:2124,Availability,error,error,2124,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata.var.highly_variable = adata.var.highly_variable_intersection; sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607
https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607:954,Integrability,depend,depends,954,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata.var.highly_variable = adata.var.highly_variable_intersection; sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607
https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607:552,Modifiability,variab,variable,552,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata.var.highly_variable = adata.var.highly_variable_intersection; sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607
https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607:637,Modifiability,variab,variable,637,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata.var.highly_variable = adata.var.highly_variable_intersection; sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607
https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607:1452,Modifiability,variab,variable,1452,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata.var.highly_variable = adata.var.highly_variable_intersection; sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607
https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607:1589,Modifiability,variab,variable,1589,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata.var.highly_variable = adata.var.highly_variable_intersection; sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607
https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607:625,Safety,detect,detected,625,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata.var.highly_variable = adata.var.highly_variable_intersection; sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607
https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607:922,Safety,avoid,avoid,922,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata.var.highly_variable = adata.var.highly_variable_intersection; sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607
https://github.com/scverse/scanpy/issues/1032#issuecomment-616820714:288,Availability,avail,available,288,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616820714
https://github.com/scverse/scanpy/issues/1032#issuecomment-616820714:237,Deployability,integrat,integration,237,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616820714
https://github.com/scverse/scanpy/issues/1032#issuecomment-616820714:237,Integrability,integrat,integration,237,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616820714
https://github.com/scverse/scanpy/issues/1032#issuecomment-616820714:249,Testability,benchmark,benchmarking,249,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616820714
https://github.com/scverse/scanpy/issues/1032#issuecomment-616845594:33,Usability,clear,clear,33,"Oh interesting, I thought it was clear :) I mean you even contributed to the function, no? . I think we also discussed why not to use intersection by default in the PR: https://github.com/theislab/scanpy/pull/614#issuecomment-485875031 . If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Edit: adata.var[""highly_variable_intersection""] wasn't even implemented in the beginning of the PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616845594
https://github.com/scverse/scanpy/issues/1032#issuecomment-616960210:182,Testability,assert,assert,182,"@gokceneraslan here's a quick example:. ```python; import scanpy as sc; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); sc.pp.highly_variable_genes(pbmc, batch_key=""louvain""). assert not pbmc.var[""highly_variable""].any(); ```. Alternatively:. ```python; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""batch""] = ""a""; sc.pp.highly_variable_genes(pbmc, batch_key=""batch""); assert not pbmc.var[""highly_variable""].any(). pbmc.obs[""batch""] = ""a""; pbmc.obs[""batch""][::2] = ""b""; sc.pp.highly_variable_genes(pbmc, batch_key=""batch""); assert not pbmc.var[""highly_variable""].any(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616960210
https://github.com/scverse/scanpy/issues/1032#issuecomment-616960210:393,Testability,assert,assert,393,"@gokceneraslan here's a quick example:. ```python; import scanpy as sc; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); sc.pp.highly_variable_genes(pbmc, batch_key=""louvain""). assert not pbmc.var[""highly_variable""].any(); ```. Alternatively:. ```python; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""batch""] = ""a""; sc.pp.highly_variable_genes(pbmc, batch_key=""batch""); assert not pbmc.var[""highly_variable""].any(). pbmc.obs[""batch""] = ""a""; pbmc.obs[""batch""][::2] = ""b""; sc.pp.highly_variable_genes(pbmc, batch_key=""batch""); assert not pbmc.var[""highly_variable""].any(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616960210
https://github.com/scverse/scanpy/issues/1032#issuecomment-616960210:548,Testability,assert,assert,548,"@gokceneraslan here's a quick example:. ```python; import scanpy as sc; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); sc.pp.highly_variable_genes(pbmc, batch_key=""louvain""). assert not pbmc.var[""highly_variable""].any(); ```. Alternatively:. ```python; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""batch""] = ""a""; sc.pp.highly_variable_genes(pbmc, batch_key=""batch""); assert not pbmc.var[""highly_variable""].any(). pbmc.obs[""batch""] = ""a""; pbmc.obs[""batch""][::2] = ""b""; sc.pp.highly_variable_genes(pbmc, batch_key=""batch""); assert not pbmc.var[""highly_variable""].any(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616960210
https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764:885,Deployability,integrat,integration,885,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no?; > ; > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031); > ; > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764
https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764:885,Integrability,integrat,integration,885,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no?; > ; > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031); > ; > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764
https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764:867,Testability,benchmark,benchmarking,867,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no?; > ; > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031); > ; > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764
https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764:35,Usability,clear,clear,35,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no?; > ; > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031); > ; > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764
https://github.com/scverse/scanpy/issues/1032#issuecomment-617446080:268,Testability,test,tested,268,"> @gokceneraslan here's a quick example:. Oh man, just noticed a horrible bug which leads to zero HVGs if batch_key is given but n_top_genes is not 😓 Somehow, highly_variable_genes with batch_key but without n_top_genes (which is the option I always use :) ) is never tested :/ Fixing now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-617446080
https://github.com/scverse/scanpy/issues/1033#issuecomment-589868726:69,Modifiability,extend,extended,69,"In lines 345 - 348, a list of dtypes was getting appended instead of extended. Fixed in PR #1070 ; ```; dtypes.append([; ('highly_variable_nbatches', int),; ('highly_variable_intersection', np.bool_),; ]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1033#issuecomment-589868726
https://github.com/scverse/scanpy/issues/1033#issuecomment-616732003:19,Testability,test,test,19,"Yeah, if you add a test, something very simple like `sc.pp.highly_variable_genes(pbmc, batch_key='louvain', inplace=False)` we can merge the PR @atarashansky. > Separately, could we return a dataframe here?. Sure, I can do after @atarashansky's PR is merged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1033#issuecomment-616732003
https://github.com/scverse/scanpy/issues/1033#issuecomment-616732003:40,Usability,simpl,simple,40,"Yeah, if you add a test, something very simple like `sc.pp.highly_variable_genes(pbmc, batch_key='louvain', inplace=False)` we can merge the PR @atarashansky. > Separately, could we return a dataframe here?. Sure, I can do after @atarashansky's PR is merged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1033#issuecomment-616732003
https://github.com/scverse/scanpy/issues/1035#issuecomment-583749392:184,Testability,test,testing,184,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```; WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]; sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'); sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'); ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data.; In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035#issuecomment-583749392
https://github.com/scverse/scanpy/issues/1035#issuecomment-583749392:392,Testability,test,test,392,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```; WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]; sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'); sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'); ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data.; In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035#issuecomment-583749392
https://github.com/scverse/scanpy/issues/1035#issuecomment-583749392:1020,Testability,test,tests,1020,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```; WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]; sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'); sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'); ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data.; In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035#issuecomment-583749392
https://github.com/scverse/scanpy/issues/1035#issuecomment-583749392:1063,Testability,test,test,1063,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```; WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]; sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'); sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'); ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data.; In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035#issuecomment-583749392
https://github.com/scverse/scanpy/issues/1035#issuecomment-583749392:1227,Testability,test,test,1227,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```; WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]; sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'); sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'); ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data.; In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035#issuecomment-583749392
https://github.com/scverse/scanpy/issues/1035#issuecomment-583749392:1235,Testability,log,log-normalized,1235,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```; WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]; sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'); sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'); ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data.; In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035#issuecomment-583749392
https://github.com/scverse/scanpy/issues/1035#issuecomment-583749392:1349,Testability,test,test,1349,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```; WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]; sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'); sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'); ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data.; In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035#issuecomment-583749392
https://github.com/scverse/scanpy/issues/1035#issuecomment-583749392:1414,Testability,log,log-normalization,1414,"Hi,. This can be done by subsetting your dataset to the cluster you're interested in and then using the `.obs` column where you store your condition information to do the differential testing. Something like this might work (note i didn't check for typos):. ```; WT_Donuts_clust1 = WT_Donuts[WT_Donuts.obs.leiden.isin(['1'])]; sc.tl.rank_genes_groups(WT_Donuts_clust1, 'condition', method='t-test', groups=['mut'], reference='ctrl', key_added='mut_up_ctrl_down'); sc.pl.rank_genes_groups(WT_Donuts_clust1, n_genes=5, key='mut_up_ctrl_down'); ```. This code makes the assumption that you have `adata.obs['condition']` which stores the categories `'mut'` and `'ctrl'`, and that you are interested in adata.obs['leiden'] == '1'`. Change these values to match your data.; In the above code you will get the top 5 genes that are up-regulated in `'mut'` compared to `'ctrl'`. If you want the up-regulated genes in `'ctrl'` compared to `'mut'`, just switch around the keywords above. There are of course better, more sensitive tests for differential expression than a t-test, but if you just want a quick top 5... then this should be fine. Also note that your use of `use_raw=False` may be dangerous. You should probably be doing a t-test on log-normalized data (not batch corrected, or scaled data, or data where any covariates were regressed out). The t-test assumes normally distributed data, which is approximated by log-normalization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035#issuecomment-583749392
https://github.com/scverse/scanpy/issues/1035#issuecomment-584256959:218,Availability,error,errors,218,"Thank you so much for your explanation, I've tried your code, it's not working yet, I think I should go through my previous code, I'm sure something is wrong. But I understand what you said, I'll try to figure out the errors. ; Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035#issuecomment-584256959
https://github.com/scverse/scanpy/issues/1035#issuecomment-584271804:259,Testability,test,test,259,"No problem. . To give you a quick example with some of the inbuilt datasets:. ```; import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); adata_sub = adata[adata.obs.bulk_labels.isin(['Dendritic'])]; sc.tl.rank_genes_groups(adata_sub, 'phase', method='t-test', groups=['G1'], reference='S', key_added='g1_upreg') ; sc.pl.rank_genes_groups(adata_sub, key='g1_upreg'); ```. This version actually works and was tested... just to rule out issues with the code I put above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035#issuecomment-584271804
https://github.com/scverse/scanpy/issues/1035#issuecomment-584271804:413,Testability,test,tested,413,"No problem. . To give you a quick example with some of the inbuilt datasets:. ```; import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); adata_sub = adata[adata.obs.bulk_labels.isin(['Dendritic'])]; sc.tl.rank_genes_groups(adata_sub, 'phase', method='t-test', groups=['G1'], reference='S', key_added='g1_upreg') ; sc.pl.rank_genes_groups(adata_sub, key='g1_upreg'); ```. This version actually works and was tested... just to rule out issues with the code I put above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035#issuecomment-584271804
https://github.com/scverse/scanpy/issues/1035#issuecomment-585508877:93,Testability,test,tested,93,"Hi,; Thank you so much! It worked this time! I got a lot ""Keyerror"" before, but I tried your tested code, it worked perfectly! ; I really appreciate it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1035#issuecomment-585508877
https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:594,Availability,FAILURE,FAILURES,594,"@Koncopd Currently breaking test for me:. ```pytb; $ pytest -k test_ingest; ===================================================== test session starts =====================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073
https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1107,Energy Efficiency,reduce,reducer,1107,"========================= test session starts =====================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 an",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073
https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1168,Energy Efficiency,reduce,reducer,1168,"=================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073
https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1205,Energy Efficiency,reduce,reducer,1205,"=========; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073
https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:361,Modifiability,plugin,plugins,361,"@Koncopd Currently breaking test for me:. ```pytb; $ pytest -k test_ingest; ===================================================== test session starts =====================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073
https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:28,Testability,test,test,28,"@Koncopd Currently breaking test for me:. ```pytb; $ pytest -k test_ingest; ===================================================== test session starts =====================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073
https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:130,Testability,test,test,130,"@Koncopd Currently breaking test for me:. ```pytb; $ pytest -k test_ingest; ===================================================== test session starts =====================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073
https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:335,Testability,test,testpaths,335,"@Koncopd Currently breaking test for me:. ```pytb; $ pytest -k test_ingest; ===================================================== test session starts =====================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073
https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:353,Testability,test,tests,353,"@Koncopd Currently breaking test for me:. ```pytb; $ pytest -k test_ingest; ===================================================== test session starts =====================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073
https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:501,Testability,test,tests,501,"@Koncopd Currently breaking test for me:. ```pytb; $ pytest -k test_ingest; ===================================================== test session starts =====================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073
https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1231,Testability,assert,assert,1231,"hon 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 st",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073
https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1294,Testability,assert,assert,1294," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073
https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1577,Testability,test,tests,1577," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073
https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:1603,Testability,Assert,AssertionError,1603," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073
https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:2044,Testability,log,logging,2044," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073
https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:2205,Usability,learn,learn,2205," /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ```. With these versions:. ```python; >>> sc.logging.print_versions() ; scanpy==1.4.5.2.dev37+g51dc038 anndata==0.7.2.dev13+g4440b90.d20200316 umap==0.4.0rc1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073
https://github.com/scverse/scanpy/issues/1036#issuecomment-627825582:27,Availability,error,error,27,I stumbled across the same error with scanpy==1.4.5.1 anndata==0.7.1 umap==0.4.2. Did not quite understand the solution for this issue. What should I do?. Best wishes,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-627825582
https://github.com/scverse/scanpy/issues/1036#issuecomment-627837413:48,Availability,down,downgrading,48,"@nahanoo ; Hi, there are 3 options for now:. 1. downgrading umap to 0.39; 2. installing scanpy from github; 3. waiting for a new release of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-627837413
https://github.com/scverse/scanpy/issues/1036#issuecomment-627837413:77,Deployability,install,installing,77,"@nahanoo ; Hi, there are 3 options for now:. 1. downgrading umap to 0.39; 2. installing scanpy from github; 3. waiting for a new release of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-627837413
https://github.com/scverse/scanpy/issues/1036#issuecomment-627837413:129,Deployability,release,release,129,"@nahanoo ; Hi, there are 3 options for now:. 1. downgrading umap to 0.39; 2. installing scanpy from github; 3. waiting for a new release of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-627837413
https://github.com/scverse/scanpy/pull/1038#issuecomment-584787583:138,Availability,robust,robust,138,"Nice! But may I ask why you’re still importing everything from umap instead of from pynndescent?. I’d assume if we’d do that we’d be more robust to further umap updates, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038#issuecomment-584787583
https://github.com/scverse/scanpy/pull/1038#issuecomment-584787583:161,Deployability,update,updates,161,"Nice! But may I ask why you’re still importing everything from umap instead of from pynndescent?. I’d assume if we’d do that we’d be more robust to further umap updates, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038#issuecomment-584787583
https://github.com/scverse/scanpy/pull/1038#issuecomment-585107629:118,Integrability,depend,dependency,118,"Hm, it is still required to follow the internals of umap to reconstruct UMAP object from anndata info. Also this adds dependency on pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038#issuecomment-585107629
https://github.com/scverse/scanpy/issues/1039#issuecomment-617838606:145,Availability,error,error,145,@pchiang5 @LuckyMD what does it mean when a gene is present in adata.raw.var_names but not in adata.var_names ? I'm running into a key not found error because of the gene being in one list and not the other. Thanks.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039#issuecomment-617838606
https://github.com/scverse/scanpy/issues/1039#issuecomment-617882284:250,Availability,mask,mask,250,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039#issuecomment-617882284
https://github.com/scverse/scanpy/issues/1039#issuecomment-617882284:14,Integrability,depend,depends,14,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039#issuecomment-617882284
https://github.com/scverse/scanpy/issues/1039#issuecomment-617882284:355,Testability,log,log-normalized,355,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039#issuecomment-617882284
https://github.com/scverse/scanpy/issues/1043#issuecomment-586153305:159,Availability,avail,available,159,"Thanks both, just to clarify, I am using the `min_in_group_fraction` and `max_out_group_fraction` args for `sc.tl.rank_genes_groups_filtered()`, which are not available as custom cutoff args for `sc.queries.enrich()` (otherwise I agree entirely)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1043#issuecomment-586153305
https://github.com/scverse/scanpy/pull/1047#issuecomment-586274304:144,Testability,test,tests,144,"Uh, no idea why this fails. We could ask Fabian for some AWS money and add this to `.travis.yml`:. ```yaml; addons:; artifacts:; paths:; - $(ls tests/figures/*-failed-diff.png | tr ""\n"" "":""); ```. [(docs)](https://docs.travis-ci.com/user/uploading-artifacts/)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1047#issuecomment-586274304
https://github.com/scverse/scanpy/pull/1047#issuecomment-777239623:68,Usability,simpl,simple,68,"@flying-sheep, does this still need to get merged?. It looks like a simple rebase, and the CI plot comparison feature is now active on azure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1047#issuecomment-777239623
https://github.com/scverse/scanpy/pull/1048#issuecomment-586269616:161,Deployability,release,release,161,"Ok, I found a workaround by subsetting the dataset to 100 obs and 100 vars and writing it back to file with this R package 😅 ; https://bioconductor.org/packages/release/bioc/html/DropletUtils.html . . This dataset now works for both `read_visium` and `pl.spatial` tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1048#issuecomment-586269616
https://github.com/scverse/scanpy/pull/1048#issuecomment-586269616:264,Testability,test,tests,264,"Ok, I found a workaround by subsetting the dataset to 100 obs and 100 vars and writing it back to file with this R package 😅 ; https://bioconductor.org/packages/release/bioc/html/DropletUtils.html . . This dataset now works for both `read_visium` and `pl.spatial` tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1048#issuecomment-586269616
https://github.com/scverse/scanpy/issues/1051#issuecomment-586474464:163,Availability,error,error,163,"That's not a bug, that's a feature ;). You can only compute as many PCs as the minimum number of dimensions in n_samples and n_features. Do you feel as though the error message is unclear on this? I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051#issuecomment-586474464
https://github.com/scverse/scanpy/issues/1051#issuecomment-586474464:169,Integrability,message,message,169,"That's not a bug, that's a feature ;). You can only compute as many PCs as the minimum number of dimensions in n_samples and n_features. Do you feel as though the error message is unclear on this? I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051#issuecomment-586474464
https://github.com/scverse/scanpy/issues/1051#issuecomment-586479275:73,Availability,error,error,73,"Three things:. 1. If that is a feature, then this is a bug (runs without error):. ```python; import numpy as np; import scanpy as sc; import anndata. adata = anndata.AnnData(np.random.normal(0, 1, (40, 10))); sc.pp.pca(adata); ```. 2. Defaults should work without tuning.; 3. > I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then. Given that I'm running `sc.pp.pca` without setting `n_comps`, I contend that the average user does not remember what the default value is. It would make more sense in both cases (`n_features <= n_comps` and `n_samples <= n_comps`) to throw a warning and set n_comps to the maximum allowable value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051#issuecomment-586479275
https://github.com/scverse/scanpy/issues/1051#issuecomment-586494795:49,Availability,error,error,49,"Hmm... you are right, that should also create an error by the above logic. It does look like this check is done for the case that `adata.n_vars < n_comps` here:; https://github.com/theislab/scanpy/blob/be1a0555252cfd97b9d00f51dc5fbab462588da0/scanpy/preprocessing/_simple.py#L472-L477. I'm not sure why that wasn't also done for `n_obs`. @Koncopd you made this fix at the time... any reason for not also checking `adata.n_obs` in the same way? Could quickly add a check for `adata.n_obs` unless there is a reason not to @ivirshup, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051#issuecomment-586494795
https://github.com/scverse/scanpy/issues/1051#issuecomment-586494795:68,Testability,log,logic,68,"Hmm... you are right, that should also create an error by the above logic. It does look like this check is done for the case that `adata.n_vars < n_comps` here:; https://github.com/theislab/scanpy/blob/be1a0555252cfd97b9d00f51dc5fbab462588da0/scanpy/preprocessing/_simple.py#L472-L477. I'm not sure why that wasn't also done for `n_obs`. @Koncopd you made this fix at the time... any reason for not also checking `adata.n_obs` in the same way? Could quickly add a check for `adata.n_obs` unless there is a reason not to @ivirshup, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051#issuecomment-586494795
https://github.com/scverse/scanpy/issues/1051#issuecomment-586662503:252,Availability,error,error,252,"I'm thinking `n_comps` should default to `None`, and we'd define behaviour like:. ```python; if n_comps is None:; min_dim = min(adata.n_vars, adata.n_obs); if 50 >= min_dim:; n_comps = min_dim - 1; else:; n_comps = 50; ```. and we let sklearn throw an error if the user specified a number of components that doesn't work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051#issuecomment-586662503
https://github.com/scverse/scanpy/issues/1053#issuecomment-586667992:15,Deployability,update,update,15,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg?; * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586667992
https://github.com/scverse/scanpy/issues/1053#issuecomment-586667992:377,Performance,perform,performance,377,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg?; * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586667992
https://github.com/scverse/scanpy/issues/1053#issuecomment-586676271:265,Performance,perform,performant,265,"The Leiden algorithm from `igraph` only implements two quality functions: CPM and modularity. Both indeed use resolution parameters, and are the equivalent of `CPMVertexPartition` and `RBConfigurationVertexPartition`. The reason the `igraph` implementation is more performant than `leidenalg` is exactly because it does not provide other quality functions or a multiplex approach. If you need the other quality functions (such as significance or surprise), you will need to stick to `leidenalg`. So if you need those other quality functions, or a multiplex approach, you might still want to stick to the `leidenalg` package after all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586676271
https://github.com/scverse/scanpy/issues/1053#issuecomment-586696126:183,Performance,perform,performance,183,"If we can cleanly switch to the igraph implementation for modularity with weights, it could make sense for that to be the default. Any chance you could point me to some benchmarks on performance? An initial test looks very impressive!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586696126
https://github.com/scverse/scanpy/issues/1053#issuecomment-586696126:169,Testability,benchmark,benchmarks,169,"If we can cleanly switch to the igraph implementation for modularity with weights, it could make sense for that to be the default. Any chance you could point me to some benchmarks on performance? An initial test looks very impressive!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586696126
https://github.com/scverse/scanpy/issues/1053#issuecomment-586696126:207,Testability,test,test,207,"If we can cleanly switch to the igraph implementation for modularity with weights, it could make sense for that to be the default. Any chance you could point me to some benchmarks on performance? An initial test looks very impressive!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586696126
https://github.com/scverse/scanpy/issues/1053#issuecomment-586969791:10,Performance,perform,performed,10,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:; ```; Running Leiden 0.7.0.post1+71.g14ba1e4.dirty; Running igraph 0.8.0; Read graph (n=63731,m=817035), starting community detection.; leidenalg: t=8.048258741036989, m=0.6175825273363675; igraph community_leiden: t=1.159165252931416, m=0.6298702028415605; ```; This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586969791
https://github.com/scverse/scanpy/issues/1053#issuecomment-586969791:98,Safety,detect,detection,98,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:; ```; Running Leiden 0.7.0.post1+71.g14ba1e4.dirty; Running igraph 0.8.0; Read graph (n=63731,m=817035), starting community detection.; leidenalg: t=8.048258741036989, m=0.6175825273363675; igraph community_leiden: t=1.159165252931416, m=0.6298702028415605; ```; This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586969791
https://github.com/scverse/scanpy/issues/1053#issuecomment-586969791:361,Safety,detect,detection,361,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:; ```; Running Leiden 0.7.0.post1+71.g14ba1e4.dirty; Running igraph 0.8.0; Read graph (n=63731,m=817035), starting community detection.; leidenalg: t=8.048258741036989, m=0.6175825273363675; igraph community_leiden: t=1.159165252931416, m=0.6298702028415605; ```; This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586969791
https://github.com/scverse/scanpy/issues/1053#issuecomment-586969791:32,Testability,benchmark,benchmark,32,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:; ```; Running Leiden 0.7.0.post1+71.g14ba1e4.dirty; Running igraph 0.8.0; Read graph (n=63731,m=817035), starting community detection.; leidenalg: t=8.048258741036989, m=0.6175825273363675; igraph community_leiden: t=1.159165252931416, m=0.6298702028415605; ```; This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586969791
https://github.com/scverse/scanpy/issues/1053#issuecomment-1038930856:189,Performance,perform,performance,189,"This hasn't been implemented yet, but a pull request would be welcome. There would also have to be documentation about changing results and how to get previous behavior. Some benchmarks of performance would also be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1038930856
https://github.com/scverse/scanpy/issues/1053#issuecomment-1038930856:175,Testability,benchmark,benchmarks,175,"This hasn't been implemented yet, but a pull request would be welcome. There would also have to be documentation about changing results and how to get previous behavior. Some benchmarks of performance would also be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1038930856
https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011:579,Availability,avail,available,579,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011
https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011:127,Deployability,release,release,127,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011
https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011:208,Integrability,depend,depend,208,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011
https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011:353,Usability,simpl,simpler,353,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011
https://github.com/scverse/scanpy/issues/1053#issuecomment-1040009925:393,Usability,clear,clear,393,"@vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe?. More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`. Thanks!. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040009925
https://github.com/scverse/scanpy/issues/1053#issuecomment-1040013593:434,Usability,simpl,simply,434,"> I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe?. Yes, you can set a random seed. You can set the RNG in `igraph` using [`set_random_number_generator`](https://igraph.org/python/api/latest/igraph._igraph.html#set_random_number_generator). When simply passing the standard `random` Python library, you can set the seed using [`random.seed`](https://docs.python.org/3/library/random.html#random.seed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040013593
https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975:167,Energy Efficiency,efficient,efficiently,167,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975
https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975:286,Energy Efficiency,efficient,efficient,286,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975
https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975:384,Energy Efficiency,efficient,efficient,384,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975
https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975:472,Energy Efficiency,efficient,efficient,472,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975
https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975:81,Modifiability,plugin,plugin,81,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975
https://github.com/scverse/scanpy/issues/1053#issuecomment-1040389317:11,Testability,test,test,11,"For the 3k test dataset, introducing edge weights with; ```; import random; random.seed(1234) . adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None); g = sc._utils.get_igraph_from_adjacency(adjacency); clustering = g.community_leiden(objective_function='modularity', weights='weight'); adata.obs['leiden_igraph_edge_weighted'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index); ```; Leads to a more similar clustering to `sc.tl.leiden`. Setting the seed makes all reproducible.; ![image](https://user-images.githubusercontent.com/25825809/154090475-9b5afd35-f254-4c30-92d5-c1a1a86d797d.png). Including igraph edge weights does not seem to impact run times on my larger datasets vs. igraph without weights.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040389317
https://github.com/scverse/scanpy/issues/1053#issuecomment-1040743473:409,Usability,clear,clear,409,"> @vtraag @ivirshup I accidentally ran the `igraph_community_leiden` in a for loop, 4 times with the same settings and noticed the leiden output differed slightly each time. Is there a parameter to set to ensure the same output each time? randomseed maybe?; > ; > ; > ; > More generally, any advice on which settings to implement with the `igraph_community_leiden` call to replicate `sc.tl.leiden`? It is not clear to me what some of the `sc.tl.leiden` default calls are to `leidenalg`.; > ; > ; > ; > Thanks!; > ; > ; > ; > Thanks!. As mentioned below, you should set the RNG. AFAIK scanpy does that by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040743473
https://github.com/scverse/scanpy/issues/1053#issuecomment-1040854081:564,Deployability,update,update,564,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python; def iterativley_cluster(; g: igraph.Graph,; *,; n_iterations: int = 10,; random_state: int = 0,; leiden_kwargs: dict = {}; ) -> list:; import random; random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}; _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]; for _ in range(n_iterations-1):; partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs); steps.append(partition). return steps; ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040854081
https://github.com/scverse/scanpy/issues/1053#issuecomment-1040854081:953,Performance,optimiz,optimization,953,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python; def iterativley_cluster(; g: igraph.Graph,; *,; n_iterations: int = 10,; random_state: int = 0,; leiden_kwargs: dict = {}; ) -> list:; import random; random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}; _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]; for _ in range(n_iterations-1):; partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs); steps.append(partition). return steps; ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040854081
https://github.com/scverse/scanpy/issues/1053#issuecomment-1047590549:108,Deployability,update,updated,108,"Sorry, i made a critical typo in the time reports, where i listed the functions the wrong way round. I have updated the comment to correct this. . To be clear. `g.community_leiden` is faster than `sc.tl.leiden` in my case, particulalrly for large datasets. > Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster.; > ; > The average of 4 leiden runs on my 185,000 cell subsampled dataset: `sc.tl.leiden`, 11.5 minutes `g.community_leiden`, 9.5 minutes; > ; > 1 leiden run on my 1,850,000 cell subsampled dataset: `sc.tl.leiden`, 11 hours, 26 minutes `g.community_leiden`, 7 hours, 30 minutes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1047590549
https://github.com/scverse/scanpy/issues/1053#issuecomment-1047590549:153,Usability,clear,clear,153,"Sorry, i made a critical typo in the time reports, where i listed the functions the wrong way round. I have updated the comment to correct this. . To be clear. `g.community_leiden` is faster than `sc.tl.leiden` in my case, particulalrly for large datasets. > Setting `n_iterations=-1` in `g.community_leiden` certainly impacts run time (vs. default `n_iterations=2`), making runtimes more similar to `sc.tl.leiden()`. For large datasets though, run times with `g.comunity_leiden` still appear faster.; > ; > The average of 4 leiden runs on my 185,000 cell subsampled dataset: `sc.tl.leiden`, 11.5 minutes `g.community_leiden`, 9.5 minutes; > ; > 1 leiden run on my 1,850,000 cell subsampled dataset: `sc.tl.leiden`, 11 hours, 26 minutes `g.community_leiden`, 7 hours, 30 minutes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1047590549
https://github.com/scverse/scanpy/issues/1053#issuecomment-1099288285:2332,Energy Efficiency,power,power,2332,"e); ```; I get the following traceback; ```; Traceback (most recent call last):; File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 92, in <module>; adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None); File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency; g.es['weight'] = weights; SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function; ```. When i run the `python-igraph` leiden implementation like so...; ```; obs_name = f""leiden {leiden_suffix}""; g = sc._utils.get_igraph_from_adjacency(adjacency); clustering = g.community_leiden(; objective_function=""modularity"", ; resolution_parameter=0.1, ; weights = 'weight',; n_iterations=2; ); adata.obs[obs_name] = (; pd.Series(; clustering.membership, ; dtype='category', ; index=adata.obs.index; ); ); ```; I get the following, similar traceback; ```; UMAP leiden clustering resolution 0.1 commenced 12:27:06.619473; running Leiden clustering; Traceback (most recent call last):; File ""/opt/notebooks/output/../scripts/sc_cluster.py"", line 88, in <module>; obs_name = f""leiden {leiden_suffix}""; File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/tools/_leiden.py"", line 129, in leiden; g = _utils.get_igraph_from_adjacency(adjacency, directed=directed); File ""/opt/conda/envs/scanpy_py3pt9/lib/python3.9/site-packages/scanpy/_utils/__init__.py"", line 219, in get_igraph_from_adjacency; g.es['weight'] = weights; SystemError: /opt/conda/conda-bld/python-split_1649141344976/work/Objects/listobject.c:138: bad argument to internal function. ```. The script runs completely fine when I subsample the adata with `sc.pp.neighbours`. So far, i have managed to run 0.5 fraction of the cells (9.25 million cells). On the cluster i am trying to run this on, memory and processing power are not limiting. . Any advice on what might be going wrong here?. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1099288285
https://github.com/scverse/scanpy/issues/1053#issuecomment-1103987652:41,Deployability,release,released,41,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1103987652
https://github.com/scverse/scanpy/issues/1053#issuecomment-1103987652:67,Deployability,release,release,67,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1103987652
https://github.com/scverse/scanpy/issues/1053#issuecomment-1103987652:172,Deployability,update,update,172,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1103987652
https://github.com/scverse/scanpy/issues/1053#issuecomment-1103987652:89,Integrability,interface,interface,89,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1103987652
https://github.com/scverse/scanpy/issues/1053#issuecomment-1892447669:217,Testability,log,logic,217,"@ivirshup The code diff is [here](https://github.com/scverse/scanpy/compare/master...ilan-gold:scanpy:igraph_leiden?expand=1#diff-9b0695a74ff6002a10a74cef4b450792a39038f204ce166f7cce1b3274b77816) but I'll explain the logic of the changes. . 1. `igraph`'s implementation does not allow directed graphs; we throw a ValueError if someone tries to do directed + `igraph`.; 2. `igraph`'s default resolution parameter is 2, so that changed as well. I don't think we should swap it back to -1 for `leidenalg`.; 3. Of course `use_igraph` is now `True` (and a new argument). I'll look into some larger datasets. ~~Also there is no plotting test from what I see. Should we add that?~~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1892447669
https://github.com/scverse/scanpy/issues/1053#issuecomment-1892447669:631,Testability,test,test,631,"@ivirshup The code diff is [here](https://github.com/scverse/scanpy/compare/master...ilan-gold:scanpy:igraph_leiden?expand=1#diff-9b0695a74ff6002a10a74cef4b450792a39038f204ce166f7cce1b3274b77816) but I'll explain the logic of the changes. . 1. `igraph`'s implementation does not allow directed graphs; we throw a ValueError if someone tries to do directed + `igraph`.; 2. `igraph`'s default resolution parameter is 2, so that changed as well. I don't think we should swap it back to -1 for `leidenalg`.; 3. Of course `use_igraph` is now `True` (and a new argument). I'll look into some larger datasets. ~~Also there is no plotting test from what I see. Should we add that?~~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1892447669
https://github.com/scverse/scanpy/issues/1061#issuecomment-588270826:184,Testability,test,test,184,"Hi, thank you for the report. However, you deleted. > Put a minimal reproducible example that reproduces the bug in the code block below. Please do that, so we can create a regression test from it:. ```py; import numpy as np; import scanpy as sc; ad = AnnData(np.array([...])); sc.tl.rank_genes_groups(ad) # this line crashes without the fix; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061#issuecomment-588270826
https://github.com/scverse/scanpy/issues/1061#issuecomment-588274013:181,Integrability,wrap,wrapper,181,Since this is an overflow any data set with 1000's of cells I can use for this? I think it is Windows specific crash and how python implements sqrt() on windows which probably is a wrapper of the native math library in C. I may be wrong. So will the regression test work in this case?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061#issuecomment-588274013
https://github.com/scverse/scanpy/issues/1061#issuecomment-588274013:261,Testability,test,test,261,Since this is an overflow any data set with 1000's of cells I can use for this? I think it is Windows specific crash and how python implements sqrt() on windows which probably is a wrapper of the native math library in C. I may be wrong. So will the regression test work in this case?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061#issuecomment-588274013
https://github.com/scverse/scanpy/issues/1061#issuecomment-588288211:13,Testability,test,test,13,"Uh, we don’t test on windows at the moment so it won’t. To create a data set, you could just have created one using some numpy random function or so. But as it is we should probably just fix it the way you propose. Care to do a PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061#issuecomment-588288211
https://github.com/scverse/scanpy/pull/1062#issuecomment-588356161:70,Deployability,a/b,a/b,70,But I am not sure about the readability and the order of operations. `a/b*c` is equal to `a/(b*c)` or `(a/b)*c`? . that why I did it with parenthesis.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1062#issuecomment-588356161
https://github.com/scverse/scanpy/pull/1062#issuecomment-588356161:104,Deployability,a/b,a/b,104,But I am not sure about the readability and the order of operations. `a/b*c` is equal to `a/(b*c)` or `(a/b)*c`? . that why I did it with parenthesis.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1062#issuecomment-588356161
https://github.com/scverse/scanpy/pull/1063#issuecomment-589704545:16,Testability,test,test,16,"hmm, you didn’t test with the new changes. please do so for the next PR, I’m trusting you with these! Check out ce06987",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1063#issuecomment-589704545
https://github.com/scverse/scanpy/pull/1063#issuecomment-589706044:18,Testability,test,test,18,"> hmm, you didn’t test with the new changes. please do so for the next PR, I’m trusting you with these! Check out [ce06987](https://github.com/theislab/scanpy/commit/ce06987e6824471d0fe22c5cc7f9faf8840bf5da). sorry about that forgot to do this with the last changes I made!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1063#issuecomment-589706044
https://github.com/scverse/scanpy/pull/1066#issuecomment-589477958:270,Testability,benchmark,benchmarks,270,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting.; * This should work with other solvers from scipy, like `lobpcg`, right?; * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that?. ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589477958
https://github.com/scverse/scanpy/pull/1066#issuecomment-589477958:333,Testability,benchmark,benchmark,333,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting.; * This should work with other solvers from scipy, like `lobpcg`, right?; * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that?. ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589477958
https://github.com/scverse/scanpy/pull/1066#issuecomment-589477958:113,Usability,learn,learn,113,"Thanks for the PR! This looks really interesting. I've got a couple questions: . * Why not submit this to scikit-learn? In general I'd be more confident in their vetting.; * This should work with other solvers from scipy, like `lobpcg`, right?; * Could you provide some benchmarks on time, memory usage, and accuracy? . From a brief benchmark on my end, this looks very good from a memory usage perspective, with similar compute times. The components also seem highly correlated, but the components are scaled differently. Would you mind commenting on that?. ------------------. Edit: It seems like the factors are making our nearest neighbor network quite different. It also looks like the calculated variances are different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589477958
https://github.com/scverse/scanpy/pull/1066#issuecomment-589489598:763,Usability,learn,learn,763,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589489598
https://github.com/scverse/scanpy/pull/1066#issuecomment-589489598:776,Usability,learn,learn,776,"Great catch! I messed up and forgot to sort the singular values prior to scaling `U`. The components should be more or less the same now. To answer your other questions,. - Submitting this PR to `scanpy` seemed like lower-hanging fruit since I'm much more familiar with your codebase. sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). If `sklearn` does eventually allow for PCA on sparse inputs, it would be really easy to replace the call to my function with a call to sklearn's implementation instead. . - This does work with `lobpcg`, but I'm a little confused by when `lobpcg` outperforms `arpack` (see the discussion here scikit-learn/scikit-learn#12794). There's some criterion that relates to the number of components and the size of the smallest dimension. In my hands, `lobpcg` is significantly slower. - Will do!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589489598
https://github.com/scverse/scanpy/pull/1066#issuecomment-589495486:190,Testability,Benchmark,Benchmarking,190,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook.; [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,; ```; %%memit; t=time.time(); sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True); print(str(time.time()-t)+' seconds'); ```; ```; 6.122049570083618 seconds; peak memory: 1332.33 MiB, increment: 1047.04 MiB; ```. With `pca_sparse=True`,; ```; %%memit; t=time.time(); sc.tl.pca(adata2,pca_sparse=True,random_state=0); print(str(time.time()-t)+' seconds'); ```; ```; 2.373802423477173 seconds; peak memory: 401.17 MiB, increment: 56.26 MiB; ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589495486
https://github.com/scverse/scanpy/pull/1066#issuecomment-589495486:1017,Testability,benchmark,benchmarking,1017,"Err, hopefully this isn't inconvenient. Here's a zip file containing the relevant notebook.; [benchmarks_PR1066.zip](https://github.com/theislab/scanpy/files/4234288/benchmarks_PR1066.zip). Benchmarking was done on the raw pbmc3k data. . Summary of the timing and memory results:. With `pca_sparse=False`,; ```; %%memit; t=time.time(); sc.tl.pca(adata1,pca_sparse=False,svd_solver='arpack',random_state=0,zero_center=True); print(str(time.time()-t)+' seconds'); ```; ```; 6.122049570083618 seconds; peak memory: 1332.33 MiB, increment: 1047.04 MiB; ```. With `pca_sparse=True`,; ```; %%memit; t=time.time(); sc.tl.pca(adata2,pca_sparse=True,random_state=0); print(str(time.time()-t)+' seconds'); ```; ```; 2.373802423477173 seconds; peak memory: 401.17 MiB, increment: 56.26 MiB; ```. There are very slight differences between the eigenvalues output by the different methods, which translates to slightly different cluster assignments when using euclidean distance (this is probably exacerbated by the fact that I am benchmarking on raw data). However, for correlation distance, the output is exactly the same. See the attached notebook for more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589495486
https://github.com/scverse/scanpy/pull/1066#issuecomment-589500984:980,Deployability,release,release,980,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you?. Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing!. Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)); * Implicit centering, densifying centering, no centering; * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589500984
https://github.com/scverse/scanpy/pull/1066#issuecomment-589500984:1216,Performance,perform,performance,1216,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you?. Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing!. Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)); * Implicit centering, densifying centering, no centering; * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589500984
https://github.com/scverse/scanpy/pull/1066#issuecomment-589500984:1427,Performance,multi-thread,multi-threaded,1427,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you?. Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing!. Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)); * Implicit centering, densifying centering, no centering; * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589500984
https://github.com/scverse/scanpy/pull/1066#issuecomment-589500984:1203,Testability,benchmark,benchmark,1203,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you?. Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing!. Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)); * Implicit centering, densifying centering, no centering; * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589500984
https://github.com/scverse/scanpy/pull/1066#issuecomment-589503209:84,Testability,benchmark,benchmarking,84,"Also, btw, I like the memory-profiler `mprof` sampling plots a lot for this kind of benchmarking. I took a look at this with this script:. <details>; <summary> `sparse_pca.py` </summary>. ```python; import scanpy as sc. pbmc = sc.datasets.pbmc3k(); sc.pp.log1p(pbmc). @profile; def implicit_mean_pca():; sc.pp.pca(pbmc, pca_sparse=True). @profile; def explicit_mean_pca():; sc.pp.pca(pbmc). @profile; def nomean_pca():; sc.pp.pca(pbmc, zero_center=False). if __name__ == ""__main__"":; implicit_mean_pca(). nomean_pca(). explicit_mean_pca(). ```. </details>. Run with. ```sh; $ mprof run --interval=0.01 ./sparse_pca.py; ...; $ mprof plot; ```. Shows:. ![pca_mem_benchmark](https://user-images.githubusercontent.com/8238804/75006460-f7266300-54c5-11ea-9378-4fedc2c6d73c.png). So this is looking very good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589503209
https://github.com/scverse/scanpy/pull/1066#issuecomment-589512273:240,Deployability,release,release,240,"> Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. I'll try and do that soon. For now, I'll focus on providing you with the benchmarks you requested!. > * Datasets size (one small, one large (>50k cells)); > * Implicit centering, densifying centering, no centering; > * single threaded, multi-threaded <---------. I could not find a `n_jobs` argument in `scanpy.pp.pca`. Can you elaborate a little on the single threaded, multi-threaded bit?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589512273
https://github.com/scverse/scanpy/pull/1066#issuecomment-589512273:485,Performance,multi-thread,multi-threaded,485,"> Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. I'll try and do that soon. For now, I'll focus on providing you with the benchmarks you requested!. > * Datasets size (one small, one large (>50k cells)); > * Implicit centering, densifying centering, no centering; > * single threaded, multi-threaded <---------. I could not find a `n_jobs` argument in `scanpy.pp.pca`. Can you elaborate a little on the single threaded, multi-threaded bit?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589512273
https://github.com/scverse/scanpy/pull/1066#issuecomment-589512273:620,Performance,multi-thread,multi-threaded,620,"> Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. I'll try and do that soon. For now, I'll focus on providing you with the benchmarks you requested!. > * Datasets size (one small, one large (>50k cells)); > * Implicit centering, densifying centering, no centering; > * single threaded, multi-threaded <---------. I could not find a `n_jobs` argument in `scanpy.pp.pca`. Can you elaborate a little on the single threaded, multi-threaded bit?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589512273
https://github.com/scverse/scanpy/pull/1066#issuecomment-589512273:322,Testability,benchmark,benchmarks,322,"> Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. I'll try and do that soon. For now, I'll focus on providing you with the benchmarks you requested!. > * Datasets size (one small, one large (>50k cells)); > * Implicit centering, densifying centering, no centering; > * single threaded, multi-threaded <---------. I could not find a `n_jobs` argument in `scanpy.pp.pca`. Can you elaborate a little on the single threaded, multi-threaded bit?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589512273
https://github.com/scverse/scanpy/pull/1066#issuecomment-589529166:254,Testability,benchmark,benchmarks,254,I used the 68k pbmc dataset from 10x genomics for the large dataset. Jupyter notebook with residuals:; [benchmarks_PR1066_residuals.ipynb.zip](https://github.com/theislab/scanpy/files/4234730/benchmarks_PR1066_residuals.ipynb.zip). The memory and timing benchmarks:; ![large](https://user-images.githubusercontent.com/16548075/75012333-97cd4200-5436-11ea-883a-94512bac16a4.png); ![small](https://user-images.githubusercontent.com/16548075/75012334-97cd4200-5436-11ea-9393-696a00b884f8.png),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589529166
https://github.com/scverse/scanpy/pull/1066#issuecomment-589921438:233,Modifiability,variab,variable,233,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit?. The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python; import os; os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas; os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas; ```. Using sc.datasets.pbmc3k:. <details>; <summary> Single threaded </summary>. ```python; %time sc.pp.pca(pbmc, pca_sparse=True) ; CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s; Wall time: 4.43 s. %time sc.pp.pca(pbmc) ; CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s; Wall time: 15.8 s; ```. </details>. <details>; <summary> Multithreaded </summary>. ```python; %time sc.pp.pca(pbmc, pca_sparse=True) ; CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s; Wall time: 2.39 s. %time sc.pp.pca(pbmc) ; CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s; Wall time: 9.92 s; ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589921438
https://github.com/scverse/scanpy/pull/1066#issuecomment-589921438:106,Performance,multi-thread,multi-threaded,106,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit?. The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python; import os; os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas; os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas; ```. Using sc.datasets.pbmc3k:. <details>; <summary> Single threaded </summary>. ```python; %time sc.pp.pca(pbmc, pca_sparse=True) ; CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s; Wall time: 4.43 s. %time sc.pp.pca(pbmc) ; CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s; Wall time: 15.8 s; ```. </details>. <details>; <summary> Multithreaded </summary>. ```python; %time sc.pp.pca(pbmc, pca_sparse=True) ; CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s; Wall time: 2.39 s. %time sc.pp.pca(pbmc) ; CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s; Wall time: 9.92 s; ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589921438
https://github.com/scverse/scanpy/pull/1066#issuecomment-590137028:193,Performance,perform,performance,193,"Interesting... I know that there can be some difference between systems I use for how time is being recorded. But I still don't think I'd expect this. Either way, it looks like single threaded performance is good, and multithreaded is adding surprisingly little for a lot of spent computation. Once you've got the similarity measurements done, I think there's a little code organization to do, and this should be pretty much ready.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-590137028
https://github.com/scverse/scanpy/pull/1066#issuecomment-590137515:75,Integrability,message,message,75,I included the notebook with the residuals above. I'll reattach it to this message:. [benchmarks_PR1066_residuals.ipynb.zip](https://github.com/theislab/scanpy/files/4242812/benchmarks_PR1066_residuals.ipynb.zip),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-590137515
https://github.com/scverse/scanpy/pull/1066#issuecomment-590349532:29,Deployability,update,update,29,If we have arpack i can also update the PR with randomized svd approach. Is it needed?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-590349532
https://github.com/scverse/scanpy/pull/1066#issuecomment-591647662:57,Performance,perform,performance,57,@ivirshup I think the benchmarks have shown satisfactory performance of this PR. Should we move on to polishing the code organization?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-591647662
https://github.com/scverse/scanpy/pull/1066#issuecomment-591647662:22,Testability,benchmark,benchmarks,22,@ivirshup I think the benchmarks have shown satisfactory performance of this PR. Should we move on to polishing the code organization?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-591647662
https://github.com/scverse/scanpy/pull/1066#issuecomment-592268817:421,Performance,perform,performance,421,"@atarashansky I'll have time to give a little more detailed notes this weekend. * Have you looked at submitting this in a PR to sklearn? I think they would be better at evaluating the stability. I'd be happy to help with this if you want.; * At this point, I think PCA should go into its own file. Could you move the `pca` function and your sparse on into a `scanpy/preprocessing/_pca.py` file?; * From the stability and performance checks, I think this could be similar enough make it the default. I think this should just be the default behaviour for when: 1) the data matrix is sparse 2) `zero_center=True` 3) `svd_solver` is `arpack` or `lobpcg`. Any objections to this @Koncopd, @flying-sheep, @falexwolf?. -----------------------------. @Koncopd, I don't think I've looked over your implementation much. Is it similar to the stalled sklearn PR? If so, do you have a sense of why the `sklearn` PR for the randomized solver is stalled?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-592268817
https://github.com/scverse/scanpy/pull/1066#issuecomment-593738303:19,Performance,perform,performance,19,"@atarashansky, the performance is looking very very good:. ```python; import scanpy as sc; import numpy as np; from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data; # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64); %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True); # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s; # Wall time: 2.93 s; # Peak memory (including dataset) is about 770 MB; %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True); # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s; # Wall time: 7min 43s; # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]); assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]); ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does?. Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593738303
https://github.com/scverse/scanpy/pull/1066#issuecomment-593738303:680,Testability,assert,assert,680,"@atarashansky, the performance is looking very very good:. ```python; import scanpy as sc; import numpy as np; from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data; # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64); %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True); # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s; # Wall time: 2.93 s; # Peak memory (including dataset) is about 770 MB; %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True); # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s; # Wall time: 7min 43s; # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]); assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]); ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does?. Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593738303
https://github.com/scverse/scanpy/pull/1066#issuecomment-593738303:748,Testability,assert,assert,748,"@atarashansky, the performance is looking very very good:. ```python; import scanpy as sc; import numpy as np; from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data; # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64); %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True); # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s; # Wall time: 2.93 s; # Peak memory (including dataset) is about 770 MB; %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True); # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s; # Wall time: 7min 43s; # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]); assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]); ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does?. Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593738303
https://github.com/scverse/scanpy/pull/1066#issuecomment-593741930:677,Availability,error,error,677,"That's odd. sklearn calculates the explained variance and variance ratio as follows:; ```python; # Calculate explained variance & explained variance ratio; X_transformed = U * Sigma; self.explained_variance_ = exp_var = np.var(X_transformed, axis=0); if sp.issparse(X):; _, full_var = mean_variance_axis(X, axis=0); full_var = full_var.sum(); else:; full_var = np.var(X, axis=0).sum(); self.explained_variance_ratio_ = exp_var / full_var; ```. I do it in the same way:; ```python; X_pca = (u * s)[:, idx] # sort PCs in decreasing order; ev = X_pca.var(0). total_var = _get_mean_var(X)[1].sum(); ev_ratio = ev / total_var; ```; I'll investigate... EDIT: Strange, your assertion error is not reproducible on my end. The code runs fine for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593741930
https://github.com/scverse/scanpy/pull/1066#issuecomment-593741930:667,Testability,assert,assertion,667,"That's odd. sklearn calculates the explained variance and variance ratio as follows:; ```python; # Calculate explained variance & explained variance ratio; X_transformed = U * Sigma; self.explained_variance_ = exp_var = np.var(X_transformed, axis=0); if sp.issparse(X):; _, full_var = mean_variance_axis(X, axis=0); full_var = full_var.sum(); else:; full_var = np.var(X, axis=0).sum(); self.explained_variance_ratio_ = exp_var / full_var; ```. I do it in the same way:; ```python; X_pca = (u * s)[:, idx] # sort PCs in decreasing order; ev = X_pca.var(0). total_var = _get_mean_var(X)[1].sum(); ev_ratio = ev / total_var; ```; I'll investigate... EDIT: Strange, your assertion error is not reproducible on my end. The code runs fine for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593741930
https://github.com/scverse/scanpy/pull/1066#issuecomment-593743978:95,Usability,learn,learn,95,"I'm not sure we're looking at the same code. I was looking [at this](https://github.com/scikit-learn/scikit-learn/blob/72b3041ed57e42817e4c5c9853b3a2597cab3654/sklearn/decomposition/_pca.py#L543):. ```python; self.n_samples_, self.n_features_ = n_samples, n_features; self.components_ = V; self.n_components_ = n_components. # Get variance explained by singular values; self.explained_variance_ = (S ** 2) / (n_samples - 1); total_var = np.var(X, ddof=1, axis=0); self.explained_variance_ratio_ = \; self.explained_variance_ / total_var.sum(); self.singular_values_ = S.copy() # Store the singular values.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593743978
https://github.com/scverse/scanpy/pull/1066#issuecomment-593743978:108,Usability,learn,learn,108,"I'm not sure we're looking at the same code. I was looking [at this](https://github.com/scikit-learn/scikit-learn/blob/72b3041ed57e42817e4c5c9853b3a2597cab3654/sklearn/decomposition/_pca.py#L543):. ```python; self.n_samples_, self.n_features_ = n_samples, n_features; self.components_ = V; self.n_components_ = n_components. # Get variance explained by singular values; self.explained_variance_ = (S ** 2) / (n_samples - 1); total_var = np.var(X, ddof=1, axis=0); self.explained_variance_ratio_ = \; self.explained_variance_ / total_var.sum(); self.singular_values_ = S.copy() # Store the singular values.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593743978
https://github.com/scverse/scanpy/pull/1066#issuecomment-593744652:272,Availability,error,error,272,"> I'm not sure we're looking at the same code. I was looking at this:. I was looking at the [TruncatedSVD](https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/decomposition/_truncated_svd.py#L186) code. Either way, I'm not able to reproduce your assertion error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593744652
https://github.com/scverse/scanpy/pull/1066#issuecomment-593744652:262,Testability,assert,assertion,262,"> I'm not sure we're looking at the same code. I was looking at this:. I was looking at the [TruncatedSVD](https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/decomposition/_truncated_svd.py#L186) code. Either way, I'm not able to reproduce your assertion error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593744652
https://github.com/scverse/scanpy/pull/1066#issuecomment-593744652:133,Usability,learn,learn,133,"> I'm not sure we're looking at the same code. I was looking at this:. I was looking at the [TruncatedSVD](https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/decomposition/_truncated_svd.py#L186) code. Either way, I'm not able to reproduce your assertion error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593744652
https://github.com/scverse/scanpy/pull/1066#issuecomment-593744652:146,Usability,learn,learn,146,"> I'm not sure we're looking at the same code. I was looking at this:. I was looking at the [TruncatedSVD](https://github.com/scikit-learn/scikit-learn/blob/b194674c4/sklearn/decomposition/_truncated_svd.py#L186) code. Either way, I'm not able to reproduce your assertion error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593744652
https://github.com/scverse/scanpy/pull/1066#issuecomment-593746949:11,Testability,test,test,11,"Here's the test I ran for commit 82e3a59b5905. ```python; import scanpy as sc; import numpy as np. pbmc = sc.datasets.pbmc3k(); pbmc.X = pbmc.X.astype(np.float64); sc.pp.log1p(pbmc). implicit = sc.pp.pca(pbmc, pca_sparse=True, dtype=np.float64, copy=True); explicit = sc.pp.pca(pbmc, pca_sparse=False, dtype=np.float64, copy=True). assert not np.allclose(implicit.uns[""pca""][""variance""], explicit.uns[""pca""][""variance""]); assert not np.allclose(implicit.uns[""pca""][""variance_ratio""], explicit.uns[""pca""][""variance_ratio""]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593746949
https://github.com/scverse/scanpy/pull/1066#issuecomment-593746949:332,Testability,assert,assert,332,"Here's the test I ran for commit 82e3a59b5905. ```python; import scanpy as sc; import numpy as np. pbmc = sc.datasets.pbmc3k(); pbmc.X = pbmc.X.astype(np.float64); sc.pp.log1p(pbmc). implicit = sc.pp.pca(pbmc, pca_sparse=True, dtype=np.float64, copy=True); explicit = sc.pp.pca(pbmc, pca_sparse=False, dtype=np.float64, copy=True). assert not np.allclose(implicit.uns[""pca""][""variance""], explicit.uns[""pca""][""variance""]); assert not np.allclose(implicit.uns[""pca""][""variance_ratio""], explicit.uns[""pca""][""variance_ratio""]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593746949
https://github.com/scverse/scanpy/pull/1066#issuecomment-593746949:422,Testability,assert,assert,422,"Here's the test I ran for commit 82e3a59b5905. ```python; import scanpy as sc; import numpy as np. pbmc = sc.datasets.pbmc3k(); pbmc.X = pbmc.X.astype(np.float64); sc.pp.log1p(pbmc). implicit = sc.pp.pca(pbmc, pca_sparse=True, dtype=np.float64, copy=True); explicit = sc.pp.pca(pbmc, pca_sparse=False, dtype=np.float64, copy=True). assert not np.allclose(implicit.uns[""pca""][""variance""], explicit.uns[""pca""][""variance""]); assert not np.allclose(implicit.uns[""pca""][""variance_ratio""], explicit.uns[""pca""][""variance_ratio""]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593746949
https://github.com/scverse/scanpy/pull/1066#issuecomment-593822877:679,Testability,test,tests,679,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)?. ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct; * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593822877
https://github.com/scverse/scanpy/pull/1066#issuecomment-593822877:107,Usability,simpl,simplifies,107,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)?. ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct; * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593822877
https://github.com/scverse/scanpy/pull/1066#issuecomment-593822877:467,Usability,learn,learn,467,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)?. ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct; * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593822877
https://github.com/scverse/scanpy/pull/1066#issuecomment-593822877:480,Usability,learn,learn,480,"Part of why I would like this to be in `sklearn` is that it lessens our responsibility to maintain it, and simplifies our code. I think it'll be easiest to do this sooner, rather than later, since these things have a tendency to lose momentum. For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and `solver=""arpack""` to `PCA.fit`, like what https://github.com/scikit-learn/scikit-learn/pull/12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)?. ----------------. About this PR, could you add tests for:. * The variance and variance explained entries being correct; * Explicit and implicit centering returning equivalent results. After that and the code reorganization I mentioned above, this should be about ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593822877
https://github.com/scverse/scanpy/pull/1066#issuecomment-593862030:202,Usability,learn,learn,202,"> For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593862030
https://github.com/scverse/scanpy/pull/1066#issuecomment-593862030:215,Usability,learn,learn,215,"> For sklearn submission, I don't think you'd have to implement any classes. Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593862030
https://github.com/scverse/scanpy/pull/1066#issuecomment-600122843:74,Deployability,release,release,74,"Hey @atarashansky, what's your status with this? We're going for a larger release soon, and I'd really like to have this PR in it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-600122843
https://github.com/scverse/scanpy/pull/1066#issuecomment-600646753:138,Testability,test,tests,138,"Sorry! I've been preoccupied with some stuff on my end (and also super distracted by this coronavirus hullabaloo). I'll add the requested tests, soon. . EDIT: Done!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-600646753
https://github.com/scverse/scanpy/pull/1066#issuecomment-601744153:252,Deployability,release,release,252,"Great! Yeah, coronavirus is pretty distracting. This last week has definitely felt like a month for me. So still to do:. * This should be rebased on master; * This should be the default, and this should be noted in the documentation; * Add this to the release notes!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-601744153
https://github.com/scverse/scanpy/pull/1066#issuecomment-630726240:153,Usability,learn,learn,153,"@ivirshup @atarashansky . > Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)?. > That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then. Has one of you opened this PR to sklearn? I just wanted to chime in and say that it'd be great if sklearn finally started to support this. Definitely worth trying to get it in there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-630726240
https://github.com/scverse/scanpy/pull/1066#issuecomment-630726240:166,Usability,learn,learn,166,"@ivirshup @atarashansky . > Your solution would just be what happened if someone passed a sparse matrix and solver=""arpack"" to PCA.fit, like what scikit-learn/scikit-learn#12841 does. Does this make it more appealing? If not, would you mind if I opened a PR to sklearn with this code (crediting you, of course)?. > That's fair! Doesn't seem like much work at all. I'll submit a PR to sklearn, then. Has one of you opened this PR to sklearn? I just wanted to chime in and say that it'd be great if sklearn finally started to support this. Definitely worth trying to get it in there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-630726240
https://github.com/scverse/scanpy/pull/1066#issuecomment-636055632:827,Availability,error,error,827,"```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); 201 ); 202 ; --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver); 204 # this is just a wrapper for the results; 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state); 293 return XHmat(x) - mhmat(ones(x)); 294 ; --> 295 XL = LinearOperator(; 296 matvec=matvec,; 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'; ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-636055632
https://github.com/scverse/scanpy/pull/1066#issuecomment-636055632:453,Integrability,wrap,wrapper,453,"```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); 201 ); 202 ; --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver); 204 # this is just a wrapper for the results; 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state); 293 return XHmat(x) - mhmat(ones(x)); 294 ; --> 295 XL = LinearOperator(; 296 matvec=matvec,; 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'; ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-636055632
https://github.com/scverse/scanpy/pull/1066#issuecomment-636055632:925,Integrability,depend,dependency,925,"```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); 201 ); 202 ; --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver); 204 # this is just a wrapper for the results; 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state); 293 return XHmat(x) - mhmat(ones(x)); 294 ; --> 295 XL = LinearOperator(; 296 matvec=matvec,; 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'; ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-636055632
https://github.com/scverse/scanpy/issues/1067#issuecomment-589649697:0,Deployability,Upgrade,Upgrade,0,"Upgrade to the real newest version 1.4.5.1, it contains the fix in 16101e7fe8269920d49a2b579125b0c1806d915d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1067#issuecomment-589649697
https://github.com/scverse/scanpy/issues/1068#issuecomment-590009483:13,Availability,avail,available,13,"Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590009483
https://github.com/scverse/scanpy/issues/1068#issuecomment-590009483:114,Availability,avail,available,114,"Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590009483
https://github.com/scverse/scanpy/issues/1068#issuecomment-590009483:62,Integrability,wrap,wrapper,62,"Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590009483
https://github.com/scverse/scanpy/issues/1068#issuecomment-590009483:158,Testability,benchmark,benchmark,158,"Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590009483
https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395:291,Availability,avail,available,291,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly; the vst R function at this address to make it work; https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den søn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <; notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for; > it via rpy2 and anndata2ri which is available here:; >; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395
https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395:391,Availability,avail,available,391,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly; the vst R function at this address to make it work; https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den søn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <; notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for; > it via rpy2 and anndata2ri which is available here:; >; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395
https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395:340,Integrability,wrap,wrapper,340,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly; the vst R function at this address to make it work; https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den søn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <; notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for; > it via rpy2 and anndata2ri which is available here:; >; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395
https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395:440,Testability,benchmark,benchmark,440,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly; the vst R function at this address to make it work; https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den søn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <; notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for; > it via rpy2 and anndata2ri which is available here:; >; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395
https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395:22,Usability,usab,usable,22,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly; the vst R function at this address to make it work; https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den søn. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <; notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for; > it via rpy2 and anndata2ri which is available here:; >; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395
https://github.com/scverse/scanpy/issues/1068#issuecomment-590116116:81,Integrability,wrap,wrappers,81,Maybe it would be a good idea to have a separate repo of `rpy2` and `anndata2ri` wrappers for `R` methods that we want to run in scanpy workflows. Would you be interested in sth like that? I could create a separate repo in theislab github? Something like `www.github.com/theislab/Rforscanpy`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590116116
https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256:567,Availability,error,error,567,"Hey!; > * What methods/ tools?; I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies?; So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer?; This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256
https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256:82,Deployability,integrat,integration,82,"Hey!; > * What methods/ tools?; I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies?; So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer?; This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256
https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256:163,Deployability,integrat,integration,163,"Hey!; > * What methods/ tools?; I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies?; So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer?; This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256
https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256:416,Deployability,install,installed,416,"Hey!; > * What methods/ tools?; I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies?; So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer?; This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256
https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256:673,Deployability,install,installable,673,"Hey!; > * What methods/ tools?; I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies?; So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer?; This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256
https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256:754,Deployability,install,install,754,"Hey!; > * What methods/ tools?; I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies?; So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer?; This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256
https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256:82,Integrability,integrat,integration,82,"Hey!; > * What methods/ tools?; I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies?; So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer?; This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256
https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256:163,Integrability,integrat,integration,163,"Hey!; > * What methods/ tools?; I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies?; So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer?; This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256
https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256:764,Integrability,depend,dependencies,764,"Hey!; > * What methods/ tools?; I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies?; So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer?; This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256
https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256:1134,Integrability,depend,depending,1134,"Hey!; > * What methods/ tools?; I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies?; So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer?; This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256
https://github.com/scverse/scanpy/issues/1068#issuecomment-590215132:53,Deployability,install,install,53,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590215132
https://github.com/scverse/scanpy/issues/1068#issuecomment-590215132:272,Deployability,install,install,272,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590215132
https://github.com/scverse/scanpy/issues/1068#issuecomment-590215132:63,Integrability,depend,dependencies,63,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590215132
https://github.com/scverse/scanpy/issues/1068#issuecomment-590215132:121,Integrability,depend,dependencies,121,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590215132
https://github.com/scverse/scanpy/issues/1068#issuecomment-590215132:332,Integrability,wrap,wrapped,332,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590215132
https://github.com/scverse/scanpy/issues/1068#issuecomment-590215132:655,Integrability,wrap,wrappers,655,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590215132
https://github.com/scverse/scanpy/issues/1068#issuecomment-590219665:223,Deployability,install,installed,223,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590219665
https://github.com/scverse/scanpy/issues/1068#issuecomment-590219665:272,Deployability,install,installed,272,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590219665
https://github.com/scverse/scanpy/issues/1068#issuecomment-590219665:287,Deployability,install,install,287,"The issue with going through conda is that not all `R` packages are on `bioconda` (e.g. Conos). And I'm not keen to create and maintain a conda `R` package. Therefore I'm using a conda environment with some python packages installed on top via `pip` and some `R` packages installed via `install.packages()`. > The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. I'm guessing this is not what already happens in `rpy2`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590219665
https://github.com/scverse/scanpy/issues/1068#issuecomment-593263880:148,Deployability,install,install,148,"> I'm not keen to create and maintain a conda R package. That's fair. Might be worth asking the `conos` developers in this case?. Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. > I'm guessing this is not what already happens in rpy2?. Nah, `rpy2` even copies the data in a particularly slow way by default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-593263880
https://github.com/scverse/scanpy/issues/1068#issuecomment-593311808:123,Availability,down,down,123,"> That's fair. Might be worth asking the `conos` developers in this case?. Yes, could and should do this... but would slow down the process for now I guess. > Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. It works if you install the R packages last and don't install anything else over the top via conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-593311808
https://github.com/scverse/scanpy/issues/1068#issuecomment-593311808:177,Deployability,install,install,177,"> That's fair. Might be worth asking the `conos` developers in this case?. Yes, could and should do this... but would slow down the process for now I guess. > Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. It works if you install the R packages last and don't install anything else over the top via conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-593311808
https://github.com/scverse/scanpy/issues/1068#issuecomment-593311808:303,Deployability,install,install,303,"> That's fair. Might be worth asking the `conos` developers in this case?. Yes, could and should do this... but would slow down the process for now I guess. > Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. It works if you install the R packages last and don't install anything else over the top via conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-593311808
https://github.com/scverse/scanpy/issues/1068#issuecomment-593311808:341,Deployability,install,install,341,"> That's fair. Might be worth asking the `conos` developers in this case?. Yes, could and should do this... but would slow down the process for now I guess. > Also, does using `install.packages` within a conda environment work for you? I recall that not working well for me in the past. It works if you install the R packages last and don't install anything else over the top via conda.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-593311808
https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:306,Deployability,install,installation,306,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553
https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:351,Deployability,install,installed,351,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553
https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:420,Deployability,install,install,420,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553
https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:430,Deployability,install,installation,430,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553
https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:533,Deployability,install,installation,533,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553
https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:655,Deployability,install,installed,655,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553
https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:489,Energy Efficiency,reduce,reduce,489,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553
https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:607,Energy Efficiency,reduce,reduced,607,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553
https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:930,Energy Efficiency,reduce,reduced,930,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553
https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:200,Integrability,depend,dependencies,200,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553
https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:408,Performance,cache,cached,408,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553
https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:1181,Performance,cache,cached,1181,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553
https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:1230,Performance,cache,cache,1230,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553
https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:792,Testability,test,test,792,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553
https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:881,Testability,test,test,881,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553
https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:980,Testability,test,test,980,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553
https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061:15,Availability,avail,available,15,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : ; invalid class “dgCMatrix” object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061
https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061:116,Availability,avail,available,116,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : ; invalid class “dgCMatrix” object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061
https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061:289,Availability,error,error,289,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : ; invalid class “dgCMatrix” object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061
https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061:342,Availability,Error,Error,342,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : ; invalid class “dgCMatrix” object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061
https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061:64,Integrability,wrap,wrapper,64,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : ; invalid class “dgCMatrix” object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061
https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061:252,Integrability,wrap,wrapper,252,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : ; invalid class “dgCMatrix” object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061
https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061:495,Modifiability,extend,extend,495,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : ; invalid class “dgCMatrix” object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061
https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061:638,Modifiability,extend,extend,638,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : ; invalid class “dgCMatrix” object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061
https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061:783,Modifiability,extend,extend,783,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : ; invalid class “dgCMatrix” object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061
https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061:926,Modifiability,extend,extend,926,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : ; invalid class “dgCMatrix” object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061
https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061:162,Testability,benchmark,benchmark,162,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : ; invalid class “dgCMatrix” object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class “dgCMatrix” object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061
https://github.com/scverse/scanpy/issues/1068#issuecomment-866769550:102,Availability,down,downgrading,102,Hey!. I think this is probably related to https://github.com/theislab/anndata2ri/issues/63. Maybe try downgrading your `anndata2ri` version.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-866769550
https://github.com/scverse/scanpy/issues/1068#issuecomment-982525338:62,Availability,error,error,62,"Hello,; I am trying to use the wrapper class and I am getting error; RRuntimeError: Error in `[.data.frame`(meta.data, , ii, drop = FALSE) : ; undefined columns selected; Could you please suggest me what should I do; Its on line ro.r('seurat_obj = as.Seurat(adata, counts=""X"",data=NULL)'); Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-982525338
https://github.com/scverse/scanpy/issues/1068#issuecomment-982525338:84,Availability,Error,Error,84,"Hello,; I am trying to use the wrapper class and I am getting error; RRuntimeError: Error in `[.data.frame`(meta.data, , ii, drop = FALSE) : ; undefined columns selected; Could you please suggest me what should I do; Its on line ro.r('seurat_obj = as.Seurat(adata, counts=""X"",data=NULL)'); Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-982525338
https://github.com/scverse/scanpy/issues/1068#issuecomment-982525338:31,Integrability,wrap,wrapper,31,"Hello,; I am trying to use the wrapper class and I am getting error; RRuntimeError: Error in `[.data.frame`(meta.data, , ii, drop = FALSE) : ; undefined columns selected; Could you please suggest me what should I do; Its on line ro.r('seurat_obj = as.Seurat(adata, counts=""X"",data=NULL)'); Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-982525338
https://github.com/scverse/scanpy/pull/1069#issuecomment-624821926:4,Deployability,update,updates,4,"Any updates on this? scanpy=1.4.6 is still not to return matplotlib figure but still GridSpec even with ""show=False"". Or is there any other way to modify the figure/ax and save it after?; Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1069#issuecomment-624821926
https://github.com/scverse/scanpy/pull/1070#issuecomment-590137170:35,Testability,test,test,35,Thanks for the PR! Could you add a test to make sure it works?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070#issuecomment-590137170
https://github.com/scverse/scanpy/pull/1070#issuecomment-590137345:21,Testability,test,test,21,"Do you mean to add a test for TravisCI (sorry, I'm new to this)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070#issuecomment-590137345
https://github.com/scverse/scanpy/pull/1070#issuecomment-590149955:75,Testability,test,tests,75,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070#issuecomment-590149955
https://github.com/scverse/scanpy/pull/1070#issuecomment-590149955:124,Testability,test,tests,124,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070#issuecomment-590149955
https://github.com/scverse/scanpy/pull/1070#issuecomment-590149955:405,Testability,test,tests,405,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070#issuecomment-590149955
https://github.com/scverse/scanpy/pull/1070#issuecomment-590149955:428,Testability,test,tests,428,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070#issuecomment-590149955
https://github.com/scverse/scanpy/pull/1070#issuecomment-590149955:513,Testability,test,tests,513,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070#issuecomment-590149955
https://github.com/scverse/scanpy/pull/1070#issuecomment-590149955:587,Testability,test,test,587,"Yeah, there are a bunch of methods which run under CI in under the `scanpy/tests` directory. These should go in the `scanpy/tests/test_highly_variable_genes.py` file. Basically just make sure the results are what they should be. In this case, the returned object should have the same values as those added to the anndata when `inplace=True`. We use [pytest](https://docs.pytest.org/en/latest/) to run the tests. You can run the tests locally by calling `pytest` at the root of the repo, and you can specify which tests to run with `pytest -k pattern` where `pattern` matches against the test names. Feel free to ask if you come across any problems!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1070#issuecomment-590149955
https://github.com/scverse/scanpy/issues/1074#issuecomment-592301994:117,Testability,assert,assert,117,Especially weird for since (at least for `scvelo 0.1.25`):. ```python; import scanpy; import scvelo; import anndata; assert scvelo.read_loom == scanpy.read_loom == anndata.read_loom; assert scvelo.read == scanpy.read; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1074#issuecomment-592301994
https://github.com/scverse/scanpy/issues/1074#issuecomment-592301994:183,Testability,assert,assert,183,Especially weird for since (at least for `scvelo 0.1.25`):. ```python; import scanpy; import scvelo; import anndata; assert scvelo.read_loom == scanpy.read_loom == anndata.read_loom; assert scvelo.read == scanpy.read; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1074#issuecomment-592301994
https://github.com/scverse/scanpy/issues/1074#issuecomment-592336678:72,Usability,simpl,simply,72,At some point we moved scvelo's loom reading to anndata/scanpy and then simply called that one from within scvelo. . `scvelo.read` and `scanpy.read` are ecaxtly the same. And `read_loom` is called internally within `read`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1074#issuecomment-592336678
https://github.com/scverse/scanpy/issues/1077#issuecomment-592423037:64,Modifiability,extend,extended,64,"sc.pl.heatmap now can execute dendrogram by clusters, can it be extended to dendrogram by genes ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077#issuecomment-592423037
https://github.com/scverse/scanpy/pull/1078#issuecomment-593925534:14,Testability,test,test,14,Added a quick test,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1078#issuecomment-593925534
https://github.com/scverse/scanpy/pull/1078#issuecomment-594055279:186,Testability,test,test,186,"Sure, only problem is that there’s now so much spatial-specific code in `embedding`. But we can fix that after this PR. I’d still like to see this change though:. > It’s good to have [a test], but it should of course also make sure that the flipping works!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1078#issuecomment-594055279
https://github.com/scverse/scanpy/pull/1080#issuecomment-702669779:181,Testability,test,test,181,"@awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`.; Could you check please?; This is certainly related to scipy 1.5. With scipy 1.4 the test works fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080#issuecomment-702669779
https://github.com/scverse/scanpy/pull/1080#issuecomment-703773746:217,Availability,error,error,217,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`.; > Could you check please?; > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080#issuecomment-703773746
https://github.com/scverse/scanpy/pull/1080#issuecomment-703773746:285,Deployability,release,release,285,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`.; > Could you check please?; > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080#issuecomment-703773746
https://github.com/scverse/scanpy/pull/1080#issuecomment-703773746:417,Deployability,release,release,417,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`.; > Could you check please?; > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080#issuecomment-703773746
https://github.com/scverse/scanpy/pull/1080#issuecomment-703773746:431,Deployability,install,install,431,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`.; > Could you check please?; > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080#issuecomment-703773746
https://github.com/scverse/scanpy/pull/1080#issuecomment-703773746:187,Testability,test,test,187,"> @awnimo , for me test_phenograph.py fails with `E TypeError: Expected list, got numpy.ndarray`.; > Could you check please?; > This is certainly related to scipy 1.5. With scipy 1.4 the test works fine. Indeed, this error is related to scipy, and we have fixed that in Phenograph new release [1.5.7](https://github.com/dpeerlab/PhenoGraph#version-157). The `test_phenograph.py` does not fail with the new Phenograph release (`pip install -U phenograph`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080#issuecomment-703773746
https://github.com/scverse/scanpy/pull/1080#issuecomment-703785741:96,Testability,test,test,96,@Koncopd ; This is weird never noticed. How can I restore it??; I only built the docs for me to test the outputs. Could be that building the docs affected somehow docs/api/scanpy.external.rst??,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1080#issuecomment-703785741
https://github.com/scverse/scanpy/pull/1081#issuecomment-595724233:96,Testability,log,logg,96,"Thank you! This looks great!. What do you think, @ivirshup?. Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value?. Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much!. Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081#issuecomment-595724233
https://github.com/scverse/scanpy/pull/1081#issuecomment-595724233:148,Testability,test,test,148,"Thank you! This looks great!. What do you think, @ivirshup?. Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value?. Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much!. Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081#issuecomment-595724233
https://github.com/scverse/scanpy/pull/1081#issuecomment-595724233:235,Testability,test,test,235,"Thank you! This looks great!. What do you think, @ivirshup?. Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value?. Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much!. Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081#issuecomment-595724233
https://github.com/scverse/scanpy/pull/1081#issuecomment-595724233:284,Testability,test,test,284,"Thank you! This looks great!. What do you think, @ivirshup?. Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value?. Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much!. Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081#issuecomment-595724233
https://github.com/scverse/scanpy/pull/1081#issuecomment-595724233:350,Testability,test,tests,350,"Thank you! This looks great!. What do you think, @ivirshup?. Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value?. Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much!. Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081#issuecomment-595724233
https://github.com/scverse/scanpy/pull/1081#issuecomment-595724233:445,Testability,test,tests,445,"Thank you! This looks great!. What do you think, @ivirshup?. Now, @Koncopd, can you also add a `logg.warn(""default of method has been changed to 't-test' from 't-test_overestim_var'"")`. And actually change the value?. Finally, can you test this on the Scanpy default tutorial and a a test based on the numerical values in addition to the image based tests? https://github.com/theislab/scanpy/blob/7e058a1a6a082e34a101d65fc7ac5e9cb6563220/scanpy/tests/notebooks/test_pbmc3k.py#L109-L111. Thank you very much!. Otherwise, this is good to be merged, IMO.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081#issuecomment-595724233
https://github.com/scverse/scanpy/pull/1081#issuecomment-595753282:27,Testability,test,tests,27,Also there are 2 numerical tests here; https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py; https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups_logreg.py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081#issuecomment-595753282
https://github.com/scverse/scanpy/pull/1081#issuecomment-595753282:93,Testability,test,tests,93,Also there are 2 numerical tests here; https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py; https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups_logreg.py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081#issuecomment-595753282
https://github.com/scverse/scanpy/pull/1081#issuecomment-595753282:180,Testability,test,tests,180,Also there are 2 numerical tests here; https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py; https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups_logreg.py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081#issuecomment-595753282
https://github.com/scverse/scanpy/pull/1081#issuecomment-595784602:0,Testability,Test,Test,0,Test fails seem unrelated to this PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081#issuecomment-595784602
https://github.com/scverse/scanpy/pull/1081#issuecomment-596456525:152,Safety,avoid,avoid,152,"@ivirshup thanks for catching these things, i'll fix them. ; In addition to some splitting and cleaning i also want to improve wilcoxon implementation (avoid densification at least, i have some code for it). But this should be another step, i think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1081#issuecomment-596456525
https://github.com/scverse/scanpy/issues/1082#issuecomment-599176533:68,Availability,down,download,68,"Oops, didn't mean to close this initially. The datasets still don't download right on master. I've opened #1102 to fix it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1082#issuecomment-599176533
https://github.com/scverse/scanpy/pull/1085#issuecomment-608561597:20,Deployability,update,updates,20,"Hi scanpy team, any updates on reviewing this PR? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1085#issuecomment-608561597
https://github.com/scverse/scanpy/issues/1086#issuecomment-603904135:70,Testability,test,test,70,"Hi @erikadudki,. Sorry for the slow replies to this post. There is no test implemented for bimodality in scanpy at the moment. For modeling you could look into [diffxpy](https://github.com/theislab/diffxpy), where you may be able to fit a gaussian mixture model to do model selection. Otherwise I guess `statsmodels` is the way forward for this in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086#issuecomment-603904135
https://github.com/scverse/scanpy/issues/1086#issuecomment-615129230:104,Deployability,release,release,104,"@erikadudki If you are willing to dabble in R, the [scDD package](https://www.bioconductor.org/packages/release/bioc/html/scDD.html) seeks to address this problem specifically. It may also be worth looking at for inspiration if you want to pursue your own implementation in python.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1086#issuecomment-615129230
https://github.com/scverse/scanpy/pull/1088#issuecomment-596448147:770,Availability,down,down,770,"That looks great Isaac, thanks a lot!. > * Restructure how elements are added to `uns`, as mentioned in [theislab/anndata#295 (comment)](https://github.com/theislab/anndata/issues/295#issuecomment-596164456). . agree, I also think that it could be best in the case of different data types (not visium). It's also best if the tree structure does not change in case of concatenation. > * Rename `obsm[""X_spatial""]` to `obsm[""coords""]` or `obsm[""spatial""]`. . what about `X_coords` ?. > * There is a natural connectivity for the observations from the adjacency of wells. This should be easy to add to obsp, or should just be added to obsp when `read_visum` is called. I'm thinking `""spatial_connectivity""` for the default key name. . This was also in the plans but further down the line (support for multiple slices should be first). I could have a look at this soon. What about re-open the `theislab/spatial` branch and merge this PR there? I could then work on how to handle the new `uns` structure in the plotting functions and have a definitive version of multiple slices support in anndata.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596448147
https://github.com/scverse/scanpy/pull/1088#issuecomment-596860000:692,Deployability,Update,Update,692,"> what about `X_coords` ?. Ha, I was mostly just trying to get rid of the `X_`!. > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596860000
https://github.com/scverse/scanpy/pull/1088#issuecomment-596860000:581,Testability,log,logic,581,"> what about `X_coords` ?. Ha, I was mostly just trying to get rid of the `X_`!. > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata. I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change. Update: heard back, the `library_id` should be fine, at least for this version. > support for multiple slices should be first. I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it. I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle. Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596860000
https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855:995,Deployability,pipeline,pipelines,995,"> > what about `X_coords` ?; > ; > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me!; > ; > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata.; > ; > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change.; > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. ; Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged.; > Update: heard back, the `library_id` should be fine, at least for this version.; > . good !. > > support for multiple slices should be first; > ; > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it.; > ; > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle.; > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:; * most people don't work with one slide; * having the same ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855
https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855:1129,Deployability,Update,Update,1129,"al branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata.; > ; > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change.; > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. ; Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged.; > Update: heard back, the `library_id` should be fine, at least for this version.; > . good !. > > support for multiple slices should be first; > ; > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it.; > ; > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle.; > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:; * most people don't work with one slide; * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855
https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855:2121,Deployability,integrat,integration,2121,"e not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. ; Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged.; > Update: heard back, the `library_id` should be fine, at least for this version.; > . good !. > > support for multiple slices should be first; > ; > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it.; > ; > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle.; > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:; * most people don't work with one slide; * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure).; ; > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855
https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855:2244,Deployability,update,update,2244,"e not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. ; Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged.; > Update: heard back, the `library_id` should be fine, at least for this version.; > . good !. > > support for multiple slices should be first; > ; > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it.; > ; > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle.; > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:; * most people don't work with one slide; * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure).; ; > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855
https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855:2121,Integrability,integrat,integration,2121,"e not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. ; Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged.; > Update: heard back, the `library_id` should be fine, at least for this version.; > . good !. > > support for multiple slices should be first; > ; > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it.; > ; > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle.; > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:; * most people don't work with one slide; * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure).; ; > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855
https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855:2875,Modifiability,inherit,inherits,2875,"e not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. ; Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged.; > Update: heard back, the `library_id` should be fine, at least for this version.; > . good !. > > support for multiple slices should be first; > ; > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it.; > ; > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle.; > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:; * most people don't work with one slide; * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure).; ; > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855
https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855:634,Testability,log,logic,634,"> > what about `X_coords` ?; > ; > Ha, I was mostly just trying to get rid of the `X_`! . ah right, anyway good for me!; > ; > > What about re-open the theislab/spatial branch and merge this PR there? I could then work on how to handle the new uns structure in the plotting functions and have a definitive version of multiple slices support in anndata.; > ; > I'd like to merge the changes currently in this PR to master since it fixes a bug with dataset reading. The changes to uns structure could go in another PR, but I'm waiting for an email back from 10x to make sure using the `library_id` as a key makes sense. Either way, the logic of getting the transformed coordinates etc. should be abstracted into a function so it's easy to change.; > . What do you mean by transformed coordinates? Also, to understand the inputs for anndata (output of spaceranger) you might have a look at this, if you are not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. ; Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged.; > Update: heard back, the `library_id` should be fine, at least for this version.; > . good !. > > support for multiple slices should be first; > ; > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it.; > ; > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle.; > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:; * most people don't work with one slide; * having the same ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855
https://github.com/scverse/scanpy/issues/1089#issuecomment-596279806:5,Modifiability,layers,layers,5,"The `layers` of an `AnnData` object are closest to the assays from Seurat. You should be able to store whatever transformations of the expression matrix you want in there. . In general, I do something like:. ```python; adata.layers[""counts""] = adata.X.copy(); sc.pp.normalize_total(adata); sc.pp.log1p(adata); ```. As a side note, I don't think we'd recommend using scaled data, but you can read more on that from these [tutorial notebooks](https://github.com/theislab/single-cell-tutorial) or [this related paper](https://www.embopress.org/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089#issuecomment-596279806
https://github.com/scverse/scanpy/issues/1089#issuecomment-596279806:225,Modifiability,layers,layers,225,"The `layers` of an `AnnData` object are closest to the assays from Seurat. You should be able to store whatever transformations of the expression matrix you want in there. . In general, I do something like:. ```python; adata.layers[""counts""] = adata.X.copy(); sc.pp.normalize_total(adata); sc.pp.log1p(adata); ```. As a side note, I don't think we'd recommend using scaled data, but you can read more on that from these [tutorial notebooks](https://github.com/theislab/single-cell-tutorial) or [this related paper](https://www.embopress.org/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089#issuecomment-596279806
https://github.com/scverse/scanpy/issues/1089#issuecomment-596466943:351,Deployability,integrat,integration,351,"Hi @chansigit,; If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089#issuecomment-596466943
https://github.com/scverse/scanpy/issues/1089#issuecomment-596466943:138,Energy Efficiency,reduce,reduce,138,"Hi @chansigit,; If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089#issuecomment-596466943
https://github.com/scverse/scanpy/issues/1089#issuecomment-596466943:351,Integrability,integrat,integration,351,"Hi @chansigit,; If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089#issuecomment-596466943
https://github.com/scverse/scanpy/pull/1090#issuecomment-596377120:111,Testability,test,tests,111,I'd appreciate a quick review from @fidelram or @flying-sheep on this. It'd be nice to get this soon since the tests failing are blocking the merging of other PRs.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1090#issuecomment-596377120
https://github.com/scverse/scanpy/issues/1092#issuecomment-597292098:73,Deployability,update,update,73,"Hi, @sarajimenez ; there is no ingest in scanpy 1.4.4.post1. You need to update scanpy to 1.4.5 and anndata to 0.7 to use ingest.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092#issuecomment-597292098
https://github.com/scverse/scanpy/issues/1092#issuecomment-623064541:169,Availability,error,error,169,"Hi, ; I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain').; I have updated my Scanpy 1.4.6 and anndata to 0.7.1.; I'm getting the following error message.; ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-71-27e22cc8f823> in <module>; ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs); 119 ; 120 for method in embedding_method:; --> 121 ing.map_embedding(method); 122 ; 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method); 407 """"""; 408 if method == 'umap':; --> 409 self._obsm['X_umap'] = self._umap_transform(); 410 elif method == 'pca':; 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self); 396 ; 397 def _umap_transform(self):; --> 398 return self._umap.transform(self._obsm['rep']); 399 ; 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X); 2006 try:; 2007 # sklearn pairwise_distances fails for callable metric on sparse data; -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func; 2009 dmat = pairwise_distances(; 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'; ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092#issuecomment-623064541
https://github.com/scverse/scanpy/issues/1092#issuecomment-623064541:96,Deployability,update,updated,96,"Hi, ; I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain').; I have updated my Scanpy 1.4.6 and anndata to 0.7.1.; I'm getting the following error message.; ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-71-27e22cc8f823> in <module>; ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs); 119 ; 120 for method in embedding_method:; --> 121 ing.map_embedding(method); 122 ; 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method); 407 """"""; 408 if method == 'umap':; --> 409 self._obsm['X_umap'] = self._umap_transform(); 410 elif method == 'pca':; 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self); 396 ; 397 def _umap_transform(self):; --> 398 return self._umap.transform(self._obsm['rep']); 399 ; 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X); 2006 try:; 2007 # sklearn pairwise_distances fails for callable metric on sparse data; -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func; 2009 dmat = pairwise_distances(; 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'; ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092#issuecomment-623064541
https://github.com/scverse/scanpy/issues/1092#issuecomment-623064541:175,Integrability,message,message,175,"Hi, ; I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain').; I have updated my Scanpy 1.4.6 and anndata to 0.7.1.; I'm getting the following error message.; ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-71-27e22cc8f823> in <module>; ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs); 119 ; 120 for method in embedding_method:; --> 121 ing.map_embedding(method); 122 ; 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method); 407 """"""; 408 if method == 'umap':; --> 409 self._obsm['X_umap'] = self._umap_transform(); 410 elif method == 'pca':; 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self); 396 ; 397 def _umap_transform(self):; --> 398 return self._umap.transform(self._obsm['rep']); 399 ; 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X); 2006 try:; 2007 # sklearn pairwise_distances fails for callable metric on sparse data; -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func; 2009 dmat = pairwise_distances(; 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'; ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092#issuecomment-623064541
https://github.com/scverse/scanpy/issues/1092#issuecomment-623084047:19,Availability,Down,Downgrading,19,"Hi, @vikram0010 .; Downgrading umap to 0.39 should help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092#issuecomment-623084047
https://github.com/scverse/scanpy/issues/1094#issuecomment-597708147:61,Deployability,install,installed,61,"There was an issue with the version of ""networkx"" that I had installed. I had version 2.2 and version 2.4 is required as version 2.2 uses old functions in matplotlib.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094#issuecomment-597708147
https://github.com/scverse/scanpy/issues/1094#issuecomment-629808985:49,Availability,down,downgrade,49,I read some forum threads saying that we need to downgrade matplotlib,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1094#issuecomment-629808985
https://github.com/scverse/scanpy/issues/1098#issuecomment-599163401:24,Deployability,release,release,24,"Hmm, there was a recent release of marplotlib that seemed to mess with a lot of our plots. Could you post an example of what you're getting and let us know your scanpy and matplotlib versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098#issuecomment-599163401
https://github.com/scverse/scanpy/issues/1098#issuecomment-599166316:589,Deployability,install,installed,589,"When I import Scanpy, I go this output:; scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.3.2 pandas==0.24.2 scikit-learn==0.21.1 statsmodels==0.10.1 python-igraph==0.7.1+5.3b99dbf6. When I import matplotlib & check version: 3.1.1. When I execute this line:; sc.pl.heatmap(adata, marker_genes_dict, groupby='leiden'). Output is:; ![image](https://user-images.githubusercontent.com/46505353/76695253-1aae7a80-663a-11ea-9fb6-5c4efbe11f3a.png); GridSpec(2, 4, height_ratios=[0.15, 6], width_ratios=[0.2, 4.8, 0, 0.2]). I did not have the issue before, but after I installed several programs because they are needed for running pyVDJ, I go this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098#issuecomment-599166316
https://github.com/scverse/scanpy/issues/1098#issuecomment-599166316:144,Usability,learn,learn,144,"When I import Scanpy, I go this output:; scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.2 scipy==1.3.2 pandas==0.24.2 scikit-learn==0.21.1 statsmodels==0.10.1 python-igraph==0.7.1+5.3b99dbf6. When I import matplotlib & check version: 3.1.1. When I execute this line:; sc.pl.heatmap(adata, marker_genes_dict, groupby='leiden'). Output is:; ![image](https://user-images.githubusercontent.com/46505353/76695253-1aae7a80-663a-11ea-9fb6-5c4efbe11f3a.png); GridSpec(2, 4, height_ratios=[0.15, 6], width_ratios=[0.2, 4.8, 0, 0.2]). I did not have the issue before, but after I installed several programs because they are needed for running pyVDJ, I go this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098#issuecomment-599166316
https://github.com/scverse/scanpy/issues/1098#issuecomment-599167801:128,Deployability,install,install,128,"I can reproduce this plotting issue with matplotlib `v3.1.1`. I think upgrading to `v3.1.3` should fix your problem, i.e. `pip3 install ""matplotlib==3.1.3""`. Unfortunately there's another bug in heatmaps introduced by `3.2.0` that just got fixed on scanpy master (#1090), and hasn't been in a release yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098#issuecomment-599167801
https://github.com/scverse/scanpy/issues/1098#issuecomment-599167801:293,Deployability,release,release,293,"I can reproduce this plotting issue with matplotlib `v3.1.1`. I think upgrading to `v3.1.3` should fix your problem, i.e. `pip3 install ""matplotlib==3.1.3""`. Unfortunately there's another bug in heatmaps introduced by `3.2.0` that just got fixed on scanpy master (#1090), and hasn't been in a release yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098#issuecomment-599167801
https://github.com/scverse/scanpy/issues/1098#issuecomment-599168621:26,Deployability,install,installed,26,"It worked! I guess when I installed pyvdj, I got 3.1.1. Many thx!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1098#issuecomment-599168621
https://github.com/scverse/scanpy/issues/1103#issuecomment-633326096:55,Usability,learn,learning,55,"Hi, I have the same problem, sorry that I just started learning python, so I don't really understand some of the improvements and I also tried some of them, it didn't work. ; Could you please let me know what kind of solution will be good for this issue?. Thanks! ; <img width=""765"" alt=""Screen Shot 2020-05-24 at 17 43 08"" src=""https://user-images.githubusercontent.com/50899584/82768801-15ae3a00-9de6-11ea-9552-88eb59b19405.png"">; <img width=""324"" alt=""Screen Shot 2020-05-24 at 17 43 42"" src=""https://user-images.githubusercontent.com/50899584/82768808-2199fc00-9de6-11ea-8624-61bd67c5aae7.png"">. Yi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1103#issuecomment-633326096
https://github.com/scverse/scanpy/issues/1103#issuecomment-634568718:98,Deployability,install,install,98,This is being fixed in #1210. for the time being you can try to set `dendrogram=False` or you can install the branch with the fix.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1103#issuecomment-634568718
https://github.com/scverse/scanpy/issues/1104#issuecomment-599203000:141,Deployability,update,update,141,"Hi @cartal!. I believe the spatial branch has been merged into master now. So you no longer need the `@spatial` in there. . @giovp could you update this in the tutorial, please?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1104#issuecomment-599203000
https://github.com/scverse/scanpy/pull/1105#issuecomment-599627259:597,Availability,mask,masked,597,"Thanks for the H/T @ivirshup ! I was able to find the library ids and I also added the metadata as suggested. This should do for now. I have also changed the plotting function to make it work with the new `uns` structure. The idea for uns concatenation is exactly that one yes. Basically, if the keys are unique, then concatenate, if they are the same, override and throw a warning. With respect to mixed anndata objects (e.g. one visium adata concatenated with one scRNA-seq), I will just concatenate the obsm and add empty entries to the one missing (like zeros) or something along the lines of masked arrays (although I don't think it's particularly useful in this case).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105#issuecomment-599627259
https://github.com/scverse/scanpy/pull/1105#issuecomment-599924513:873,Availability,mask,masked,873,"No problem!. > The idea for uns concatenation is exactly that one yes. Basically, if the keys are unique, then concatenate, if they are the same, override and throw a warning. I was thinking that this should have multiple modes, chosen by an argument like `merge_uns`. I'm thinking options would be:. * `None`: the default. Maintain current behaviour of just not merging.; * `""unique""`: Only keep values which are uniquely specified; * `""identical""`: Only keep values which are the same in all objects; * `""override""`: Just take the first value from each. You wouldn't have to implement all of these, just one that makes spatial concatenation work for now. > With respect to mixed anndata objects (e.g. one visium adata concatenated with one scRNA-seq), I will just concatenate the obsm and add empty entries to the one missing (like zeros) or something along the lines of masked arrays (although I don't think it's particularly useful in this case). Could there be a `fill_value ` argument here? A way for people to specify what the fill value should be?. I'm mainly against masked arrays since I don't think they're going to work with sparse matrices, and I'm not sure about other array subtypes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105#issuecomment-599924513
https://github.com/scverse/scanpy/pull/1105#issuecomment-599924513:1076,Availability,mask,masked,1076,"No problem!. > The idea for uns concatenation is exactly that one yes. Basically, if the keys are unique, then concatenate, if they are the same, override and throw a warning. I was thinking that this should have multiple modes, chosen by an argument like `merge_uns`. I'm thinking options would be:. * `None`: the default. Maintain current behaviour of just not merging.; * `""unique""`: Only keep values which are uniquely specified; * `""identical""`: Only keep values which are the same in all objects; * `""override""`: Just take the first value from each. You wouldn't have to implement all of these, just one that makes spatial concatenation work for now. > With respect to mixed anndata objects (e.g. one visium adata concatenated with one scRNA-seq), I will just concatenate the obsm and add empty entries to the one missing (like zeros) or something along the lines of masked arrays (although I don't think it's particularly useful in this case). Could there be a `fill_value ` argument here? A way for people to specify what the fill value should be?. I'm mainly against masked arrays since I don't think they're going to work with sparse matrices, and I'm not sure about other array subtypes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105#issuecomment-599924513
https://github.com/scverse/scanpy/pull/1105#issuecomment-600196432:79,Availability,error,errors,79,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:; ```; assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105#issuecomment-600196432
https://github.com/scverse/scanpy/pull/1105#issuecomment-600196432:119,Availability,Error,Error,119,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:; ```; assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105#issuecomment-600196432
https://github.com/scverse/scanpy/pull/1105#issuecomment-600196432:5,Testability,test,tests,5,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:; ```; assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105#issuecomment-600196432
https://github.com/scverse/scanpy/pull/1105#issuecomment-600196432:111,Testability,assert,assert,111,"Some tests are still failing, but not because of `uns/spatial`. They all throw errors along these lines:; ```; assert 'Error: Image files did not match.\n RMS Value: 15.114361035293829\n; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105#issuecomment-600196432
https://github.com/scverse/scanpy/pull/1105#issuecomment-600684687:218,Testability,test,tests,218,"Is that in embedding? I had thought we'd removed all that. There shouldn't be any special meaning to strings starting with `X_` anyways, it's just a convention. I'd like to give this a review, but I'd need to see that tests are passing. Do you know why travis isn't running on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105#issuecomment-600684687
https://github.com/scverse/scanpy/pull/1105#issuecomment-600685036:17,Testability,test,tests,17,"Just re run it, [tests passed](https://travis-ci.org/github/theislab/scanpy/builds/663089197?utm_medium=notification&utm_source=github_status); No sure why it does not appear on github",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105#issuecomment-600685036
https://github.com/scverse/scanpy/pull/1105#issuecomment-601303079:49,Availability,error,errors,49,"> If you change `X_coords` to `coords`, where do errors occur? Could you also point me to where you're seeing this code? I've definitely been using embeddings in `obsm` whose keys don't start with `""X_""`. Right, my bad, switched to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105#issuecomment-601303079
https://github.com/scverse/scanpy/pull/1105#issuecomment-606045652:54,Testability,test,test,54,"Hi @ivirshup this would be ready for review. ; Travis test are failing for some figure in diffusion maps, not sure why though :(. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105#issuecomment-606045652
https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328:114,Deployability,integrat,integrated,114,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python; def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]; G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:; part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]; # then run the optimizer; else:; membership, improv = la.find_partitions_multiplex(**params). for a in adata:; a.obs['multiplex'] = pd.Categorical(membership). ```; where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`.; Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328
https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328:114,Integrability,integrat,integrated,114,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python; def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]; G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:; part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]; # then run the optimizer; else:; membership, improv = la.find_partitions_multiplex(**params). for a in adata:; a.obs['multiplex'] = pd.Categorical(membership). ```; where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`.; Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328
https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328:585,Performance,optimiz,optimizer,585,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python; def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]; G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:; part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]; # then run the optimizer; else:; membership, improv = la.find_partitions_multiplex(**params). for a in adata:; a.obs['multiplex'] = pd.Categorical(membership). ```; where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`.; Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328
https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328:849,Performance,optimiz,optimize,849,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python; def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]; G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:; part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]; # then run the optimizer; else:; membership, improv = la.find_partitions_multiplex(**params). for a in adata:; a.obs['multiplex'] = pd.Categorical(membership). ```; where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`.; Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328
https://github.com/scverse/scanpy/pull/1109#issuecomment-600094624:35,Deployability,integrat,integrated,35,"Closing in favor of #1116, where I integrated the commit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1109#issuecomment-600094624
https://github.com/scverse/scanpy/pull/1109#issuecomment-600094624:35,Integrability,integrat,integrated,35,"Closing in favor of #1116, where I integrated the commit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1109#issuecomment-600094624
https://github.com/scverse/scanpy/pull/1111#issuecomment-629592851:17,Testability,test,test,17,"Could this get a test?. Also, should this be documented somewhere?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1111#issuecomment-629592851
https://github.com/scverse/scanpy/pull/1113#issuecomment-600052786:9,Performance,cache,caches,9,cleaning caches did the trick. let’s see if the absolute path is also needed (rebuilding master): https://travis-ci.org/github/theislab/scanpy/builds/663442852. /edit: it did,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1113#issuecomment-600052786
https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021:1091,Integrability,wrap,wrap,1091," tried with a minimum reproducible example and it seemed to work:; ```python; sc.__version__; >>> '1.4.5.1'; ```; ```python; adata = sc.datasets.pbmc68k_reduced(); sc.pp.neighbors(adata); sc.tl.louvain(adata); sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8); ```; ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object?. Also, does upgrading pandas help?. <details>; <summary> Versions</summary>. ```python; anndata==0.7.1; appnope==0.1.0; attrs==19.3.0; backcall==0.1.0; bleach==3.1.0; certifi==2019.11.28; cycler==0.10.0; decorator==4.4.2; defusedxml==0.6.0; entrypoints==0.3; get-version==2.1; h5py==2.10.0; importlib-metadata==1.5.0; ipykernel==5.1.4; ipython==7.13.0; ipython-genutils==0.2.0; jedi==0.16.0; Jinja2==2.11.1; joblib==0.14.1; json5==0.9.1; jsonschema==3.2.0; jupyter-client==5.3.4; jupyter-core==4.6.1; jupyterlab==1.2.6; jupyterlab-server==1.0.6; kiwisolver==1.1.0; legacy-api-wrap==1.2; leidenalg==0.7.0; llvmlite==0.31.0; louvain==0.6.1; MarkupSafe==1.1.1; matplotlib==3.2.0; mistune==0.8.4; natsort==7.0.1; nbconvert==5.6.1; nbformat==5.0.4; networkx==2.4; notebook==6.0.3; numba==0.48.0; numexpr==2.7.1; numpy==1.18.2; packaging==20.3; pandas==1.0.2; pandocfilters==1.4.2; parso==0.6.1; patsy==0.5.1; pexpect==4.8.0; pickleshare==0.7.5; prometheus-client==0.7.1; prompt-toolkit==3.0.3; ptyprocess==0.6.0; pycairo==1.19.0; Pygments==2.5.2; pyparsing==2.4.6; pyrsistent==0.15.7; python-dateutil==2.8.1; python-igraph==0.7.1.post7; pytz==2019.3; pyzmq==18.1.1; scanpy==1.4.5.1; scikit-learn==0.22.2.post1; scipy==1.4.1; seaborn==0.10.0; Send2Trash==1.5.0; setuptools-scm==3.5.0; six==1.14.0; statsmodels==0.11.1; tables==3.6.1; terminado==0.8.3; testpath==0.4.4; tornado==6.0.4; tqdm==4.43.0; traitlets==4.3.3; umap-learn==0.3.10; wcwidth==0.1.8; webencodings==0.5.1; zipp==2.2.0; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021
https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021:1861,Testability,test,testpath,1861," tried with a minimum reproducible example and it seemed to work:; ```python; sc.__version__; >>> '1.4.5.1'; ```; ```python; adata = sc.datasets.pbmc68k_reduced(); sc.pp.neighbors(adata); sc.tl.louvain(adata); sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8); ```; ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object?. Also, does upgrading pandas help?. <details>; <summary> Versions</summary>. ```python; anndata==0.7.1; appnope==0.1.0; attrs==19.3.0; backcall==0.1.0; bleach==3.1.0; certifi==2019.11.28; cycler==0.10.0; decorator==4.4.2; defusedxml==0.6.0; entrypoints==0.3; get-version==2.1; h5py==2.10.0; importlib-metadata==1.5.0; ipykernel==5.1.4; ipython==7.13.0; ipython-genutils==0.2.0; jedi==0.16.0; Jinja2==2.11.1; joblib==0.14.1; json5==0.9.1; jsonschema==3.2.0; jupyter-client==5.3.4; jupyter-core==4.6.1; jupyterlab==1.2.6; jupyterlab-server==1.0.6; kiwisolver==1.1.0; legacy-api-wrap==1.2; leidenalg==0.7.0; llvmlite==0.31.0; louvain==0.6.1; MarkupSafe==1.1.1; matplotlib==3.2.0; mistune==0.8.4; natsort==7.0.1; nbconvert==5.6.1; nbformat==5.0.4; networkx==2.4; notebook==6.0.3; numba==0.48.0; numexpr==2.7.1; numpy==1.18.2; packaging==20.3; pandas==1.0.2; pandocfilters==1.4.2; parso==0.6.1; patsy==0.5.1; pexpect==4.8.0; pickleshare==0.7.5; prometheus-client==0.7.1; prompt-toolkit==3.0.3; ptyprocess==0.6.0; pycairo==1.19.0; Pygments==2.5.2; pyparsing==2.4.6; pyrsistent==0.15.7; python-dateutil==2.8.1; python-igraph==0.7.1.post7; pytz==2019.3; pyzmq==18.1.1; scanpy==1.4.5.1; scikit-learn==0.22.2.post1; scipy==1.4.1; seaborn==0.10.0; Send2Trash==1.5.0; setuptools-scm==3.5.0; six==1.14.0; statsmodels==0.11.1; tables==3.6.1; terminado==0.8.3; testpath==0.4.4; tornado==6.0.4; tqdm==4.43.0; traitlets==4.3.3; umap-learn==0.3.10; wcwidth==0.1.8; webencodings==0.5.1; zipp==2.2.0; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021
https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021:1700,Usability,learn,learn,1700," tried with a minimum reproducible example and it seemed to work:; ```python; sc.__version__; >>> '1.4.5.1'; ```; ```python; adata = sc.datasets.pbmc68k_reduced(); sc.pp.neighbors(adata); sc.tl.louvain(adata); sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8); ```; ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object?. Also, does upgrading pandas help?. <details>; <summary> Versions</summary>. ```python; anndata==0.7.1; appnope==0.1.0; attrs==19.3.0; backcall==0.1.0; bleach==3.1.0; certifi==2019.11.28; cycler==0.10.0; decorator==4.4.2; defusedxml==0.6.0; entrypoints==0.3; get-version==2.1; h5py==2.10.0; importlib-metadata==1.5.0; ipykernel==5.1.4; ipython==7.13.0; ipython-genutils==0.2.0; jedi==0.16.0; Jinja2==2.11.1; joblib==0.14.1; json5==0.9.1; jsonschema==3.2.0; jupyter-client==5.3.4; jupyter-core==4.6.1; jupyterlab==1.2.6; jupyterlab-server==1.0.6; kiwisolver==1.1.0; legacy-api-wrap==1.2; leidenalg==0.7.0; llvmlite==0.31.0; louvain==0.6.1; MarkupSafe==1.1.1; matplotlib==3.2.0; mistune==0.8.4; natsort==7.0.1; nbconvert==5.6.1; nbformat==5.0.4; networkx==2.4; notebook==6.0.3; numba==0.48.0; numexpr==2.7.1; numpy==1.18.2; packaging==20.3; pandas==1.0.2; pandocfilters==1.4.2; parso==0.6.1; patsy==0.5.1; pexpect==4.8.0; pickleshare==0.7.5; prometheus-client==0.7.1; prompt-toolkit==3.0.3; ptyprocess==0.6.0; pycairo==1.19.0; Pygments==2.5.2; pyparsing==2.4.6; pyrsistent==0.15.7; python-dateutil==2.8.1; python-igraph==0.7.1.post7; pytz==2019.3; pyzmq==18.1.1; scanpy==1.4.5.1; scikit-learn==0.22.2.post1; scipy==1.4.1; seaborn==0.10.0; Send2Trash==1.5.0; setuptools-scm==3.5.0; six==1.14.0; statsmodels==0.11.1; tables==3.6.1; terminado==0.8.3; testpath==0.4.4; tornado==6.0.4; tqdm==4.43.0; traitlets==4.3.3; umap-learn==0.3.10; wcwidth==0.1.8; webencodings==0.5.1; zipp==2.2.0; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021
https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021:1931,Usability,learn,learn,1931," tried with a minimum reproducible example and it seemed to work:; ```python; sc.__version__; >>> '1.4.5.1'; ```; ```python; adata = sc.datasets.pbmc68k_reduced(); sc.pp.neighbors(adata); sc.tl.louvain(adata); sc.tl.rank_genes_groups(adata, 'louvain', method='wilcoxon'); sc.pl.rank_genes_groups_violin(adata, groups='2', n_genes=8); ```; ![image](https://user-images.githubusercontent.com/25887487/76887535-b7635900-6882-11ea-9a1c-65bd7d2e6721.png). Could you provide more inputs on the anndata object?. Also, does upgrading pandas help?. <details>; <summary> Versions</summary>. ```python; anndata==0.7.1; appnope==0.1.0; attrs==19.3.0; backcall==0.1.0; bleach==3.1.0; certifi==2019.11.28; cycler==0.10.0; decorator==4.4.2; defusedxml==0.6.0; entrypoints==0.3; get-version==2.1; h5py==2.10.0; importlib-metadata==1.5.0; ipykernel==5.1.4; ipython==7.13.0; ipython-genutils==0.2.0; jedi==0.16.0; Jinja2==2.11.1; joblib==0.14.1; json5==0.9.1; jsonschema==3.2.0; jupyter-client==5.3.4; jupyter-core==4.6.1; jupyterlab==1.2.6; jupyterlab-server==1.0.6; kiwisolver==1.1.0; legacy-api-wrap==1.2; leidenalg==0.7.0; llvmlite==0.31.0; louvain==0.6.1; MarkupSafe==1.1.1; matplotlib==3.2.0; mistune==0.8.4; natsort==7.0.1; nbconvert==5.6.1; nbformat==5.0.4; networkx==2.4; notebook==6.0.3; numba==0.48.0; numexpr==2.7.1; numpy==1.18.2; packaging==20.3; pandas==1.0.2; pandocfilters==1.4.2; parso==0.6.1; patsy==0.5.1; pexpect==4.8.0; pickleshare==0.7.5; prometheus-client==0.7.1; prompt-toolkit==3.0.3; ptyprocess==0.6.0; pycairo==1.19.0; Pygments==2.5.2; pyparsing==2.4.6; pyrsistent==0.15.7; python-dateutil==2.8.1; python-igraph==0.7.1.post7; pytz==2019.3; pyzmq==18.1.1; scanpy==1.4.5.1; scikit-learn==0.22.2.post1; scipy==1.4.1; seaborn==0.10.0; Send2Trash==1.5.0; setuptools-scm==3.5.0; six==1.14.0; statsmodels==0.11.1; tables==3.6.1; terminado==0.8.3; testpath==0.4.4; tornado==6.0.4; tqdm==4.43.0; traitlets==4.3.3; umap-learn==0.3.10; wcwidth==0.1.8; webencodings==0.5.1; zipp==2.2.0; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-600224021
https://github.com/scverse/scanpy/issues/1114#issuecomment-601076097:156,Availability,error,error,156,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. ; After running your example I will just look into how I created adata to see if I can find the error. ; This is what it looks like now: ; ```; AnnData object with n_obs × n_vars = 2773 × 3783 ; obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'; var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'; obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'; varm: 'PCs'; layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'; ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`; My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-601076097
https://github.com/scverse/scanpy/issues/1114#issuecomment-601076097:499,Availability,error,error,499,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. ; After running your example I will just look into how I created adata to see if I can find the error. ; This is what it looks like now: ; ```; AnnData object with n_obs × n_vars = 2773 × 3783 ; obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'; var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'; obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'; varm: 'PCs'; layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'; ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`; My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-601076097
https://github.com/scverse/scanpy/issues/1114#issuecomment-601076097:38,Deployability,upgrade,upgraded,38,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. ; After running your example I will just look into how I created adata to see if I can find the error. ; This is what it looks like now: ; ```; AnnData object with n_obs × n_vars = 2773 × 3783 ; obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'; var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'; obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'; varm: 'PCs'; layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'; ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`; My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-601076097
https://github.com/scverse/scanpy/issues/1114#issuecomment-601076097:1545,Modifiability,layers,layers,1545,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. ; After running your example I will just look into how I created adata to see if I can find the error. ; This is what it looks like now: ; ```; AnnData object with n_obs × n_vars = 2773 × 3783 ; obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'; var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'; obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'; varm: 'PCs'; layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'; ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`; My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-601076097
https://github.com/scverse/scanpy/issues/1114#issuecomment-615097979:28,Availability,error,error,28,"Yes I am still getting this error, my version of anndata==0.7.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-615097979
https://github.com/scverse/scanpy/issues/1114#issuecomment-627888788:188,Availability,error,error,188,Can you provide an example to reproduce. From this issue #28 it seems to be related to dense matrices. Can try transforming `adata=sc.datasets.pbmc68k_reduced()` to see if you trigger the error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-627888788
https://github.com/scverse/scanpy/pull/1116#issuecomment-600108886:81,Availability,error,errors,81,"@flying-sheep, for this, were you thinking to update `adata.obs_vector` to throw errors with ambiguities , `sc.get.obsdf`, or both?. I'm wondering if there should be some period of deprecation warnings for that. I also think it's fair to consider it a bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116#issuecomment-600108886
https://github.com/scverse/scanpy/pull/1116#issuecomment-600108886:46,Deployability,update,update,46,"@flying-sheep, for this, were you thinking to update `adata.obs_vector` to throw errors with ambiguities , `sc.get.obsdf`, or both?. I'm wondering if there should be some period of deprecation warnings for that. I also think it's fair to consider it a bug.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116#issuecomment-600108886
https://github.com/scverse/scanpy/pull/1116#issuecomment-2435182681:74,Energy Efficiency,power,powerful,74,@flying-sheep do you want to close this in favor of your future much more powerful plotting revamp?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1116#issuecomment-2435182681
https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:924,Availability,down,downstream,924,"thanks for getting this started!. since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691
https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:2533,Integrability,depend,depending,2533,"s of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = np.arcsinh(df / cofactor); return T_cytof. def geometric_transform(df):; '''; implements the scanpy transform originating from ivirshup:multimodal; '''; from scipy.stats.mstats import gmean; T_geometric = np.divide(df, gmean(df + 1, axis=0)); return T_geometric. #optionally, for each of these, similar to some cytof workflows, ; #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range; #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale; #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691
https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:989,Modifiability,variab,variable,989,"thanks for getting this started!. since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691
https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:1062,Testability,test,testing,1062,"thanks for getting this started!. since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691
https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:1533,Testability,test,tested,1533," the question, now that similar panels are being used, which transform makes the most sense in terms of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = np.arcsinh(df / cofactor); return T_cytof. def geometric_transform(df):; '''; implements the scanpy transform originating from ivirshup:multimodal; '''; from scipy.stats.mstats import gmean; T_geometric = np.divide(df, gmean(df + 1, axis=0)); return T_geometric. #optionally, for each of these, similar to some cytof workflows, ; #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range; #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the sca",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691
https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:1760,Testability,log,log,1760,"s of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = np.arcsinh(df / cofactor); return T_cytof. def geometric_transform(df):; '''; implements the scanpy transform originating from ivirshup:multimodal; '''; from scipy.stats.mstats import gmean; T_geometric = np.divide(df, gmean(df + 1, axis=0)); return T_geometric. #optionally, for each of these, similar to some cytof workflows, ; #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range; #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale; #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691
https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:2436,Usability,simpl,simple,2436,"s of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = np.arcsinh(df / cofactor); return T_cytof. def geometric_transform(df):; '''; implements the scanpy transform originating from ivirshup:multimodal; '''; from scipy.stats.mstats import gmean; T_geometric = np.divide(df, gmean(df + 1, axis=0)); return T_geometric. #optionally, for each of these, similar to some cytof workflows, ; #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range; #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale; #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691
https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:407,Availability,reliab,reliable,407,"Here's some initial distribution plots for comparison:. Legend: ; - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![im",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530
https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:794,Availability,error,error,794,"Here's some initial distribution plots for comparison:. Legend: ; - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![im",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530
https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:1203,Energy Efficiency,power,power,1203," cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530
https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:2178,Integrability,inject,injecting,2178,"# simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530
https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:2178,Security,inject,injecting,2178,"# simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530
https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:2692,Security,validat,validated,2692,"# simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530
https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:858,Testability,log,log,858,"Here's some initial distribution plots for comparison:. Legend: ; - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![im",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530
https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:1115,Testability,log,log,1115," cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530
https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:1380,Testability,log,log,1380,"ctice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530
https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:1411,Testability,assert,assert,1411,"ctice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530
https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:1959,Testability,log,log,1959,"# simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![image](https://user-images.githubusercontent.com/20694664/83345643-a9e73800-a2e3-11ea-8303-365fccca16cc.png). not only does this have good smoothing, but it differs in that it is injecting an additional aspect beyond just bringing high values into a linear range; specifically, the centering feature seems to impart an assumption about compositional data, giving higher preference to relative ratios, even if the absolute magnitude might be different -- this has the effect of counteracting cell size, but I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". for the time being though, the last few mentioned are all good candidates to include as transform options",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530
https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:218,Usability,simpl,simply,218,"Here's some initial distribution plots for comparison:. Legend: ; - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![im",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530
https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530:851,Usability,simpl,simple,851,"Here's some initial distribution plots for comparison:. Legend: ; - red lines are 0.5% and 99.5% quantiles, a trick used in some cytof papers to deal with extreme outliers (believed to be technical artifacts); here, I simply use it to move the bulk of the data in visible range for the first two plots; while the values are merely a heuristic, the spirit of it follows nonparametric statistics so is pretty reliable in practice. ### raw (ADT counts):; ![image](https://user-images.githubusercontent.com/20694664/83345454-4956fb80-a2e1-11ea-8ae7-e13dfcc10cac.png). ### geometric mean (as used in Issac's notebook); ![image](https://user-images.githubusercontent.com/20694664/83345468-6f7c9b80-a2e1-11ea-8a42-acad50bfb66b.png). seems to only changes the scale, not the shape, so unless I made an error in implementation... it's probably not useful. ### simple log(n+1) (as used in RNAseq); ![image](https://user-images.githubusercontent.com/20694664/83345487-a05cd080-a2e1-11ea-858e-4d98621d12e6.png). can suffer from discretization at low values... note: even though Seurat/Scanpy/Loupe all use different bases, the log base doesn't really matter; it just changes the scale, not the shape/distinguishing power. ### hyperbolic arcsin (as used in CyTOF); ![image](https://user-images.githubusercontent.com/20694664/83345476-81f6d500-a2e1-11ea-8f68-ddff22ffe853.png). not as noisy as log at low values, and doesn't assert that zeros have to be Laplace smoothed with a pseudocount of +1. ### biexponential family (as used in flow cytometry); ![image](https://user-images.githubusercontent.com/20694664/83345554-6fc96680-a2e2-11ea-8112-3bdc09260e63.png). best smoothing so far in the low counts, because that's what it was designed to do. in this case, it is the newest of this family: `vlog(alpha=0, beta=12, xmax=70000, zmax=1)`; - https://doi.org/10.1002/cyto.a.23017; - https://doi.org/10.1002/cyto.a.22030; - https://doi.org/10.1002/cyto.a.20258. ### centered log ratio (as used in CITEseq paper); ![im",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636429530
https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:729,Integrability,depend,depending,729,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:; - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215
https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:1543,Integrability,inject,injects,1543,"n't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explana",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215
https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:806,Modifiability,extend,extends,806,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:; - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215
https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:1941,Performance,perform,performs,1941,"thout proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215
https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:189,Security,validat,validated,189,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:; - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215
https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:1543,Security,inject,injects,1543,"n't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explana",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215
https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:1722,Security,validat,validation,1722,"n't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explana",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215
https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:2099,Testability,log,log,2099,"thout proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215
https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:2125,Testability,log,logicle,2125,"thout proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215
https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:2164,Testability,test,tested,2164,"-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215
https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:1850,Usability,simpl,simply,1850," effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215
https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:2197,Usability,clear,clear,2197,"-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215
https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:3028,Usability,intuit,intuitively,3028,"tematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier for biologists to intuitively grasp ""negative/positive"" cell types for a particular marker without having to worry about subtle shifts in intensity (and therefore would be a better first transformation to more easily interpret data), there will still be utility for a sophisticated algorithm to take advantage of those subtle intensity shifts in building more confidence about a celltype identity, especially since those subtleties seem plausible (not technical artifacts). . TLDR; these transforms are better for different use-cases, so having a variety will be helpful. Probably lean towards CLR as the default, but explictly document why/when it won't be useful for finding absolute signal subtleties.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215
https://github.com/scverse/scanpy/pull/1117#issuecomment-777280791:80,Availability,avail,available,80,@ivirshup Is the code for all the normalisations (including quantile rescaling) available somewhere?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-777280791
https://github.com/scverse/scanpy/pull/1117#issuecomment-863081651:90,Testability,log,log,90,Very interesting discussion. ; Can someone help me to interpret the axis of the `centered log ratio (as used in CITEseq paper)`? I see that each Antibody has a different scale. How to compare them?; Thanks,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-863081651
https://github.com/scverse/scanpy/pull/1117#issuecomment-1054761336:272,Performance,perform,perform,272,"> Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html. You can still use the CLR transformation function provided in the previous posts to transform the data and perform the rest of the analysis steps using scanpy. Alternatively, you can use the clr transform function from scikit-bio (http://scikit-bio.org/docs/0.4.1/generated/generated/skbio.stats.composition.clr.html). Either way, you will need to convert the adata to pandas dataframe using the `to_df()` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-1054761336
https://github.com/scverse/scanpy/pull/1118#issuecomment-600689830:29,Testability,test,tests,29,"@ivirshup ; Thanks, i'll add tests ofc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118#issuecomment-600689830
https://github.com/scverse/scanpy/pull/1118#issuecomment-600748485:45,Testability,test,test,45,Without fallback to .uns this fails on every test which uses `sc.datasets.` with neighbors precomputed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118#issuecomment-600748485
https://github.com/scverse/scanpy/pull/1118#issuecomment-616757843:28,Usability,feedback,feedback,28,"@Koncopd sorry for the late feedback, but I don't see the ""neighbors_key"" in the scanpy.tl.paga function. It'd be great to make sure that everything that uses the neighbor graph is covered :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118#issuecomment-616757843
https://github.com/scverse/scanpy/pull/1118#issuecomment-616759417:51,Testability,test,tested,51,"Also sc.tl.dpt and sc.tl.diffmap functions are not tested in tests, AFAICS.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118#issuecomment-616759417
https://github.com/scverse/scanpy/pull/1118#issuecomment-616759417:61,Testability,test,tests,61,"Also sc.tl.dpt and sc.tl.diffmap functions are not tested in tests, AFAICS.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1118#issuecomment-616759417
https://github.com/scverse/scanpy/issues/1120#issuecomment-603826676:2,Usability,simpl,simple,2,"A simple way to do it is by creating a new `.obs` categorical column as follows:. ```python; adata = sc.datasets.pbmc68k_reduced(); # create new categorical column called `selection`; adata.obs['selection'] = pd.Categorical((adata.obs_vector('CD3G') > 2) & (adata.obs_vector('CD4') < 3)); # adjust colors; adata.uns['selection_colors'] = ['blue', 'yellow']; sc.pl.umap(adata, color='selection', add_outline=True, s=20); ```; ![image](https://user-images.githubusercontent.com/4964309/77539317-75996a80-6ea1-11ea-8762-bb29b00d8e43.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1120#issuecomment-603826676
https://github.com/scverse/scanpy/issues/1121#issuecomment-604799158:218,Availability,down,downgrading,218,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121#issuecomment-604799158
https://github.com/scverse/scanpy/issues/1121#issuecomment-604799158:47,Deployability,update,update,47,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121#issuecomment-604799158
https://github.com/scverse/scanpy/issues/1121#issuecomment-604799158:89,Deployability,release,releases,89,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121#issuecomment-604799158
https://github.com/scverse/scanpy/issues/1121#issuecomment-604799158:32,Integrability,depend,dependency,32,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121#issuecomment-604799158
https://github.com/scverse/scanpy/pull/1123#issuecomment-603993724:43,Usability,feedback,feedback,43,"Hey @ivirshup , could you please give me a feedback on this if there should be any improvements?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-603993724
https://github.com/scverse/scanpy/pull/1123#issuecomment-604224882:191,Testability,test,test,191,"I'm not super familiar with this code, I had no idea a pie chart could even be used here. @falexwolf or @fidelram may be able to say more here. A few points:. * This should have a regression test, similar to [these](https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/scanpy/tests/test_plotting.py#L742-L774); * Could you give some more details about the benchmark? 14 seconds seems far too slow for that plot. Also, is that plotting right? The pie charts all look the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-604224882
https://github.com/scverse/scanpy/pull/1123#issuecomment-604224882:304,Testability,test,tests,304,"I'm not super familiar with this code, I had no idea a pie chart could even be used here. @falexwolf or @fidelram may be able to say more here. A few points:. * This should have a regression test, similar to [these](https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/scanpy/tests/test_plotting.py#L742-L774); * Could you give some more details about the benchmark? 14 seconds seems far too slow for that plot. Also, is that plotting right? The pie charts all look the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-604224882
https://github.com/scverse/scanpy/pull/1123#issuecomment-604224882:384,Testability,benchmark,benchmark,384,"I'm not super familiar with this code, I had no idea a pie chart could even be used here. @falexwolf or @fidelram may be able to say more here. A few points:. * This should have a regression test, similar to [these](https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/scanpy/tests/test_plotting.py#L742-L774); * Could you give some more details about the benchmark? 14 seconds seems far too slow for that plot. Also, is that plotting right? The pie charts all look the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-604224882
https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387:127,Energy Efficiency,green,green,127,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact).; As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:; ```; for node in nodes:; for pie_fraction in fractions[node]:; ...; ```; I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example; ```; foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}; foo[0] = {'black': 0.5}; ```; the the nodes don't contain the same colors, which user could (although not sure why) specify.; I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387
https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387:49,Testability,test,test,49,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact).; As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:; ```; for node in nodes:; for pie_fraction in fractions[node]:; ...; ```; I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example; ```; foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}; foo[0] = {'black': 0.5}; ```; the the nodes don't contain the same colors, which user could (although not sure why) specify.; I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387
https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387:874,Testability,test,test,874,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact).; As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:; ```; for node in nodes:; for pie_fraction in fractions[node]:; ...; ```; I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example; ```; foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}; foo[0] = {'black': 0.5}; ```; the the nodes don't contain the same colors, which user could (although not sure why) specify.; I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387
https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387:15,Usability,feedback,feedback,15,"Thanks for the feedback. I will add a regression test soon. The plot should be right - there's the color gradient fron blue to green and is a bit more than 1/4 (0.255 to be exact).; As for time, the problem is, that each pie charts are being plotted one by one - that is - there are 2 loops:; ```; for node in nodes:; for pie_fraction in fractions[node]:; ...; ```; I did it because this is the general case of the following matplotlib [example](https://matplotlib.org/3.2.0/gallery/lines_bars_and_markers/scatter_piecharts.html), where they in essence do only `for pie_fraction in fractions`. However, this approach would fail if in the above example; ```; foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}; foo[0] = {'black': 0.5}; ```; the the nodes don't contain the same colors, which user could (although not sure why) specify.; I will test out how much speedup can be gained by using the matplotlib approach (assuming the colors for every node are the same) and get back to you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-604356387
https://github.com/scverse/scanpy/pull/1123#issuecomment-605284449:375,Testability,test,test,375,"Ok, while trying to implement what I've suggested, I realized I made a mistake and it won't work - I can't specify different markers per 1 call of `ax.scatter`. I don't think the speed is a major issue, since the above example is an extreme case (255 categories). Currently, I don't have any trick up my sleeve on how to speed it up. I've also tried including the regression test, but I can't seem to produce an expected figure. I have the default parameters + 40 dpi as it's in `test_plotting.py`, but the plots that I save are always larger for some reason (tried running it from CLI as well, saving the result from the test case [both options failed]). I've tried whether this is related specifically to the pie chart - it isn't - plotting it without still produces larger plots. @ivirshup any idea what I'm doing wrong?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-605284449
https://github.com/scverse/scanpy/pull/1123#issuecomment-605284449:622,Testability,test,test,622,"Ok, while trying to implement what I've suggested, I realized I made a mistake and it won't work - I can't specify different markers per 1 call of `ax.scatter`. I don't think the speed is a major issue, since the above example is an extreme case (255 categories). Currently, I don't have any trick up my sleeve on how to speed it up. I've also tried including the regression test, but I can't seem to produce an expected figure. I have the default parameters + 40 dpi as it's in `test_plotting.py`, but the plots that I save are always larger for some reason (tried running it from CLI as well, saving the result from the test case [both options failed]). I've tried whether this is related specifically to the pie chart - it isn't - plotting it without still produces larger plots. @ivirshup any idea what I'm doing wrong?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-605284449
https://github.com/scverse/scanpy/pull/1123#issuecomment-605811146:63,Usability,simpl,simple,63,"The time to plot seems like an issue to me because it's such a simple plot that ends up being generated. It's not obvious to me what part of making that plot would take a long time to calculate, so maybe something unexpected is happening. I really think @falexwolf or @fidelram are in a better position to give advice on how to implement this plot, and troubleshoot matplotlib. It would be useful to see examples of the output you're getting, along with the code that generated them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-605811146
https://github.com/scverse/scanpy/pull/1123#issuecomment-653803793:200,Availability,robust,robust,200,"@fidelram again sorry for being late. I hope it doesn't matter that I've blackified one of the files (can revert this).; Not sure how relevant this PR will be, since @VolkerBergen has faster and more robust implementation, but as a triage version, it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123#issuecomment-653803793
https://github.com/scverse/scanpy/issues/1125#issuecomment-602818921:72,Deployability,update,update,72,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125#issuecomment-602818921
https://github.com/scverse/scanpy/issues/1125#issuecomment-602818921:233,Deployability,install,installed,233,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125#issuecomment-602818921
https://github.com/scverse/scanpy/issues/1125#issuecomment-602818921:305,Deployability,install,installed,305,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125#issuecomment-602818921
https://github.com/scverse/scanpy/issues/1125#issuecomment-602818921:4,Integrability,depend,dependencies,4,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125#issuecomment-602818921
https://github.com/scverse/scanpy/pull/1127#issuecomment-607236910:27,Modifiability,extend,extend,27,@LuckyMD should be easy to extend the `Plot` class to add this. I will think about it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607236910
https://github.com/scverse/scanpy/pull/1127#issuecomment-607813085:366,Modifiability,extend,extending,366,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607813085
https://github.com/scverse/scanpy/pull/1127#issuecomment-607813085:469,Modifiability,extend,extended,469,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607813085
https://github.com/scverse/scanpy/pull/1127#issuecomment-607813085:35,Usability,undo,undoubtedly,35,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607813085
https://github.com/scverse/scanpy/pull/1127#issuecomment-607888729:543,Deployability,integrat,integrates,543,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**; ```PYTHON; import plotly.graph_objects as go; fig = go.Figure(; data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],; layout=go.Layout(; title=go.layout.Title(text=""A Bar Chart""); ); ); fig.show(); ```; **Plotnine:**; ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap; from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')); + geom_point(); + stat_smooth(method='lm'); + facet_wrap('~gear')); ```; **Altair:**; ```PYTHON; import altair as alt; from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(; x='Horsepower',; y='Miles_per_Gallon',; color='Origin',; tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']; ).interactive(); ```; The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607888729
https://github.com/scverse/scanpy/pull/1127#issuecomment-607888729:543,Integrability,integrat,integrates,543,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**; ```PYTHON; import plotly.graph_objects as go; fig = go.Figure(; data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],; layout=go.Layout(; title=go.layout.Title(text=""A Bar Chart""); ); ); fig.show(); ```; **Plotnine:**; ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap; from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')); + geom_point(); + stat_smooth(method='lm'); + facet_wrap('~gear')); ```; **Altair:**; ```PYTHON; import altair as alt; from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(; x='Horsepower',; y='Miles_per_Gallon',; color='Origin',; tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']; ).interactive(); ```; The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607888729
https://github.com/scverse/scanpy/pull/1127#issuecomment-607888729:220,Modifiability,extend,extended,220,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**; ```PYTHON; import plotly.graph_objects as go; fig = go.Figure(; data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],; layout=go.Layout(; title=go.layout.Title(text=""A Bar Chart""); ); ); fig.show(); ```; **Plotnine:**; ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap; from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')); + geom_point(); + stat_smooth(method='lm'); + facet_wrap('~gear')); ```; **Altair:**; ```PYTHON; import altair as alt; from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(; x='Horsepower',; y='Miles_per_Gallon',; color='Origin',; tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']; ).interactive(); ```; The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607888729
https://github.com/scverse/scanpy/pull/1127#issuecomment-608262723:207,Deployability,update,update,207,"@ivirshup ; > * Could you show some examples of the new additions/ let me know where you are on tutorials?. I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: ; * embedding scatter plots which are separated already; * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this?. > * Could you run `black` over this?. Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-608262723
https://github.com/scverse/scanpy/pull/1127#issuecomment-608262723:357,Testability,test,tests,357,"@ivirshup ; > * Could you show some examples of the new additions/ let me know where you are on tutorials?. I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: ; * embedding scatter plots which are separated already; * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this?. > * Could you run `black` over this?. Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-608262723
https://github.com/scverse/scanpy/pull/1127#issuecomment-608262723:488,Testability,test,tests,488,"@ivirshup ; > * Could you show some examples of the new additions/ let me know where you are on tutorials?. I will do that once we are happy with the current code and naming conventions used. My goal was to update this tutorial https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html but suggestions are welcome. . > * Could you add tests for functionality where the underlying code is changing, but doesn't have coverage yet? I would mostly just like to see more tests of the plotting code. I will try to do that. > * What do you think about the idea of splitting up `_anndata.py` into a few more files? I think it's getting a bit too big, which can make reviewing difficult. We should to that. Currently, as I see it we have two types of plots: ; * embedding scatter plots which are separated already; * the type of plots in this PR that I would describe as `grouping` plots, because they visualize the AnnData matrix subdivided based on a .`obs` column. Any better name for this?. > * Could you run `black` over this?. Will do it at the end. Do we have some style policies for black or the defaults are fine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-608262723
https://github.com/scverse/scanpy/issues/1128#issuecomment-603848540:249,Modifiability,variab,variables,249,"Hi, @andrea-tango ; About the second issue - you dont need pca; you can do something like. ```; # project reference adata to latent dimensions with your autoencoder; adata_ref.obsm['X_latent'] = autoencoder.to_latent(adata_ref.X); # use your latent variables to calculate neighbors; sc.pp.neighbors(adata_ref, use_rep='X_latent'); sc.tl.umap(adata_ref); # project your new adata to latent dimensions with your autoencoder; adata_new.obsm['X_latent'] = autoencoder.to_latent(adata_new.X); sc.tl.ingest(adata_new, adata_ref, embedding_method='umap'); ```. About the first, yes, ingest needs vars in the same order. The ordering thing you describe is definitely not the issue with ingest.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1128#issuecomment-603848540
https://github.com/scverse/scanpy/pull/1130#issuecomment-634565246:259,Deployability,install,installed,259,"I've decided to split the baby a bit here, and now we make sure `ipywidgets` before import `tqdm.auto`. If it's not present, we just use `tqdm`. Unfortunately, I think this can still result in bad progress bars in Jupyterlab unless appropriate extensions are installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130#issuecomment-634565246
https://github.com/scverse/scanpy/pull/1130#issuecomment-634565246:197,Usability,progress bar,progress bars,197,"I've decided to split the baby a bit here, and now we make sure `ipywidgets` before import `tqdm.auto`. If it's not present, we just use `tqdm`. Unfortunately, I think this can still result in bad progress bars in Jupyterlab unless appropriate extensions are installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130#issuecomment-634565246
https://github.com/scverse/scanpy/issues/1133#issuecomment-1739825913:175,Availability,avail,available,175,"Hi, I've recently been searching for the functionalities listed above and came across this issue from 2020. Are there any updates on when these functions might potentially be available? :) Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1133#issuecomment-1739825913
https://github.com/scverse/scanpy/issues/1133#issuecomment-1739825913:122,Deployability,update,updates,122,"Hi, I've recently been searching for the functionalities listed above and came across this issue from 2020. Are there any updates on when these functions might potentially be available? :) Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1133#issuecomment-1739825913
https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:1599,Deployability,update,updates,1599,"nt handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>; <summary> Alternative implementation of scale </summary>. ```python; @singledispatch; def scale(X, *args, **kwargs):; """"""\; Scale data to unit variance and zero mean.; .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735
https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:1365,Energy Efficiency,efficient,efficiently,1365,"nData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>; <summary> Alternative implementation of scale </summary>. ```python; @singledispatch; def scale(X, *args, **kwargs):; """"""\; Scale data to unit variance and zero mean.; .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735
https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:3172,Energy Efficiency,consumption,consumption,3172,"ated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adata.copy() if copy else adata; view_to_actual(adata); adata.X, adata.var[""mean""], adata.var[""std""] = scale(; X, ; zero_center=zero_center, ; max_value=max_value, ; copy=False, # because a copy has already been made, if it were to be made; return_mean_var=True; ); if copy:; return adata. @scale.register(sparse.spmatrix); def scale_sparse(; X, ; *, ; zero_center: bool = True,; copy=False,; **kwargs; ):; # need to add the following here to make inplace logic work; if zero_center:; logg.info(; '... as `zero_center=True`, sparse input is '; 'densified and may lead to large memory consumption'; ); X = X.toarray(); copy = False # Since the data has been copied; return scale_array(X, zero_center=zero_center, copy=copy, **kwargs); ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735
https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:1568,Integrability,Depend,Depending,1568,"nt handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>; <summary> Alternative implementation of scale </summary>. ```python; @singledispatch; def scale(X, *args, **kwargs):; """"""\; Scale data to unit variance and zero mean.; .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735
https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:3462,Integrability,wrap,wrappers,3462,"ated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adata.copy() if copy else adata; view_to_actual(adata); adata.X, adata.var[""mean""], adata.var[""std""] = scale(; X, ; zero_center=zero_center, ; max_value=max_value, ; copy=False, # because a copy has already been made, if it were to be made; return_mean_var=True; ); if copy:; return adata. @scale.register(sparse.spmatrix); def scale_sparse(; X, ; *, ; zero_center: bool = True,; copy=False,; **kwargs; ):; # need to add the following here to make inplace logic work; if zero_center:; logg.info(; '... as `zero_center=True`, sparse input is '; 'densified and may lead to large memory consumption'; ); X = X.toarray(); copy = False # Since the data has been copied; return scale_array(X, zero_center=zero_center, copy=copy, **kwargs); ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735
https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:951,Modifiability,Variab,Variables,951,"iterally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>; <summary> Alternative implementation of scale </summary>. ```python; @singledispatch; def scale(X, *args, **kwargs):; """"""\; Scale data to unit variance and zero mean.; .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735
https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:1317,Modifiability,variab,variables,1317,"nData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>; <summary> Alternative implementation of scale </summary>. ```python; @singledispatch; def scale(X, *args, **kwargs):; """"""\; Scale data to unit variance and zero mean.; .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735
https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:3555,Modifiability,flexible,flexible,3555,"ated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adata.copy() if copy else adata; view_to_actual(adata); adata.X, adata.var[""mean""], adata.var[""std""] = scale(; X, ; zero_center=zero_center, ; max_value=max_value, ; copy=False, # because a copy has already been made, if it were to be made; return_mean_var=True; ); if copy:; return adata. @scale.register(sparse.spmatrix); def scale_sparse(; X, ; *, ; zero_center: bool = True,; copy=False,; **kwargs; ):; # need to add the following here to make inplace logic work; if zero_center:; logg.info(; '... as `zero_center=True`, sparse input is '; 'densified and may lead to large memory consumption'; ); X = X.toarray(); copy = False # Since the data has been copied; return scale_array(X, zero_center=zero_center, copy=copy, **kwargs); ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735
https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:695,Testability,test,tested,695,"> I think that literally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>; <summary> Alternative implementation of scale </summary>. ```python; @singledispatch; def scale(X, *args, **kwargs):; """"""\; Scale data to unit variance and zero mean.; .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735
https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:1973,Testability,log,logg,1973,":; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adata.copy() if copy else adata; view_to_actual(adata); adata.X, adata.var[""mean""], adata.var[""std""] = scale(; X, ; zero_center=zero_center, ; max_value=max_value, ; copy=False, # because a copy has already been made, if it were to be made; return_mean_var=True; ); if copy:; return adata. @scale.register(sparse.spmatrix); def scale_sparse(; X, ; *, ; zero_ce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735
https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:2134,Testability,log,logg,2134,"e set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adata.copy() if copy else adata; view_to_actual(adata); adata.X, adata.var[""mean""], adata.var[""std""] = scale(; X, ; zero_center=zero_center, ; max_value=max_value, ; copy=False, # because a copy has already been made, if it were to be made; return_mean_var=True; ); if copy:; return adata. @scale.register(sparse.spmatrix); def scale_sparse(; X, ; *, ; zero_center: bool = True,; copy=False,; **kwargs; ):; # need to add the following here to make inplace logic work; if zero_center:; logg.info(; '... as `zero_center=True`, sparse ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735
https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:3044,Testability,log,logic,3044,"ated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adata.copy() if copy else adata; view_to_actual(adata); adata.X, adata.var[""mean""], adata.var[""std""] = scale(; X, ; zero_center=zero_center, ; max_value=max_value, ; copy=False, # because a copy has already been made, if it were to be made; return_mean_var=True; ); if copy:; return adata. @scale.register(sparse.spmatrix); def scale_sparse(; X, ; *, ; zero_center: bool = True,; copy=False,; **kwargs; ):; # need to add the following here to make inplace logic work; if zero_center:; logg.info(; '... as `zero_center=True`, sparse input is '; 'densified and may lead to large memory consumption'; ); X = X.toarray(); copy = False # Since the data has been copied; return scale_array(X, zero_center=zero_center, copy=copy, **kwargs); ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735
https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:3073,Testability,log,logg,3073,"ated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adata.copy() if copy else adata; view_to_actual(adata); adata.X, adata.var[""mean""], adata.var[""std""] = scale(; X, ; zero_center=zero_center, ; max_value=max_value, ; copy=False, # because a copy has already been made, if it were to be made; return_mean_var=True; ); if copy:; return adata. @scale.register(sparse.spmatrix); def scale_sparse(; X, ; *, ; zero_center: bool = True,; copy=False,; **kwargs; ):; # need to add the following here to make inplace logic work; if zero_center:; logg.info(; '... as `zero_center=True`, sparse input is '; 'densified and may lead to large memory consumption'; ); X = X.toarray(); copy = False # Since the data has been copied; return scale_array(X, zero_center=zero_center, copy=copy, **kwargs); ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735
https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:3426,Testability,log,logic,3426,"ated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adata.copy() if copy else adata; view_to_actual(adata); adata.X, adata.var[""mean""], adata.var[""std""] = scale(; X, ; zero_center=zero_center, ; max_value=max_value, ; copy=False, # because a copy has already been made, if it were to be made; return_mean_var=True; ); if copy:; return adata. @scale.register(sparse.spmatrix); def scale_sparse(; X, ; *, ; zero_center: bool = True,; copy=False,; **kwargs; ):; # need to add the following here to make inplace logic work; if zero_center:; logg.info(; '... as `zero_center=True`, sparse input is '; 'densified and may lead to large memory consumption'; ); X = X.toarray(); copy = False # Since the data has been copied; return scale_array(X, zero_center=zero_center, copy=copy, **kwargs); ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735
https://github.com/scverse/scanpy/pull/1135#issuecomment-609057722:430,Integrability,wrap,wrap,430,"I'd like to +1 @ivirshup 's prototype there -- perhaps `scale` is not the best example of a function that people would use on data other than anndata, but this would be a good pattern to follow throughout `scanpy`. Not infrequently I run the following workflow:; ```; adata = scanpy.AnnData(data); dpt = scanpy.tl.dpt(adata); del adata; ```; and it would be awfully nice if `dpt` just accepted my data matrix without me having to wrap it in an AnnData object. This could be true for any scanpy function that doesn't require row/column data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-609057722
https://github.com/scverse/scanpy/issues/1136#issuecomment-614491704:116,Deployability,update,updates,116,I'm going to close this since no more information was provided. I'd be happy to re-open this if @ yuxiaokang-source updates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1136#issuecomment-614491704
https://github.com/scverse/scanpy/issues/1139#issuecomment-608251007:140,Testability,log,logging,140,"I think you might be using an out of date version of scanpy. Try updating that. If that fails, please show your version info by running `sc.logging.print_versions()` and pasting the result here. Probable duplicate of: #781",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1139#issuecomment-608251007
https://github.com/scverse/scanpy/issues/1142#issuecomment-608191630:140,Deployability,install,installation,140,"That's weird. Why would cuda be a dependency?. I'm not sure who is maintaining the bioconda recipe, so that might just be wrong. Do the new installation instructions work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-608191630
https://github.com/scverse/scanpy/issues/1142#issuecomment-608191630:34,Integrability,depend,dependency,34,"That's weird. Why would cuda be a dependency?. I'm not sure who is maintaining the bioconda recipe, so that might just be wrong. Do the new installation instructions work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-608191630
https://github.com/scverse/scanpy/issues/1142#issuecomment-608270536:26,Deployability,install,install,26,"I had the issue trying to install it in a new environment when python itself wasn't installed there yet. After having installed python, it worked though. However, uing scanpy within a script doesn't work properly after the installation. I'll have a closer look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-608270536
https://github.com/scverse/scanpy/issues/1142#issuecomment-608270536:84,Deployability,install,installed,84,"I had the issue trying to install it in a new environment when python itself wasn't installed there yet. After having installed python, it worked though. However, uing scanpy within a script doesn't work properly after the installation. I'll have a closer look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-608270536
https://github.com/scverse/scanpy/issues/1142#issuecomment-608270536:118,Deployability,install,installed,118,"I had the issue trying to install it in a new environment when python itself wasn't installed there yet. After having installed python, it worked though. However, uing scanpy within a script doesn't work properly after the installation. I'll have a closer look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-608270536
https://github.com/scverse/scanpy/issues/1142#issuecomment-608270536:223,Deployability,install,installation,223,"I had the issue trying to install it in a new environment when python itself wasn't installed there yet. After having installed python, it worked though. However, uing scanpy within a script doesn't work properly after the installation. I'll have a closer look at that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-608270536
https://github.com/scverse/scanpy/issues/1142#issuecomment-609514112:679,Availability,Avail,Available,679,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```; (base) $ conda activate SCA. (SCA) $ conda --version; conda 4.8.2. (SCA) $ python --version; Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \ ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-609514112
https://github.com/scverse/scanpy/issues/1142#issuecomment-609514112:20,Deployability,install,install,20,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```; (base) $ conda activate SCA. (SCA) $ conda --version; conda 4.8.2. (SCA) $ python --version; Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \ ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-609514112
https://github.com/scverse/scanpy/issues/1142#issuecomment-609514112:236,Deployability,install,install,236,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```; (base) $ conda activate SCA. (SCA) $ conda --version; conda 4.8.2. (SCA) $ python --version; Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \ ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-609514112
https://github.com/scverse/scanpy/issues/1142#issuecomment-609514112:384,Modifiability,flexible,flexible,384,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```; (base) $ conda activate SCA. (SCA) $ conda --version; conda 4.8.2. (SCA) $ python --version; Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \ ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-609514112
https://github.com/scverse/scanpy/issues/1142#issuecomment-609514112:526,Safety,abort,abort,526,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```; (base) $ conda activate SCA. (SCA) $ conda --version; conda 4.8.2. (SCA) $ python --version; Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \ ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-609514112
https://github.com/scverse/scanpy/issues/1142#issuecomment-613475004:76,Deployability,install,install,76,"Make sure you are searching the `conda-forge` channel, too. ; Either `conda install -c conda-forge -c bioconda scanpy` or [configure the default channels](https://bioconda.github.io/user/install.html?highlight=conda%20forge#set-up-channels).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-613475004
https://github.com/scverse/scanpy/issues/1142#issuecomment-613475004:187,Deployability,install,install,187,"Make sure you are searching the `conda-forge` channel, too. ; Either `conda install -c conda-forge -c bioconda scanpy` or [configure the default channels](https://bioconda.github.io/user/install.html?highlight=conda%20forge#set-up-channels).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-613475004
https://github.com/scverse/scanpy/issues/1142#issuecomment-613475004:123,Modifiability,config,configure,123,"Make sure you are searching the `conda-forge` channel, too. ; Either `conda install -c conda-forge -c bioconda scanpy` or [configure the default channels](https://bioconda.github.io/user/install.html?highlight=conda%20forge#set-up-channels).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-613475004
https://github.com/scverse/scanpy/issues/1143#issuecomment-608192740:110,Testability,log,logging,110,"Not sure what's going on here, but it sounds like your environment. Could you post your version info with `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1143#issuecomment-608192740
https://github.com/scverse/scanpy/issues/1146#issuecomment-613471671:99,Availability,avail,available,99,Seems this has been fixed in https://github.com/bioconda/bioconda-recipes/pull/21423. ; `1.4.6` is available again.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1146#issuecomment-613471671
https://github.com/scverse/scanpy/issues/1147#issuecomment-609455598:190,Deployability,install,installing,190,"Seen this recently exactly on a windows laptop. Not sure but sound like something messed up with the environment, are you working on the base env? Try creating a fresh conda environment and installing scanpy there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147#issuecomment-609455598
https://github.com/scverse/scanpy/issues/1147#issuecomment-609495323:361,Availability,error,error,361,"> ; > ; > Seen this recently exactly on a windows laptop. Not sure but sound like something messed up with the environment, are you working on the base env? Try creating a fresh conda environment and installing scanpy there. Thanks for the suggestion, @giovp. Was doing it in the base env earlier. Made a new env and tried it again, but ran into the same exact error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147#issuecomment-609495323
https://github.com/scverse/scanpy/issues/1147#issuecomment-609495323:200,Deployability,install,installing,200,"> ; > ; > Seen this recently exactly on a windows laptop. Not sure but sound like something messed up with the environment, are you working on the base env? Try creating a fresh conda environment and installing scanpy there. Thanks for the suggestion, @giovp. Was doing it in the base env earlier. Made a new env and tried it again, but ran into the same exact error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147#issuecomment-609495323
https://github.com/scverse/scanpy/issues/1147#issuecomment-610413337:229,Availability,error,error,229,"> ; > ; > What's your version of `numba` in this environment?. It's version 0.48.0, @ivirshup. For what it's worth, the build is py38he350917_0 and the source is conda-forge. (In the base environment where I was getting the same error, the version is the same but the build is py37h47e9c7a_0.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147#issuecomment-610413337
https://github.com/scverse/scanpy/pull/1149#issuecomment-613283777:1383,Modifiability,refactor,refactored,1383,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301; > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. ; Now the point would be:; * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior; * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:; * we probably should assume that also `scalefactors` are empty.; * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do.; * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right?. The reason for flipping is that the coords from space ranger are given with upper origin. ```python; sc.pl.embedding(adata_spatial, basis = ""coords""); ```; ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point.; And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149#issuecomment-613283777
https://github.com/scverse/scanpy/pull/1149#issuecomment-613283777:547,Usability,simpl,simple,547,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301; > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. ; Now the point would be:; * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior; * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:; * we probably should assume that also `scalefactors` are empty.; * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do.; * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right?. The reason for flipping is that the coords from space ranger are given with upper origin. ```python; sc.pl.embedding(adata_spatial, basis = ""coords""); ```; ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point.; And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149#issuecomment-613283777
https://github.com/scverse/scanpy/pull/1149#issuecomment-613283777:1875,Usability,feedback,feedback,1875,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301; > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. ; Now the point would be:; * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior; * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:; * we probably should assume that also `scalefactors` are empty.; * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do.; * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldn’t involve flipping the data right?. The reason for flipping is that the coords from space ranger are given with upper origin. ```python; sc.pl.embedding(adata_spatial, basis = ""coords""); ```; ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point.; And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149#issuecomment-613283777
https://github.com/scverse/scanpy/pull/1149#issuecomment-628371370:131,Deployability,update,updates,131,"@giovp, I've thought about this a bit more and think it should be treated as an image rather than a scatterplot. This is so future updates to the spatial plotting function (like plotting hexagons) won't require a special case here. If you can't find a scale factor, could we just say it's 1?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149#issuecomment-628371370
https://github.com/scverse/scanpy/pull/1149#issuecomment-628521440:75,Testability,test,test,75,I think it makes sense. I will open a new PR cause I'll have to change the test as well.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149#issuecomment-628521440
https://github.com/scverse/scanpy/issues/1151#issuecomment-611362371:13,Availability,error,error,13,Exactly same error here.; info:; scanpy==1.4.6 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.5 scipy==1.3.1 pandas==1.0.0 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151#issuecomment-611362371
https://github.com/scverse/scanpy/issues/1151#issuecomment-611362371:130,Usability,learn,learn,130,Exactly same error here.; info:; scanpy==1.4.6 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.5 scipy==1.3.1 pandas==1.0.0 scikit-learn==0.22.1 statsmodels==0.11.0 python-igraph==0.7.1 louvain==0.6.1,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151#issuecomment-611362371
https://github.com/scverse/scanpy/issues/1151#issuecomment-616673759:80,Deployability,update,updated,80,Indeed this is an issue related to an older version of AnnData. Once AnnData is updated the problem is gone.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1151#issuecomment-616673759
https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335:740,Availability,robust,robust,740,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here.; 1. Not all methods have log fold changes (`'logreg'` for example); 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be...; 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335
https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335:163,Testability,log,log,163,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here.; 1. Not all methods have log fold changes (`'logreg'` for example); 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be...; 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335
https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335:183,Testability,log,logreg,183,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here.; 1. Not all methods have log fold changes (`'logreg'` for example); 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be...; 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335
https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335:227,Testability,log,log,227,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here.; 1. Not all methods have log fold changes (`'logreg'` for example); 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be...; 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335
https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335:328,Testability,log,logFC,328,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here.; 1. Not all methods have log fold changes (`'logreg'` for example); 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be...; 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335
https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335:493,Testability,test,test,493,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here.; 1. Not all methods have log fold changes (`'logreg'` for example); 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be...; 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335
https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335:524,Testability,test,test,524,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here.; 1. Not all methods have log fold changes (`'logreg'` for example); 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be...; 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335
https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335:600,Testability,test,test,600,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here.; 1. Not all methods have log fold changes (`'logreg'` for example); 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be...; 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335
https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335:697,Testability,test,test,697,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here.; 1. Not all methods have log fold changes (`'logreg'` for example); 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be...; 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335
https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335:759,Testability,test,test,759,"I think the plotting parameter would make a lot of sense. We should take a few things into account though when determining defaults here.; 1. Not all methods have log fold changes (`'logreg'` for example); 2. Ordering based on log FC will be different than based on the scoring (lowly expressed genes will typically have higher logFC). I'm not sure how meaningful the plot would then be...; 3. We initially didn't have any fold changes or p-values at all, partially because the marker gene DE test setup is ill-defined. You test gene in two groups where the groups are defined based on the genes you test... that will generate inflated p-values. Hence it might be a good idea to only consider the test as a way to order genes rather than a robust statistical test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152#issuecomment-610607335
https://github.com/scverse/scanpy/issues/1152#issuecomment-610639662:25,Testability,log,log,25,> * Not all methods have log fold changes (`'logreg'` for example). This will hopefully be fixed: https://github.com/theislab/scanpy/pull/1081#discussion_r393315428,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152#issuecomment-610639662
https://github.com/scverse/scanpy/issues/1152#issuecomment-610639662:45,Testability,log,logreg,45,> * Not all methods have log fold changes (`'logreg'` for example). This will hopefully be fixed: https://github.com/theislab/scanpy/pull/1081#discussion_r393315428,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152#issuecomment-610639662
https://github.com/scverse/scanpy/issues/1152#issuecomment-610653258:136,Testability,log,logFC,136,"> This will hopefully be fixed: [#1081 (comment)](https://github.com/theislab/scanpy/pull/1081#discussion_r393315428). Interesting... a logFC is not sth that comes naturally out of a logistic regression model, no? Sergei would have to add a separate calculation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152#issuecomment-610653258
https://github.com/scverse/scanpy/issues/1152#issuecomment-610653258:183,Testability,log,logistic,183,"> This will hopefully be fixed: [#1081 (comment)](https://github.com/theislab/scanpy/pull/1081#discussion_r393315428). Interesting... a logFC is not sth that comes naturally out of a logistic regression model, no? Sergei would have to add a separate calculation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152#issuecomment-610653258
https://github.com/scverse/scanpy/issues/1154#issuecomment-610787898:72,Deployability,update,update,72,"Hi, @KabitaBaral1. These happens because of changes in umap 0.4. Please update scanpy to solve this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-610787898
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:112,Availability,error,error,112,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:39,Deployability,update,updated,39,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:606,Deployability,Configurat,Configuration,606,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:3719,Deployability,install,install,3719,"-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); <ipython-input-11-495a6d84c058> in <module>; 1 import os; ----> 2 import scanpy as sc; 3 import numpy as np; 4 import pandas as pd; 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>; 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>; 17 from pandas.api.types import CategoricalDtype; 18 ; ---> 19 from ._settings import settings; 20 from . import logging as logg; 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>; 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional; 8 ; ----> 9 from . import logging; 10 from .logging import _set_log_level, _set_log_file, RootLogger; 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>; 7 from typing import Optional; 8 ; ----> 9 import anndata.logging; 10 ; 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 93 from .compat import pkg_version; 94 ; ---> 95 __version__ = pkg_version(__name__); 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 57 from importlib.metadata import version as v; 58 except ImportError:; ---> 59 from importlib_metadata import version as v; 60 return version.parse(v(package)); 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:3762,Deployability,install,installed,3762,"-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); <ipython-input-11-495a6d84c058> in <module>; 1 import os; ----> 2 import scanpy as sc; 3 import numpy as np; 4 import pandas as pd; 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>; 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>; 17 from pandas.api.types import CategoricalDtype; 18 ; ---> 19 from ._settings import settings; 20 from . import logging as logg; 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>; 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional; 8 ; ----> 9 from . import logging; 10 from .logging import _set_log_level, _set_log_file, RootLogger; 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>; 7 from typing import Optional; 8 ; ----> 9 import anndata.logging; 10 ; 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 93 from .compat import pkg_version; 94 ; ---> 95 __version__ = pkg_version(__name__); 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 57 from importlib.metadata import version as v; 58 except ImportError:; ---> 59 from importlib_metadata import version as v; 60 return version.parse(v(package)); 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:597,Modifiability,config,config,597,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:606,Modifiability,Config,Configuration,606,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:661,Modifiability,config,config,661,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:772,Modifiability,config,config,772,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:802,Modifiability,config,config,802,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:847,Modifiability,config,config,847,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:955,Modifiability,config,config,955,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:1082,Modifiability,config,config,1082,"t scanpy as sc I get the following error:. LookupError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:1153,Safety,detect,detect,1153,"/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); <ipython-input-11-495a6d84c058> in <module>; 1 import os; ----",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:2721,Testability,log,logging,2721,"last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); <ipython-input-11-495a6d84c058> in <module>; 1 import os; ----> 2 import scanpy as sc; 3 import numpy as np; 4 import pandas as pd; 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>; 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>; 17 from pandas.api.types import CategoricalDtype; 18 ; ---> 19 from ._settings import settings; 20 from . import logging as logg; 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>; 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional; 8 ; ----> 9 from . import logging; 10 from .logging import _set_log_level, _set_log_file, RootLogger; 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>; 7 from typing import Optional; 8 ; ----> 9 import anndata.logging; 10 ; 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 93 from .compat import pkg_version; 94 ; ---> 95 __version__ = pkg_version(__name__); 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 57 from importlib.metadata import version as v; 58 except ImportError:; ---> 59 from importlib_metadata import version as v; 60 return version.parse(v(package)); 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the modu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:2732,Testability,log,logg,2732,"last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); <ipython-input-11-495a6d84c058> in <module>; 1 import os; ----> 2 import scanpy as sc; 3 import numpy as np; 4 import pandas as pd; 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>; 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>; 17 from pandas.api.types import CategoricalDtype; 18 ; ---> 19 from ._settings import settings; 20 from . import logging as logg; 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>; 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional; 8 ; ----> 9 from . import logging; 10 from .logging import _set_log_level, _set_log_file, RootLogger; 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>; 7 from typing import Optional; 8 ; ----> 9 import anndata.logging; 10 ; 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 93 from .compat import pkg_version; 94 ; ---> 95 __version__ = pkg_version(__name__); 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 57 from importlib.metadata import version as v; 58 except ImportError:; ---> 59 from importlib_metadata import version as v; 60 return version.parse(v(package)); 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the modu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:2944,Testability,log,logging,2944,"-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); <ipython-input-11-495a6d84c058> in <module>; 1 import os; ----> 2 import scanpy as sc; 3 import numpy as np; 4 import pandas as pd; 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>; 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>; 17 from pandas.api.types import CategoricalDtype; 18 ; ---> 19 from ._settings import settings; 20 from . import logging as logg; 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>; 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional; 8 ; ----> 9 from . import logging; 10 from .logging import _set_log_level, _set_log_file, RootLogger; 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>; 7 from typing import Optional; 8 ; ----> 9 import anndata.logging; 10 ; 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 93 from .compat import pkg_version; 94 ; ---> 95 __version__ = pkg_version(__name__); 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 57 from importlib.metadata import version as v; 58 except ImportError:; ---> 59 from importlib_metadata import version as v; 60 return version.parse(v(package)); 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:2962,Testability,log,logging,2962,"-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); <ipython-input-11-495a6d84c058> in <module>; 1 import os; ----> 2 import scanpy as sc; 3 import numpy as np; 4 import pandas as pd; 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>; 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>; 17 from pandas.api.types import CategoricalDtype; 18 ; ---> 19 from ._settings import settings; 20 from . import logging as logg; 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>; 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional; 8 ; ----> 9 from . import logging; 10 from .logging import _set_log_level, _set_log_file, RootLogger; 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>; 7 from typing import Optional; 8 ; ----> 9 import anndata.logging; 10 ; 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 93 from .compat import pkg_version; 94 ; ---> 95 __version__ = pkg_version(__name__); 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 57 from importlib.metadata import version as v; 58 except ImportError:; ---> 59 from importlib_metadata import version as v; 60 return version.parse(v(package)); 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:3085,Testability,log,logging,3085,"-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); <ipython-input-11-495a6d84c058> in <module>; 1 import os; ----> 2 import scanpy as sc; 3 import numpy as np; 4 import pandas as pd; 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>; 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>; 17 from pandas.api.types import CategoricalDtype; 18 ; ---> 19 from ._settings import settings; 20 from . import logging as logg; 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>; 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional; 8 ; ----> 9 from . import logging; 10 from .logging import _set_log_level, _set_log_file, RootLogger; 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>; 7 from typing import Optional; 8 ; ----> 9 import anndata.logging; 10 ; 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 93 from .compat import pkg_version; 94 ; ---> 95 __version__ = pkg_version(__name__); 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 57 from importlib.metadata import version as v; 58 except ImportError:; ---> 59 from importlib_metadata import version as v; 60 return version.parse(v(package)); 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:3167,Testability,log,logging,3167,"-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); <ipython-input-11-495a6d84c058> in <module>; 1 import os; ----> 2 import scanpy as sc; 3 import numpy as np; 4 import pandas as pd; 5 import loompy as lp. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/__init__.py in <module>; 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/utils.py in <module>; 17 from pandas.api.types import CategoricalDtype; 18 ; ---> 19 from ._settings import settings; 20 from . import logging as logg; 21 import warnings. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/_settings.py in <module>; 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional; 8 ; ----> 9 from . import logging; 10 from .logging import _set_log_level, _set_log_file, RootLogger; 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/logging.py in <module>; 7 from typing import Optional; 8 ; ----> 9 import anndata.logging; 10 ; 11 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 93 from .compat import pkg_version; 94 ; ---> 95 __version__ = pkg_version(__name__); 96 del pkg_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 57 from importlib.metadata import version as v; 58 except ImportError:; ---> 59 from importlib_metadata import version as v; 60 return version.parse(v(package)); 61 . ModuleNotFoundError: No module named 'importlib_metadata'. And, when I try to install the module, it says that I already installed it. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845
https://github.com/scverse/scanpy/issues/1154#issuecomment-611248366:72,Availability,error,error,72,"Hello,; I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1; numpy==1.18.2. The full command and error:; ```; adata_pp = adata.copy(); sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6); sc.pp.log1p(adata_pp); sc.pp.pca(adata_pp, n_comps=15); sc.pp.neighbors(adata_pp); sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5); ```. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-13-50ed2ff29926> in <module>; 4 sc.pp.log1p(adata_pp); 5 sc.pp.pca(adata_pp, n_comps=15); ----> 6 sc.pp.neighbors(adata_pp); 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy); 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,; 94 method=method, metric=metric, metric_kwds=metric_kwds,; ---> 95 random_state=random_state,; 96 ); 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 681 knn_distances,; 682 self._adata.shape[0],; --> 683 self.n_neighbors,; 684 ); 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors); 323 ; --> 324 return distances, connectivities.tocsr(); 325 ; 326 . AttributeError: 'tuple' object has no attribute 'tocsr'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611248366
https://github.com/scverse/scanpy/issues/1154#issuecomment-611248366:212,Availability,error,error,212,"Hello,; I am also getting the ""'tuple' object has no attribute 'tocsr'"" error and appreciate your help with that. I am using the latest scanpy and numpy:. scanpy==1.4.4.post1; numpy==1.18.2. The full command and error:; ```; adata_pp = adata.copy(); sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6); sc.pp.log1p(adata_pp); sc.pp.pca(adata_pp, n_comps=15); sc.pp.neighbors(adata_pp); sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5); ```. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-13-50ed2ff29926> in <module>; 4 sc.pp.log1p(adata_pp); 5 sc.pp.pca(adata_pp, n_comps=15); ----> 6 sc.pp.neighbors(adata_pp); 7 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy); 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,; 94 method=method, metric=metric, metric_kwds=metric_kwds,; ---> 95 random_state=random_state,; 96 ); 97 adata.uns['neighbors'] = {}. Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 681 knn_distances,; 682 self._adata.shape[0],; --> 683 self.n_neighbors,; 684 ); 685 # overwrite the umap connectivities if method is 'gauss'. /Anaconda_python3/lib/python3.7/site-packages/scanpy/neighbors/__init__.py in compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 322 distances = get_sparse_matrix_from_indices_distances_umap(knn_indices, knn_dists, n_obs, n_neighbors); 323 ; --> 324 return distances, connectivities.tocsr(); 325 ; 326 . AttributeError: 'tuple' object has no attribute 'tocsr'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611248366
https://github.com/scverse/scanpy/issues/1154#issuecomment-611273954:60,Deployability,update,update,60,"Hi @Koncopd I tried updating scanpy, but it does not let me update from 1.4.4.post1. How do I update it to 1.4.6 using conda?. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611273954
https://github.com/scverse/scanpy/issues/1154#issuecomment-611273954:94,Deployability,update,update,94,"Hi @Koncopd I tried updating scanpy, but it does not let me update from 1.4.4.post1. How do I update it to 1.4.6 using conda?. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611273954
https://github.com/scverse/scanpy/issues/1154#issuecomment-611324832:28,Deployability,update,update,28,"Hi @KabitaBaral1 ,; You can update it using:; ```; pip install --upgrade scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611324832
https://github.com/scverse/scanpy/issues/1154#issuecomment-611324832:55,Deployability,install,install,55,"Hi @KabitaBaral1 ,; You can update it using:; ```; pip install --upgrade scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611324832
https://github.com/scverse/scanpy/issues/1154#issuecomment-611324832:65,Deployability,upgrade,upgrade,65,"Hi @KabitaBaral1 ,; You can update it using:; ```; pip install --upgrade scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611324832
https://github.com/scverse/scanpy/issues/1154#issuecomment-613413914:32,Deployability,update,update,32,"> Hi @KabitaBaral1 ,; > You can update it using:; > ; > ```; > pip install --upgrade scanpy; > ```; in conda; ```; conda install -c bioconda scanpy=1.4.6; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-613413914
https://github.com/scverse/scanpy/issues/1154#issuecomment-613413914:67,Deployability,install,install,67,"> Hi @KabitaBaral1 ,; > You can update it using:; > ; > ```; > pip install --upgrade scanpy; > ```; in conda; ```; conda install -c bioconda scanpy=1.4.6; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-613413914
https://github.com/scverse/scanpy/issues/1154#issuecomment-613413914:77,Deployability,upgrade,upgrade,77,"> Hi @KabitaBaral1 ,; > You can update it using:; > ; > ```; > pip install --upgrade scanpy; > ```; in conda; ```; conda install -c bioconda scanpy=1.4.6; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-613413914
https://github.com/scverse/scanpy/issues/1154#issuecomment-613413914:121,Deployability,install,install,121,"> Hi @KabitaBaral1 ,; > You can update it using:; > ; > ```; > pip install --upgrade scanpy; > ```; in conda; ```; conda install -c bioconda scanpy=1.4.6; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-613413914
https://github.com/scverse/scanpy/issues/1154#issuecomment-614331672:11,Availability,error,error,11,"I get same error as @KabitaBaral1 after updating to scanp==1.4.6, namely: `AttributeError: module 'cairo' has no attribute 'version_info'` also see the issue #1166",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-614331672
https://github.com/scverse/scanpy/issues/1154#issuecomment-614611920:81,Deployability,install,installation,81,"Great! I'm going to close this then, since the discussion seems to be more about installation issues now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-614611920
https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994:18,Availability,error,error,18,"I am getting this error when I run scanpy.pp.neighbors(adata); As far as I know, I have the latest packages mentioned here.; anndata 0.7.6 pypi_0 pypi; louvain 0.7.0 py38h9dedd22_1 conda-forge; pandas 1.1.3 py38hb1e8313_0; python-igraph 0.9.1 py38h3dab7cd_0 conda-forge; scanpy 1.7.2 pypi_0 pypi; scikit-learn 0.23.2 py38h959d312_0; scipy 1.6.3 py38h431c0a8_0 conda-forge; statsmodels 0.12.0 py38haf1e3a3_0; umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated?. **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994
https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994:537,Deployability,update,updated,537,"I am getting this error when I run scanpy.pp.neighbors(adata); As far as I know, I have the latest packages mentioned here.; anndata 0.7.6 pypi_0 pypi; louvain 0.7.0 py38h9dedd22_1 conda-forge; pandas 1.1.3 py38hb1e8313_0; python-igraph 0.9.1 py38h3dab7cd_0 conda-forge; scanpy 1.7.2 pypi_0 pypi; scikit-learn 0.23.2 py38h959d312_0; scipy 1.6.3 py38h431c0a8_0 conda-forge; statsmodels 0.12.0 py38haf1e3a3_0; umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated?. **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994
https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994:576,Deployability,update,update,576,"I am getting this error when I run scanpy.pp.neighbors(adata); As far as I know, I have the latest packages mentioned here.; anndata 0.7.6 pypi_0 pypi; louvain 0.7.0 py38h9dedd22_1 conda-forge; pandas 1.1.3 py38hb1e8313_0; python-igraph 0.9.1 py38h3dab7cd_0 conda-forge; scanpy 1.7.2 pypi_0 pypi; scikit-learn 0.23.2 py38h959d312_0; scipy 1.6.3 py38h431c0a8_0 conda-forge; statsmodels 0.12.0 py38haf1e3a3_0; umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated?. **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994
https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994:304,Usability,learn,learn,304,"I am getting this error when I run scanpy.pp.neighbors(adata); As far as I know, I have the latest packages mentioned here.; anndata 0.7.6 pypi_0 pypi; louvain 0.7.0 py38h9dedd22_1 conda-forge; pandas 1.1.3 py38hb1e8313_0; python-igraph 0.9.1 py38h3dab7cd_0 conda-forge; scanpy 1.7.2 pypi_0 pypi; scikit-learn 0.23.2 py38h959d312_0; scipy 1.6.3 py38h431c0a8_0 conda-forge; statsmodels 0.12.0 py38haf1e3a3_0; umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated?. **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994
https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994:413,Usability,learn,learn,413,"I am getting this error when I run scanpy.pp.neighbors(adata); As far as I know, I have the latest packages mentioned here.; anndata 0.7.6 pypi_0 pypi; louvain 0.7.0 py38h9dedd22_1 conda-forge; pandas 1.1.3 py38hb1e8313_0; python-igraph 0.9.1 py38h3dab7cd_0 conda-forge; scanpy 1.7.2 pypi_0 pypi; scikit-learn 0.23.2 py38h959d312_0; scipy 1.6.3 py38h431c0a8_0 conda-forge; statsmodels 0.12.0 py38haf1e3a3_0; umap-learn 0.5.1 py38h50d1736_0 conda-forge. Is there probably another package that is outdated?. **EDIT: Not sure what module I updated but now it works. I use 'conda update --all' and others to do that.** . thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-835038994
https://github.com/scverse/scanpy/issues/1154#issuecomment-860196125:148,Usability,learn,learn,148,"I face the same problem. However I used latest version ; </scanpy==1.4.6 anndata==0.7.4 umap==0.5.1 numpy==1.20.3 scipy==1.6.3 pandas==1.2.4 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.2 , bbknn : 1.5.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-860196125
https://github.com/scverse/scanpy/pull/1156#issuecomment-614656197:422,Performance,perform,performance,422,"Hi, @ivirshup . Thanks for the review. I'll address the comments soon.; No, this currently doesn't deal with correctness, tie correction and the other things in the issues, just general structure. But i'll definitely turn to them after this. About the reference thing. Yes, you are right, i can use this approach of course. However, i didn't want to store the same value multiple times, it doesn't make much difference in performance, but still gives uneasy feelings, i would say. upd: oh, i see that `numpy.broadcast_to` creates a view, so it should avoid the problem of storing the same value multiple times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-614656197
https://github.com/scverse/scanpy/pull/1156#issuecomment-614656197:551,Safety,avoid,avoid,551,"Hi, @ivirshup . Thanks for the review. I'll address the comments soon.; No, this currently doesn't deal with correctness, tie correction and the other things in the issues, just general structure. But i'll definitely turn to them after this. About the reference thing. Yes, you are right, i can use this approach of course. However, i didn't want to store the same value multiple times, it doesn't make much difference in performance, but still gives uneasy feelings, i would say. upd: oh, i see that `numpy.broadcast_to` creates a view, so it should avoid the problem of storing the same value multiple times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-614656197
https://github.com/scverse/scanpy/pull/1156#issuecomment-615322862:265,Testability,log,logfoldchanges,265,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-615322862
https://github.com/scverse/scanpy/pull/1156#issuecomment-615322862:420,Testability,log,logfoldchanges,420,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-615322862
https://github.com/scverse/scanpy/pull/1156#issuecomment-615322862:708,Testability,log,logfoldchanges,708,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-615322862
https://github.com/scverse/scanpy/pull/1156#issuecomment-615322862:876,Usability,simpl,simple,876,"Thanks so much for the amazing work @Koncopd 🎊. I think it'd be more convenient to have the same type of ""keys"" regardless of the provided `reference` option in sc.tl.rank_genes_groups. Right now I get . `['params', 'pts', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . keys in .uns['rank_genes_groups'], if I set a reference and . `['params', 'pts', 'pts_rest', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']`. if I don't. Wouldn't it be better to replace all *_rest in the code with *_reference, and calculate them based on whatever the reference is, rest or a specific group? . Then we can provide . `['params', 'pts', 'pts_reference', 'scores', 'names', 'pvals', 'pvals_adj', 'logfoldchanges']` . without thinking about what the reference is. Seurat reports pct1 for the first group and pct2 for the second group, for example, which is nice and simple, IMO. Also `sc.get.rank_genes_groups_df` should return pct and pct_reference. It'd be also great to have `pct` cutoffs in the `sc.get.rank_genes_groups_df` function like `pct_min`. . People typically define DE genes using cutoffs like pval_adj < 0.05, log2fc>1, pct>0.1. Edit: Just noticed similar comments from @ivirshup, sorry about replicating :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-615322862
https://github.com/scverse/scanpy/pull/1156#issuecomment-616484573:448,Usability,simpl,simple,448,"Thank you all for this! In particular, @Koncopd!. I know that this will cause a little more headache, but could we consider renaming to `rank_genes`?. The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-616484573
https://github.com/scverse/scanpy/pull/1156#issuecomment-616484573:802,Usability,feedback,feedback,802,"Thank you all for this! In particular, @Koncopd!. I know that this will cause a little more headache, but could we consider renaming to `rank_genes`?. The function has its name with a `_groups` suffix from very early considerations about distinguishing it from a second function that would rank genes by fitting a model on time or pseudotime. But I don't see that function ever coming within the current Scanpy main API. So, it'd be nice to have a simple and short name. As backward compat will be broken to some degree anyway in 1.5, and a small converter for legacy `h5ad` (in the scanpy read function - renaming `.uns['rank_genes_groups']` to `.uns['rank_genes']` and making entries a dataframe instead of structured arrays). It'd be a good point in time, now. Please slack me if you really want my feedback. Sorry that I'm so absent. 😔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-616484573
https://github.com/scverse/scanpy/pull/1156#issuecomment-616490080:583,Integrability,wrap,wrapper,583,"Instead of having `self.d` as the central attribute, why not having a central property `self.stats` and that's a DataFrame with a multi-index for the columns. The outer index is the group that you're comparing against a reference, the inner index loops over `gene`, `score`, `pval`, `pval_adj`, etc. Meaning, there is a class for this besides the convenience function:; ```; rg = sc.RankGenes(); ```. And if you just want a dataframe and not store something in `adata`, you can do; ```; rg.compute_stats(...); ```. After this `rg.stats` contains your results. Within the convenience wrapper `sc.tl.rank_genes`, this `stats` attribute will be written to `adata`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-616490080
https://github.com/scverse/scanpy/pull/1156#issuecomment-616525603:191,Deployability,release,releases,191,"> I know that this will cause a little more headache, but could we consider renaming to `rank_genes`?. Would you not maybe do this in stages with a `DeprecationWarning` first for a couple of releases? This change would break nearly everyone's pipelines and published notebooks. It's not a lot of work to change... but it might warrant a longer warning time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-616525603
https://github.com/scverse/scanpy/pull/1156#issuecomment-616525603:243,Deployability,pipeline,pipelines,243,"> I know that this will cause a little more headache, but could we consider renaming to `rank_genes`?. Would you not maybe do this in stages with a `DeprecationWarning` first for a couple of releases? This change would break nearly everyone's pipelines and published notebooks. It's not a lot of work to change... but it might warrant a longer warning time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-616525603
https://github.com/scverse/scanpy/pull/1156#issuecomment-620393866:915,Testability,test,test,915,"> adata.rename_categories doesn't work with dataframes in uns. * What problem is this causing?; * @falexwolf, why did this method get un-deprecated?. > Looks fine with miltindex dataframe. However a bit awkward with this names column when n_genes is set. I will set n_genes to all by default but do we need this at all?. I personally find MultiIndexes a bit hard to work with. Could you show how they would be used here? For example, how would I just get a dataframe for the naive T cells vs. rest?. I'm also not sure I get why we'd order genes by rank, when there are multiple comparisons in the table. What operations does this make easier?. **Most importantly**, I don't think we have support for reading and writing multi indexes in anndata. An alternative would be to just have an entry in `uns` that looked like:. ```python; adata.uns[key_added] = {; ""params"": {; ""groupby"": ""leiden"",; ""reference"": ""rest"",; ""test"": ""wilcoxon"",; ""rep"": ""X""; },; ""results"": {; ""1"": ..., # pd.DataFrame, with index of .var_names; ""2"": ..., #etc; },; }; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-620393866
https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020:212,Energy Efficiency,adapt,adapting,212,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020
https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020:143,Integrability,message,message,143,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020
https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020:212,Modifiability,adapt,adapting,212,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020
https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020:74,Usability,simpl,simpler,74,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020
https://github.com/scverse/scanpy/issues/1157#issuecomment-614700185:632,Modifiability,layers,layers,632,"A few things!. 1. What version is UMAP and numba?. 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data.; ; 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-614700185
https://github.com/scverse/scanpy/issues/1157#issuecomment-614700185:825,Modifiability,layers,layers,825,"A few things!. 1. What version is UMAP and numba?. 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data.; ; 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-614700185
https://github.com/scverse/scanpy/issues/1157#issuecomment-614700185:921,Modifiability,layers,layers,921,"A few things!. 1. What version is UMAP and numba?. 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data.; ; 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-614700185
https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989:1135,Availability,error,error,1135,"So it will only work on non-negative expression values without any pre-process?; I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing?. I tried removed all the antibody read counts from adata.X and ran it once, still got same error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989
https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989:1141,Integrability,message,message,1141,"So it will only work on non-negative expression values without any pre-process?; I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing?. I tried removed all the antibody read counts from adata.X and ran it once, still got same error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989
https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989:457,Testability,test,test,457,"So it will only work on non-negative expression values without any pre-process?; I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing?. I tried removed all the antibody read counts from adata.X and ran it once, still got same error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989
https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989:249,Usability,learn,learn,249,"So it will only work on non-negative expression values without any pre-process?; I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing?. I tried removed all the antibody read counts from adata.X and ran it once, still got same error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989
https://github.com/scverse/scanpy/issues/1157#issuecomment-615359662:72,Deployability,update,update,72,I fixed this issue!. It should be all good in SAM version 0.7.2. Please update SAM using either pip or github. Let me know if there are still any other problems.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-615359662
https://github.com/scverse/scanpy/issues/1158#issuecomment-614525787:65,Deployability,update,update,65,"Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again?; For me it seems to work; ```python; fig, ax = plt.subplots(1,3, figsize=(20,6)); sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False); sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False); sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False); plt.tight_layout(pad=3.0); plt.show(); ```; ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-614525787
https://github.com/scverse/scanpy/issues/1158#issuecomment-614676325:2,Deployability,update,updated,2,"I updated scanpy:. ```; pip install git+https://github.com/theislab/scanpy.git; pip install spatialde; ```. ```; Successfully built scanpy; Installing collected packages: scanpy; Attempting uninstall: scanpy; Found existing installation: scanpy 1.4.5.2.dev38+gae88b949; Uninstalling scanpy-1.4.5.2.dev38+gae88b949:; Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949; Successfully installed scanpy-1.4.7.dev47+gf6a49e81; ```. Now I'm not able to generate a spatial plot at all. ```; ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs); 763 """"""; 764 if library_id is _empty:; --> 765 library_id = next((i for i in adata.uns['spatial'].keys())); 766 else:; 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-614676325
https://github.com/scverse/scanpy/issues/1158#issuecomment-614676325:28,Deployability,install,install,28,"I updated scanpy:. ```; pip install git+https://github.com/theislab/scanpy.git; pip install spatialde; ```. ```; Successfully built scanpy; Installing collected packages: scanpy; Attempting uninstall: scanpy; Found existing installation: scanpy 1.4.5.2.dev38+gae88b949; Uninstalling scanpy-1.4.5.2.dev38+gae88b949:; Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949; Successfully installed scanpy-1.4.7.dev47+gf6a49e81; ```. Now I'm not able to generate a spatial plot at all. ```; ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs); 763 """"""; 764 if library_id is _empty:; --> 765 library_id = next((i for i in adata.uns['spatial'].keys())); 766 else:; 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-614676325
https://github.com/scverse/scanpy/issues/1158#issuecomment-614676325:84,Deployability,install,install,84,"I updated scanpy:. ```; pip install git+https://github.com/theislab/scanpy.git; pip install spatialde; ```. ```; Successfully built scanpy; Installing collected packages: scanpy; Attempting uninstall: scanpy; Found existing installation: scanpy 1.4.5.2.dev38+gae88b949; Uninstalling scanpy-1.4.5.2.dev38+gae88b949:; Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949; Successfully installed scanpy-1.4.7.dev47+gf6a49e81; ```. Now I'm not able to generate a spatial plot at all. ```; ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs); 763 """"""; 764 if library_id is _empty:; --> 765 library_id = next((i for i in adata.uns['spatial'].keys())); 766 else:; 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-614676325
https://github.com/scverse/scanpy/issues/1158#issuecomment-614676325:140,Deployability,Install,Installing,140,"I updated scanpy:. ```; pip install git+https://github.com/theislab/scanpy.git; pip install spatialde; ```. ```; Successfully built scanpy; Installing collected packages: scanpy; Attempting uninstall: scanpy; Found existing installation: scanpy 1.4.5.2.dev38+gae88b949; Uninstalling scanpy-1.4.5.2.dev38+gae88b949:; Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949; Successfully installed scanpy-1.4.7.dev47+gf6a49e81; ```. Now I'm not able to generate a spatial plot at all. ```; ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs); 763 """"""; 764 if library_id is _empty:; --> 765 library_id = next((i for i in adata.uns['spatial'].keys())); 766 else:; 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-614676325
https://github.com/scverse/scanpy/issues/1158#issuecomment-614676325:224,Deployability,install,installation,224,"I updated scanpy:. ```; pip install git+https://github.com/theislab/scanpy.git; pip install spatialde; ```. ```; Successfully built scanpy; Installing collected packages: scanpy; Attempting uninstall: scanpy; Found existing installation: scanpy 1.4.5.2.dev38+gae88b949; Uninstalling scanpy-1.4.5.2.dev38+gae88b949:; Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949; Successfully installed scanpy-1.4.7.dev47+gf6a49e81; ```. Now I'm not able to generate a spatial plot at all. ```; ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs); 763 """"""; 764 if library_id is _empty:; --> 765 library_id = next((i for i in adata.uns['spatial'].keys())); 766 else:; 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-614676325
https://github.com/scverse/scanpy/issues/1158#issuecomment-614676325:386,Deployability,install,installed,386,"I updated scanpy:. ```; pip install git+https://github.com/theislab/scanpy.git; pip install spatialde; ```. ```; Successfully built scanpy; Installing collected packages: scanpy; Attempting uninstall: scanpy; Found existing installation: scanpy 1.4.5.2.dev38+gae88b949; Uninstalling scanpy-1.4.5.2.dev38+gae88b949:; Successfully uninstalled scanpy-1.4.5.2.dev38+gae88b949; Successfully installed scanpy-1.4.7.dev47+gf6a49e81; ```. Now I'm not able to generate a spatial plot at all. ```; ----> 5 sc.pl.spatial(adata, img_key=""hires"", color=""clusters"", size=1.5). ~/opt/anaconda3/envs/scanpy/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py in spatial(adata, img_key, library_id, crop_coord, alpha_img, bw, **kwargs); 763 """"""; 764 if library_id is _empty:; --> 765 library_id = next((i for i in adata.uns['spatial'].keys())); 766 else:; 767 if library_id not in adata.uns['spatial'].keys():. KeyError: 'spatial'. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-614676325
https://github.com/scverse/scanpy/issues/1158#issuecomment-616214678:283,Testability,test,tested,283,"You can have a look at the new structure in the function definition [here](https://github.com/theislab/scanpy/blob/4156314407c5368fa0b66ac18470d80f3748a71f/scanpy/readwrite.py#L281) . . Regarding multiple samples support, that is on its way! Will post here as soon as it's ready and tested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-616214678
https://github.com/scverse/scanpy/issues/1158#issuecomment-640496084:112,Deployability,integrat,integration-scanorama,112,"@vitkl now multiple samples are supported, see [here](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html) for description on how to use the new concat strategy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-640496084
https://github.com/scverse/scanpy/issues/1158#issuecomment-640496084:112,Integrability,integrat,integration-scanorama,112,"@vitkl now multiple samples are supported, see [here](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html) for description on how to use the new concat strategy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-640496084
https://github.com/scverse/scanpy/issues/1158#issuecomment-640632513:229,Deployability,integrat,integration-scanorama,229,"Nice! Thanks!. On Mon, 8 Jun 2020, 10:46 giovp, <notifications@github.com> wrote:. > @vitkl <https://github.com/vitkl> now multiple samples are supported, see; > here; > <https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html>; > for description on how to use the new concat strategy; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1158#issuecomment-640496084>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AFMFTVZVVWII7Z7Q34ZPTQ3RVSXOVANCNFSM4MEXUAPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-640632513
https://github.com/scverse/scanpy/issues/1158#issuecomment-640632513:229,Integrability,integrat,integration-scanorama,229,"Nice! Thanks!. On Mon, 8 Jun 2020, 10:46 giovp, <notifications@github.com> wrote:. > @vitkl <https://github.com/vitkl> now multiple samples are supported, see; > here; > <https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html>; > for description on how to use the new concat strategy; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1158#issuecomment-640496084>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AFMFTVZVVWII7Z7Q34ZPTQ3RVSXOVANCNFSM4MEXUAPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-640632513
https://github.com/scverse/scanpy/issues/1158#issuecomment-1454679327:67,Deployability,update,update,67,"> Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work; > ; > ```python; > fig, ax = plt.subplots(1,3, figsize=(20,6)); > sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False); > sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False); > sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False); > plt.tight_layout(pad=3.0); > plt.show(); > ```; > ; > ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png). Hello, I have a problem, that is why some plots show colorbar but other plots show legend? It seems using same code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-1454679327
https://github.com/scverse/scanpy/issues/1158#issuecomment-1454702754:69,Deployability,update,update,69,"> > Sorry for late reply, I think this was fixed in #1138. Could you update your scanpy and try again? For me it seems to work; > > ```python; > > fig, ax = plt.subplots(1,3, figsize=(20,6)); > > sc.pl.spatial(adata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[0], show=False); > > sc.pl.spatial(bdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[1], show=False); > > sc.pl.spatial(cdata, img_key=""hires"", color=""array_row"", size=1.5, ax=ax[2], show=False); > > plt.tight_layout(pad=3.0); > > plt.show(); > > ```; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ; > > ![image](https://user-images.githubusercontent.com/25887487/79438766-41165b80-7fd4-11ea-8ed7-f297b22da7c0.png); > ; > Hello, I have a problem, that is why some plots show colorbar but other plots show legend? It seems using same code. I've got the reason: if the data type is category, sc.pl.spetial would append lengend, in others conditon it would append colorbar.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-1454702754
https://github.com/scverse/scanpy/pull/1160#issuecomment-613874802:155,Deployability,release,release,155,"Thanks for catching this! Could you add a test so this doesn't happen again in the future?. Also, I believe the tests that were failing were due to a umap release, not anything you changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-613874802
https://github.com/scverse/scanpy/pull/1160#issuecomment-613874802:42,Testability,test,test,42,"Thanks for catching this! Could you add a test so this doesn't happen again in the future?. Also, I believe the tests that were failing were due to a umap release, not anything you changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-613874802
https://github.com/scverse/scanpy/pull/1160#issuecomment-613874802:112,Testability,test,tests,112,"Thanks for catching this! Could you add a test so this doesn't happen again in the future?. Also, I believe the tests that were failing were due to a umap release, not anything you changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-613874802
https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:449,Availability,error,errors,449,"Just added a test and changed the behaviour of scale a little more.; The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`.; Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/cus",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330
https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1212,Availability,error,error,1212," `scale[scale == 0] = 1e-12`.; Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ""9""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl; if min_ver i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330
https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1515,Availability,error,errors,1515,"odepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ""9""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl; if min_ver is not None and version < min_ver:; ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__; c = self._cmp(other); ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp; if self.version < other.version:; E TypeError: '<' not supported between instances of 'str' and 'int''`; I have the current ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330
https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1561,Availability,ERROR,ERROR,1561,"odepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ""9""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl; if min_ver is not None and version < min_ver:; ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__; c = self._cmp(other); ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp; if self.version < other.version:; E TypeError: '<' not supported between instances of 'str' and 'int''`; I have the current ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330
https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:13,Testability,test,test,13,"Just added a test and changed the behaviour of scale a little more.; The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`.; Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/cus",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330
https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1097,Testability,test,test,1097," arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`.; Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330
https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1318,Testability,test,tests,1318," all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ""9""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl; if min_ver is not None and version < min_ver:; ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__; c = self._cmp(other);",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330
https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1419,Testability,test,testing,1419,"s and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ""9""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl; if min_ver is not None and version < min_ver:; ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__; c = self._cmp(other); ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330
https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1585,Testability,test,tests,1585,"odepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ""9""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl; if min_ver is not None and version < min_ver:; ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__; c = self._cmp(other); ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp; if self.version < other.version:; E TypeError: '<' not supported between instances of 'str' and 'int''`; I have the current ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330
https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1633,Testability,test,tests,1633,"ering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ""9""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl; if min_ver is not None and version < min_ver:; ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__; c = self._cmp(other); ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp; if self.version < other.version:; E TypeError: '<' not supported between instances of 'str' and 'int''`; I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330
https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1689,Testability,test,testing,1689,"ering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ""9""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl; if min_ver is not None and version < min_ver:; ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__; c = self._cmp(other); ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp; if self.version < other.version:; E TypeError: '<' not supported between instances of 'str' and 'int''`; I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330
https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1788,Testability,test,testing,1788,"ering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ""9""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl; if min_ver is not None and version < min_ver:; ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__; c = self._cmp(other); ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp; if self.version < other.version:; E TypeError: '<' not supported between instances of 'str' and 'int''`; I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330
https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1906,Testability,test,testing,1906,"ering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:364: in _get_executable_info; return impl([e, ""--version""], ""(.*)"", ""9""); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/__init__.py:346: in impl; if min_ver is not None and version < min_ver:; ~/.conda/envs/custom/lib/python3.8/distutils/version.py:52: in __lt__; c = self._cmp(other); ~/.conda/envs/custom/lib/python3.8/distutils/version.py:337: in _cmp; if self.version < other.version:; E TypeError: '<' not supported between instances of 'str' and 'int''`; I have the current matplotlib version 3.2.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330
https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330:1031,Usability,undo,undocumented,1031," behaviour of scale a little more.; The case of zero variance was until now replace with an arbitrary tiny variance, which arbitrarily blew up the scaled value and made it completely meaningless `scale[scale == 0] = 1e-12`.; Now I put instead `scale[scale == 0] = 1`. This yields the same result for `zero_center == True`: all values set to `0`, anyway (but with less arbitrary magic numbers and maybe less rounding errors). But if `zero_zenter == False`, unscalable values are untouched. This only affected the dense codepath where zero-centering was done afterwards anyway due to the original bug. Therefore this is no code breaking change.; But I also moved this statement before the sparse check to have consistent handling of sparse and dense data. Before that the sparse path wrote infs in the values (unchecked divison by zero) - this is a potentially code breaking change, but it only leads to the behaviour already stated in the documentation. I personally think that code relying on this undocumented behaviour should be rewritten, anyway...; In the new test I explicitly check for this behaviour to make it well defined.; Similar for integer datatypes (resulted in an error), they are now converted to floating point for scaling and return a copy. BTW: In order to make the tests run in my conda environment, I had to remove every reference to compare_images from matplotlib.testing.compare. There seems to be a version conflict in the version checking... It always gave errors like the following:; `________________ ERROR collecting scanpy/tests/test_plotting.py ________________; scanpy/tests/test_plotting.py:16: in <module>; from matplotlib.testing.compare import compare_images; ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:240: in <module>; _update_converter(); ~/.conda/envs/custom/lib/python3.8/site-packages/matplotlib/testing/compare.py:222: in _update_converter; mpl._get_executable_info(""gs""); ~/.conda/envs/custom/lib/python3.8/site-packages/m",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-615407330
https://github.com/scverse/scanpy/pull/1160#issuecomment-622244074:235,Usability,simpl,simplifying,235,"Sorry for the late response!. In general, it's good to keep PR's focussed on the initial topic, and make any other changes in new PRs. This will make the review process go a bit quicker. I've merged some code (already in progress) for simplifying the control flow of `scale`, and am going to ask that you rebase on that. Please let me know if you run into any trouble there. For the change in substituted values (from `1e-12` to `1`), could you provide an example I could run of `inf` values being generated? I just get zeros.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-622244074
https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221:1159,Energy Efficiency,efficient,efficient,1159," me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:; ```; X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]); X = sc.pp.scale(Xtest, copy=True, zero_center=False); X; ```; If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`; if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`; if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`.; But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]; Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221
https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221:1953,Modifiability,Variab,Variables,1953,"htly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:; ```; X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]); X = sc.pp.scale(Xtest, copy=True, zero_center=False); X; ```; If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`; if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`; if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`.; But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]; Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these values to `inf` without zero centering and to `0` or `nan` with zero centering. That is something for you to decide (I guess).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221
https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221:1282,Performance,optimiz,optimized,1282," me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:; ```; X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]); X = sc.pp.scale(Xtest, copy=True, zero_center=False); X; ```; If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`; if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`; if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`.; But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]; Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221
https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221:416,Testability,test,tests,416,"I'll keep the focused PRs in mind. The new control flow seems reasonbale. I was unfamiliar with the `@singledispatch` decorator, so it was not directly self explanatory for me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:; ```; X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]); X = sc.pp.scale(Xtest, copy=True, zero_center=False); X; ```; If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`; if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`; if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`.; But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]; Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221
https://github.com/scverse/scanpy/pull/1161#issuecomment-620661211:161,Testability,test,tests,161,@Marius1311 Thanks a lot for adding this. I reviewed the code and do not have any objections to merge it. Would be possible to add or modify one of the plotting tests where this new parameter is used?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1161#issuecomment-620661211
https://github.com/scverse/scanpy/pull/1161#issuecomment-621051643:36,Testability,test,test,36,"@michalk8 , would be great to add a test for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1161#issuecomment-621051643
https://github.com/scverse/scanpy/pull/1161#issuecomment-653802860:70,Testability,test,tests,70,@fidelram sorry it took so long... I've added the param to one of the tests.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1161#issuecomment-653802860
https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:1042,Availability,down,downloads,1042,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254
https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:912,Deployability,integrat,integrated,912,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Grea",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254
https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:966,Deployability,Integrat,Integration,966,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254
https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:1098,Deployability,integrat,integrate,1098,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254
https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:1327,Deployability,integrat,integration,1327,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254
https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:912,Integrability,integrat,integrated,912,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Grea",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254
https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:966,Integrability,Integrat,Integration,966,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254
https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:1098,Integrability,integrat,integrate,1098,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254
https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:1327,Integrability,integrat,integration,1327,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254
https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:1015,Performance,load,loaders,1015,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254
https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:392,Safety,predict,prediction,392,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Grea",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254
https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:846,Usability,learn,learned,846,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Grea",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254
https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1072,Availability,down,downloads,1072,"up and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910
https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:996,Deployability,Integrat,Integration,996,"up and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910
https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1128,Deployability,integrat,integrate,1128,"up and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910
https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1716,Deployability,integrat,integration,1716,"lmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, ; Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910
https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:996,Integrability,Integrat,Integration,996,"up and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910
https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1128,Integrability,integrat,integrate,1128,"up and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910
https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1716,Integrability,integrat,integration,1716,"lmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, ; Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910
https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1045,Performance,load,loaders,1045,"up and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910
https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:234,Safety,predict,prediction,234,"Hi David, . thanks for your reply and your interest!. > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or cust",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910
https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:150,Usability,learn,learned,150,"Hi David, . thanks for your reply and your interest!. > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or cust",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910
https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:591,Usability,learn,learned,591,"Hi David, . thanks for your reply and your interest!. > I had a superficial look at the functionalities and setup and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or cust",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910
https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:2344,Usability,simpl,simply,2344,"lmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, ; Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910
https://github.com/scverse/scanpy/issues/1164#issuecomment-614494639:207,Energy Efficiency,reduce,reduced,207,"Could you report your numba version, and also try updating your version of anndata? I'm not able to reproduce this on my end with either dense or sparse arrays in ""X"". If issues still occur, can you make an reduced example that replicates the bug which I could run on my machine?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164#issuecomment-614494639
https://github.com/scverse/scanpy/issues/1164#issuecomment-614594656:136,Availability,down,downgrade,136,"Hi @ivirshup ,; Thanks for your help.; Versions:; ```; In [1]: import numba; In [2]: numba.__version__; Out[2]: '0.45.0'; ```; I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post.; Now I updated anndata through conda:; ```conda update anndata```; And ran this code (minus highly variable gene calculation):; ```; adataCombat = sc.read_h5ad(results_file); #Run combat:; # sc.pp.highly_variable_genes(adataCombat); sc.pp.pca(adataCombat, svd_solver='arpack'); sc.pp.combat(adataCombat, key='sample'); sc.pp.neighbors(adataCombat, n_pcs =50); ```; with even worse output:; ```; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old); sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])); ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; sum2 = sum2 ** 2; sum2 = sum2.sum(axis=1); ^. @numba.jit; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITHOUT looplifting enabled be",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164#issuecomment-614594656
https://github.com/scverse/scanpy/issues/1164#issuecomment-614594656:247,Deployability,update,updated,247,"Hi @ivirshup ,; Thanks for your help.; Versions:; ```; In [1]: import numba; In [2]: numba.__version__; Out[2]: '0.45.0'; ```; I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post.; Now I updated anndata through conda:; ```conda update anndata```; And ran this code (minus highly variable gene calculation):; ```; adataCombat = sc.read_h5ad(results_file); #Run combat:; # sc.pp.highly_variable_genes(adataCombat); sc.pp.pca(adataCombat, svd_solver='arpack'); sc.pp.combat(adataCombat, key='sample'); sc.pp.neighbors(adataCombat, n_pcs =50); ```; with even worse output:; ```; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old); sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])); ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; sum2 = sum2 ** 2; sum2 = sum2.sum(axis=1); ^. @numba.jit; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITHOUT looplifting enabled be",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164#issuecomment-614594656
https://github.com/scverse/scanpy/issues/1164#issuecomment-614594656:288,Deployability,update,update,288,"Hi @ivirshup ,; Thanks for your help.; Versions:; ```; In [1]: import numba; In [2]: numba.__version__; Out[2]: '0.45.0'; ```; I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post.; Now I updated anndata through conda:; ```conda update anndata```; And ran this code (minus highly variable gene calculation):; ```; adataCombat = sc.read_h5ad(results_file); #Run combat:; # sc.pp.highly_variable_genes(adataCombat); sc.pp.pca(adataCombat, svd_solver='arpack'); sc.pp.combat(adataCombat, key='sample'); sc.pp.neighbors(adataCombat, n_pcs =50); ```; with even worse output:; ```; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old); sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])); ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; sum2 = sum2 ** 2; sum2 = sum2.sum(axis=1); ^. @numba.jit; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITHOUT looplifting enabled be",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164#issuecomment-614594656
https://github.com/scverse/scanpy/issues/1164#issuecomment-614594656:339,Modifiability,variab,variable,339,"Hi @ivirshup ,; Thanks for your help.; Versions:; ```; In [1]: import numba; In [2]: numba.__version__; Out[2]: '0.45.0'; ```; I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post.; Now I updated anndata through conda:; ```conda update anndata```; And ran this code (minus highly variable gene calculation):; ```; adataCombat = sc.read_h5ad(results_file); #Run combat:; # sc.pp.highly_variable_genes(adataCombat); sc.pp.pca(adataCombat, svd_solver='arpack'); sc.pp.combat(adataCombat, key='sample'); sc.pp.neighbors(adataCombat, n_pcs =50); ```; with even worse output:; ```; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old); sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])); ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; sum2 = sum2 ** 2; sum2 = sum2.sum(axis=1); ^. @numba.jit; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITHOUT looplifting enabled be",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164#issuecomment-614594656
https://github.com/scverse/scanpy/issues/1164#issuecomment-614654094:63,Deployability,install,installs,63,"Ah, looks like that might be it. I think your scanpy and numba installs might be old versions. Could you upgrade those and let me know if that works?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164#issuecomment-614654094
https://github.com/scverse/scanpy/issues/1164#issuecomment-614654094:105,Deployability,upgrade,upgrade,105,"Ah, looks like that might be it. I think your scanpy and numba installs might be old versions. Could you upgrade those and let me know if that works?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164#issuecomment-614654094
https://github.com/scverse/scanpy/issues/1166#issuecomment-614743174:12,Deployability,install,installing,12,Hi! so when installing via pip this is not an issue... somehow with conda it doesnt work,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166#issuecomment-614743174
https://github.com/scverse/scanpy/issues/1166#issuecomment-614798701:109,Availability,error,error,109,"Ok so it doesnt work, again, even with pip... @ivirshup I tried the code you wrote, and it gives me the same error:. `` AttributeError: module 'cairo' has no attribute 'version_info'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166#issuecomment-614798701
https://github.com/scverse/scanpy/issues/1166#issuecomment-615054303:53,Deployability,install,install,53,"So, it looks like the conda environment has a broken install of Matplotlib, which I don't think I can help with too much. Are you able to create an environment with just Matplotlib, where you're able to import `pyplot`? Does adding scanpy to this environment break matplotlib?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166#issuecomment-615054303
https://github.com/scverse/scanpy/issues/1167#issuecomment-615006407:207,Availability,error,error,207,"This looks like an issue with `mnn_correct` , and is probably more appropriate for that repo (https://github.com/chriscainx/mnnpy). I would note that on my end I'm able to replicate the warnings but not the error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1167#issuecomment-615006407
https://github.com/scverse/scanpy/issues/1168#issuecomment-615069994:43,Availability,error,error,43,"@rsggsr, that looks like a warning, not an error to me. Do the plots look wrong to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168#issuecomment-615069994
https://github.com/scverse/scanpy/issues/1168#issuecomment-615878967:2464,Integrability,wrap,wrap,2464,"8 return_data=True,; ---> 19 show=False); 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)); 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1037 if n_avg > 1:; 1038 old_len_x = len(x); -> 1039 x = moving_average(x); 1040 if ikey == 0:; 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a); 980 ; 981 def moving_average(a):; --> 982 return _sc_utils.moving_average(a, n_avg); 983 ; 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n); 374 An array view storing the moving average.; 375 """"""; --> 376 ret = np.cumsum(a, dtype=float); 377 ret[n:] = ret[n:] - ret[:-n]; 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out); 2421 ; 2422 """"""; -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out); 2424 ; 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds); 56 bound = getattr(obj, method, None); 57 if bound is None:; ---> 58 return _wrapit(obj, method, *args, **kwds); 59 ; 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds); 45 except AttributeError:; 46 wrap = None; ---> 47 result = getattr(asarray(obj), method)(*args, **kwds); 48 if wrap:; 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168#issuecomment-615878967
https://github.com/scverse/scanpy/issues/1168#issuecomment-615878967:2546,Integrability,wrap,wrap,2546,"8 return_data=True,; ---> 19 show=False); 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)); 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1037 if n_avg > 1:; 1038 old_len_x = len(x); -> 1039 x = moving_average(x); 1040 if ikey == 0:; 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a); 980 ; 981 def moving_average(a):; --> 982 return _sc_utils.moving_average(a, n_avg); 983 ; 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n); 374 An array view storing the moving average.; 375 """"""; --> 376 ret = np.cumsum(a, dtype=float); 377 ret[n:] = ret[n:] - ret[:-n]; 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out); 2421 ; 2422 """"""; -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out); 2424 ; 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds); 56 bound = getattr(obj, method, None); 57 if bound is None:; ---> 58 return _wrapit(obj, method, *args, **kwds); 59 ; 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds); 45 except AttributeError:; 46 wrap = None; ---> 47 result = getattr(asarray(obj), method)(*args, **kwds); 48 if wrap:; 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168#issuecomment-615878967
https://github.com/scverse/scanpy/issues/1168#issuecomment-615878967:369,Performance,perform,perform,369,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right?. And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it?. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-44-72c504b15b2e> in <module>; 17 title='{} path'.format(descr),; 18 return_data=True,; ---> 19 show=False); 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)); 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1037 if n_avg > 1:; 1038 old_len_x = len(x); -> 1039 x = moving_average(x); 1040 if ikey == 0:; 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a); 980 ; 981 def moving_average(a):; --> 982 return _sc_utils.moving_average(a, n_avg); 983 ; 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n); 374 An array view storing the moving average.; 375 """"""; --> 376 ret = np.cumsum(a, dtype=float); 377 ret[n:] = ret[n:] - ret[:-n]; 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out); 2421 ; 2422 """"""; ->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168#issuecomment-615878967
https://github.com/scverse/scanpy/issues/1168#issuecomment-619031543:80925,Integrability,wrap,wrap,80925,"True,; ---> 20 use_raw=False); 21 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)); 22 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1038 old_len_x = len(x); 1039 print(x); -> 1040 x = moving_average(x); 1041 if ikey == 0:; 1042 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a); 980 ; 981 def moving_average(a):; --> 982 return _sc_utils.moving_average(a, n_avg); 983 ; 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n); 374 An array view storing the moving average.; 375 """"""; --> 376 ret = np.cumsum(a, dtype=float); 377 ret[n:] = ret[n:] - ret[:-n]; 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out); 2421 ; 2422 """"""; -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out); 2424 ; 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds); 56 bound = getattr(obj, method, None); 57 if bound is None:; ---> 58 return _wrapit(obj, method, *args, **kwds); 59 ; 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds); 45 except AttributeError:; 46 wrap = None; ---> 47 result = getattr(asarray(obj), method)(*args, **kwds); 48 if wrap:; 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168#issuecomment-619031543
https://github.com/scverse/scanpy/issues/1168#issuecomment-619031543:81007,Integrability,wrap,wrap,81007,"True,; ---> 20 use_raw=False); 21 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)); 22 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1038 old_len_x = len(x); 1039 print(x); -> 1040 x = moving_average(x); 1041 if ikey == 0:; 1042 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a); 980 ; 981 def moving_average(a):; --> 982 return _sc_utils.moving_average(a, n_avg); 983 ; 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n); 374 An array view storing the moving average.; 375 """"""; --> 376 ret = np.cumsum(a, dtype=float); 377 ret[n:] = ret[n:] - ret[:-n]; 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out); 2421 ; 2422 """"""; -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out); 2424 ; 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds); 56 bound = getattr(obj, method, None); 57 if bound is None:; ---> 58 return _wrapit(obj, method, *args, **kwds); 59 ; 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds); 45 except AttributeError:; 46 wrap = None; ---> 47 result = getattr(asarray(obj), method)(*args, **kwds); 48 if wrap:; 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168#issuecomment-619031543
https://github.com/scverse/scanpy/issues/1168#issuecomment-619083501:99,Availability,error,error,99,"@HypoChloremic,; I did but it comes out with weird PAGA pathway analysis plot (shown blow) and new error:. ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-46-42a11a5bd10f> in <module>; 19 show=True,; 20 use_raw=False); ---> 21 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)); 22 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""); 23 pl.show(). ~\Miniconda3\envs\project\lib\site-packages\pandas\core\generic.py in to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal); 3226 decimal=decimal,; 3227 ); -> 3228 formatter.save(); 3229 ; 3230 if path_or_buf is None:. ~\Miniconda3\envs\project\lib\site-packages\pandas\io\formats\csvs.py in save(self); 181 self.mode,; 182 encoding=self.encoding,; --> 183 compression=self.compression,; 184 ); 185 close = True. ~\Miniconda3\envs\project\lib\site-packages\pandas\io\common.py in _get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text); 397 if encoding:; 398 # Encoding; --> 399 f = open(path_or_buf, mode, encoding=encoding, newline=""""); 400 elif is_text:; 401 # No explicit encoding. FileNotFoundError: [Errno 2] No such file or directory: 'C:/Users/Lin/write/paga_path_DC/LC.csv'. [下載](https://user-images.githubusercontent.com/57272642/80230003-47818480-861f-11ea-96ce-db1128b4a6eb.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168#issuecomment-619083501
https://github.com/scverse/scanpy/issues/1168#issuecomment-619398744:111,Availability,error,errors,111,"@rsggsr remove the `data.to_csv()` part, it seems to have worked otherwise, if the ``tl.paga_path` ran without errors",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168#issuecomment-619398744
https://github.com/scverse/scanpy/issues/1168#issuecomment-619409104:31,Availability,error,error,31,@HypoChloremic . Right now the error is gone but still got three weird PAGA pathay graphs shown below:. ![下載 (1)](https://user-images.githubusercontent.com/57272642/80285540-c1833d80-86f3-11ea-9a35-4fdf7385eaab.png); ![下載 (2)](https://user-images.githubusercontent.com/57272642/80285541-c47e2e00-86f3-11ea-9b10-1427ccfc978e.png); ![下載 (3)](https://user-images.githubusercontent.com/57272642/80285543-c7791e80-86f3-11ea-8c66-9ec7226de7c6.png),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168#issuecomment-619409104
https://github.com/scverse/scanpy/issues/1169#issuecomment-833182712:37,Deployability,release,release,37,I would like to do this for the next release series.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1169#issuecomment-833182712
https://github.com/scverse/scanpy/issues/1170#issuecomment-618177408:134,Usability,simpl,simplify,134,"The best thing to do would just be to put a link to the data here, and paste the code. It's actually easier to debug the more you can simplify the data that replicates the bug. Ideally, you could just send code that generates the data to replicate the bug. If that isn't working out, you could send me a DM on the discourse forum?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1170#issuecomment-618177408
https://github.com/scverse/scanpy/issues/1170#issuecomment-628401842:45,Availability,error,error,45,"@Zifeng1995, @ivirshup . I received the same error caused by wrong shape of ""numer"" in line 268 in _combat.py. ; In my case, I could resolved this problem by generating unique cell names ( i.e. adata.obs_names) as following. . **Workaround** ; Before combat execution, in concatenate process of batch data, I specified index_unique='-' . ; e.g.) adata1.concatenate(adata2, adata3, ..., index_unique='-'). When index_unique=None, the error was occurred. However index_unique='-' was fine in my case.; I'm afraid that couldn't trace root cause of this problem due to busy day, but I hope this helps you. . Sincerely.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1170#issuecomment-628401842
https://github.com/scverse/scanpy/issues/1170#issuecomment-628401842:433,Availability,error,error,433,"@Zifeng1995, @ivirshup . I received the same error caused by wrong shape of ""numer"" in line 268 in _combat.py. ; In my case, I could resolved this problem by generating unique cell names ( i.e. adata.obs_names) as following. . **Workaround** ; Before combat execution, in concatenate process of batch data, I specified index_unique='-' . ; e.g.) adata1.concatenate(adata2, adata3, ..., index_unique='-'). When index_unique=None, the error was occurred. However index_unique='-' was fine in my case.; I'm afraid that couldn't trace root cause of this problem due to busy day, but I hope this helps you. . Sincerely.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1170#issuecomment-628401842
https://github.com/scverse/scanpy/issues/1172#issuecomment-616458854:556,Modifiability,variab,variables,556,"In theory Combat knows how to take care of zero variance genes according to the; [code](https://github.com/theislab/scanpy/blob/4156314407c5368fa0b66ac18470d80f3748a71f/scanpy/preprocessing/_combat.py#L124).; Well, post-Combat apparently NaNs are everywhere:; ```; np.sum(np.isnan(adata_Combat.X)); Out[2]: 8089368. np.sum(~np.isnan(adata_Combat.X)); Out[3]: 0; ```; This is really weird if only 3 genes have zero variance, right? Could it have anything to do with this warnings?:; ```; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Found 3 genes with zero variance.; Fitting L/S model and finding priors. Finding parametric adjustments. Adjusting data. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: invalid value encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616458854
https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922:249,Availability,error,error,249,"OK, I got rid of a few genes that were not expressed in the dataset (its a subset of cells from the full dataset) with ```sc.pp.filter_genes(adata, min_counts=1)``` and now the zero variance genes are gone but still same warning, same NaNs and same error when trying to run ```sc.pp.highly_variable_genes()```:. ```; In [1]: sc.pp.combat(adata_Combat, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922
https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922:570,Availability,avail,available,570,"OK, I got rid of a few genes that were not expressed in the dataset (its a subset of cells from the full dataset) with ```sc.pp.filter_genes(adata, min_counts=1)``` and now the zero variance genes are gone but still same warning, same NaNs and same error when trying to run ```sc.pp.highly_variable_genes()```:. ```; In [1]: sc.pp.combat(adata_Combat, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922
https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922:1228,Modifiability,variab,variables,1228,"ing, same NaNs and same error when trying to run ```sc.pp.highly_variable_genes()```:. ```; In [1]: sc.pp.combat(adata_Combat, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 65, in _highly_variabl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922
https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922:1756,Modifiability,variab,variable,1756,"ng: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 65, in _highly_variable_genes_single_batch; df['mean_bin'] = pd.cut(df['means'], bins=n_bins). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 265, in cut; duplicates=duplicates,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 381, in _bins_to_cuts; f""Bin edges must be unique: {repr(bins)}.\n"". ValueError: Bin edges must be unique: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,; nan, nan, nan, nan, nan, nan, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922
https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922:1097,Usability,learn,learn,1097,"he full dataset) with ```sc.pp.filter_genes(adata, min_counts=1)``` and now the zero variance genes are gone but still same warning, same NaNs and same error when trying to run ```sc.pp.highly_variable_genes()```:. ```; In [1]: sc.pp.combat(adata_Combat, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/aues",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922
https://github.com/scverse/scanpy/issues/1172#issuecomment-616528291:467,Availability,error,error,467,"It's only 538 cells, so it's a small file, I can send it to you, if you want to have a look.; ________________________________; From: MalteDLuecken <notifications@github.com>; Sent: Monday, April 20, 2020 2:42:07 PM; To: theislab/scanpy <scanpy@noreply.github.com>; Cc: Augusto Escalante <ae_rodriguez_@hotmail.com>; Author <author@noreply.github.com>; Subject: Re: [theislab/scanpy] Combat populates adata.X with NANs so sc.pp.highly_variable_genes function outputs error (#1172). It's hard to say what's going on here. There seems to be sth happening in sc.pp.combat() so I would evaluate this separately from sc.pp.highly_variable_genes(). It would be important to have a minimal reproducible example it seems. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/1172#issuecomment-616527285>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AFZKH4VCL573RRAU55AHS23RNQ7J7ANCNFSM4ML4AVXA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616528291
https://github.com/scverse/scanpy/issues/1176#issuecomment-617141601:27,Deployability,patch,patch,27,"Excellent, thank you. I'll patch my code accordingly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1176#issuecomment-617141601
https://github.com/scverse/scanpy/issues/1177#issuecomment-618719727:427,Energy Efficiency,allocate,allocated,427,"@quasiben As far as I know Cusparse is being used under Cupy currently for a lot of the operations. I’m not quite sure why those slicing strategies aren’t supported yet. I just figured maybe they were less trivial than the others and weren’t immediately needed so they were pushed off to future feature requests. . The issue #2360 I can’t imagine is too hard- I imagine the output array the size of the selection list could be allocated and a Cuda kernel scheduled to write the selected entries in parallel. I’m not as sure about the other issue, but what Dask is trying to do seems more like an API compatibility issue than one of performance/compute.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618719727
https://github.com/scverse/scanpy/issues/1177#issuecomment-618719727:455,Energy Efficiency,schedul,scheduled,455,"@quasiben As far as I know Cusparse is being used under Cupy currently for a lot of the operations. I’m not quite sure why those slicing strategies aren’t supported yet. I just figured maybe they were less trivial than the others and weren’t immediately needed so they were pushed off to future feature requests. . The issue #2360 I can’t imagine is too hard- I imagine the output array the size of the selection list could be allocated and a Cuda kernel scheduled to write the selected entries in parallel. I’m not as sure about the other issue, but what Dask is trying to do seems more like an API compatibility issue than one of performance/compute.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618719727
https://github.com/scverse/scanpy/issues/1177#issuecomment-618719727:632,Performance,perform,performance,632,"@quasiben As far as I know Cusparse is being used under Cupy currently for a lot of the operations. I’m not quite sure why those slicing strategies aren’t supported yet. I just figured maybe they were less trivial than the others and weren’t immediately needed so they were pushed off to future feature requests. . The issue #2360 I can’t imagine is too hard- I imagine the output array the size of the selection list could be allocated and a Cuda kernel scheduled to write the selected entries in parallel. I’m not as sure about the other issue, but what Dask is trying to do seems more like an API compatibility issue than one of performance/compute.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618719727
https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:289,Availability,avail,available,289,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788
https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:416,Deployability,deploy,deploys,416,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788
https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:583,Deployability,deploy,deployed,583,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788
https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:957,Deployability,pipeline,pipelines,957,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788
https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:33,Performance,bottleneck,bottleneck,33,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788
https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:186,Performance,bottleneck,bottlenecks,186,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788
https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:342,Performance,perform,perform,342,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788
https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:1270,Performance,bottleneck,bottlenecks,1270,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788
https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:1440,Performance,perform,performance,1440,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788
https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:166,Usability,clear,clear,166,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788
https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:1733,Usability,clear,clear,1733,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788
https://github.com/scverse/scanpy/issues/1181#issuecomment-617895856:124,Deployability,install,install,124,"Hi, @plrlhb12 .; Yes, this is a known problem. The easiest fix for now is to use umap 0.3.9 instead of 0.4.1.; You can also install scanpy from github where it is fixed or just wait for a new scanpy release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181#issuecomment-617895856
https://github.com/scverse/scanpy/issues/1181#issuecomment-617895856:199,Deployability,release,release,199,"Hi, @plrlhb12 .; Yes, this is a known problem. The easiest fix for now is to use umap 0.3.9 instead of 0.4.1.; You can also install scanpy from github where it is fixed or just wait for a new scanpy release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181#issuecomment-617895856
https://github.com/scverse/scanpy/issues/1181#issuecomment-617911861:285,Availability,error,error,285,"Hi Sergei,. Thank you very much for your fast reply. Do you mean I can still use the latest version of scanpy but installing a lower version of umap?. I tried different versions of scanpy, including pastiest, stable, 1.4.5, 1.4.5post3, 1.4.4post1... They seem to either have different error messages or packages not compatible. Do you know which version of the scanpy has it fixed?. Thank you for your kind help. Best regards,. Lirong. 获取 Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; 发件人: Sergei R. <notifications@github.com>; 发送时间: Wednesday, April 22, 2020 12:44:36 PM; 收件人: theislab/scanpy <scanpy@noreply.github.com>; 抄送: plrlhb12 <lrpeng@hotmail.com>; Mention <mention@noreply.github.com>; 主题: Re: [theislab/scanpy] Issue with ingest (#1181). Hi, @plrlhb12<https://github.com/plrlhb12> .; Yes, this is a known problem. The easiest fix for now is to use umap 0.3.9 instead of 0.4.1.; You can also install scanpy from github where it is fixed or just wait for a new scanpy release. ―; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/1181#issuecomment-617895856>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AKHCBZKH4TYT5672QNGFW33RN4NHJANCNFSM4MNX44PA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181#issuecomment-617911861
https://github.com/scverse/scanpy/issues/1181#issuecomment-617911861:114,Deployability,install,installing,114,"Hi Sergei,. Thank you very much for your fast reply. Do you mean I can still use the latest version of scanpy but installing a lower version of umap?. I tried different versions of scanpy, including pastiest, stable, 1.4.5, 1.4.5post3, 1.4.4post1... They seem to either have different error messages or packages not compatible. Do you know which version of the scanpy has it fixed?. Thank you for your kind help. Best regards,. Lirong. 获取 Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; 发件人: Sergei R. <notifications@github.com>; 发送时间: Wednesday, April 22, 2020 12:44:36 PM; 收件人: theislab/scanpy <scanpy@noreply.github.com>; 抄送: plrlhb12 <lrpeng@hotmail.com>; Mention <mention@noreply.github.com>; 主题: Re: [theislab/scanpy] Issue with ingest (#1181). Hi, @plrlhb12<https://github.com/plrlhb12> .; Yes, this is a known problem. The easiest fix for now is to use umap 0.3.9 instead of 0.4.1.; You can also install scanpy from github where it is fixed or just wait for a new scanpy release. ―; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/1181#issuecomment-617895856>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AKHCBZKH4TYT5672QNGFW33RN4NHJANCNFSM4MNX44PA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181#issuecomment-617911861
https://github.com/scverse/scanpy/issues/1181#issuecomment-617911861:930,Deployability,install,install,930,"Hi Sergei,. Thank you very much for your fast reply. Do you mean I can still use the latest version of scanpy but installing a lower version of umap?. I tried different versions of scanpy, including pastiest, stable, 1.4.5, 1.4.5post3, 1.4.4post1... They seem to either have different error messages or packages not compatible. Do you know which version of the scanpy has it fixed?. Thank you for your kind help. Best regards,. Lirong. 获取 Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; 发件人: Sergei R. <notifications@github.com>; 发送时间: Wednesday, April 22, 2020 12:44:36 PM; 收件人: theislab/scanpy <scanpy@noreply.github.com>; 抄送: plrlhb12 <lrpeng@hotmail.com>; Mention <mention@noreply.github.com>; 主题: Re: [theislab/scanpy] Issue with ingest (#1181). Hi, @plrlhb12<https://github.com/plrlhb12> .; Yes, this is a known problem. The easiest fix for now is to use umap 0.3.9 instead of 0.4.1.; You can also install scanpy from github where it is fixed or just wait for a new scanpy release. ―; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/1181#issuecomment-617895856>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AKHCBZKH4TYT5672QNGFW33RN4NHJANCNFSM4MNX44PA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181#issuecomment-617911861
https://github.com/scverse/scanpy/issues/1181#issuecomment-617911861:1005,Deployability,release,release,1005,"Hi Sergei,. Thank you very much for your fast reply. Do you mean I can still use the latest version of scanpy but installing a lower version of umap?. I tried different versions of scanpy, including pastiest, stable, 1.4.5, 1.4.5post3, 1.4.4post1... They seem to either have different error messages or packages not compatible. Do you know which version of the scanpy has it fixed?. Thank you for your kind help. Best regards,. Lirong. 获取 Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; 发件人: Sergei R. <notifications@github.com>; 发送时间: Wednesday, April 22, 2020 12:44:36 PM; 收件人: theislab/scanpy <scanpy@noreply.github.com>; 抄送: plrlhb12 <lrpeng@hotmail.com>; Mention <mention@noreply.github.com>; 主题: Re: [theislab/scanpy] Issue with ingest (#1181). Hi, @plrlhb12<https://github.com/plrlhb12> .; Yes, this is a known problem. The easiest fix for now is to use umap 0.3.9 instead of 0.4.1.; You can also install scanpy from github where it is fixed or just wait for a new scanpy release. ―; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/1181#issuecomment-617895856>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AKHCBZKH4TYT5672QNGFW33RN4NHJANCNFSM4MNX44PA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181#issuecomment-617911861
https://github.com/scverse/scanpy/issues/1181#issuecomment-617911861:291,Integrability,message,messages,291,"Hi Sergei,. Thank you very much for your fast reply. Do you mean I can still use the latest version of scanpy but installing a lower version of umap?. I tried different versions of scanpy, including pastiest, stable, 1.4.5, 1.4.5post3, 1.4.4post1... They seem to either have different error messages or packages not compatible. Do you know which version of the scanpy has it fixed?. Thank you for your kind help. Best regards,. Lirong. 获取 Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; 发件人: Sergei R. <notifications@github.com>; 发送时间: Wednesday, April 22, 2020 12:44:36 PM; 收件人: theislab/scanpy <scanpy@noreply.github.com>; 抄送: plrlhb12 <lrpeng@hotmail.com>; Mention <mention@noreply.github.com>; 主题: Re: [theislab/scanpy] Issue with ingest (#1181). Hi, @plrlhb12<https://github.com/plrlhb12> .; Yes, this is a known problem. The easiest fix for now is to use umap 0.3.9 instead of 0.4.1.; You can also install scanpy from github where it is fixed or just wait for a new scanpy release. ―; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/1181#issuecomment-617895856>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AKHCBZKH4TYT5672QNGFW33RN4NHJANCNFSM4MNX44PA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181#issuecomment-617911861
https://github.com/scverse/scanpy/issues/1181#issuecomment-617940220:572,Deployability,install,install,572,"It works now by changing umap to 0.3.9. Thanks a lot!. Best regards,. Lirong. 获取 Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; 发件人: Sergei R. <notifications@github.com>; 发送时间: Wednesday, April 22, 2020 12:44:36 PM; 收件人: theislab/scanpy <scanpy@noreply.github.com>; 抄送: plrlhb12 <lrpeng@hotmail.com>; Mention <mention@noreply.github.com>; 主题: Re: [theislab/scanpy] Issue with ingest (#1181). Hi, @plrlhb12<https://github.com/plrlhb12> .; Yes, this is a known problem. The easiest fix for now is to use umap 0.3.9 instead of 0.4.1.; You can also install scanpy from github where it is fixed or just wait for a new scanpy release. ―; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/1181#issuecomment-617895856>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AKHCBZKH4TYT5672QNGFW33RN4NHJANCNFSM4MNX44PA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181#issuecomment-617940220
https://github.com/scverse/scanpy/issues/1181#issuecomment-617940220:647,Deployability,release,release,647,"It works now by changing umap to 0.3.9. Thanks a lot!. Best regards,. Lirong. 获取 Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; 发件人: Sergei R. <notifications@github.com>; 发送时间: Wednesday, April 22, 2020 12:44:36 PM; 收件人: theislab/scanpy <scanpy@noreply.github.com>; 抄送: plrlhb12 <lrpeng@hotmail.com>; Mention <mention@noreply.github.com>; 主题: Re: [theislab/scanpy] Issue with ingest (#1181). Hi, @plrlhb12<https://github.com/plrlhb12> .; Yes, this is a known problem. The easiest fix for now is to use umap 0.3.9 instead of 0.4.1.; You can also install scanpy from github where it is fixed or just wait for a new scanpy release. ―; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/1181#issuecomment-617895856>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AKHCBZKH4TYT5672QNGFW33RN4NHJANCNFSM4MNX44PA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181#issuecomment-617940220
