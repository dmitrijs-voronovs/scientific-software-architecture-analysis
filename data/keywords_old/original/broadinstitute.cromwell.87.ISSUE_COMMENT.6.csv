id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279:1330,Modifiability,config,configure,1330,"Looks like one test is legit failing. ```; - should successfully run drs_wf_level_read_size *** FAILED *** (44 seconds, 659 milliseconds); centaur.test.CentaurTestException: Unexpected terminal status Failed but was waiting for Succeeded (workflow ID: 0f9eb46c-44ce-4c92-99f6-0184196298eb). Metadata 'failures' content: [; {; ""causedBy"" : [; {; ""causedBy"" : [; ],; ""message"" : ""Failed to evaluate 'wf_level_read_and_size.fileSize1' (reason 1 of 1): Evaluating size(input1) failed: java.lang.IllegalArgumentException: Could not build the path \""dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, Google Cloud Storage, LinuxFileSystem. Failures: \nHTTP: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 does not have an http or https scheme (IllegalArgumentException)\nGoogle Cloud Storage: Cloud Storage URIs must have 'gs' scheme: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (IllegalArgumentException)\nLinuxFileSystem: Cannot build a local path from dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (RuntimeException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""; },. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279
https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279:15,Testability,test,test,15,"Looks like one test is legit failing. ```; - should successfully run drs_wf_level_read_size *** FAILED *** (44 seconds, 659 milliseconds); centaur.test.CentaurTestException: Unexpected terminal status Failed but was waiting for Succeeded (workflow ID: 0f9eb46c-44ce-4c92-99f6-0184196298eb). Metadata 'failures' content: [; {; ""causedBy"" : [; {; ""causedBy"" : [; ],; ""message"" : ""Failed to evaluate 'wf_level_read_and_size.fileSize1' (reason 1 of 1): Evaluating size(input1) failed: java.lang.IllegalArgumentException: Could not build the path \""dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, Google Cloud Storage, LinuxFileSystem. Failures: \nHTTP: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 does not have an http or https scheme (IllegalArgumentException)\nGoogle Cloud Storage: Cloud Storage URIs must have 'gs' scheme: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (IllegalArgumentException)\nLinuxFileSystem: Cannot build a local path from dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (RuntimeException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""; },. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279
https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279:147,Testability,test,test,147,"Looks like one test is legit failing. ```; - should successfully run drs_wf_level_read_size *** FAILED *** (44 seconds, 659 milliseconds); centaur.test.CentaurTestException: Unexpected terminal status Failed but was waiting for Succeeded (workflow ID: 0f9eb46c-44ce-4c92-99f6-0184196298eb). Metadata 'failures' content: [; {; ""causedBy"" : [; {; ""causedBy"" : [; ],; ""message"" : ""Failed to evaluate 'wf_level_read_and_size.fileSize1' (reason 1 of 1): Evaluating size(input1) failed: java.lang.IllegalArgumentException: Could not build the path \""dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, Google Cloud Storage, LinuxFileSystem. Failures: \nHTTP: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 does not have an http or https scheme (IllegalArgumentException)\nGoogle Cloud Storage: Cloud Storage URIs must have 'gs' scheme: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (IllegalArgumentException)\nLinuxFileSystem: Cannot build a local path from dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (RuntimeException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""; },. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279
https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279:553,Testability,mock,mock-drs-dev,553,"Looks like one test is legit failing. ```; - should successfully run drs_wf_level_read_size *** FAILED *** (44 seconds, 659 milliseconds); centaur.test.CentaurTestException: Unexpected terminal status Failed but was waiting for Succeeded (workflow ID: 0f9eb46c-44ce-4c92-99f6-0184196298eb). Metadata 'failures' content: [; {; ""causedBy"" : [; {; ""causedBy"" : [; ],; ""message"" : ""Failed to evaluate 'wf_level_read_and_size.fileSize1' (reason 1 of 1): Evaluating size(input1) failed: java.lang.IllegalArgumentException: Could not build the path \""dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, Google Cloud Storage, LinuxFileSystem. Failures: \nHTTP: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 does not have an http or https scheme (IllegalArgumentException)\nGoogle Cloud Storage: Cloud Storage URIs must have 'gs' scheme: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (IllegalArgumentException)\nLinuxFileSystem: Cannot build a local path from dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (RuntimeException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""; },. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279
https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279:801,Testability,mock,mock-drs-dev,801,"Looks like one test is legit failing. ```; - should successfully run drs_wf_level_read_size *** FAILED *** (44 seconds, 659 milliseconds); centaur.test.CentaurTestException: Unexpected terminal status Failed but was waiting for Succeeded (workflow ID: 0f9eb46c-44ce-4c92-99f6-0184196298eb). Metadata 'failures' content: [; {; ""causedBy"" : [; {; ""causedBy"" : [; ],; ""message"" : ""Failed to evaluate 'wf_level_read_and_size.fileSize1' (reason 1 of 1): Evaluating size(input1) failed: java.lang.IllegalArgumentException: Could not build the path \""dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, Google Cloud Storage, LinuxFileSystem. Failures: \nHTTP: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 does not have an http or https scheme (IllegalArgumentException)\nGoogle Cloud Storage: Cloud Storage URIs must have 'gs' scheme: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (IllegalArgumentException)\nLinuxFileSystem: Cannot build a local path from dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (RuntimeException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""; },. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279
https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279:1013,Testability,mock,mock-drs-dev,1013,"Looks like one test is legit failing. ```; - should successfully run drs_wf_level_read_size *** FAILED *** (44 seconds, 659 milliseconds); centaur.test.CentaurTestException: Unexpected terminal status Failed but was waiting for Succeeded (workflow ID: 0f9eb46c-44ce-4c92-99f6-0184196298eb). Metadata 'failures' content: [; {; ""causedBy"" : [; {; ""causedBy"" : [; ],; ""message"" : ""Failed to evaluate 'wf_level_read_and_size.fileSize1' (reason 1 of 1): Evaluating size(input1) failed: java.lang.IllegalArgumentException: Could not build the path \""dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, Google Cloud Storage, LinuxFileSystem. Failures: \nHTTP: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 does not have an http or https scheme (IllegalArgumentException)\nGoogle Cloud Storage: Cloud Storage URIs must have 'gs' scheme: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (IllegalArgumentException)\nLinuxFileSystem: Cannot build a local path from dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (RuntimeException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""; },. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279
https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279:1171,Testability,mock,mock-drs-dev,1171,"Looks like one test is legit failing. ```; - should successfully run drs_wf_level_read_size *** FAILED *** (44 seconds, 659 milliseconds); centaur.test.CentaurTestException: Unexpected terminal status Failed but was waiting for Succeeded (workflow ID: 0f9eb46c-44ce-4c92-99f6-0184196298eb). Metadata 'failures' content: [; {; ""causedBy"" : [; {; ""causedBy"" : [; ],; ""message"" : ""Failed to evaluate 'wf_level_read_and_size.fileSize1' (reason 1 of 1): Evaluating size(input1) failed: java.lang.IllegalArgumentException: Could not build the path \""dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, Google Cloud Storage, LinuxFileSystem. Failures: \nHTTP: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 does not have an http or https scheme (IllegalArgumentException)\nGoogle Cloud Storage: Cloud Storage URIs must have 'gs' scheme: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (IllegalArgumentException)\nLinuxFileSystem: Cannot build a local path from dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (RuntimeException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""; },. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279
https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505092978:99,Modifiability,config,config,99,"@mcovarr Yes. I am trying to add a centaur test case for DRS and I am still working on getting the config right. I thought I corrected it with the last commit, but apparently not...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505092978
https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505092978:43,Testability,test,test,43,"@mcovarr Yes. I am trying to add a centaur test case for DRS and I am still working on getting the config right. I thought I corrected it with the last commit, but apparently not...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505092978
https://github.com/broadinstitute/cromwell/issues/5041#issuecomment-504764351:198,Availability,error,error,198,"Hi @antonkulaga - as far as I know there is no provision in the WDL spec to subset a `Directory` like this (nor do I believe this was intended behavior). As such, I'm closing this issue because the error you describe indicates Cromwell is correctly implementing the WDL spec. . If I'm incorrect and there is indeed something in the spec allowing this please reopen.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5041#issuecomment-504764351
https://github.com/broadinstitute/cromwell/pull/5042#issuecomment-505061221:62,Availability,Error,Error,62,"Happy for a future PR to propose catching SOE, or even other `Error`s. In past lives I've been indoctrinated that `java.lang.Error` means ""stop and shutdown, something is horribly wrong"". For those who want to read debates on the issue:; https://www.google.com/search?q=java+catch+stack+overflow+error+site:stackoverflow.com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5042#issuecomment-505061221
https://github.com/broadinstitute/cromwell/pull/5042#issuecomment-505061221:125,Availability,Error,Error,125,"Happy for a future PR to propose catching SOE, or even other `Error`s. In past lives I've been indoctrinated that `java.lang.Error` means ""stop and shutdown, something is horribly wrong"". For those who want to read debates on the issue:; https://www.google.com/search?q=java+catch+stack+overflow+error+site:stackoverflow.com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5042#issuecomment-505061221
https://github.com/broadinstitute/cromwell/pull/5042#issuecomment-505061221:296,Availability,error,error,296,"Happy for a future PR to propose catching SOE, or even other `Error`s. In past lives I've been indoctrinated that `java.lang.Error` means ""stop and shutdown, something is horribly wrong"". For those who want to read debates on the issue:; https://www.google.com/search?q=java+catch+stack+overflow+error+site:stackoverflow.com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5042#issuecomment-505061221
https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505180523:61,Deployability,patch,patch,61,Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505180523
https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505180523:39,Testability,test,test,39,Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505180523
https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505180523:120,Testability,test,test,120,Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505180523
https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313:87,Deployability,patch,patch,87,Thanks for the review!. > Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising... running `sbt coverageOn sfsBackend/test coverageAggregate coverageReport`; On this commit does generate a coverageReport. It shows that the extra code is being tested. So the codecov results are incorrect. (This PR is 75% test code for a reason!). Maybe it has something to do with me being an external contributor? I do not get access to travis secret variables. So if these are needed to report the coverage it will show up as 0%.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313
https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313:545,Modifiability,variab,variables,545,Thanks for the review!. > Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising... running `sbt coverageOn sfsBackend/test coverageAggregate coverageReport`; On this commit does generate a coverageReport. It shows that the extra code is being tested. So the codecov results are incorrect. (This PR is 75% test code for a reason!). Maybe it has something to do with me being an external contributor? I do not get access to travis secret variables. So if these are needed to report the coverage it will show up as 0%.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313
https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313:521,Security,access,access,521,Thanks for the review!. > Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising... running `sbt coverageOn sfsBackend/test coverageAggregate coverageReport`; On this commit does generate a coverageReport. It shows that the extra code is being tested. So the codecov results are incorrect. (This PR is 75% test code for a reason!). Maybe it has something to do with me being an external contributor? I do not get access to travis secret variables. So if these are needed to report the coverage it will show up as 0%.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313
https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313:65,Testability,test,test,65,Thanks for the review!. > Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising... running `sbt coverageOn sfsBackend/test coverageAggregate coverageReport`; On this commit does generate a coverageReport. It shows that the extra code is being tested. So the codecov results are incorrect. (This PR is 75% test code for a reason!). Maybe it has something to do with me being an external contributor? I do not get access to travis secret variables. So if these are needed to report the coverage it will show up as 0%.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313
https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313:146,Testability,test,test,146,Thanks for the review!. > Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising... running `sbt coverageOn sfsBackend/test coverageAggregate coverageReport`; On this commit does generate a coverageReport. It shows that the extra code is being tested. So the codecov results are incorrect. (This PR is 75% test code for a reason!). Maybe it has something to do with me being an external contributor? I do not get access to travis secret variables. So if these are needed to report the coverage it will show up as 0%.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313
https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313:227,Testability,test,test,227,Thanks for the review!. > Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising... running `sbt coverageOn sfsBackend/test coverageAggregate coverageReport`; On this commit does generate a coverageReport. It shows that the extra code is being tested. So the codecov results are incorrect. (This PR is 75% test code for a reason!). Maybe it has something to do with me being an external contributor? I do not get access to travis secret variables. So if these are needed to report the coverage it will show up as 0%.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313
https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313:352,Testability,test,tested,352,Thanks for the review!. > Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising... running `sbt coverageOn sfsBackend/test coverageAggregate coverageReport`; On this commit does generate a coverageReport. It shows that the extra code is being tested. So the codecov results are incorrect. (This PR is 75% test code for a reason!). Maybe it has something to do with me being an external contributor? I do not get access to travis secret variables. So if these are needed to report the coverage it will show up as 0%.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313
https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313:414,Testability,test,test,414,Thanks for the review!. > Actually @kshakir pointed out that the test coverage on this patch is reported as 0% which given what it looks like the test is trying to do is kind of surprising... running `sbt coverageOn sfsBackend/test coverageAggregate coverageReport`; On this commit does generate a coverageReport. It shows that the extra code is being tested. So the codecov results are incorrect. (This PR is 75% test code for a reason!). Maybe it has something to do with me being an external contributor? I do not get access to travis secret variables. So if these are needed to report the coverage it will show up as 0%.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505298313
https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505357240:20,Deployability,patch,patch,20,For some reason the patch coverage works now. Weird that it intermittently works and not works.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5043#issuecomment-505357240
https://github.com/broadinstitute/cromwell/pull/5046#issuecomment-505924531:64,Testability,test,test-runner,64,✅https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/4769/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5046#issuecomment-505924531
https://github.com/broadinstitute/cromwell/pull/5050#issuecomment-506500477:0,Deployability,Hotfix,Hotfix,0,Hotfix worthy?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5050#issuecomment-506500477
https://github.com/broadinstitute/cromwell/pull/5050#issuecomment-506524118:89,Deployability,Hotfix,Hotfix,89,"Meh. On Thu, Jun 27, 2019, 4:33 PM Chris Llanwarne <notifications@github.com>; wrote:. > Hotfix worthy?; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5050?email_source=notifications&email_token=AABILSEPI46IZV46NXKJVRLP4UP2FA5CNFSM4H363QWKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYYJK7I#issuecomment-506500477>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AABILSD3E6NIWBT63BF3Y3DP4UP2FANCNFSM4H363QWA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5050#issuecomment-506524118
https://github.com/broadinstitute/cromwell/pull/5050#issuecomment-506820781:25,Testability,test,test,25,For the record I ran the test against Cromwell 39 and it failed,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5050#issuecomment-506820781
https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509432152:21,Testability,test,test,21,"There is one failing test. I tried merging with the up to date development branch, in the hopes that it will resolve the problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509432152
https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509798159:18,Testability,test,test,18,It turns out that test `cromwell.services.womtool.DescriberSpec` in sbt project `services` tests the values returned by `describe`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509798159
https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509798159:91,Testability,test,tests,91,It turns out that test `cromwell.services.womtool.DescriberSpec` in sbt project `services` tests the values returned by `describe`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509798159
https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758:4346,Deployability,update,update,4346,"yYy9tYWluL3NjYWxhL2N3bC9Xb3JrZmxvdy5zY2FsYQ==) | `95.52% <100%> (+7.46%)` | :arrow_up: |; | [...ain/scala/wdl/transforms/base/wdlom2wom/Util.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvbmV3LWJhc2Uvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmFzZS93ZGxvbTJ3b20vVXRpbC5zY2FsYQ==) | `100% <100%> (ø)` | :arrow_up: |; | [...dlom2wom/WdlDraft2WomWorkflowDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tV29ya2Zsb3dEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `87.5% <75%> (-12.5%)` | :arrow_down: |; | [...m2wom/WdlDraft2WomCommandTaskDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tQ29tbWFuZFRhc2tEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `95.23% <75%> (-4.77%)` | :arrow_down: |; | [.../scala/cromiam/webservice/EngineRouteSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-Q3JvbUlBTS9zcmMvbWFpbi9zY2FsYS9jcm9taWFtL3dlYnNlcnZpY2UvRW5naW5lUm91dGVTdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | ... and [626 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=footer). Last update [da601c8...ae566b9](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758
https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758:4245,Energy Efficiency,Power,Powered,4245,"yYy9tYWluL3NjYWxhL2N3bC9Xb3JrZmxvdy5zY2FsYQ==) | `95.52% <100%> (+7.46%)` | :arrow_up: |; | [...ain/scala/wdl/transforms/base/wdlom2wom/Util.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvbmV3LWJhc2Uvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmFzZS93ZGxvbTJ3b20vVXRpbC5zY2FsYQ==) | `100% <100%> (ø)` | :arrow_up: |; | [...dlom2wom/WdlDraft2WomWorkflowDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tV29ya2Zsb3dEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `87.5% <75%> (-12.5%)` | :arrow_down: |; | [...m2wom/WdlDraft2WomCommandTaskDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tQ29tbWFuZFRhc2tEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `95.23% <75%> (-4.77%)` | :arrow_down: |; | [.../scala/cromiam/webservice/EngineRouteSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-Q3JvbUlBTS9zcmMvbWFpbi9zY2FsYS9jcm9taWFtL3dlYnNlcnZpY2UvRW5naW5lUm91dGVTdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | ... and [626 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=footer). Last update [da601c8...ae566b9](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758
https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758:4108,Usability,learn,learn,4108,"yYy9tYWluL3NjYWxhL2N3bC9Xb3JrZmxvdy5zY2FsYQ==) | `95.52% <100%> (+7.46%)` | :arrow_up: |; | [...ain/scala/wdl/transforms/base/wdlom2wom/Util.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvbmV3LWJhc2Uvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmFzZS93ZGxvbTJ3b20vVXRpbC5zY2FsYQ==) | `100% <100%> (ø)` | :arrow_up: |; | [...dlom2wom/WdlDraft2WomWorkflowDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tV29ya2Zsb3dEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `87.5% <75%> (-12.5%)` | :arrow_down: |; | [...m2wom/WdlDraft2WomCommandTaskDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tQ29tbWFuZFRhc2tEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `95.23% <75%> (-4.77%)` | :arrow_down: |; | [.../scala/cromiam/webservice/EngineRouteSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-Q3JvbUlBTS9zcmMvbWFpbi9zY2FsYS9jcm9taWFtL3dlYnNlcnZpY2UvRW5naW5lUm91dGVTdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | ... and [626 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=footer). Last update [da601c8...ae566b9](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758
https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-510137858:18,Testability,test,test,18,"@cjllanwarne, the test `cromwell.services.womtool.DescriberSpec` in sbt project `services` tests the values returned by describe.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-510137858
https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-510137858:91,Testability,test,tests,91,"@cjllanwarne, the test `cromwell.services.womtool.DescriberSpec` in sbt project `services` tests the values returned by describe.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-510137858
https://github.com/broadinstitute/cromwell/pull/5054#issuecomment-507519998:123,Deployability,update,updates,123,Thanks for the fix. Whenever updating the swagger you'll also need to run `sbt generateRestApiDocs` then commit & push the updates.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5054#issuecomment-507519998
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:78,Availability,error,error,78,"The AWS cromwell-aio template for Cromwell 43 (latest) returned the following error after a workflow is submitted. fyi. This template works fine with Cromwell 42. 2019-07-02 19:16:36,824 cromwell-system-akka.dispatchers.api-dispatcher-73 INFO - Unspecified type (Unspecified version) workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 submitted; 2019-07-02 19:16:37,222 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - 1 new workflows fetched by cromid-271b774: 10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,239 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor Starting workflow UUID(10f172e8-b7ba-416f-964e-22ab8c7b38e3); 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor Successfully started WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2019-07-02 19:16:37,271 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2019-07-02 19:16:37,932 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credent",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:1257,Availability,ERROR,ERROR,1257,"ecified version) workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 submitted; 2019-07-02 19:16:37,222 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - 1 new workflows fetched by cromid-271b774: 10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,239 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor Starting workflow UUID(10f172e8-b7ba-416f-964e-22ab8c7b38e3); 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor Successfully started WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2019-07-02 19:16:37,271 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2019-07-02 19:16:37,932 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderF",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:8992,Availability,ERROR,ERROR,8992,"zon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderFactory.scala:59); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:8545,Energy Efficiency,adapt,adapted,8545,"ncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:16256,Energy Efficiency,adapt,adapted,16256,"ader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 more. 2019-07-02 19:16:37,991 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3 is in a terminal state: WorkflowFailedState",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:6778,Integrability,wrap,wrapAndCopyInto,6778,ider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at java.util.ServiceLoader.fail(ServiceLoader.java:239); 	at java.util.ServiceLoader.access$300(ServiceLoader.java:185); 	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372); 	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); 	at java.util.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssd,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:14489,Integrability,wrap,wrapAndCopyInto,14489,ider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at java.util.ServiceLoader.fail(ServiceLoader.java:239); 	at java.util.ServiceLoader.access$300(ServiceLoader.java:185); 	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372); 	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); 	at java.util.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssd,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:1108,Modifiability,config,configured,1108,"d. fyi. This template works fine with Cromwell 42. 2019-07-02 19:16:36,824 cromwell-system-akka.dispatchers.api-dispatcher-73 INFO - Unspecified type (Unspecified version) workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 submitted; 2019-07-02 19:16:37,222 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - 1 new workflows fetched by cromid-271b774: 10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,239 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor Starting workflow UUID(10f172e8-b7ba-416f-964e-22ab8c7b38e3); 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor Successfully started WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2019-07-02 19:16:37,271 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2019-07-02 19:16:37,932 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:8545,Modifiability,adapt,adapted,8545,"ncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:16256,Modifiability,adapt,adapted,16256,"ader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 more. 2019-07-02 19:16:37,991 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3 is in a terminal state: WorkflowFailedState",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:6243,Performance,load,loader,6243,ox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.util.ServiceConfigurationError: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at java.util.ServiceLoader.fail(ServiceLoader.java:239); 	at java.util.ServiceLoader.access$300(ServiceLoader.java:185); 	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372); 	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); 	at java.util.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttp,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:6282,Performance,load,loadService,6282,ilbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.util.ServiceConfigurationError: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at java.util.ServiceLoader.fail(ServiceLoader.java:239); 	at java.util.ServiceLoader.access$300(ServiceLoader.java:185); 	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372); 	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); 	at java.util.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:7095,Performance,load,loader,7095,Iterator.next(ServiceLoader.java:404); 	at java.util.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.bui,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:7130,Performance,load,loadService,7130,.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at soft,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:7226,Performance,load,loader,7226,internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsCl,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:7263,Performance,load,loadService,7263,adService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:7361,Performance,load,loader,7361,cePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:13954,Performance,load,loader,13954,ox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.util.ServiceConfigurationError: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at java.util.ServiceLoader.fail(ServiceLoader.java:239); 	at java.util.ServiceLoader.access$300(ServiceLoader.java:185); 	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372); 	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); 	at java.util.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttp,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:13993,Performance,load,loadService,13993,ilbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.util.ServiceConfigurationError: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at java.util.ServiceLoader.fail(ServiceLoader.java:239); 	at java.util.ServiceLoader.access$300(ServiceLoader.java:185); 	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372); 	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); 	at java.util.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:14806,Performance,load,loader,14806,Iterator.next(ServiceLoader.java:404); 	at java.util.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.bui,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:14841,Performance,load,loadService,14841,.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at soft,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:14937,Performance,load,loader,14937,internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsCl,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:14974,Performance,load,loadService,14974,adService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:15072,Performance,load,loader,15072,cePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464); 	at software.amazon.awssdk.core.internal.http.loader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:1630,Security,validat,validateCredential,1630,"b7ba-416f-964e-22ab8c7b38e3); 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor Successfully started WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2019-07-02 19:16:37,271 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2019-07-02 19:16:37,932 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderFactory.scala:59); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$s",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:1719,Security,validat,validateCredential,1719,"ine-dispatcher-31 INFO - WorkflowManagerActor Successfully started WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2019-07-02 19:16:37,271 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2019-07-02 19:16:37,932 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderFactory.scala:59); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:1809,Security,validat,validateCredential,1809,"-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2019-07-02 19:16:37,271 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2019-07-02 19:16:37,932 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderFactory.scala:59); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.Li",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:3490,Security,Validat,Validated,3490,nstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); 	at cromwell.core.path.PathBuilderFactory$.instantiatePathBuilders(PathBuilderFactory.scala:23); 	at cromwell.engine.EngineFilesystems$.pathBuildersForWorkflow(EngineFilesystems.scala:29); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$workflowOptionsAndPathBuilders$1(MaterializeWorkflowDescriptorActor.scala:226); 	at cats.data.Validated.map(Validated.scala:204); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowOptionsAndPathBuilders(MaterializeWorkflowDescriptorActor.scala:225); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:159); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:155); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEv,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:3504,Security,Validat,Validated,3504,on$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); 	at cromwell.core.path.PathBuilderFactory$.instantiatePathBuilders(PathBuilderFactory.scala:23); 	at cromwell.engine.EngineFilesystems$.pathBuildersForWorkflow(EngineFilesystems.scala:29); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$workflowOptionsAndPathBuilders$1(MaterializeWorkflowDescriptorActor.scala:226); 	at cats.data.Validated.map(Validated.scala:204); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowOptionsAndPathBuilders(MaterializeWorkflowDescriptorActor.scala:225); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:159); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:155); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scal,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:5952,Security,access,access,5952,well.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.util.ServiceConfigurationError: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at java.util.ServiceLoader.fail(ServiceLoader.java:239); 	at java.util.ServiceLoader.access$300(ServiceLoader.java:185); 	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372); 	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); 	at java.util.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:8632,Security,validat,validateCredential,8632,"t software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:8841,Security,validat,validateCredential,8841,"figuration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:9341,Security,validat,validateCredential,9341,"a:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderFactory.scala:59); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$s",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:9430,Security,validat,validateCredential,9430,"AuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderFactory.scala:59); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:9520,Security,validat,validateCredential,9520,"idation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential(AwsAuthMode.scala:117); 	at cromwell.cloudsupport.aws.auth.DefaultMode.credential(AwsAuthMode.scala:130); 	at cromwell.filesystems.s3.S3PathBuilder$.fromAuthMode(S3PathBuilder.scala:118); 	at cromwell.filesystems.s3.S3PathBuilderFactory.withOptions(S3PathBuilderFactory.scala:59); 	at cromwell.core.path.PathBuilderFactory$.$anonfun$instantiatePathBuilders$2(PathBuilderFactory.scala:23); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.Li",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:11201,Security,Validat,Validated,11201,nstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); 	at cromwell.core.path.PathBuilderFactory$.instantiatePathBuilders(PathBuilderFactory.scala:23); 	at cromwell.engine.EngineFilesystems$.pathBuildersForWorkflow(EngineFilesystems.scala:29); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$workflowOptionsAndPathBuilders$1(MaterializeWorkflowDescriptorActor.scala:226); 	at cats.data.Validated.map(Validated.scala:204); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowOptionsAndPathBuilders(MaterializeWorkflowDescriptorActor.scala:225); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:159); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:155); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEv,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:11215,Security,Validat,Validated,11215,on$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$$anon$11.$anonfun$start$3(Eval.scala:275); 	at cats.Eval$.loop$1(Eval.scala:336); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); 	at cromwell.core.path.PathBuilderFactory$.instantiatePathBuilders(PathBuilderFactory.scala:23); 	at cromwell.engine.EngineFilesystems$.pathBuildersForWorkflow(EngineFilesystems.scala:29); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$workflowOptionsAndPathBuilders$1(MaterializeWorkflowDescriptorActor.scala:226); 	at cats.data.Validated.map(Validated.scala:204); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowOptionsAndPathBuilders(MaterializeWorkflowDescriptorActor.scala:225); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:159); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:155); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scal,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:13663,Security,access,access,13663,well.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.util.ServiceConfigurationError: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at java.util.ServiceLoader.fail(ServiceLoader.java:239); 	at java.util.ServiceLoader.access$300(ServiceLoader.java:185); 	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:372); 	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404); 	at java.util.ServiceLoader$1.next(ServiceLoader.java:480); 	at software.amazon.awssdk.core.internal.http.loader.ClasspathSdkHttpServiceProvider.loadService(ClasspathSdkHttpServiceProvider.java:53); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.tryAdvance(Spliterators.java:958); 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126); 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:16343,Security,validat,validateCredential,16343,"ader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 more. 2019-07-02 19:16:37,991 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3 is in a terminal state: WorkflowFailedState",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:16552,Security,validat,validateCredential,16552,"ader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 more. 2019-07-02 19:16:37,991 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3 is in a terminal state: WorkflowFailedState",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:4382,Testability,Log,LoggingFSM,4382,nsAndPathBuilders$1(MaterializeWorkflowDescriptorActor.scala:226); 	at cats.data.Validated.map(Validated.scala:204); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowOptionsAndPathBuilders(MaterializeWorkflowDescriptorActor.scala:225); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:159); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:155); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoi,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:4475,Testability,Log,LoggingFSM,4475,cats.data.Validated.map(Validated.scala:204); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowOptionsAndPathBuilders(MaterializeWorkflowDescriptorActor.scala:225); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:159); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:155); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoi,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:4530,Testability,Log,LoggingFSM,4530,ell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowOptionsAndPathBuilders(MaterializeWorkflowDescriptorActor.scala:225); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:159); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:155); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:12093,Testability,Log,LoggingFSM,12093,nsAndPathBuilders$1(MaterializeWorkflowDescriptorActor.scala:226); 	at cats.data.Validated.map(Validated.scala:204); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowOptionsAndPathBuilders(MaterializeWorkflowDescriptorActor.scala:225); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:159); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:155); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoi,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:12186,Testability,Log,LoggingFSM,12186,cats.data.Validated.map(Validated.scala:204); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowOptionsAndPathBuilders(MaterializeWorkflowDescriptorActor.scala:225); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:159); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:155); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoi,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:12241,Testability,Log,LoggingFSM,12241,ell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowOptionsAndPathBuilders(MaterializeWorkflowDescriptorActor.scala:225); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:159); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:155); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-516195804:56,Availability,error,error,56,@geoffjentry I didn't dig into to the root cause of the error. I launched two Cromwell servers (via the cfn template on the AWS docs page) against the same AWS Batch setup and tested a hello world wdl.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-516195804
https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-516195804:176,Testability,test,tested,176,@geoffjentry I didn't dig into to the root cause of the error. I launched two Cromwell servers (via the cfn template on the AWS docs page) against the same AWS Batch setup and tested a hello world wdl.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-516195804
https://github.com/broadinstitute/cromwell/pull/5058#issuecomment-509633947:13,Deployability,patch,patch,13,Will need to patch a lack-of-jdbc-`convert()` breaking call caching in MariaDB.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5058#issuecomment-509633947
https://github.com/broadinstitute/cromwell/pull/5058#issuecomment-510463770:69,Safety,sanity check,sanity check,69,Submitted a new commit that works around a MariaDB issue. Requesting sanity check for that new commit.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5058#issuecomment-510463770
https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938418:172,Deployability,install,installed,172,"I'm a :-1: on this idea. Seems like it is overly demanding as compilation is less of an ask than committing code. For instance, now our build servers must have git secrets installed where it is irrelevant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938418
https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:131,Availability,error,error,131,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820
https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:291,Availability,error,error,291,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820
https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:561,Availability,error,error,561,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820
https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:796,Availability,error,error,796,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820
https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:91,Deployability,install,installed,91,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820
https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:726,Deployability,install,installed,726,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820
https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:139,Integrability,message,messages,139,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820
https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:431,Modifiability,config,config,431,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820
https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:221,Performance,perform,performance,221,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820
https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:12,Testability,test,tested,12,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820
https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510948597:60,Deployability,install,installed,60,"> For instance, now our build servers must have git secrets installed where it is irrelevant. @danbills That's not true - unless they want to commit code. This only asks them to configure a set of git hooks which they'll never end up using (or to add a compile time option which makes the compilation ignore the check).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510948597
https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510948597:178,Modifiability,config,configure,178,"> For instance, now our build servers must have git secrets installed where it is irrelevant. @danbills That's not true - unless they want to commit code. This only asks them to configure a set of git hooks which they'll never end up using (or to add a compile time option which makes the compilation ignore the check).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510948597
https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510955574:17,Usability,feedback,feedback,17,Closing based on feedback.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510955574
https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-510842794:146,Deployability,configurat,configuration,146,@orodeh I would probably start with something in `WdlFileToWomSpec`. If you wanted to do that you'll probably need to find a way to customize the configuration for just that test case,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-510842794
https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-510842794:146,Modifiability,config,configuration,146,@orodeh I would probably start with something in `WdlFileToWomSpec`. If you wanted to do that you'll probably need to find a way to customize the configuration for just that test case,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-510842794
https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-510842794:174,Testability,test,test,174,@orodeh I would probably start with something in `WdlFileToWomSpec`. If you wanted to do that you'll probably need to find a way to customize the configuration for just that test case,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-510842794
https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511051253:75,Deployability,configurat,configuration,75,"I added a test to `WdlFileToWomSpec`. However, I don't know how to set the configuration flag `wom-parse.convert-nested-scatter-to-subworkflow` inside of it. I can do it manually, and it works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511051253
https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511051253:75,Modifiability,config,configuration,75,"I added a test to `WdlFileToWomSpec`. However, I don't know how to set the configuration flag `wom-parse.convert-nested-scatter-to-subworkflow` inside of it. I can do it manually, and it works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511051253
https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511051253:10,Testability,test,test,10,"I added a test to `WdlFileToWomSpec`. However, I don't know how to set the configuration flag `wom-parse.convert-nested-scatter-to-subworkflow` inside of it. I can do it manually, and it works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511051253
https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511603946:32,Integrability,depend,dependency,32,"@aednichols, rewrote using the ""dependency injection pattern"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511603946
https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511603946:43,Integrability,inject,injection,43,"@aednichols, rewrote using the ""dependency injection pattern"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511603946
https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511603946:43,Security,inject,injection,43,"@aednichols, rewrote using the ""dependency injection pattern"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511603946
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509638085:697,Availability,avail,available,697,"I'm just reading a bit [more](https://linux.die.net/man/1/flock) about `flock` - so with `-x` the idea is that you are creating a write lock - this means that this particular script will be run by many workers, and the first that gets to creating the lock will get it, continue, and do the build. ```; -s, --shared; Obtain a shared lock, sometimes called a read lock.; -x, -e, --exclusive; Obtain an exclusive lock, sometimes called a write lock. This is the default.; ```; I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.) . Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509638085
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509638449:35,Modifiability,variab,variables,35,"I also wouldn't be sparse with the variables, for some future user coming to read this, I would use `--exclusive` instead of `-x` and then `--unlock` instead of `-u` so it's explicitly clear.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509638449
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509638449:185,Usability,clear,clear,185,"I also wouldn't be sparse with the variables, for some future user coming to read this, I would use `--exclusive` instead of `-x` and then `--unlock` instead of `-u` so it's explicitly clear.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509638449
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:225,Availability,avail,available,225,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:402,Availability,avail,available,402,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:899,Modifiability,variab,variables,899,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:692,Performance,cache,cached,692,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:721,Performance,cache,cache,721,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:846,Performance,cache,cache,846,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:815,Safety,avoid,avoid,815,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973:1041,Usability,clear,clear,1041,"> I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.). This is the default behaviour of `flock`, I believe. My flock man page says: ""By default, if the lock cannot be immediately acquired, flock waits until the lock is available."". > Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry. My understanding of Singularity was that the actual *pulling* would be cached using the Singularity cache, and only the *building* would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. > I also wouldn't be sparse with the variables, for some future user coming to read this, I would use --exclusive instead of -x and then --unlock instead of -u so it's explicitly clear. Agreed! I'll edit the OP.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509639973
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616:71,Performance,cache,cached,71,"> My understanding of Singularity was that the actual pulling would be cached using the Singularity cache, and only the building would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. I think that if it's the case that the (finished image) isn't in the cache, all of the workers would start building (and then not copy to the final thing given that another worker got there first). It's been a while, so it might be good to check with others on slack.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616:100,Performance,cache,cache,100,"> My understanding of Singularity was that the actual pulling would be cached using the Singularity cache, and only the building would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. I think that if it's the case that the (finished image) isn't in the cache, all of the workers would start building (and then not copy to the final thing given that another worker got there first). It's been a while, so it might be good to check with others on slack.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616:223,Performance,cache,cache,223,"> My understanding of Singularity was that the actual pulling would be cached using the Singularity cache, and only the building would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. I think that if it's the case that the (finished image) isn't in the cache, all of the workers would start building (and then not copy to the final thing given that another worker got there first). It's been a while, so it might be good to check with others on slack.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616:308,Performance,cache,cache,308,"> My understanding of Singularity was that the actual pulling would be cached using the Singularity cache, and only the building would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. I think that if it's the case that the (finished image) isn't in the cache, all of the workers would start building (and then not copy to the final thing given that another worker got there first). It's been a while, so it might be good to check with others on slack.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616:192,Safety,avoid,avoid,192,"> My understanding of Singularity was that the actual pulling would be cached using the Singularity cache, and only the building would be duplicated. Is this not right? In any case, this will avoid smashing the Singularity cache at least. I think that if it's the case that the (finished image) isn't in the cache, all of the workers would start building (and then not copy to the final thing given that another worker got there first). It's been a while, so it might be good to check with others on slack.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509643616
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509650419:69,Deployability,update,updated,69,"I think I misunderstood how to use `flock` in a bash script, so I've updated this with a tentative improvement:. ```bash; (; flock --exclusive 200; # Build the image; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi; ) 200>/var/lock/$IMAGE; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509650419
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509871160:17,Energy Efficiency,power,power,17,"I don't have the power to invite people, but the signup link is here: https://sylabs.io/singularity/slack/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509871160
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510760560:277,Availability,avail,available,277,"Also I've done some more research on `flock`, and although people claim that `flock` doesn't work on certain filesystems (NFS, for example), this has been fixed for a very long time in the Linux kernel (since Linux 2.6.12, released in 2005). In addition, it seems to be widely available in Linux distros, and is installed by default, unlike other locking tools like `lockfile`. So I still think `flock` is the best option for this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510760560
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510760560:223,Deployability,release,released,223,"Also I've done some more research on `flock`, and although people claim that `flock` doesn't work on certain filesystems (NFS, for example), this has been fixed for a very long time in the Linux kernel (since Linux 2.6.12, released in 2005). In addition, it seems to be widely available in Linux distros, and is installed by default, unlike other locking tools like `lockfile`. So I still think `flock` is the best option for this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510760560
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510760560:312,Deployability,install,installed,312,"Also I've done some more research on `flock`, and although people claim that `flock` doesn't work on certain filesystems (NFS, for example), this has been fixed for a very long time in the Linux kernel (since Linux 2.6.12, released in 2005). In addition, it seems to be widely available in Linux distros, and is installed by default, unlike other locking tools like `lockfile`. So I still think `flock` is the best option for this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510760560
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510867281:15,Testability,test,test,15,Did you get to test this out @TMiguelT ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510867281
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510918985:40,Testability,test,testing,40,"I gather @illusional managed to do some testing. I haven't heard any details from him yet, though",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-510918985
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637:962,Deployability,release,released,962,"Hey everyone, I did manage to do some testing today (after some discussion with @TMiguelT as well). Some small notes about my setup:. - I'm using Singularity that has the ability to store and run an OCI container to/from a file.; - I have one place `/path/to/containers/*` where I store all my containers.; - I transform the container digest (returned by Cromwell as `${docker}`) to generate a filename and use that to uniquely reference the container (per this PR: #4797). Notes about my (slightly modified) config below:. - My `$image` var has slashes in it (because it's a path to a file) which isn't correct, as `flock` expects a valid path, so I've just used `$docker_subbed` which is the transformed docker file.; - I didn't have write permission to `/var/lock/$imagename`, I've opted instead for the container directory.; - I wanted the output of `flock` to be redirected to Cromwell's `stderr.submit`.; - I do the second image check for when the lock is released, the locked processes will find the image and skip the pull (per @rherban's [comment](https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509677104)); ```bash; # transformed docker digest; docker_subbed=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); # output path of container (.sif); image=/path/to/containers/$docker_subbed.sif; # declare a very similar path (.lock) where Cromwell can access; lockpath=/path/to/containers/$docker_subbed.lock . if [ ! -f ""$image"" ]; then # If we already have the image, skip everything; (; flock --verbose --exclusive 200 1>&2; if [ ! -f ""$image"" ]; then # do a second check once the lock has been released ; singularity pull ""$image"" docker://${docker}; fi; ) 200>/var/lock/$lockpath; fi; ```. Hope this helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637:1623,Deployability,release,released,1623,"Hey everyone, I did manage to do some testing today (after some discussion with @TMiguelT as well). Some small notes about my setup:. - I'm using Singularity that has the ability to store and run an OCI container to/from a file.; - I have one place `/path/to/containers/*` where I store all my containers.; - I transform the container digest (returned by Cromwell as `${docker}`) to generate a filename and use that to uniquely reference the container (per this PR: #4797). Notes about my (slightly modified) config below:. - My `$image` var has slashes in it (because it's a path to a file) which isn't correct, as `flock` expects a valid path, so I've just used `$docker_subbed` which is the transformed docker file.; - I didn't have write permission to `/var/lock/$imagename`, I've opted instead for the container directory.; - I wanted the output of `flock` to be redirected to Cromwell's `stderr.submit`.; - I do the second image check for when the lock is released, the locked processes will find the image and skip the pull (per @rherban's [comment](https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509677104)); ```bash; # transformed docker digest; docker_subbed=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); # output path of container (.sif); image=/path/to/containers/$docker_subbed.sif; # declare a very similar path (.lock) where Cromwell can access; lockpath=/path/to/containers/$docker_subbed.lock . if [ ! -f ""$image"" ]; then # If we already have the image, skip everything; (; flock --verbose --exclusive 200 1>&2; if [ ! -f ""$image"" ]; then # do a second check once the lock has been released ; singularity pull ""$image"" docker://${docker}; fi; ) 200>/var/lock/$lockpath; fi; ```. Hope this helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637:509,Modifiability,config,config,509,"Hey everyone, I did manage to do some testing today (after some discussion with @TMiguelT as well). Some small notes about my setup:. - I'm using Singularity that has the ability to store and run an OCI container to/from a file.; - I have one place `/path/to/containers/*` where I store all my containers.; - I transform the container digest (returned by Cromwell as `${docker}`) to generate a filename and use that to uniquely reference the container (per this PR: #4797). Notes about my (slightly modified) config below:. - My `$image` var has slashes in it (because it's a path to a file) which isn't correct, as `flock` expects a valid path, so I've just used `$docker_subbed` which is the transformed docker file.; - I didn't have write permission to `/var/lock/$imagename`, I've opted instead for the container directory.; - I wanted the output of `flock` to be redirected to Cromwell's `stderr.submit`.; - I do the second image check for when the lock is released, the locked processes will find the image and skip the pull (per @rherban's [comment](https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509677104)); ```bash; # transformed docker digest; docker_subbed=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); # output path of container (.sif); image=/path/to/containers/$docker_subbed.sif; # declare a very similar path (.lock) where Cromwell can access; lockpath=/path/to/containers/$docker_subbed.lock . if [ ! -f ""$image"" ]; then # If we already have the image, skip everything; (; flock --verbose --exclusive 200 1>&2; if [ ! -f ""$image"" ]; then # do a second check once the lock has been released ; singularity pull ""$image"" docker://${docker}; fi; ) 200>/var/lock/$lockpath; fi; ```. Hope this helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637:1377,Security,access,access,1377,"Hey everyone, I did manage to do some testing today (after some discussion with @TMiguelT as well). Some small notes about my setup:. - I'm using Singularity that has the ability to store and run an OCI container to/from a file.; - I have one place `/path/to/containers/*` where I store all my containers.; - I transform the container digest (returned by Cromwell as `${docker}`) to generate a filename and use that to uniquely reference the container (per this PR: #4797). Notes about my (slightly modified) config below:. - My `$image` var has slashes in it (because it's a path to a file) which isn't correct, as `flock` expects a valid path, so I've just used `$docker_subbed` which is the transformed docker file.; - I didn't have write permission to `/var/lock/$imagename`, I've opted instead for the container directory.; - I wanted the output of `flock` to be redirected to Cromwell's `stderr.submit`.; - I do the second image check for when the lock is released, the locked processes will find the image and skip the pull (per @rherban's [comment](https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509677104)); ```bash; # transformed docker digest; docker_subbed=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); # output path of container (.sif); image=/path/to/containers/$docker_subbed.sif; # declare a very similar path (.lock) where Cromwell can access; lockpath=/path/to/containers/$docker_subbed.lock . if [ ! -f ""$image"" ]; then # If we already have the image, skip everything; (; flock --verbose --exclusive 200 1>&2; if [ ! -f ""$image"" ]; then # do a second check once the lock has been released ; singularity pull ""$image"" docker://${docker}; fi; ) 200>/var/lock/$lockpath; fi; ```. Hope this helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637:38,Testability,test,testing,38,"Hey everyone, I did manage to do some testing today (after some discussion with @TMiguelT as well). Some small notes about my setup:. - I'm using Singularity that has the ability to store and run an OCI container to/from a file.; - I have one place `/path/to/containers/*` where I store all my containers.; - I transform the container digest (returned by Cromwell as `${docker}`) to generate a filename and use that to uniquely reference the container (per this PR: #4797). Notes about my (slightly modified) config below:. - My `$image` var has slashes in it (because it's a path to a file) which isn't correct, as `flock` expects a valid path, so I've just used `$docker_subbed` which is the transformed docker file.; - I didn't have write permission to `/var/lock/$imagename`, I've opted instead for the container directory.; - I wanted the output of `flock` to be redirected to Cromwell's `stderr.submit`.; - I do the second image check for when the lock is released, the locked processes will find the image and skip the pull (per @rherban's [comment](https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509677104)); ```bash; # transformed docker digest; docker_subbed=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); # output path of container (.sif); image=/path/to/containers/$docker_subbed.sif; # declare a very similar path (.lock) where Cromwell can access; lockpath=/path/to/containers/$docker_subbed.lock . if [ ! -f ""$image"" ]; then # If we already have the image, skip everything; (; flock --verbose --exclusive 200 1>&2; if [ ! -f ""$image"" ]; then # do a second check once the lock has been released ; singularity pull ""$image"" docker://${docker}; fi; ) 200>/var/lock/$lockpath; fi; ```. Hope this helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517123637
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517265945:93,Modifiability,variab,variables,93,"Would it be possible to use realistic paths, perhaps the `SINGULARITY_CACHEDIR`? And to make variables explicitly clear (and in quotes), maybe ""${docker_subbed}.sif"" instead of $docker_subbed.sif. The goal would be to have a general script that can work for most, correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517265945
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517265945:114,Usability,clear,clear,114,"Would it be possible to use realistic paths, perhaps the `SINGULARITY_CACHEDIR`? And to make variables explicitly clear (and in quotes), maybe ""${docker_subbed}.sif"" instead of $docker_subbed.sif. The goal would be to have a general script that can work for most, correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-517265945
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921:269,Availability,down,download,269,"Here is a slightly more general script (it assumes the lock file and saved image are in the current directory). It also does the pull into a temp file with a rename into the destination name at the end so that for a large image the -f check won't trigger for a partial download. I do some work here to deduce a filename that should match (as I understand the rules anyway) the one that the pull would create. I also include an option to force the path, since in my automation I tend to wish to define everything for my self. The derived or given image filename is echoed at the end. YOUR_HOST is the name of the sregistry host (this is the context I'm doing this in). ```; #!/bin/bash . lock_dir=. if [[ $# -ne 1 && $# -ne 2 ]] ; then; echo ""Usage: $0 image-name [output-file]"" 1>&2; exit 1; fi; name=$1; output_file=$2. if [[ ""$output_file"" = """" ]] ; then. # deduce filename . output_file=`basename $name`; if [[ $output_file =~ (.*):([^:]+)$ ]] ; then; base=${BASH_REMATCH[1]}; tag=${BASH_REMATCH[2]}; else; base=$output_file; tag=latest; fi; output_file=""${base}_$tag.sif""; fi. url=shub://YOUR_HOST/$name. # declare a very similar path (.lock) where Cromwell can access ; lock=$lock_dir/$output_file.lock; tmp=$output_file.tmp. if [ ! -f ""$output_file"" ]; then # If we already have the image, skip everything ; (; flock --exclusive 200; if [ ! -f ""$output_file"" ]; then # do a second check once the lock has been released ; singularity pull --nohttps $tmp $url; mv $tmp $output_file; fi; ) 200>$lock; fi. rm -f $lock. echo $output_file; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921:564,Availability,echo,echoed,564,"Here is a slightly more general script (it assumes the lock file and saved image are in the current directory). It also does the pull into a temp file with a rename into the destination name at the end so that for a large image the -f check won't trigger for a partial download. I do some work here to deduce a filename that should match (as I understand the rules anyway) the one that the pull would create. I also include an option to force the path, since in my automation I tend to wish to define everything for my self. The derived or given image filename is echoed at the end. YOUR_HOST is the name of the sregistry host (this is the context I'm doing this in). ```; #!/bin/bash . lock_dir=. if [[ $# -ne 1 && $# -ne 2 ]] ; then; echo ""Usage: $0 image-name [output-file]"" 1>&2; exit 1; fi; name=$1; output_file=$2. if [[ ""$output_file"" = """" ]] ; then. # deduce filename . output_file=`basename $name`; if [[ $output_file =~ (.*):([^:]+)$ ]] ; then; base=${BASH_REMATCH[1]}; tag=${BASH_REMATCH[2]}; else; base=$output_file; tag=latest; fi; output_file=""${base}_$tag.sif""; fi. url=shub://YOUR_HOST/$name. # declare a very similar path (.lock) where Cromwell can access ; lock=$lock_dir/$output_file.lock; tmp=$output_file.tmp. if [ ! -f ""$output_file"" ]; then # If we already have the image, skip everything ; (; flock --exclusive 200; if [ ! -f ""$output_file"" ]; then # do a second check once the lock has been released ; singularity pull --nohttps $tmp $url; mv $tmp $output_file; fi; ) 200>$lock; fi. rm -f $lock. echo $output_file; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921:736,Availability,echo,echo,736,"Here is a slightly more general script (it assumes the lock file and saved image are in the current directory). It also does the pull into a temp file with a rename into the destination name at the end so that for a large image the -f check won't trigger for a partial download. I do some work here to deduce a filename that should match (as I understand the rules anyway) the one that the pull would create. I also include an option to force the path, since in my automation I tend to wish to define everything for my self. The derived or given image filename is echoed at the end. YOUR_HOST is the name of the sregistry host (this is the context I'm doing this in). ```; #!/bin/bash . lock_dir=. if [[ $# -ne 1 && $# -ne 2 ]] ; then; echo ""Usage: $0 image-name [output-file]"" 1>&2; exit 1; fi; name=$1; output_file=$2. if [[ ""$output_file"" = """" ]] ; then. # deduce filename . output_file=`basename $name`; if [[ $output_file =~ (.*):([^:]+)$ ]] ; then; base=${BASH_REMATCH[1]}; tag=${BASH_REMATCH[2]}; else; base=$output_file; tag=latest; fi; output_file=""${base}_$tag.sif""; fi. url=shub://YOUR_HOST/$name. # declare a very similar path (.lock) where Cromwell can access ; lock=$lock_dir/$output_file.lock; tmp=$output_file.tmp. if [ ! -f ""$output_file"" ]; then # If we already have the image, skip everything ; (; flock --exclusive 200; if [ ! -f ""$output_file"" ]; then # do a second check once the lock has been released ; singularity pull --nohttps $tmp $url; mv $tmp $output_file; fi; ) 200>$lock; fi. rm -f $lock. echo $output_file; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921:1521,Availability,echo,echo,1521,"Here is a slightly more general script (it assumes the lock file and saved image are in the current directory). It also does the pull into a temp file with a rename into the destination name at the end so that for a large image the -f check won't trigger for a partial download. I do some work here to deduce a filename that should match (as I understand the rules anyway) the one that the pull would create. I also include an option to force the path, since in my automation I tend to wish to define everything for my self. The derived or given image filename is echoed at the end. YOUR_HOST is the name of the sregistry host (this is the context I'm doing this in). ```; #!/bin/bash . lock_dir=. if [[ $# -ne 1 && $# -ne 2 ]] ; then; echo ""Usage: $0 image-name [output-file]"" 1>&2; exit 1; fi; name=$1; output_file=$2. if [[ ""$output_file"" = """" ]] ; then. # deduce filename . output_file=`basename $name`; if [[ $output_file =~ (.*):([^:]+)$ ]] ; then; base=${BASH_REMATCH[1]}; tag=${BASH_REMATCH[2]}; else; base=$output_file; tag=latest; fi; output_file=""${base}_$tag.sif""; fi. url=shub://YOUR_HOST/$name. # declare a very similar path (.lock) where Cromwell can access ; lock=$lock_dir/$output_file.lock; tmp=$output_file.tmp. if [ ! -f ""$output_file"" ]; then # If we already have the image, skip everything ; (; flock --exclusive 200; if [ ! -f ""$output_file"" ]; then # do a second check once the lock has been released ; singularity pull --nohttps $tmp $url; mv $tmp $output_file; fi; ) 200>$lock; fi. rm -f $lock. echo $output_file; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921:1416,Deployability,release,released,1416,"Here is a slightly more general script (it assumes the lock file and saved image are in the current directory). It also does the pull into a temp file with a rename into the destination name at the end so that for a large image the -f check won't trigger for a partial download. I do some work here to deduce a filename that should match (as I understand the rules anyway) the one that the pull would create. I also include an option to force the path, since in my automation I tend to wish to define everything for my self. The derived or given image filename is echoed at the end. YOUR_HOST is the name of the sregistry host (this is the context I'm doing this in). ```; #!/bin/bash . lock_dir=. if [[ $# -ne 1 && $# -ne 2 ]] ; then; echo ""Usage: $0 image-name [output-file]"" 1>&2; exit 1; fi; name=$1; output_file=$2. if [[ ""$output_file"" = """" ]] ; then. # deduce filename . output_file=`basename $name`; if [[ $output_file =~ (.*):([^:]+)$ ]] ; then; base=${BASH_REMATCH[1]}; tag=${BASH_REMATCH[2]}; else; base=$output_file; tag=latest; fi; output_file=""${base}_$tag.sif""; fi. url=shub://YOUR_HOST/$name. # declare a very similar path (.lock) where Cromwell can access ; lock=$lock_dir/$output_file.lock; tmp=$output_file.tmp. if [ ! -f ""$output_file"" ]; then # If we already have the image, skip everything ; (; flock --exclusive 200; if [ ! -f ""$output_file"" ]; then # do a second check once the lock has been released ; singularity pull --nohttps $tmp $url; mv $tmp $output_file; fi; ) 200>$lock; fi. rm -f $lock. echo $output_file; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921:1166,Security,access,access,1166,"Here is a slightly more general script (it assumes the lock file and saved image are in the current directory). It also does the pull into a temp file with a rename into the destination name at the end so that for a large image the -f check won't trigger for a partial download. I do some work here to deduce a filename that should match (as I understand the rules anyway) the one that the pull would create. I also include an option to force the path, since in my automation I tend to wish to define everything for my self. The derived or given image filename is echoed at the end. YOUR_HOST is the name of the sregistry host (this is the context I'm doing this in). ```; #!/bin/bash . lock_dir=. if [[ $# -ne 1 && $# -ne 2 ]] ; then; echo ""Usage: $0 image-name [output-file]"" 1>&2; exit 1; fi; name=$1; output_file=$2. if [[ ""$output_file"" = """" ]] ; then. # deduce filename . output_file=`basename $name`; if [[ $output_file =~ (.*):([^:]+)$ ]] ; then; base=${BASH_REMATCH[1]}; tag=${BASH_REMATCH[2]}; else; base=$output_file; tag=latest; fi; output_file=""${base}_$tag.sif""; fi. url=shub://YOUR_HOST/$name. # declare a very similar path (.lock) where Cromwell can access ; lock=$lock_dir/$output_file.lock; tmp=$output_file.tmp. if [ ! -f ""$output_file"" ]; then # If we already have the image, skip everything ; (; flock --exclusive 200; if [ ! -f ""$output_file"" ]; then # do a second check once the lock has been released ; singularity pull --nohttps $tmp $url; mv $tmp $output_file; fi; ) 200>$lock; fi. rm -f $lock. echo $output_file; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537240373:35,Testability,log,login,35,"I hope you are doing the pull on a login / dev node and not on something running massively in parallel? Or that the shub:// uri is interchangeable with docker:// or library:// ? Doing exec/run/pull in parallel is what led to devastating events in July that warranted adding extreme limits for all users to the server, and almost was the end of Singularity Hub. Ideally this really needs to be done with just one pull, and done before anything is run in parallel.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537240373
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537242867:93,Availability,down,downloads,93,"I had actually assumed that the singularity pull/caching mechanism would handle simultaneous downloads properly (by allowing only one to progress to fill the cache), but it doesn't appear to.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537242867
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537242867:158,Performance,cache,cache,158,"I had actually assumed that the singularity pull/caching mechanism would handle simultaneous downloads properly (by allowing only one to progress to fill the cache), but it doesn't appear to.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537242867
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243005:111,Energy Efficiency,charge,charges,111,"Ah, if it's a local sregistry, then the build limits are not the same, and Singularity Hub isn't incurring any charges on Google Cloud :) . Carry on!. Are you running a local sregistry? I put in a PR today to add a keystore, in case you want to test it out :) https://github.com/singularityhub/sregistry/pull/235",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243005
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243005:245,Testability,test,test,245,"Ah, if it's a local sregistry, then the build limits are not the same, and Singularity Hub isn't incurring any charges on Google Cloud :) . Carry on!. Are you running a local sregistry? I put in a PR today to add a keystore, in case you want to test it out :) https://github.com/singularityhub/sregistry/pull/235",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243005
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243320:21,Integrability,depend,depends,21,"@olsonanl I think it depends on the version - I think(?) in newer versions a singularity hub image is cached, but (from actual experience) in _some_ version it's definitely not!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243320
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243320:102,Performance,cache,cached,102,"@olsonanl I think it depends on the version - I think(?) in newer versions a singularity hub image is cached, but (from actual experience) in _some_ version it's definitely not!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243320
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537244158:67,Availability,down,download,67,"I was using 3.4.1 and saw behavior where one of the copies did the download into the cache, and the second saw the partial file in the cache and tried to run it and failed. And yes, it's a local registry that I convinced to run inside singularity instead of docker since my production hosts are centos6 and are not happy with docker currently (and I'd rather have the control singularity gives me).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537244158
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537244158:85,Performance,cache,cache,85,"I was using 3.4.1 and saw behavior where one of the copies did the download into the cache, and the second saw the partial file in the cache and tried to run it and failed. And yes, it's a local registry that I convinced to run inside singularity instead of docker since my production hosts are centos6 and are not happy with docker currently (and I'd rather have the control singularity gives me).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537244158
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537244158:135,Performance,cache,cache,135,"I was using 3.4.1 and saw behavior where one of the copies did the download into the cache, and the second saw the partial file in the cache and tried to run it and failed. And yes, it's a local registry that I convinced to run inside singularity instead of docker since my production hosts are centos6 and are not happy with docker currently (and I'd rather have the control singularity gives me).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537244158
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537245722:608,Integrability,depend,dependencies,608,"Oh that's wicked! I haven't seen / heard of registries (being run with Singularity!) in the wild - did you use the same base and convert to Singularity recipes? Did you run as an instance, or run as instances with singularity-compose? Apologies for many questions, you've greatly peaked my interest! . Interesting story - the _very first_ design for a Singularity Registry (I was doing back in 2016/early 2017) was totally based with Singularity - but because we didn't have instances proper yet, there were too many issues to make it feasible to develop (and I jumped to Docker). But now that we have those dependencies / instances, I could definitely look at it again. The caveat (I suspect) is that we'd still need to run services with sudo - did you get around that?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537245722
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537248811:146,Deployability,deploy,deployment,146,"Ahh okay, so you did need root permission for what I expected. If you needed root for those 2, arguably another user (or cluster admin) doing the deployment would consider the additional root as trivial. And the benefits to this would be - having the registry deployed on a cluster (still needing root) but without Docker?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537248811
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537248811:260,Deployability,deploy,deployed,260,"Ahh okay, so you did need root permission for what I expected. If you needed root for those 2, arguably another user (or cluster admin) doing the deployment would consider the additional root as trivial. And the benefits to this would be - having the registry deployed on a cluster (still needing root) but without Docker?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537248811
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685:301,Availability,down,down,301,"The main reason was that I don't have a native environment with python3 + the latest openssl that was usable with the github auth requirements. I have python3 + openssl that I've built, but need xmlsec which segfaulted when I used the centos6 version with the hand-built python environment. I started down the path of building xmlsec which didn't immediately work properly so wanted to stop spiraling down the rathole of building chained dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685:401,Availability,down,down,401,"The main reason was that I don't have a native environment with python3 + the latest openssl that was usable with the github auth requirements. I have python3 + openssl that I've built, but need xmlsec which segfaulted when I used the centos6 version with the hand-built python environment. I started down the path of building xmlsec which didn't immediately work properly so wanted to stop spiraling down the rathole of building chained dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685:438,Integrability,depend,dependencies,438,"The main reason was that I don't have a native environment with python3 + the latest openssl that was usable with the github auth requirements. I have python3 + openssl that I've built, but need xmlsec which segfaulted when I used the centos6 version with the hand-built python environment. I started down the path of building xmlsec which didn't immediately work properly so wanted to stop spiraling down the rathole of building chained dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685:102,Usability,usab,usable,102,"The main reason was that I don't have a native environment with python3 + the latest openssl that was usable with the github auth requirements. I have python3 + openssl that I've built, but need xmlsec which segfaulted when I used the centos6 version with the hand-built python environment. I started down the path of building xmlsec which didn't immediately work properly so wanted to stop spiraling down the rathole of building chained dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756:298,Availability,down,download,298,"I did not know of this thread.; At our institute we have solved this differently. We use `singularity exec` and no specific pull command. This will try to locate the image in the cache whis is located in `SINGULARITY_CACHEDIR` (env variable). If it is already there it will use it. If not, it will download it. This will lead to race condition if it is used in a scatter. We use https://github.com/biowdl/prepull-singularity to pull the images beforehand, so no race conditions occur. I am also thinking of adding a `docker_pull` thing to the config, so you can do `singularity exec {image} echo done!` or something similar to make sure the cache is populated at workflow initialization time. I have no ETA on this though, for now the prepull singularity script works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756:591,Availability,echo,echo,591,"I did not know of this thread.; At our institute we have solved this differently. We use `singularity exec` and no specific pull command. This will try to locate the image in the cache whis is located in `SINGULARITY_CACHEDIR` (env variable). If it is already there it will use it. If not, it will download it. This will lead to race condition if it is used in a scatter. We use https://github.com/biowdl/prepull-singularity to pull the images beforehand, so no race conditions occur. I am also thinking of adding a `docker_pull` thing to the config, so you can do `singularity exec {image} echo done!` or something similar to make sure the cache is populated at workflow initialization time. I have no ETA on this though, for now the prepull singularity script works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756:232,Modifiability,variab,variable,232,"I did not know of this thread.; At our institute we have solved this differently. We use `singularity exec` and no specific pull command. This will try to locate the image in the cache whis is located in `SINGULARITY_CACHEDIR` (env variable). If it is already there it will use it. If not, it will download it. This will lead to race condition if it is used in a scatter. We use https://github.com/biowdl/prepull-singularity to pull the images beforehand, so no race conditions occur. I am also thinking of adding a `docker_pull` thing to the config, so you can do `singularity exec {image} echo done!` or something similar to make sure the cache is populated at workflow initialization time. I have no ETA on this though, for now the prepull singularity script works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756:543,Modifiability,config,config,543,"I did not know of this thread.; At our institute we have solved this differently. We use `singularity exec` and no specific pull command. This will try to locate the image in the cache whis is located in `SINGULARITY_CACHEDIR` (env variable). If it is already there it will use it. If not, it will download it. This will lead to race condition if it is used in a scatter. We use https://github.com/biowdl/prepull-singularity to pull the images beforehand, so no race conditions occur. I am also thinking of adding a `docker_pull` thing to the config, so you can do `singularity exec {image} echo done!` or something similar to make sure the cache is populated at workflow initialization time. I have no ETA on this though, for now the prepull singularity script works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756:179,Performance,cache,cache,179,"I did not know of this thread.; At our institute we have solved this differently. We use `singularity exec` and no specific pull command. This will try to locate the image in the cache whis is located in `SINGULARITY_CACHEDIR` (env variable). If it is already there it will use it. If not, it will download it. This will lead to race condition if it is used in a scatter. We use https://github.com/biowdl/prepull-singularity to pull the images beforehand, so no race conditions occur. I am also thinking of adding a `docker_pull` thing to the config, so you can do `singularity exec {image} echo done!` or something similar to make sure the cache is populated at workflow initialization time. I have no ETA on this though, for now the prepull singularity script works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756:329,Performance,race condition,race condition,329,"I did not know of this thread.; At our institute we have solved this differently. We use `singularity exec` and no specific pull command. This will try to locate the image in the cache whis is located in `SINGULARITY_CACHEDIR` (env variable). If it is already there it will use it. If not, it will download it. This will lead to race condition if it is used in a scatter. We use https://github.com/biowdl/prepull-singularity to pull the images beforehand, so no race conditions occur. I am also thinking of adding a `docker_pull` thing to the config, so you can do `singularity exec {image} echo done!` or something similar to make sure the cache is populated at workflow initialization time. I have no ETA on this though, for now the prepull singularity script works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756:462,Performance,race condition,race conditions,462,"I did not know of this thread.; At our institute we have solved this differently. We use `singularity exec` and no specific pull command. This will try to locate the image in the cache whis is located in `SINGULARITY_CACHEDIR` (env variable). If it is already there it will use it. If not, it will download it. This will lead to race condition if it is used in a scatter. We use https://github.com/biowdl/prepull-singularity to pull the images beforehand, so no race conditions occur. I am also thinking of adding a `docker_pull` thing to the config, so you can do `singularity exec {image} echo done!` or something similar to make sure the cache is populated at workflow initialization time. I have no ETA on this though, for now the prepull singularity script works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756:641,Performance,cache,cache,641,"I did not know of this thread.; At our institute we have solved this differently. We use `singularity exec` and no specific pull command. This will try to locate the image in the cache whis is located in `SINGULARITY_CACHEDIR` (env variable). If it is already there it will use it. If not, it will download it. This will lead to race condition if it is used in a scatter. We use https://github.com/biowdl/prepull-singularity to pull the images beforehand, so no race conditions occur. I am also thinking of adding a `docker_pull` thing to the config, so you can do `singularity exec {image} echo done!` or something similar to make sure the cache is populated at workflow initialization time. I have no ETA on this though, for now the prepull singularity script works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558:157,Availability,echo,echo,157,"And here is the config:; ```hocon; sbatch \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /cluster-shared-filesystem,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558:213,Availability,error,error,213,"And here is the config:; ```hocon; sbatch \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /cluster-shared-filesystem,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558:406,Availability,echo,echo,406,"And here is the config:; ```hocon; sbatch \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /cluster-shared-filesystem,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558:233,Integrability,wrap,wrap,233,"And here is the config:; ```hocon; sbatch \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /cluster-shared-filesystem,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558:16,Modifiability,config,config,16,"And here is the config:; ```hocon; sbatch \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /cluster-shared-filesystem,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627148558
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:470,Availability,redundant,redundant,470,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:1477,Availability,echo,echo,1477,"ution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is becau",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:1878,Availability,echo,echo,1878,"@TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is because pulling two images at the same time that have a shared layer might also corrupt the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:1934,Availability,error,error,1934,"@TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is because pulling two images at the same time that have a shared layer might also corrupt the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:2120,Availability,echo,echo,2120,"@TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is because pulling two images at the same time that have a shared layer might also corrupt the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:2233,Availability,alive,alive,2233,"@TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is because pulling two images at the same time that have a shared layer might also corrupt the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:274,Integrability,depend,dependent,274,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:1954,Integrability,wrap,wrap,1954,"@TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is because pulling two images at the same time that have a shared layer might also corrupt the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:688,Modifiability,variab,variable,688,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:762,Modifiability,config,config,762,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:837,Modifiability,config,config,837,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:844,Modifiability,Config,ConfigBackendLifecycleActorFactory,844,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:881,Modifiability,config,config,881,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:2337,Modifiability,config,config,2337,"@TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is because pulling two images at the same time that have a shared layer might also corrupt the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:891,Performance,concurren,concurrent-job-limit,891,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:1189,Performance,cache,cache,1189,"s due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""sque",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:2531,Performance,cache,cache,2531,"@TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is because pulling two images at the same time that have a shared layer might also corrupt the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:470,Safety,redund,redundant,470,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:929,Safety,timeout,timeout-seconds,929,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:1405,Safety,timeout,timeout,1405,"ution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is becau",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598:92,Availability,redundant,redundant,92,"> So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. I looked into this a while ago so I might be wrong, but I think a `docker_pull` hook would still be useful, even at runtime, because it would be run only once, and *not* scattered, unlike the actual `submit_docker` hook. This would give it time to pull/convert the container without any race condition issues, meaning we don't have to use `flock` or any of that messy bash. The execution graph would look like:; ```; pull_docker; ⭩ ↓ ⭨; submit submit submit; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598:437,Performance,race condition,race condition,437,"> So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. I looked into this a while ago so I might be wrong, but I think a `docker_pull` hook would still be useful, even at runtime, because it would be run only once, and *not* scattered, unlike the actual `submit_docker` hook. This would give it time to pull/convert the container without any race condition issues, meaning we don't have to use `flock` or any of that messy bash. The execution graph would look like:; ```; pull_docker; ⭩ ↓ ⭨; submit submit submit; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598:92,Safety,redund,redundant,92,"> So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. I looked into this a while ago so I might be wrong, but I think a `docker_pull` hook would still be useful, even at runtime, because it would be run only once, and *not* scattered, unlike the actual `submit_docker` hook. This would give it time to pull/convert the container without any race condition issues, meaning we don't have to use `flock` or any of that messy bash. The execution graph would look like:; ```; pull_docker; ⭩ ↓ ⭨; submit submit submit; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627386598
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781:19,Modifiability,config,config,19,"BTW, I changed the config above because the only viable way to implement a lock in singularity is one lock per cache. If two images that are different but with shared layers are written to the cache, it can still become corrupted!. @TMiguelT This will be hard to pull of in the code because the runtime attributes are evaluated just before task submissions (not before scattering). Also there will still be race conditions. Only one process should pull to the same cache at the same time. This is because images have shared layers. Also this requires additional code and is only applicable to the singularity use case. The few lines in the script are way way way less than any code that would be required to make this happen. As regards tot he messy bash, no bash is required. Flock + the pull command is just one line. The only bash that is required is to set a few variables, but that is not really scripting. Flock is a great solution. Thanks a lot for suggesting it! The only better solution would have been that singularity has implemented such a filelock itself. As long as the pull process happens on the same node, the kernel will ensure that there are no race conditions. Of course this doesn't work on another node, so the flock command should be placed before the submit command.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781:167,Modifiability,layers,layers,167,"BTW, I changed the config above because the only viable way to implement a lock in singularity is one lock per cache. If two images that are different but with shared layers are written to the cache, it can still become corrupted!. @TMiguelT This will be hard to pull of in the code because the runtime attributes are evaluated just before task submissions (not before scattering). Also there will still be race conditions. Only one process should pull to the same cache at the same time. This is because images have shared layers. Also this requires additional code and is only applicable to the singularity use case. The few lines in the script are way way way less than any code that would be required to make this happen. As regards tot he messy bash, no bash is required. Flock + the pull command is just one line. The only bash that is required is to set a few variables, but that is not really scripting. Flock is a great solution. Thanks a lot for suggesting it! The only better solution would have been that singularity has implemented such a filelock itself. As long as the pull process happens on the same node, the kernel will ensure that there are no race conditions. Of course this doesn't work on another node, so the flock command should be placed before the submit command.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781:524,Modifiability,layers,layers,524,"BTW, I changed the config above because the only viable way to implement a lock in singularity is one lock per cache. If two images that are different but with shared layers are written to the cache, it can still become corrupted!. @TMiguelT This will be hard to pull of in the code because the runtime attributes are evaluated just before task submissions (not before scattering). Also there will still be race conditions. Only one process should pull to the same cache at the same time. This is because images have shared layers. Also this requires additional code and is only applicable to the singularity use case. The few lines in the script are way way way less than any code that would be required to make this happen. As regards tot he messy bash, no bash is required. Flock + the pull command is just one line. The only bash that is required is to set a few variables, but that is not really scripting. Flock is a great solution. Thanks a lot for suggesting it! The only better solution would have been that singularity has implemented such a filelock itself. As long as the pull process happens on the same node, the kernel will ensure that there are no race conditions. Of course this doesn't work on another node, so the flock command should be placed before the submit command.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781:867,Modifiability,variab,variables,867,"BTW, I changed the config above because the only viable way to implement a lock in singularity is one lock per cache. If two images that are different but with shared layers are written to the cache, it can still become corrupted!. @TMiguelT This will be hard to pull of in the code because the runtime attributes are evaluated just before task submissions (not before scattering). Also there will still be race conditions. Only one process should pull to the same cache at the same time. This is because images have shared layers. Also this requires additional code and is only applicable to the singularity use case. The few lines in the script are way way way less than any code that would be required to make this happen. As regards tot he messy bash, no bash is required. Flock + the pull command is just one line. The only bash that is required is to set a few variables, but that is not really scripting. Flock is a great solution. Thanks a lot for suggesting it! The only better solution would have been that singularity has implemented such a filelock itself. As long as the pull process happens on the same node, the kernel will ensure that there are no race conditions. Of course this doesn't work on another node, so the flock command should be placed before the submit command.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781:111,Performance,cache,cache,111,"BTW, I changed the config above because the only viable way to implement a lock in singularity is one lock per cache. If two images that are different but with shared layers are written to the cache, it can still become corrupted!. @TMiguelT This will be hard to pull of in the code because the runtime attributes are evaluated just before task submissions (not before scattering). Also there will still be race conditions. Only one process should pull to the same cache at the same time. This is because images have shared layers. Also this requires additional code and is only applicable to the singularity use case. The few lines in the script are way way way less than any code that would be required to make this happen. As regards tot he messy bash, no bash is required. Flock + the pull command is just one line. The only bash that is required is to set a few variables, but that is not really scripting. Flock is a great solution. Thanks a lot for suggesting it! The only better solution would have been that singularity has implemented such a filelock itself. As long as the pull process happens on the same node, the kernel will ensure that there are no race conditions. Of course this doesn't work on another node, so the flock command should be placed before the submit command.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781:193,Performance,cache,cache,193,"BTW, I changed the config above because the only viable way to implement a lock in singularity is one lock per cache. If two images that are different but with shared layers are written to the cache, it can still become corrupted!. @TMiguelT This will be hard to pull of in the code because the runtime attributes are evaluated just before task submissions (not before scattering). Also there will still be race conditions. Only one process should pull to the same cache at the same time. This is because images have shared layers. Also this requires additional code and is only applicable to the singularity use case. The few lines in the script are way way way less than any code that would be required to make this happen. As regards tot he messy bash, no bash is required. Flock + the pull command is just one line. The only bash that is required is to set a few variables, but that is not really scripting. Flock is a great solution. Thanks a lot for suggesting it! The only better solution would have been that singularity has implemented such a filelock itself. As long as the pull process happens on the same node, the kernel will ensure that there are no race conditions. Of course this doesn't work on another node, so the flock command should be placed before the submit command.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781:407,Performance,race condition,race conditions,407,"BTW, I changed the config above because the only viable way to implement a lock in singularity is one lock per cache. If two images that are different but with shared layers are written to the cache, it can still become corrupted!. @TMiguelT This will be hard to pull of in the code because the runtime attributes are evaluated just before task submissions (not before scattering). Also there will still be race conditions. Only one process should pull to the same cache at the same time. This is because images have shared layers. Also this requires additional code and is only applicable to the singularity use case. The few lines in the script are way way way less than any code that would be required to make this happen. As regards tot he messy bash, no bash is required. Flock + the pull command is just one line. The only bash that is required is to set a few variables, but that is not really scripting. Flock is a great solution. Thanks a lot for suggesting it! The only better solution would have been that singularity has implemented such a filelock itself. As long as the pull process happens on the same node, the kernel will ensure that there are no race conditions. Of course this doesn't work on another node, so the flock command should be placed before the submit command.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781:465,Performance,cache,cache,465,"BTW, I changed the config above because the only viable way to implement a lock in singularity is one lock per cache. If two images that are different but with shared layers are written to the cache, it can still become corrupted!. @TMiguelT This will be hard to pull of in the code because the runtime attributes are evaluated just before task submissions (not before scattering). Also there will still be race conditions. Only one process should pull to the same cache at the same time. This is because images have shared layers. Also this requires additional code and is only applicable to the singularity use case. The few lines in the script are way way way less than any code that would be required to make this happen. As regards tot he messy bash, no bash is required. Flock + the pull command is just one line. The only bash that is required is to set a few variables, but that is not really scripting. Flock is a great solution. Thanks a lot for suggesting it! The only better solution would have been that singularity has implemented such a filelock itself. As long as the pull process happens on the same node, the kernel will ensure that there are no race conditions. Of course this doesn't work on another node, so the flock command should be placed before the submit command.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781
https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781:1164,Performance,race condition,race conditions,1164,"BTW, I changed the config above because the only viable way to implement a lock in singularity is one lock per cache. If two images that are different but with shared layers are written to the cache, it can still become corrupted!. @TMiguelT This will be hard to pull of in the code because the runtime attributes are evaluated just before task submissions (not before scattering). Also there will still be race conditions. Only one process should pull to the same cache at the same time. This is because images have shared layers. Also this requires additional code and is only applicable to the singularity use case. The few lines in the script are way way way less than any code that would be required to make this happen. As regards tot he messy bash, no bash is required. Flock + the pull command is just one line. The only bash that is required is to set a few variables, but that is not really scripting. Flock is a great solution. Thanks a lot for suggesting it! The only better solution would have been that singularity has implemented such a filelock itself. As long as the pull process happens on the same node, the kernel will ensure that there are no race conditions. Of course this doesn't work on another node, so the flock command should be placed before the submit command.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627835781
https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-510118963:53,Deployability,pipeline,pipeline,53,"The intention is only to sever the logging to sentry pipeline - exceptions still go to Sentry. This file configures the log appender, i.e. where logs get sent, and does not say anything on exceptions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-510118963
https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-510118963:105,Modifiability,config,configures,105,"The intention is only to sever the logging to sentry pipeline - exceptions still go to Sentry. This file configures the log appender, i.e. where logs get sent, and does not say anything on exceptions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-510118963
https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-510118963:35,Testability,log,logging,35,"The intention is only to sever the logging to sentry pipeline - exceptions still go to Sentry. This file configures the log appender, i.e. where logs get sent, and does not say anything on exceptions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-510118963
https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-510118963:120,Testability,log,log,120,"The intention is only to sever the logging to sentry pipeline - exceptions still go to Sentry. This file configures the log appender, i.e. where logs get sent, and does not say anything on exceptions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-510118963
https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-510118963:145,Testability,log,logs,145,"The intention is only to sever the logging to sentry pipeline - exceptions still go to Sentry. This file configures the log appender, i.e. where logs get sent, and does not say anything on exceptions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-510118963
https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511462544:65,Availability,down,downgrade,65,"After reviewing cromwell code with Dan, we think that it's OK to downgrade the log level of **WorkflowFailedResponse** event in **WorkflowManagerActor** to INFO so it won't propagate to Sentry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511462544
https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511462544:79,Testability,log,log,79,"After reviewing cromwell code with Dan, we think that it's OK to downgrade the log level of **WorkflowFailedResponse** event in **WorkflowManagerActor** to INFO so it won't propagate to Sentry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511462544
https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511471851:46,Testability,log,logs,46,"@ichengchang are we confident that INFO-level logs show up in Kibana?. Should be very easy to check, simply identify a single INFO entry in Kibana that came from Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511471851
https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511471851:101,Usability,simpl,simply,101,"@ichengchang are we confident that INFO-level logs show up in Kibana?. Should be very easy to check, simply identify a single INFO entry in Kibana that came from Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511471851
https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511514188:26,Testability,log,log,26,Verified that Kibana does log INFO level log in production.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511514188
https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511514188:41,Testability,log,log,41,Verified that Kibana does log INFO level log in production.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065#issuecomment-511514188
https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580533224:43,Availability,error,error,43,"Hi @tAndreani ,. Were you able to fix this error? I'm actually having a similar issue where I'm providing the files in a singularity container as an input to the workflow and localization via hard link and copy fails. I would appreciate your help. Thanks,; Chetana",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580533224
https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580619402:543,Availability,error,error,543,"Hi Chetana,. in my case, the problem was the variable""file_format"" that I was passing to cutadapt . `cutadapt -f ${file_format}`. in the json file, one of the input was: `""scMeth.file_format"": ""fastq""`, but cutadapt didn't like it. Therefore I have substituted the initial command above with:. `cutadapt -f fastq`. or I have substitute `File file_format` with `String file_fomat` in the first step of the pipeline. Basically I was passing a file but in reality, was just a string for cutadapt. I don't know if this might help. If you type the error from Cromwell maybe I can help you better. Best; Tommaso",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580619402
https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580619402:405,Deployability,pipeline,pipeline,405,"Hi Chetana,. in my case, the problem was the variable""file_format"" that I was passing to cutadapt . `cutadapt -f ${file_format}`. in the json file, one of the input was: `""scMeth.file_format"": ""fastq""`, but cutadapt didn't like it. Therefore I have substituted the initial command above with:. `cutadapt -f fastq`. or I have substitute `File file_format` with `String file_fomat` in the first step of the pipeline. Basically I was passing a file but in reality, was just a string for cutadapt. I don't know if this might help. If you type the error from Cromwell maybe I can help you better. Best; Tommaso",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580619402
https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580619402:45,Modifiability,variab,variable,45,"Hi Chetana,. in my case, the problem was the variable""file_format"" that I was passing to cutadapt . `cutadapt -f ${file_format}`. in the json file, one of the input was: `""scMeth.file_format"": ""fastq""`, but cutadapt didn't like it. Therefore I have substituted the initial command above with:. `cutadapt -f fastq`. or I have substitute `File file_format` with `String file_fomat` in the first step of the pipeline. Basically I was passing a file but in reality, was just a string for cutadapt. I don't know if this might help. If you type the error from Cromwell maybe I can help you better. Best; Tommaso",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580619402
https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511571536:172,Testability,test,tested,172,"Hey @mikebaumann -- can you describe for the workflow/task you're running -- what are the # of input files?. When you say it worked for the ""commons"" in 2018, how was this tested? Just for my understanding of the before/after.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511571536
https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:551,Availability,Error,Error,551,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334
https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:649,Availability,error,error,649,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334
https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:1565,Availability,avail,available,1565,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334
https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:117,Deployability,configurat,configuration,117,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334
https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:451,Deployability,configurat,configuration,451,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334
https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:117,Modifiability,config,configuration,117,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334
https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:451,Modifiability,config,configuration,451,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334
https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:1597,Security,access,access,1597,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334
https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:1664,Security,access,access,1664,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334
https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:35,Testability,test,test,35,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334
https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:112,Testability,test,test,112,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334
https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:446,Testability,test,test,446,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334
https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:728,Testability,Log,Log,728,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334
https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:1136,Testability,test,test,1136,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-510697922:66,Deployability,patch,patch,66,Would like to know the expectations regrading unit tests. codecov/patch is not successful. Hence the query.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-510697922
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-510697922:51,Testability,test,tests,51,Would like to know the expectations regrading unit tests. codecov/patch is not successful. Hence the query.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-510697922
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-511475292:518,Energy Efficiency,schedul,schedule,518,"Hi @vanajasmy and thanks for your contribution. Codecov is a nice-to-have, we report it as a useful indicator but don't mandate that every single PR continue a monotonic march towards 100%. The real measure we care about is a matter of judgment - i.e. does all functionality have reasonable tests, and does critical functionality have exhaustive tests. In order to set expectations, it may be a bit before we have cycles to review this PR. Reviewing does take a substantial team effort and has to be included into the schedule alongside other tasks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-511475292
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-511475292:291,Testability,test,tests,291,"Hi @vanajasmy and thanks for your contribution. Codecov is a nice-to-have, we report it as a useful indicator but don't mandate that every single PR continue a monotonic march towards 100%. The real measure we care about is a matter of judgment - i.e. does all functionality have reasonable tests, and does critical functionality have exhaustive tests. In order to set expectations, it may be a bit before we have cycles to review this PR. Reviewing does take a substantial team effort and has to be included into the schedule alongside other tasks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-511475292
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-511475292:346,Testability,test,tests,346,"Hi @vanajasmy and thanks for your contribution. Codecov is a nice-to-have, we report it as a useful indicator but don't mandate that every single PR continue a monotonic march towards 100%. The real measure we care about is a matter of judgment - i.e. does all functionality have reasonable tests, and does critical functionality have exhaustive tests. In order to set expectations, it may be a bit before we have cycles to review this PR. Reviewing does take a substantial team effort and has to be included into the schedule alongside other tasks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-511475292
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852:18,Deployability,update,update,18,"Hi,; Is there any update on this PR? Can you give me an approximate timeline; for looking into this PR for review and integration?. Thanks,; Vanaja. On Mon, Jul 15, 2019 at 9:34 AM Adam Nichols <notifications@github.com>; wrote:. > Hi @vanajasmy <https://github.com/vanajasmy> and thanks for your; > contribution.; >; > Codecov is a nice-to-have, we report it as a useful indicator but don't; > mandate that every single PR continue a monotonic march towards 100%. The; > real measure we care about is a matter of judgment - i.e. does all; > functionality have reasonable tests, and does critical functionality have; > exhaustive tests.; >; > In order to set expectations, it may be a bit before we have cycles to; > review this PR. Reviewing does take a substantial team effort and has to be; > included into the schedule alongside other tasks.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATS73U3ASS2XEBHYMJLP7SRI3A5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ6H4XA#issuecomment-511475292>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ALILATQDJNZOYLMFIMT5C5TP7SRI3ANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852:118,Deployability,integrat,integration,118,"Hi,; Is there any update on this PR? Can you give me an approximate timeline; for looking into this PR for review and integration?. Thanks,; Vanaja. On Mon, Jul 15, 2019 at 9:34 AM Adam Nichols <notifications@github.com>; wrote:. > Hi @vanajasmy <https://github.com/vanajasmy> and thanks for your; > contribution.; >; > Codecov is a nice-to-have, we report it as a useful indicator but don't; > mandate that every single PR continue a monotonic march towards 100%. The; > real measure we care about is a matter of judgment - i.e. does all; > functionality have reasonable tests, and does critical functionality have; > exhaustive tests.; >; > In order to set expectations, it may be a bit before we have cycles to; > review this PR. Reviewing does take a substantial team effort and has to be; > included into the schedule alongside other tasks.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATS73U3ASS2XEBHYMJLP7SRI3A5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ6H4XA#issuecomment-511475292>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ALILATQDJNZOYLMFIMT5C5TP7SRI3ANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852:814,Energy Efficiency,schedul,schedule,814,"Hi,; Is there any update on this PR? Can you give me an approximate timeline; for looking into this PR for review and integration?. Thanks,; Vanaja. On Mon, Jul 15, 2019 at 9:34 AM Adam Nichols <notifications@github.com>; wrote:. > Hi @vanajasmy <https://github.com/vanajasmy> and thanks for your; > contribution.; >; > Codecov is a nice-to-have, we report it as a useful indicator but don't; > mandate that every single PR continue a monotonic march towards 100%. The; > real measure we care about is a matter of judgment - i.e. does all; > functionality have reasonable tests, and does critical functionality have; > exhaustive tests.; >; > In order to set expectations, it may be a bit before we have cycles to; > review this PR. Reviewing does take a substantial team effort and has to be; > included into the schedule alongside other tasks.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATS73U3ASS2XEBHYMJLP7SRI3A5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ6H4XA#issuecomment-511475292>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ALILATQDJNZOYLMFIMT5C5TP7SRI3ANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852:118,Integrability,integrat,integration,118,"Hi,; Is there any update on this PR? Can you give me an approximate timeline; for looking into this PR for review and integration?. Thanks,; Vanaja. On Mon, Jul 15, 2019 at 9:34 AM Adam Nichols <notifications@github.com>; wrote:. > Hi @vanajasmy <https://github.com/vanajasmy> and thanks for your; > contribution.; >; > Codecov is a nice-to-have, we report it as a useful indicator but don't; > mandate that every single PR continue a monotonic march towards 100%. The; > real measure we care about is a matter of judgment - i.e. does all; > functionality have reasonable tests, and does critical functionality have; > exhaustive tests.; >; > In order to set expectations, it may be a bit before we have cycles to; > review this PR. Reviewing does take a substantial team effort and has to be; > included into the schedule alongside other tasks.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATS73U3ASS2XEBHYMJLP7SRI3A5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ6H4XA#issuecomment-511475292>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ALILATQDJNZOYLMFIMT5C5TP7SRI3ANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852:572,Testability,test,tests,572,"Hi,; Is there any update on this PR? Can you give me an approximate timeline; for looking into this PR for review and integration?. Thanks,; Vanaja. On Mon, Jul 15, 2019 at 9:34 AM Adam Nichols <notifications@github.com>; wrote:. > Hi @vanajasmy <https://github.com/vanajasmy> and thanks for your; > contribution.; >; > Codecov is a nice-to-have, we report it as a useful indicator but don't; > mandate that every single PR continue a monotonic march towards 100%. The; > real measure we care about is a matter of judgment - i.e. does all; > functionality have reasonable tests, and does critical functionality have; > exhaustive tests.; >; > In order to set expectations, it may be a bit before we have cycles to; > review this PR. Reviewing does take a substantial team effort and has to be; > included into the schedule alongside other tasks.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATS73U3ASS2XEBHYMJLP7SRI3A5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ6H4XA#issuecomment-511475292>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ALILATQDJNZOYLMFIMT5C5TP7SRI3ANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852:630,Testability,test,tests,630,"Hi,; Is there any update on this PR? Can you give me an approximate timeline; for looking into this PR for review and integration?. Thanks,; Vanaja. On Mon, Jul 15, 2019 at 9:34 AM Adam Nichols <notifications@github.com>; wrote:. > Hi @vanajasmy <https://github.com/vanajasmy> and thanks for your; > contribution.; >; > Codecov is a nice-to-have, we report it as a useful indicator but don't; > mandate that every single PR continue a monotonic march towards 100%. The; > real measure we care about is a matter of judgment - i.e. does all; > functionality have reasonable tests, and does critical functionality have; > exhaustive tests.; >; > In order to set expectations, it may be a bit before we have cycles to; > review this PR. Reviewing does take a substantial team effort and has to be; > included into the schedule alongside other tasks.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATS73U3ASS2XEBHYMJLP7SRI3A5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ6H4XA#issuecomment-511475292>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ALILATQDJNZOYLMFIMT5C5TP7SRI3ANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-521748577:30,Testability,test,tests,30,I've created a PR so that our tests can run against your fork: https://github.com/broadinstitute/cromwell/pull/5118,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-521748577
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-585446149:48,Modifiability,Config,Config,48,How do you use this support? Working example? ; Config wise,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-585446149
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-585500837:407,Modifiability,config,config,407,"Here is an example for cromwell.conf backend for AWS-EFS or any shared mountable file system for AWSBATCH.; Please make sure you mount the EFS to /your-root on cromwell-server host and batch-computes.; One way of doing this automatically is thro' a LaunchTemplate. . backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""/your-root/cromwell_execution""; auth = ""default""; default-runtime-attributes { queueArn = ""xxxx"" }; filesystems { local { auth = ""default"" } }; }; }; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-585500837
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-585500837:562,Performance,queue,queueArn,562,"Here is an example for cromwell.conf backend for AWS-EFS or any shared mountable file system for AWSBATCH.; Please make sure you mount the EFS to /your-root on cromwell-server host and batch-computes.; One way of doing this automatically is thro' a LaunchTemplate. . backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""/your-root/cromwell_execution""; auth = ""default""; default-runtime-attributes { queueArn = ""xxxx"" }; filesystems { local { auth = ""default"" } }; }; }; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-585500837
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015:760,Availability,error,error,760,"Can you upload the complete stack trace from the cromwell-log?. On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com> wrote:. > I have /mnt/efs on both batch nodes and cromwell server which is the; > mounted EFS.; >; > Then; > backend {; > // this configures the AWS Batch Backend for Cromwell; > default = ""AWSBATCH""; > providers {; > AWSBATCH {; > actor-factory =; > ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; > config {; > root = ""/mnt/efs/cromwell_execution""; > auth = ""default""; >; > numSubmitAttempts = 3; > numCreateDefinitionAttempts = 3; >; > default-runtime-attributes {; > queueArn: ""${BatchQueue}""; > }; >; > filesystems {; > local { auth = ""default"" }; > }; > }; > }; >; > }; > }; >; > And I always get this error:; > ERROR - AwsBatchAsyncBackendJobExecutionActor; > [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; > java.util.NoSuchElementException: None.get; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015:770,Availability,ERROR,ERROR,770,"Can you upload the complete stack trace from the cromwell-log?. On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com> wrote:. > I have /mnt/efs on both batch nodes and cromwell server which is the; > mounted EFS.; >; > Then; > backend {; > // this configures the AWS Batch Backend for Cromwell; > default = ""AWSBATCH""; > providers {; > AWSBATCH {; > actor-factory =; > ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; > config {; > root = ""/mnt/efs/cromwell_execution""; > auth = ""default""; >; > numSubmitAttempts = 3; > numCreateDefinitionAttempts = 3; >; > default-runtime-attributes {; > queueArn: ""${BatchQueue}""; > }; >; > filesystems {; > local { auth = ""default"" }; > }; > }; > }; >; > }; > }; >; > And I always get this error:; > ERROR - AwsBatchAsyncBackendJobExecutionActor; > [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; > java.util.NoSuchElementException: None.get; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015:857,Availability,Error,Error,857,"Can you upload the complete stack trace from the cromwell-log?. On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com> wrote:. > I have /mnt/efs on both batch nodes and cromwell server which is the; > mounted EFS.; >; > Then; > backend {; > // this configures the AWS Batch Backend for Cromwell; > default = ""AWSBATCH""; > providers {; > AWSBATCH {; > actor-factory =; > ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; > config {; > root = ""/mnt/efs/cromwell_execution""; > auth = ""default""; >; > numSubmitAttempts = 3; > numCreateDefinitionAttempts = 3; >; > default-runtime-attributes {; > queueArn: ""${BatchQueue}""; > }; >; > filesystems {; > local { auth = ""default"" }; > }; > }; > }; >; > }; > }; >; > And I always get this error:; > ERROR - AwsBatchAsyncBackendJobExecutionActor; > [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; > java.util.NoSuchElementException: None.get; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015:264,Modifiability,config,configures,264,"Can you upload the complete stack trace from the cromwell-log?. On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com> wrote:. > I have /mnt/efs on both batch nodes and cromwell server which is the; > mounted EFS.; >; > Then; > backend {; > // this configures the AWS Batch Backend for Cromwell; > default = ""AWSBATCH""; > providers {; > AWSBATCH {; > actor-factory =; > ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; > config {; > root = ""/mnt/efs/cromwell_execution""; > auth = ""default""; >; > numSubmitAttempts = 3; > numCreateDefinitionAttempts = 3; >; > default-runtime-attributes {; > queueArn: ""${BatchQueue}""; > }; >; > filesystems {; > local { auth = ""default"" }; > }; > }; > }; >; > }; > }; >; > And I always get this error:; > ERROR - AwsBatchAsyncBackendJobExecutionActor; > [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; > java.util.NoSuchElementException: None.get; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015:453,Modifiability,config,config,453,"Can you upload the complete stack trace from the cromwell-log?. On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com> wrote:. > I have /mnt/efs on both batch nodes and cromwell server which is the; > mounted EFS.; >; > Then; > backend {; > // this configures the AWS Batch Backend for Cromwell; > default = ""AWSBATCH""; > providers {; > AWSBATCH {; > actor-factory =; > ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; > config {; > root = ""/mnt/efs/cromwell_execution""; > auth = ""default""; >; > numSubmitAttempts = 3; > numCreateDefinitionAttempts = 3; >; > default-runtime-attributes {; > queueArn: ""${BatchQueue}""; > }; >; > filesystems {; > local { auth = ""default"" }; > }; > }; > }; >; > }; > }; >; > And I always get this error:; > ERROR - AwsBatchAsyncBackendJobExecutionActor; > [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; > java.util.NoSuchElementException: None.get; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015:623,Performance,queue,queueArn,623,"Can you upload the complete stack trace from the cromwell-log?. On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com> wrote:. > I have /mnt/efs on both batch nodes and cromwell server which is the; > mounted EFS.; >; > Then; > backend {; > // this configures the AWS Batch Backend for Cromwell; > default = ""AWSBATCH""; > providers {; > AWSBATCH {; > actor-factory =; > ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; > config {; > root = ""/mnt/efs/cromwell_execution""; > auth = ""default""; >; > numSubmitAttempts = 3; > numCreateDefinitionAttempts = 3; >; > default-runtime-attributes {; > queueArn: ""${BatchQueue}""; > }; >; > filesystems {; > local { auth = ""default"" }; > }; > }; > }; >; > }; > }; >; > And I always get this error:; > ERROR - AwsBatchAsyncBackendJobExecutionActor; > [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; > java.util.NoSuchElementException: None.get; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015:58,Testability,log,log,58,"Can you upload the complete stack trace from the cromwell-log?. On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com> wrote:. > I have /mnt/efs on both batch nodes and cromwell server which is the; > mounted EFS.; >; > Then; > backend {; > // this configures the AWS Batch Backend for Cromwell; > default = ""AWSBATCH""; > providers {; > AWSBATCH {; > actor-factory =; > ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; > config {; > root = ""/mnt/efs/cromwell_execution""; > auth = ""default""; >; > numSubmitAttempts = 3; > numCreateDefinitionAttempts = 3; >; > default-runtime-attributes {; > queueArn: ""${BatchQueue}""; > }; >; > filesystems {; > local { auth = ""default"" }; > }; > }; > }; >; > }; > }; >; > And I always get this error:; > ERROR - AwsBatchAsyncBackendJobExecutionActor; > [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; > java.util.NoSuchElementException: None.get; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015:842,Testability,test,testjob,842,"Can you upload the complete stack trace from the cromwell-log?. On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com> wrote:. > I have /mnt/efs on both batch nodes and cromwell server which is the; > mounted EFS.; >; > Then; > backend {; > // this configures the AWS Batch Backend for Cromwell; > default = ""AWSBATCH""; > providers {; > AWSBATCH {; > actor-factory =; > ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; > config {; > root = ""/mnt/efs/cromwell_execution""; > auth = ""default""; >; > numSubmitAttempts = 3; > numCreateDefinitionAttempts = 3; >; > default-runtime-attributes {; > queueArn: ""${BatchQueue}""; > }; >; > filesystems {; > local { auth = ""default"" }; > }; > }; > }; >; > }; > }; >; > And I always get this error:; > ERROR - AwsBatchAsyncBackendJobExecutionActor; > [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; > java.util.NoSuchElementException: None.get; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586395015
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147:1187,Availability,error,error,1187,"Also, please make sure wherever you specify the runtime-attributes; ""docker"", ""memory"" etc, you include ""disks"" also. Example:; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""memory"": ""21G"",; ""disks"" : ""/mnt/efs 3 EFS""; }; The size,3 , in this case does not matter. It gets ignored. On Fri, Feb 14, 2020 at 9:45 AM Vanaja Narayanaswamy <vanajasmy@gmail.com>; wrote:. > Can you upload the complete stack trace from the cromwell-log?; >; > On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com>; > wrote:; >; >> I have /mnt/efs on both batch nodes and cromwell server which is the; >> mounted EFS.; >>; >> Then; >> backend {; >> // this configures the AWS Batch Backend for Cromwell; >> default = ""AWSBATCH""; >> providers {; >> AWSBATCH {; >> actor-factory =; >> ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; >> config {; >> root = ""/mnt/efs/cromwell_execution""; >> auth = ""default""; >>; >> numSubmitAttempts = 3; >> numCreateDefinitionAttempts = 3; >>; >> default-runtime-attributes {; >> queueArn: ""${BatchQueue}""; >> }; >>; >> filesystems {; >> local { auth = ""default"" }; >> }; >> }; >> }; >>; >> }; >> }; >>; >> And I always get this error:; >> ERROR - AwsBatchAsyncBackendJobExecutionActor; >> [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; >> java.util.NoSuchElementException: None.get; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; >> .; >>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147:1198,Availability,ERROR,ERROR,1198,"Also, please make sure wherever you specify the runtime-attributes; ""docker"", ""memory"" etc, you include ""disks"" also. Example:; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""memory"": ""21G"",; ""disks"" : ""/mnt/efs 3 EFS""; }; The size,3 , in this case does not matter. It gets ignored. On Fri, Feb 14, 2020 at 9:45 AM Vanaja Narayanaswamy <vanajasmy@gmail.com>; wrote:. > Can you upload the complete stack trace from the cromwell-log?; >; > On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com>; > wrote:; >; >> I have /mnt/efs on both batch nodes and cromwell server which is the; >> mounted EFS.; >>; >> Then; >> backend {; >> // this configures the AWS Batch Backend for Cromwell; >> default = ""AWSBATCH""; >> providers {; >> AWSBATCH {; >> actor-factory =; >> ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; >> config {; >> root = ""/mnt/efs/cromwell_execution""; >> auth = ""default""; >>; >> numSubmitAttempts = 3; >> numCreateDefinitionAttempts = 3; >>; >> default-runtime-attributes {; >> queueArn: ""${BatchQueue}""; >> }; >>; >> filesystems {; >> local { auth = ""default"" }; >> }; >> }; >> }; >>; >> }; >> }; >>; >> And I always get this error:; >> ERROR - AwsBatchAsyncBackendJobExecutionActor; >> [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; >> java.util.NoSuchElementException: None.get; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; >> .; >>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147:1286,Availability,Error,Error,1286,"Also, please make sure wherever you specify the runtime-attributes; ""docker"", ""memory"" etc, you include ""disks"" also. Example:; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""memory"": ""21G"",; ""disks"" : ""/mnt/efs 3 EFS""; }; The size,3 , in this case does not matter. It gets ignored. On Fri, Feb 14, 2020 at 9:45 AM Vanaja Narayanaswamy <vanajasmy@gmail.com>; wrote:. > Can you upload the complete stack trace from the cromwell-log?; >; > On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com>; > wrote:; >; >> I have /mnt/efs on both batch nodes and cromwell server which is the; >> mounted EFS.; >>; >> Then; >> backend {; >> // this configures the AWS Batch Backend for Cromwell; >> default = ""AWSBATCH""; >> providers {; >> AWSBATCH {; >> actor-factory =; >> ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; >> config {; >> root = ""/mnt/efs/cromwell_execution""; >> auth = ""default""; >>; >> numSubmitAttempts = 3; >> numCreateDefinitionAttempts = 3; >>; >> default-runtime-attributes {; >> queueArn: ""${BatchQueue}""; >> }; >>; >> filesystems {; >> local { auth = ""default"" }; >> }; >> }; >> }; >>; >> }; >> }; >>; >> And I always get this error:; >> ERROR - AwsBatchAsyncBackendJobExecutionActor; >> [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; >> java.util.NoSuchElementException: None.get; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; >> .; >>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147:665,Modifiability,config,configures,665,"Also, please make sure wherever you specify the runtime-attributes; ""docker"", ""memory"" etc, you include ""disks"" also. Example:; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""memory"": ""21G"",; ""disks"" : ""/mnt/efs 3 EFS""; }; The size,3 , in this case does not matter. It gets ignored. On Fri, Feb 14, 2020 at 9:45 AM Vanaja Narayanaswamy <vanajasmy@gmail.com>; wrote:. > Can you upload the complete stack trace from the cromwell-log?; >; > On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com>; > wrote:; >; >> I have /mnt/efs on both batch nodes and cromwell server which is the; >> mounted EFS.; >>; >> Then; >> backend {; >> // this configures the AWS Batch Backend for Cromwell; >> default = ""AWSBATCH""; >> providers {; >> AWSBATCH {; >> actor-factory =; >> ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; >> config {; >> root = ""/mnt/efs/cromwell_execution""; >> auth = ""default""; >>; >> numSubmitAttempts = 3; >> numCreateDefinitionAttempts = 3; >>; >> default-runtime-attributes {; >> queueArn: ""${BatchQueue}""; >> }; >>; >> filesystems {; >> local { auth = ""default"" }; >> }; >> }; >> }; >>; >> }; >> }; >>; >> And I always get this error:; >> ERROR - AwsBatchAsyncBackendJobExecutionActor; >> [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; >> java.util.NoSuchElementException: None.get; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; >> .; >>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147:860,Modifiability,config,config,860,"Also, please make sure wherever you specify the runtime-attributes; ""docker"", ""memory"" etc, you include ""disks"" also. Example:; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""memory"": ""21G"",; ""disks"" : ""/mnt/efs 3 EFS""; }; The size,3 , in this case does not matter. It gets ignored. On Fri, Feb 14, 2020 at 9:45 AM Vanaja Narayanaswamy <vanajasmy@gmail.com>; wrote:. > Can you upload the complete stack trace from the cromwell-log?; >; > On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com>; > wrote:; >; >> I have /mnt/efs on both batch nodes and cromwell server which is the; >> mounted EFS.; >>; >> Then; >> backend {; >> // this configures the AWS Batch Backend for Cromwell; >> default = ""AWSBATCH""; >> providers {; >> AWSBATCH {; >> actor-factory =; >> ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; >> config {; >> root = ""/mnt/efs/cromwell_execution""; >> auth = ""default""; >>; >> numSubmitAttempts = 3; >> numCreateDefinitionAttempts = 3; >>; >> default-runtime-attributes {; >> queueArn: ""${BatchQueue}""; >> }; >>; >> filesystems {; >> local { auth = ""default"" }; >> }; >> }; >> }; >>; >> }; >> }; >>; >> And I always get this error:; >> ERROR - AwsBatchAsyncBackendJobExecutionActor; >> [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; >> java.util.NoSuchElementException: None.get; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; >> .; >>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147:1038,Performance,queue,queueArn,1038,"Also, please make sure wherever you specify the runtime-attributes; ""docker"", ""memory"" etc, you include ""disks"" also. Example:; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""memory"": ""21G"",; ""disks"" : ""/mnt/efs 3 EFS""; }; The size,3 , in this case does not matter. It gets ignored. On Fri, Feb 14, 2020 at 9:45 AM Vanaja Narayanaswamy <vanajasmy@gmail.com>; wrote:. > Can you upload the complete stack trace from the cromwell-log?; >; > On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com>; > wrote:; >; >> I have /mnt/efs on both batch nodes and cromwell server which is the; >> mounted EFS.; >>; >> Then; >> backend {; >> // this configures the AWS Batch Backend for Cromwell; >> default = ""AWSBATCH""; >> providers {; >> AWSBATCH {; >> actor-factory =; >> ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; >> config {; >> root = ""/mnt/efs/cromwell_execution""; >> auth = ""default""; >>; >> numSubmitAttempts = 3; >> numCreateDefinitionAttempts = 3; >>; >> default-runtime-attributes {; >> queueArn: ""${BatchQueue}""; >> }; >>; >> filesystems {; >> local { auth = ""default"" }; >> }; >> }; >> }; >>; >> }; >> }; >>; >> And I always get this error:; >> ERROR - AwsBatchAsyncBackendJobExecutionActor; >> [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; >> java.util.NoSuchElementException: None.get; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; >> .; >>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147:442,Testability,log,log,442,"Also, please make sure wherever you specify the runtime-attributes; ""docker"", ""memory"" etc, you include ""disks"" also. Example:; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""memory"": ""21G"",; ""disks"" : ""/mnt/efs 3 EFS""; }; The size,3 , in this case does not matter. It gets ignored. On Fri, Feb 14, 2020 at 9:45 AM Vanaja Narayanaswamy <vanajasmy@gmail.com>; wrote:. > Can you upload the complete stack trace from the cromwell-log?; >; > On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com>; > wrote:; >; >> I have /mnt/efs on both batch nodes and cromwell server which is the; >> mounted EFS.; >>; >> Then; >> backend {; >> // this configures the AWS Batch Backend for Cromwell; >> default = ""AWSBATCH""; >> providers {; >> AWSBATCH {; >> actor-factory =; >> ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; >> config {; >> root = ""/mnt/efs/cromwell_execution""; >> auth = ""default""; >>; >> numSubmitAttempts = 3; >> numCreateDefinitionAttempts = 3; >>; >> default-runtime-attributes {; >> queueArn: ""${BatchQueue}""; >> }; >>; >> filesystems {; >> local { auth = ""default"" }; >> }; >> }; >> }; >>; >> }; >> }; >>; >> And I always get this error:; >> ERROR - AwsBatchAsyncBackendJobExecutionActor; >> [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; >> java.util.NoSuchElementException: None.get; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; >> .; >>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147
https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147:1271,Testability,test,testjob,1271,"Also, please make sure wherever you specify the runtime-attributes; ""docker"", ""memory"" etc, you include ""disks"" also. Example:; ""default_runtime_attributes"": {; ""docker"": ""ubuntu:latest"",; ""memory"": ""21G"",; ""disks"" : ""/mnt/efs 3 EFS""; }; The size,3 , in this case does not matter. It gets ignored. On Fri, Feb 14, 2020 at 9:45 AM Vanaja Narayanaswamy <vanajasmy@gmail.com>; wrote:. > Can you upload the complete stack trace from the cromwell-log?; >; > On Fri, Feb 14, 2020 at 9:29 AM pjongeneel <notifications@github.com>; > wrote:; >; >> I have /mnt/efs on both batch nodes and cromwell server which is the; >> mounted EFS.; >>; >> Then; >> backend {; >> // this configures the AWS Batch Backend for Cromwell; >> default = ""AWSBATCH""; >> providers {; >> AWSBATCH {; >> actor-factory =; >> ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; >> config {; >> root = ""/mnt/efs/cromwell_execution""; >> auth = ""default""; >>; >> numSubmitAttempts = 3; >> numCreateDefinitionAttempts = 3; >>; >> default-runtime-attributes {; >> queueArn: ""${BatchQueue}""; >> }; >>; >> filesystems {; >> local { auth = ""default"" }; >> }; >> }; >> }; >>; >> }; >> }; >>; >> And I always get this error:; >> ERROR - AwsBatchAsyncBackendJobExecutionActor; >> [UUID(8512304b)bioinfx.testjob:NA:1]: Error attempting to Execute; >> java.util.NoSuchElementException: None.get; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATR2AVXQXLFRQKER6W3RC3IIXA5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELZZF7A#issuecomment-586388220>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ALILATWPGUN66MUEOCVPYULRC3IIXANCNFSM4IBORPIQ>; >> .; >>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-586416147
https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511111210:90,Deployability,hotfix,hotfix,90,"@aednichols a couple of interesting points in that comment: 😉 . 1) IMHO there should be a hotfix for this considering the impact and narrowness of the code changes. I can do that once this is merged.; 2) I added an entry to the CHANGELOG.md, requesting re-review because words are hard.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511111210
https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623:281,Availability,error,error,281,"Since you specifically asked for a ""words"" review, I personally found the CHANGELOG comment came across as a bit defensive (and IMO slightly overplays the certainty of one new heuristic). My suggestion:. > Added a new heuristic for detecting preemption jobs in PAPIv2 based on the error message ""The assigned worker has failed to complete the operation"". Jobs with this message will from now on be treated as preempted rather than failed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623
https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623:287,Integrability,message,message,287,"Since you specifically asked for a ""words"" review, I personally found the CHANGELOG comment came across as a bit defensive (and IMO slightly overplays the certainty of one new heuristic). My suggestion:. > Added a new heuristic for detecting preemption jobs in PAPIv2 based on the error message ""The assigned worker has failed to complete the operation"". Jobs with this message will from now on be treated as preempted rather than failed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623
https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623:370,Integrability,message,message,370,"Since you specifically asked for a ""words"" review, I personally found the CHANGELOG comment came across as a bit defensive (and IMO slightly overplays the certainty of one new heuristic). My suggestion:. > Added a new heuristic for detecting preemption jobs in PAPIv2 based on the error message ""The assigned worker has failed to complete the operation"". Jobs with this message will from now on be treated as preempted rather than failed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623
https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623:232,Safety,detect,detecting,232,"Since you specifically asked for a ""words"" review, I personally found the CHANGELOG comment came across as a bit defensive (and IMO slightly overplays the certainty of one new heuristic). My suggestion:. > Added a new heuristic for detecting preemption jobs in PAPIv2 based on the error message ""The assigned worker has failed to complete the operation"". Jobs with this message will from now on be treated as preempted rather than failed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623
https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511400403:192,Availability,error,error,192,"Even though it is accurate to blame Google, from the user’s point of view it’s Cromwell that’s behaving unexpectedly. So I agree with Chris’s sentiment. I also think it’s worth it to mention “error code 10”, it’s a phrase now widely known and feared in DSP. (Perhaps the right qualification is, “this condition manifests itself as error code 10, but not all code 10s indicate this error”.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511400403
https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511400403:331,Availability,error,error,331,"Even though it is accurate to blame Google, from the user’s point of view it’s Cromwell that’s behaving unexpectedly. So I agree with Chris’s sentiment. I also think it’s worth it to mention “error code 10”, it’s a phrase now widely known and feared in DSP. (Perhaps the right qualification is, “this condition manifests itself as error code 10, but not all code 10s indicate this error”.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511400403
https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511400403:381,Availability,error,error,381,"Even though it is accurate to blame Google, from the user’s point of view it’s Cromwell that’s behaving unexpectedly. So I agree with Chris’s sentiment. I also think it’s worth it to mention “error code 10”, it’s a phrase now widely known and feared in DSP. (Perhaps the right qualification is, “this condition manifests itself as error code 10, but not all code 10s indicate this error”.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511400403
https://github.com/broadinstitute/cromwell/pull/5074#issuecomment-511914223:34,Testability,test,test,34,Apparently you broke a Spark unit test with this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5074#issuecomment-511914223
https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251:1101,Deployability,Pipeline,PipelinesParameterConversions,1101,https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=desc) into [44_hotfix](https://codecov.io/gh/broadinstitute/cromwell/commit/8055dad79afe29bbfb6b4b558f997a03d38dabd4?src=pr&el=desc) will **increase** coverage by `16.84%`.; > The diff coverage is `28.57%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## 44_hotfix #5076 +/- ##; =============================================; + Coverage 62.75% 79.6% +16.84% ; =============================================; Files 1031 1031 ; Lines 26424 26433 +9 ; Branches 869 819 -50 ; =============================================; + Hits 16582 21041 +4459 ; + Misses 9842 5392 -4450; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [...lines/v2alpha1/PipelinesParameterConversions.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvUGlwZWxpbmVzUGFyYW1ldGVyQ29udmVyc2lvbnMuc2NhbGE=) | `82.85% <100%> (+54.91%)` | :arrow_up: |; | [.../main/scala/cromwell/filesystems/drs/DrsPath.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZmlsZXN5c3RlbXMvZHJzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2ZpbGVzeXN0ZW1zL2Rycy9EcnNQYXRoLnNjYWxh) | `100% <100%> (ø)` | :arrow_up: |; | [...cala/cromwell/filesystems/drs/DrsPathBuilder.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZmlsZXN5c3RlbXMvZHJzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2ZpbGVzeXN0ZW1zL2Rycy9EcnNQYXRoQnVpbGRlci5zY2FsYQ==) | `92.85% <100%> (+7.14%)` | :arrow_up: |; | [...omwell/filesystems/drs/DrsPathBuilderFactory.scala](https://codecov.io/gh/broadinstitute/cromwell,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251
https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251:4537,Deployability,update,update,4537,"e#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | [...ll/engine/workflow/WorkflowDockerLookupActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9Xb3JrZmxvd0RvY2tlckxvb2t1cEFjdG9yLnNjYWxh) | `95.34% <0%> (+1.16%)` | :arrow_up: |; | [...cle/execution/callcaching/CallCacheDiffActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9saWZlY3ljbGUvZXhlY3V0aW9uL2NhbGxjYWNoaW5nL0NhbGxDYWNoZURpZmZBY3Rvci5zY2FsYQ==) | `96.38% <0%> (+1.2%)` | :arrow_up: |; | [...scala/cromwell/languages/util/ImportResolver.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-bGFuZ3VhZ2VGYWN0b3JpZXMvbGFuZ3VhZ2UtZmFjdG9yeS1jb3JlL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2xhbmd1YWdlcy91dGlsL0ltcG9ydFJlc29sdmVyLnNjYWxh) | `98.7% <0%> (+1.29%)` | :arrow_up: |; | [...src/main/scala/wdl/draft2/model/WdlNamespace.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL1dkbE5hbWVzcGFjZS5zY2FsYQ==) | `92.37% <0%> (+1.34%)` | :arrow_up: |; | ... and [402 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=footer). Last update [8055dad...803ebd8](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251
https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251:4436,Energy Efficiency,Power,Powered,4436,"e#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | [...ll/engine/workflow/WorkflowDockerLookupActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9Xb3JrZmxvd0RvY2tlckxvb2t1cEFjdG9yLnNjYWxh) | `95.34% <0%> (+1.16%)` | :arrow_up: |; | [...cle/execution/callcaching/CallCacheDiffActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9saWZlY3ljbGUvZXhlY3V0aW9uL2NhbGxjYWNoaW5nL0NhbGxDYWNoZURpZmZBY3Rvci5zY2FsYQ==) | `96.38% <0%> (+1.2%)` | :arrow_up: |; | [...scala/cromwell/languages/util/ImportResolver.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-bGFuZ3VhZ2VGYWN0b3JpZXMvbGFuZ3VhZ2UtZmFjdG9yeS1jb3JlL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2xhbmd1YWdlcy91dGlsL0ltcG9ydFJlc29sdmVyLnNjYWxh) | `98.7% <0%> (+1.29%)` | :arrow_up: |; | [...src/main/scala/wdl/draft2/model/WdlNamespace.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL1dkbE5hbWVzcGFjZS5zY2FsYQ==) | `92.37% <0%> (+1.34%)` | :arrow_up: |; | ... and [402 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=footer). Last update [8055dad...803ebd8](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251
https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251:4299,Usability,learn,learn,4299,"e#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | [...ll/engine/workflow/WorkflowDockerLookupActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9Xb3JrZmxvd0RvY2tlckxvb2t1cEFjdG9yLnNjYWxh) | `95.34% <0%> (+1.16%)` | :arrow_up: |; | [...cle/execution/callcaching/CallCacheDiffActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9saWZlY3ljbGUvZXhlY3V0aW9uL2NhbGxjYWNoaW5nL0NhbGxDYWNoZURpZmZBY3Rvci5zY2FsYQ==) | `96.38% <0%> (+1.2%)` | :arrow_up: |; | [...scala/cromwell/languages/util/ImportResolver.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-bGFuZ3VhZ2VGYWN0b3JpZXMvbGFuZ3VhZ2UtZmFjdG9yeS1jb3JlL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2xhbmd1YWdlcy91dGlsL0ltcG9ydFJlc29sdmVyLnNjYWxh) | `98.7% <0%> (+1.29%)` | :arrow_up: |; | [...src/main/scala/wdl/draft2/model/WdlNamespace.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL1dkbE5hbWVzcGFjZS5zY2FsYQ==) | `92.37% <0%> (+1.34%)` | :arrow_up: |; | ... and [402 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=footer). Last update [8055dad...803ebd8](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251
https://github.com/broadinstitute/cromwell/pull/5082#issuecomment-516492845:54,Testability,test,test,54,Thanks - I'll merge this as soon as the one remaining test finishes,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5082#issuecomment-516492845
https://github.com/broadinstitute/cromwell/issues/5083#issuecomment-513595663:0,Availability,Ping,Pinging,0,Pinging @natechols and @kshakir in case they have thoughts,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083#issuecomment-513595663
https://github.com/broadinstitute/cromwell/issues/5083#issuecomment-513595965:65,Testability,test,tested,65,"First guess: I targeted Postgres 9.6.1, I don't know if @kshakir tested anything earlier but I would try upgrading.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083#issuecomment-513595965
https://github.com/broadinstitute/cromwell/issues/5084#issuecomment-625457700:58,Availability,error,error,58,I am having the same issue. Was there a solution for this error?; cromwell version: 47; MySQL version: 5.5.64-MariaDB; centos-release-7-7.1908.0.el7.centos.x86_64,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084#issuecomment-625457700
https://github.com/broadinstitute/cromwell/issues/5084#issuecomment-625457700:126,Deployability,release,release-,126,I am having the same issue. Was there a solution for this error?; cromwell version: 47; MySQL version: 5.5.64-MariaDB; centos-release-7-7.1908.0.el7.centos.x86_64,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084#issuecomment-625457700
https://github.com/broadinstitute/cromwell/issues/5085#issuecomment-519932595:90,Availability,echo,echo,90,"I can reproduce this on my system. If I do not include `--type cwl` it fails to recognize echo.cwl as a valid workflow input, if I do include it the tool succeeds. Furthermore, if I run an actual workflow with the tools inside the directory (so no import) it succeeds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085#issuecomment-519932595
https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119:237,Availability,error,error-reference,237,"# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@9ec815d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `95.12%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #5086 +/- ##; ==========================================; Coverage ? 78.36% ; ==========================================; Files ? 1038 ; Lines ? 26695 ; Branches ? 887 ; ==========================================; Hits ? 20920 ; Misses ? 5775 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [core/src/main/scala/cromwell/util/JsonEditor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC91dGlsL0pzb25FZGl0b3Iuc2NhbGE=) | `95.12% <95.12%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=footer). Last update [9ec815d...d2705e2](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119
https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119:1600,Deployability,update,update,1600,"# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@9ec815d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `95.12%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #5086 +/- ##; ==========================================; Coverage ? 78.36% ; ==========================================; Files ? 1038 ; Lines ? 26695 ; Branches ? 887 ; ==========================================; Hits ? 20920 ; Misses ? 5775 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [core/src/main/scala/cromwell/util/JsonEditor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC91dGlsL0pzb25FZGl0b3Iuc2NhbGE=) | `95.12% <95.12%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=footer). Last update [9ec815d...d2705e2](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119
https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119:1499,Energy Efficiency,Power,Powered,1499,"# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@9ec815d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `95.12%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #5086 +/- ##; ==========================================; Coverage ? 78.36% ; ==========================================; Files ? 1038 ; Lines ? 26695 ; Branches ? 887 ; ==========================================; Hits ? 20920 ; Misses ? 5775 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [core/src/main/scala/cromwell/util/JsonEditor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC91dGlsL0pzb25FZGl0b3Iuc2NhbGE=) | `95.12% <95.12%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=footer). Last update [9ec815d...d2705e2](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119
https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119:185,Usability,learn,learn,185,"# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@9ec815d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `95.12%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #5086 +/- ##; ==========================================; Coverage ? 78.36% ; ==========================================; Files ? 1038 ; Lines ? 26695 ; Branches ? 887 ; ==========================================; Hits ? 20920 ; Misses ? 5775 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [core/src/main/scala/cromwell/util/JsonEditor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC91dGlsL0pzb25FZGl0b3Iuc2NhbGE=) | `95.12% <95.12%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=footer). Last update [9ec815d...d2705e2](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119
https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119:1362,Usability,learn,learn,1362,"# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@9ec815d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `95.12%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #5086 +/- ##; ==========================================; Coverage ? 78.36% ; ==========================================; Files ? 1038 ; Lines ? 26695 ; Branches ? 887 ; ==========================================; Hits ? 20920 ; Misses ? 5775 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [core/src/main/scala/cromwell/util/JsonEditor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC91dGlsL0pzb25FZGl0b3Iuc2NhbGE=) | `95.12% <95.12%> (ø)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=footer). Last update [9ec815d...d2705e2](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119
https://github.com/broadinstitute/cromwell/pull/5087#issuecomment-514434249:91,Modifiability,config,configurable,91,"In particular, the summarizer that uploads metadata to GCS should have a really high value configurable!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5087#issuecomment-514434249
https://github.com/broadinstitute/cromwell/pull/5087#issuecomment-517824618:35,Testability,test,test,35,Re-requesting reviews now that the test is in.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5087#issuecomment-517824618
https://github.com/broadinstitute/cromwell/pull/5088#issuecomment-516670868:202,Deployability,integrat,integration,202,"@alexagrf Would it be possible to add some tests here? I realize that it can be difficult to do that w/ auth code, so if this seems like a challenge perhaps we can work out a way w/ you to develop some integration tests we could fold into our internal system",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5088#issuecomment-516670868
https://github.com/broadinstitute/cromwell/pull/5088#issuecomment-516670868:202,Integrability,integrat,integration,202,"@alexagrf Would it be possible to add some tests here? I realize that it can be difficult to do that w/ auth code, so if this seems like a challenge perhaps we can work out a way w/ you to develop some integration tests we could fold into our internal system",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5088#issuecomment-516670868
https://github.com/broadinstitute/cromwell/pull/5088#issuecomment-516670868:43,Testability,test,tests,43,"@alexagrf Would it be possible to add some tests here? I realize that it can be difficult to do that w/ auth code, so if this seems like a challenge perhaps we can work out a way w/ you to develop some integration tests we could fold into our internal system",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5088#issuecomment-516670868
https://github.com/broadinstitute/cromwell/pull/5088#issuecomment-516670868:214,Testability,test,tests,214,"@alexagrf Would it be possible to add some tests here? I realize that it can be difficult to do that w/ auth code, so if this seems like a challenge perhaps we can work out a way w/ you to develop some integration tests we could fold into our internal system",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5088#issuecomment-516670868
https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-516197457:4,Testability,test,test,4,The test is to identify a scenario when exclude key does not work.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-516197457
https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-516198097:31,Testability,test,test,31,"Yeah the ask was for a failing test, next time I think a ""draft"" PR is more appropriate",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-516198097
https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-516570637:64,Testability,test,tests,64,"I added back the missing reference to ""nested"" node in the unit tests against Json array.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-516570637
https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-517025325:12,Performance,perform,performance,12,"Much better performance, 8-fold improvement. <img width=""1253"" alt=""Screen Shot 2019-07-31 at 5 15 50 PM"" src=""https://user-images.githubusercontent.com/50877414/62248953-5a7c6680-b3b7-11e9-9b28-393c8a9202a8.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-517025325
https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-517407444:511,Integrability,message,message,511,"Benchmarking results. ::Benchmark JsonEditor with circe.includeExcludeJson(_, None, Some(NonEmptyList.of(<some exclude key>)))::; cores: 12; hostname: wm963-226; name: Java HotSpot(TM) 64-Bit Server VM; osArch: x86_64; osName: Mac OS X; vendor: Oracle Corporation; version: 25.211-b12; Parameters(Exclude keys traversal: json size MB -> 14): 22.716215929999994 ms; Parameters(Exclude keys traversal: json size MB -> 32): 222.96187330999993 ms. ::Benchmark JsonEditor with circe.includeJson(_, NonEmptyList.one(""message""))::; cores: 12; hostname: wm963-226; name: Java HotSpot(TM) 64-Bit Server VM; osArch: x86_64; osName: Mac OS X; vendor: Oracle Corporation; version: 25.211-b12; Parameters(Include keys traversal: json size MB -> 14): 23.666746470000003 ms; Parameters(Include keys traversal: json size MB -> 32): 151.92092569000005 ms",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-517407444
https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-517407444:0,Testability,Benchmark,Benchmarking,0,"Benchmarking results. ::Benchmark JsonEditor with circe.includeExcludeJson(_, None, Some(NonEmptyList.of(<some exclude key>)))::; cores: 12; hostname: wm963-226; name: Java HotSpot(TM) 64-Bit Server VM; osArch: x86_64; osName: Mac OS X; vendor: Oracle Corporation; version: 25.211-b12; Parameters(Exclude keys traversal: json size MB -> 14): 22.716215929999994 ms; Parameters(Exclude keys traversal: json size MB -> 32): 222.96187330999993 ms. ::Benchmark JsonEditor with circe.includeJson(_, NonEmptyList.one(""message""))::; cores: 12; hostname: wm963-226; name: Java HotSpot(TM) 64-Bit Server VM; osArch: x86_64; osName: Mac OS X; vendor: Oracle Corporation; version: 25.211-b12; Parameters(Include keys traversal: json size MB -> 14): 23.666746470000003 ms; Parameters(Include keys traversal: json size MB -> 32): 151.92092569000005 ms",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-517407444
https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-517407444:24,Testability,Benchmark,Benchmark,24,"Benchmarking results. ::Benchmark JsonEditor with circe.includeExcludeJson(_, None, Some(NonEmptyList.of(<some exclude key>)))::; cores: 12; hostname: wm963-226; name: Java HotSpot(TM) 64-Bit Server VM; osArch: x86_64; osName: Mac OS X; vendor: Oracle Corporation; version: 25.211-b12; Parameters(Exclude keys traversal: json size MB -> 14): 22.716215929999994 ms; Parameters(Exclude keys traversal: json size MB -> 32): 222.96187330999993 ms. ::Benchmark JsonEditor with circe.includeJson(_, NonEmptyList.one(""message""))::; cores: 12; hostname: wm963-226; name: Java HotSpot(TM) 64-Bit Server VM; osArch: x86_64; osName: Mac OS X; vendor: Oracle Corporation; version: 25.211-b12; Parameters(Include keys traversal: json size MB -> 14): 23.666746470000003 ms; Parameters(Include keys traversal: json size MB -> 32): 151.92092569000005 ms",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-517407444
https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-517407444:446,Testability,Benchmark,Benchmark,446,"Benchmarking results. ::Benchmark JsonEditor with circe.includeExcludeJson(_, None, Some(NonEmptyList.of(<some exclude key>)))::; cores: 12; hostname: wm963-226; name: Java HotSpot(TM) 64-Bit Server VM; osArch: x86_64; osName: Mac OS X; vendor: Oracle Corporation; version: 25.211-b12; Parameters(Exclude keys traversal: json size MB -> 14): 22.716215929999994 ms; Parameters(Exclude keys traversal: json size MB -> 32): 222.96187330999993 ms. ::Benchmark JsonEditor with circe.includeJson(_, NonEmptyList.one(""message""))::; cores: 12; hostname: wm963-226; name: Java HotSpot(TM) 64-Bit Server VM; osArch: x86_64; osName: Mac OS X; vendor: Oracle Corporation; version: 25.211-b12; Parameters(Include keys traversal: json size MB -> 14): 23.666746470000003 ms; Parameters(Include keys traversal: json size MB -> 32): 151.92092569000005 ms",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5093#issuecomment-517407444
https://github.com/broadinstitute/cromwell/pull/5095#issuecomment-522021556:53,Safety,avoid,avoid,53,">Don't just create the IO, run it. That's one way to avoid side effects, for sure",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5095#issuecomment-522021556
https://github.com/broadinstitute/cromwell/pull/5095#issuecomment-522687367:75,Testability,test,tests,75,"I think I got everything, hopefully without creating new issues. 🤞 Failing tests are due to GPU issues that I believe are being investigated elsewhere.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5095#issuecomment-522687367
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-516888364:362,Integrability,message,message,362,"This looks remarkably similar to the change made in https://github.com/broadinstitute/cromwell/pull/4952 (and seems to re-invent the ""don't read too much"" logic with a slightly different maximum size). I wonder whether it would be possible to combine the ""read from stderr"" logic from these two changes to always happen in the same place - even if the resulting message text then ends up going in different directions?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-516888364
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-516888364:155,Testability,log,logic,155,"This looks remarkably similar to the change made in https://github.com/broadinstitute/cromwell/pull/4952 (and seems to re-invent the ""don't read too much"" logic with a slightly different maximum size). I wonder whether it would be possible to combine the ""read from stderr"" logic from these two changes to always happen in the same place - even if the resulting message text then ends up going in different directions?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-516888364
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-516888364:274,Testability,log,logic,274,"This looks remarkably similar to the change made in https://github.com/broadinstitute/cromwell/pull/4952 (and seems to re-invent the ""don't read too much"" logic with a slightly different maximum size). I wonder whether it would be possible to combine the ""read from stderr"" logic from these two changes to always happen in the same place - even if the resulting message text then ends up going in different directions?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-516888364
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-517014518:102,Integrability,message,messages,102,"Hi @cjllanwarne -; I pushed a new commit. Can you check it now? Tests are failing because they expect messages formatted differently. I'll fix them if you say that other things are okay.; The contents of the file are still read twice, but now the main logic is located in one method. I tried to do this with one reading, but it caused a lot of headaches :) If necessary, I can explain in more detail what problems arise when reading a file only once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-517014518
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-517014518:64,Testability,Test,Tests,64,"Hi @cjllanwarne -; I pushed a new commit. Can you check it now? Tests are failing because they expect messages formatted differently. I'll fix them if you say that other things are okay.; The contents of the file are still read twice, but now the main logic is located in one method. I tried to do this with one reading, but it caused a lot of headaches :) If necessary, I can explain in more detail what problems arise when reading a file only once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-517014518
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-517014518:252,Testability,log,logic,252,"Hi @cjllanwarne -; I pushed a new commit. Can you check it now? Tests are failing because they expect messages formatted differently. I'll fix them if you say that other things are okay.; The contents of the file are still read twice, but now the main logic is located in one method. I tried to do this with one reading, but it caused a lot of headaches :) If necessary, I can explain in more detail what problems arise when reading a file only once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-517014518
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045:323,Availability,down,download,323,"Hi @myazinn, sorry for the slow response time, and I haven't really looked at this in detail, but it looks like we'll now be calling the same function twice for the same stderr file (once for the job message and once for the workflow message)? Is that right?. What I mean is, when a task within a workflow fails, do we now download the same stderr file twice?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045:200,Integrability,message,message,200,"Hi @myazinn, sorry for the slow response time, and I haven't really looked at this in detail, but it looks like we'll now be calling the same function twice for the same stderr file (once for the job message and once for the workflow message)? Is that right?. What I mean is, when a task within a workflow fails, do we now download the same stderr file twice?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045:234,Integrability,message,message,234,"Hi @myazinn, sorry for the slow response time, and I haven't really looked at this in detail, but it looks like we'll now be calling the same function twice for the same stderr file (once for the job message and once for the workflow message)? Is that right?. What I mean is, when a task within a workflow fails, do we now download the same stderr file twice?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045:32,Performance,response time,response time,32,"Hi @myazinn, sorry for the slow response time, and I haven't really looked at this in detail, but it looks like we'll now be calling the same function twice for the same stderr file (once for the job message and once for the workflow message)? Is that right?. What I mean is, when a task within a workflow fails, do we now download the same stderr file twice?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809:331,Availability,error,error,331,"Hi @cjllanwarne ; You are right, the file is indeed read twice. There is a problem with reading it only once. . TL;DR This is because reading that file requires an execution context. The metadata that the user sees is taken from the `getMessage` method of the exception (at least right now it's done that way). This means that the error file must be read when creating the `WrongReturnCode` exception and this information must be embedded in the exception object.; In the meantime, [`expandFailureReasons`](https://github.com/broadinstitute/cromwell/blob/c45343ec33f922c9784667a9aa1d42ed68f8c9ba/engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala#L323) tends to add the content of the error file to every `KnownJobFailureException` exception (`WrongReturnCode` is the subclass of it). This means that if we want to get rid of double reading an error file, we must read this file at the moment the exception is created. In that case, the `expandFailureReasons` will have guarantees that there is no need to read it again.; The ideal solution is to let every `KnownJobFailureException` read the error file automatically during creation. This would have solved the metadata issue and eliminated the need to read the file in the `expandFailureReasons`.; But there is a huge problem. Reading the error file requires an execution context. Which means creating such an exception would require an execution context. This may break existing code.; That is the essence of a problem. In order not to read the file twice, the `expandFailureReasons` method needs guarantees that the file has already been read. To give these guarantees, we need to change the signature of the exception constructor so it will require execution context. But it may break existing code.; Although there is a workaround. I can add an `optionalErrorMessage` field to the `KnownJobFailureException` exception with the default value `None`. The `expandFailureReasons` method will check whether this field `Some` or",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809:708,Availability,error,error,708,"Hi @cjllanwarne ; You are right, the file is indeed read twice. There is a problem with reading it only once. . TL;DR This is because reading that file requires an execution context. The metadata that the user sees is taken from the `getMessage` method of the exception (at least right now it's done that way). This means that the error file must be read when creating the `WrongReturnCode` exception and this information must be embedded in the exception object.; In the meantime, [`expandFailureReasons`](https://github.com/broadinstitute/cromwell/blob/c45343ec33f922c9784667a9aa1d42ed68f8c9ba/engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala#L323) tends to add the content of the error file to every `KnownJobFailureException` exception (`WrongReturnCode` is the subclass of it). This means that if we want to get rid of double reading an error file, we must read this file at the moment the exception is created. In that case, the `expandFailureReasons` will have guarantees that there is no need to read it again.; The ideal solution is to let every `KnownJobFailureException` read the error file automatically during creation. This would have solved the metadata issue and eliminated the need to read the file in the `expandFailureReasons`.; But there is a huge problem. Reading the error file requires an execution context. Which means creating such an exception would require an execution context. This may break existing code.; That is the essence of a problem. In order not to read the file twice, the `expandFailureReasons` method needs guarantees that the file has already been read. To give these guarantees, we need to change the signature of the exception constructor so it will require execution context. But it may break existing code.; Although there is a workaround. I can add an `optionalErrorMessage` field to the `KnownJobFailureException` exception with the default value `None`. The `expandFailureReasons` method will check whether this field `Some` or",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809:867,Availability,error,error,867,"Hi @cjllanwarne ; You are right, the file is indeed read twice. There is a problem with reading it only once. . TL;DR This is because reading that file requires an execution context. The metadata that the user sees is taken from the `getMessage` method of the exception (at least right now it's done that way). This means that the error file must be read when creating the `WrongReturnCode` exception and this information must be embedded in the exception object.; In the meantime, [`expandFailureReasons`](https://github.com/broadinstitute/cromwell/blob/c45343ec33f922c9784667a9aa1d42ed68f8c9ba/engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala#L323) tends to add the content of the error file to every `KnownJobFailureException` exception (`WrongReturnCode` is the subclass of it). This means that if we want to get rid of double reading an error file, we must read this file at the moment the exception is created. In that case, the `expandFailureReasons` will have guarantees that there is no need to read it again.; The ideal solution is to let every `KnownJobFailureException` read the error file automatically during creation. This would have solved the metadata issue and eliminated the need to read the file in the `expandFailureReasons`.; But there is a huge problem. Reading the error file requires an execution context. Which means creating such an exception would require an execution context. This may break existing code.; That is the essence of a problem. In order not to read the file twice, the `expandFailureReasons` method needs guarantees that the file has already been read. To give these guarantees, we need to change the signature of the exception constructor so it will require execution context. But it may break existing code.; Although there is a workaround. I can add an `optionalErrorMessage` field to the `KnownJobFailureException` exception with the default value `None`. The `expandFailureReasons` method will check whether this field `Some` or",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809:1116,Availability,error,error,1116,"nly once. . TL;DR This is because reading that file requires an execution context. The metadata that the user sees is taken from the `getMessage` method of the exception (at least right now it's done that way). This means that the error file must be read when creating the `WrongReturnCode` exception and this information must be embedded in the exception object.; In the meantime, [`expandFailureReasons`](https://github.com/broadinstitute/cromwell/blob/c45343ec33f922c9784667a9aa1d42ed68f8c9ba/engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala#L323) tends to add the content of the error file to every `KnownJobFailureException` exception (`WrongReturnCode` is the subclass of it). This means that if we want to get rid of double reading an error file, we must read this file at the moment the exception is created. In that case, the `expandFailureReasons` will have guarantees that there is no need to read it again.; The ideal solution is to let every `KnownJobFailureException` read the error file automatically during creation. This would have solved the metadata issue and eliminated the need to read the file in the `expandFailureReasons`.; But there is a huge problem. Reading the error file requires an execution context. Which means creating such an exception would require an execution context. This may break existing code.; That is the essence of a problem. In order not to read the file twice, the `expandFailureReasons` method needs guarantees that the file has already been read. To give these guarantees, we need to change the signature of the exception constructor so it will require execution context. But it may break existing code.; Although there is a workaround. I can add an `optionalErrorMessage` field to the `KnownJobFailureException` exception with the default value `None`. The `expandFailureReasons` method will check whether this field `Some` or `None`, then it will read the file only if it is `None`. This is ugly, but it will work.; I hope I",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809:1314,Availability,error,error,1314,"ontext. The metadata that the user sees is taken from the `getMessage` method of the exception (at least right now it's done that way). This means that the error file must be read when creating the `WrongReturnCode` exception and this information must be embedded in the exception object.; In the meantime, [`expandFailureReasons`](https://github.com/broadinstitute/cromwell/blob/c45343ec33f922c9784667a9aa1d42ed68f8c9ba/engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala#L323) tends to add the content of the error file to every `KnownJobFailureException` exception (`WrongReturnCode` is the subclass of it). This means that if we want to get rid of double reading an error file, we must read this file at the moment the exception is created. In that case, the `expandFailureReasons` will have guarantees that there is no need to read it again.; The ideal solution is to let every `KnownJobFailureException` read the error file automatically during creation. This would have solved the metadata issue and eliminated the need to read the file in the `expandFailureReasons`.; But there is a huge problem. Reading the error file requires an execution context. Which means creating such an exception would require an execution context. This may break existing code.; That is the essence of a problem. In order not to read the file twice, the `expandFailureReasons` method needs guarantees that the file has already been read. To give these guarantees, we need to change the signature of the exception constructor so it will require execution context. But it may break existing code.; Although there is a workaround. I can add an `optionalErrorMessage` field to the `KnownJobFailureException` exception with the default value `None`. The `expandFailureReasons` method will check whether this field `Some` or `None`, then it will read the file only if it is `None`. This is ugly, but it will work.; I hope I gave you an understanding of a problem. Maybe you can give me some advice?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-528078536:208,Availability,error,error,208,Do you have an example before & after?. It seems like the output would contain `[First $limitBytes bytes]` from `annotatedContentAsStringWithLimit` which is a pretty strange thing to have in the middle of an error message.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-528078536
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-528078536:214,Integrability,message,message,214,Do you have an example before & after?. It seems like the output would contain `[First $limitBytes bytes]` from `annotatedContentAsStringWithLimit` which is a pretty strange thing to have in the middle of an error message.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-528078536
https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-530531351:85,Modifiability,refactor,refactoring,85,"@aednichols ; Yeah, it seems pretty ugly. Implementing it less ugly may require some refactoring and I'm not sure that I can do it in the right way. I think it would be better to close this PR and leave this issue to someone who knows Cromwell better than me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-530531351
https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-517282793:24,Testability,test,test,24,"Hi @geoffjentry -; This test works fine on my local backend and fails on GCP and AWS backends. I assume this is expected, given that the test should only run on the local backend. ; If you asked me to do this because Travis tests fell yesterday then I can only say that I don’t know what was wrong yesterday. I pushed the same changes again and now everything is fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-517282793
https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-517282793:137,Testability,test,test,137,"Hi @geoffjentry -; This test works fine on my local backend and fails on GCP and AWS backends. I assume this is expected, given that the test should only run on the local backend. ; If you asked me to do this because Travis tests fell yesterday then I can only say that I don’t know what was wrong yesterday. I pushed the same changes again and now everything is fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-517282793
https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-517282793:224,Testability,test,tests,224,"Hi @geoffjentry -; This test works fine on my local backend and fails on GCP and AWS backends. I assume this is expected, given that the test should only run on the local backend. ; If you asked me to do this because Travis tests fell yesterday then I can only say that I don’t know what was wrong yesterday. I pushed the same changes again and now everything is fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-517282793
https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-518352796:206,Testability,test,test,206,"@myazinn Hmm sorry - I was just looking for another example of something using `cwl.inputs.json` and figured that this **should** work on a cloud backend, but I was just looking by eye. Ultimately the real test is if you can run the stuff described by @chapmanb in #4586",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-518352796
https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-527998279:68,Testability,test,tests,68,Created https://github.com/broadinstitute/cromwell/pull/5159 to run tests,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-527998279
https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-528088789:0,Testability,Test,Tests,0,Tests passed in https://github.com/broadinstitute/cromwell/pull/5159,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-528088789
https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-533685512:70,Testability,test,test,70,"Hi @myazinn, can you please resolve the merge conflict so that we can test this again?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-533685512
https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-533906281:24,Deployability,update,updated,24,Hi @salonishah11 ; I've updated PR and resolved conflicts,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-533906281
https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-535962720:334,Energy Efficiency,green,green,334,"Hi @myazinn, ~can you please rebase with develop? We just merged a fix for a bug that was causing our tests to fail. Once you rebase I can run the tests again. Thank you!~; Please disregard my comment. Actually we have some mechanism in place where our tests pick up the changes from develop and then run the tests. Since it has gone green I will merge this PR. Thank you for the contribution! :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-535962720
https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-535962720:102,Testability,test,tests,102,"Hi @myazinn, ~can you please rebase with develop? We just merged a fix for a bug that was causing our tests to fail. Once you rebase I can run the tests again. Thank you!~; Please disregard my comment. Actually we have some mechanism in place where our tests pick up the changes from develop and then run the tests. Since it has gone green I will merge this PR. Thank you for the contribution! :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-535962720
https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-535962720:147,Testability,test,tests,147,"Hi @myazinn, ~can you please rebase with develop? We just merged a fix for a bug that was causing our tests to fail. Once you rebase I can run the tests again. Thank you!~; Please disregard my comment. Actually we have some mechanism in place where our tests pick up the changes from develop and then run the tests. Since it has gone green I will merge this PR. Thank you for the contribution! :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-535962720
https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-535962720:253,Testability,test,tests,253,"Hi @myazinn, ~can you please rebase with develop? We just merged a fix for a bug that was causing our tests to fail. Once you rebase I can run the tests again. Thank you!~; Please disregard my comment. Actually we have some mechanism in place where our tests pick up the changes from develop and then run the tests. Since it has gone green I will merge this PR. Thank you for the contribution! :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-535962720
https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-535962720:309,Testability,test,tests,309,"Hi @myazinn, ~can you please rebase with develop? We just merged a fix for a bug that was causing our tests to fail. Once you rebase I can run the tests again. Thank you!~; Please disregard my comment. Actually we have some mechanism in place where our tests pick up the changes from develop and then run the tests. Since it has gone green I will merge this PR. Thank you for the contribution! :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-535962720
https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518311368:88,Energy Efficiency,monitor,monitoring,88,Would it be easy to set it for _all_ text outputs? The other one I'm thinking about is `monitoring.log`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518311368
https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518311368:99,Testability,log,log,99,Would it be easy to set it for _all_ text outputs? The other one I'm thinking about is `monitoring.log`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518311368
https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518329395:87,Energy Efficiency,monitor,monitoring,87,"> Would it be easy to set it for all text outputs? The other one I'm thinking about is monitoring.log. It's pretty much a per-file thing, so each file needs to be considered on its own. gsutil has some logic to infer file type from extension so a `.txt` file should have the correct content type already. It does look like the monitoring file isn't text/plain and it would be easy to add. @mcovarr what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518329395
https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518329395:327,Energy Efficiency,monitor,monitoring,327,"> Would it be easy to set it for all text outputs? The other one I'm thinking about is monitoring.log. It's pretty much a per-file thing, so each file needs to be considered on its own. gsutil has some logic to infer file type from extension so a `.txt` file should have the correct content type already. It does look like the monitoring file isn't text/plain and it would be easy to add. @mcovarr what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518329395
https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518329395:98,Testability,log,log,98,"> Would it be easy to set it for all text outputs? The other one I'm thinking about is monitoring.log. It's pretty much a per-file thing, so each file needs to be considered on its own. gsutil has some logic to infer file type from extension so a `.txt` file should have the correct content type already. It does look like the monitoring file isn't text/plain and it would be easy to add. @mcovarr what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518329395
https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518329395:202,Testability,log,logic,202,"> Would it be easy to set it for all text outputs? The other one I'm thinking about is monitoring.log. It's pretty much a per-file thing, so each file needs to be considered on its own. gsutil has some logic to infer file type from extension so a `.txt` file should have the correct content type already. It does look like the monitoring file isn't text/plain and it would be easy to add. @mcovarr what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518329395
https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518583719:35,Deployability,integrat,integration,35,"Actually I'd like to add a Centaur integration test for this, I can hopefully get this done today.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518583719
https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518583719:35,Integrability,integrat,integration,35,"Actually I'd like to add a Centaur integration test for this, I can hopefully get this done today.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518583719
https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518583719:47,Testability,test,test,47,"Actually I'd like to add a Centaur integration test for this, I can hopefully get this done today.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518583719
https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518663198:37,Deployability,integrat,integration,37,"> Actually I'd like to add a Centaur integration test for this, I can hopefully get this done today. Thanks, I thought about doing this but wasn't sure how to do it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518663198
https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518663198:37,Integrability,integrat,integration,37,"> Actually I'd like to add a Centaur integration test for this, I can hopefully get this done today. Thanks, I thought about doing this but wasn't sure how to do it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518663198
https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518663198:49,Testability,test,test,49,"> Actually I'd like to add a Centaur integration test for this, I can hopefully get this done today. Thanks, I thought about doing this but wasn't sure how to do it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518663198
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-518982880:31,Testability,test,tests,31,Something is wrong with Travis tests. I had to reopen PR to force Travis run them again since they failed due to an external reason.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-518982880
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519045497:30,Testability,test,tests,30,"Not that I'm complaining, but tests failed due to external reasons again. Does anybody have an idea of what is going on?; Although I found that the tests are falling due to my changes too, this was not the only reason.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519045497
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519045497:148,Testability,test,tests,148,"Not that I'm complaining, but tests failed due to external reasons again. Does anybody have an idea of what is going on?; Although I found that the tests are falling due to my changes too, this was not the only reason.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519045497
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990:241,Performance,concurren,concurrent,241,"@myazinn You can restart the tests by going to the ""checks"" tab on this PR and restarting failed tests that way, no need to reset the PR. We do have a number of flaky tests, unfortunately. There are a lot of things which rely on timing in a concurrent environment which we haven't rooted out. I'm pretty skeptical that the test fix you pushed is actually correct unless it very recently changed - more likely is that something is timing out before the test can properly complete and the incorrect value is coming through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990:29,Testability,test,tests,29,"@myazinn You can restart the tests by going to the ""checks"" tab on this PR and restarting failed tests that way, no need to reset the PR. We do have a number of flaky tests, unfortunately. There are a lot of things which rely on timing in a concurrent environment which we haven't rooted out. I'm pretty skeptical that the test fix you pushed is actually correct unless it very recently changed - more likely is that something is timing out before the test can properly complete and the incorrect value is coming through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990:97,Testability,test,tests,97,"@myazinn You can restart the tests by going to the ""checks"" tab on this PR and restarting failed tests that way, no need to reset the PR. We do have a number of flaky tests, unfortunately. There are a lot of things which rely on timing in a concurrent environment which we haven't rooted out. I'm pretty skeptical that the test fix you pushed is actually correct unless it very recently changed - more likely is that something is timing out before the test can properly complete and the incorrect value is coming through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990:167,Testability,test,tests,167,"@myazinn You can restart the tests by going to the ""checks"" tab on this PR and restarting failed tests that way, no need to reset the PR. We do have a number of flaky tests, unfortunately. There are a lot of things which rely on timing in a concurrent environment which we haven't rooted out. I'm pretty skeptical that the test fix you pushed is actually correct unless it very recently changed - more likely is that something is timing out before the test can properly complete and the incorrect value is coming through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990:323,Testability,test,test,323,"@myazinn You can restart the tests by going to the ""checks"" tab on this PR and restarting failed tests that way, no need to reset the PR. We do have a number of flaky tests, unfortunately. There are a lot of things which rely on timing in a concurrent environment which we haven't rooted out. I'm pretty skeptical that the test fix you pushed is actually correct unless it very recently changed - more likely is that something is timing out before the test can properly complete and the incorrect value is coming through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990:452,Testability,test,test,452,"@myazinn You can restart the tests by going to the ""checks"" tab on this PR and restarting failed tests that way, no need to reset the PR. We do have a number of flaky tests, unfortunately. There are a lot of things which rely on timing in a concurrent environment which we haven't rooted out. I'm pretty skeptical that the test fix you pushed is actually correct unless it very recently changed - more likely is that something is timing out before the test can properly complete and the incorrect value is coming through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519088990
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366:78,Security,validat,validateRunArguments,78,"Hi @geoffjentry - ; The changes that I pushed are related to the test of the `validateRunArguments` method. Since I changed him, I had to change the tests. I did not notice this test initially.; This bothers me a bit because I did not expect the behavior of the method to change when processing WDL files. But in the end, now this method just processes the files in the same way as in server mode. Moreover, maybe the new behavior makes sense. For example, `validation.get.workflowUrl` indeed should be `None`, since we are supplying workflow without any `url`.; Just in case, I ran simple WDL workflows and didn't encounter any issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366:458,Security,validat,validation,458,"Hi @geoffjentry - ; The changes that I pushed are related to the test of the `validateRunArguments` method. Since I changed him, I had to change the tests. I did not notice this test initially.; This bothers me a bit because I did not expect the behavior of the method to change when processing WDL files. But in the end, now this method just processes the files in the same way as in server mode. Moreover, maybe the new behavior makes sense. For example, `validation.get.workflowUrl` indeed should be `None`, since we are supplying workflow without any `url`.; Just in case, I ran simple WDL workflows and didn't encounter any issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366:65,Testability,test,test,65,"Hi @geoffjentry - ; The changes that I pushed are related to the test of the `validateRunArguments` method. Since I changed him, I had to change the tests. I did not notice this test initially.; This bothers me a bit because I did not expect the behavior of the method to change when processing WDL files. But in the end, now this method just processes the files in the same way as in server mode. Moreover, maybe the new behavior makes sense. For example, `validation.get.workflowUrl` indeed should be `None`, since we are supplying workflow without any `url`.; Just in case, I ran simple WDL workflows and didn't encounter any issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366:149,Testability,test,tests,149,"Hi @geoffjentry - ; The changes that I pushed are related to the test of the `validateRunArguments` method. Since I changed him, I had to change the tests. I did not notice this test initially.; This bothers me a bit because I did not expect the behavior of the method to change when processing WDL files. But in the end, now this method just processes the files in the same way as in server mode. Moreover, maybe the new behavior makes sense. For example, `validation.get.workflowUrl` indeed should be `None`, since we are supplying workflow without any `url`.; Just in case, I ran simple WDL workflows and didn't encounter any issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366:178,Testability,test,test,178,"Hi @geoffjentry - ; The changes that I pushed are related to the test of the `validateRunArguments` method. Since I changed him, I had to change the tests. I did not notice this test initially.; This bothers me a bit because I did not expect the behavior of the method to change when processing WDL files. But in the end, now this method just processes the files in the same way as in server mode. Moreover, maybe the new behavior makes sense. For example, `validation.get.workflowUrl` indeed should be `None`, since we are supplying workflow without any `url`.; Just in case, I ran simple WDL workflows and didn't encounter any issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366:583,Usability,simpl,simple,583,"Hi @geoffjentry - ; The changes that I pushed are related to the test of the `validateRunArguments` method. Since I changed him, I had to change the tests. I did not notice this test initially.; This bothers me a bit because I did not expect the behavior of the method to change when processing WDL files. But in the end, now this method just processes the files in the same way as in server mode. Moreover, maybe the new behavior makes sense. For example, `validation.get.workflowUrl` indeed should be `None`, since we are supplying workflow without any `url`.; Just in case, I ran simple WDL workflows and didn't encounter any issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519103366
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519665821:68,Testability,test,test,68,"@myazinn Ah, I see. Sorry - I had assumed that you spotted a flakey test and were fixing that as well",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-519665821
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-523196132:77,Testability,test,tests,77,"Hi @myazinn, thank you for the contribution! I have created a PR so that our tests can run against your fork: #5130",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-523196132
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-523922121:43,Testability,test,tests,43,"Hi @myazinn, there was a recent fix in our tests. Can you please rebase against develop so that I can run the test suite again for you PR?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-523922121
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-523922121:110,Testability,test,test,110,"Hi @myazinn, there was a recent fix in our tests. Can you please rebase against develop so that I can run the test suite again for you PR?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-523922121
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-524348234:193,Testability,test,test,193,It does look like something went wrong while rebasing/merging. You can `git cherry-pick` your commits and force push it to the branch. Can you please try this? It would be much cleaner to just test just your changes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-524348234
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-524382440:38,Testability,test,tests,38,@myazinn That's great! I will run the tests again.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-524382440
https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-524422240:4,Testability,test,tests,4,The tests passed. Merging this. @myazinn thank you again for the contribution!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5104#issuecomment-524422240
https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519570433:144,Deployability,integrat,integration,144,"@geoffjentry Very nice, thanks for the link! Wish I did know this earlier... :+1: ; Could this file then be provided to `cromwell` when running integration test via `centaur`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519570433
https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519570433:144,Integrability,integrat,integration,144,"@geoffjentry Very nice, thanks for the link! Wish I did know this earlier... :+1: ; Could this file then be provided to `cromwell` when running integration test via `centaur`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519570433
https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519570433:156,Testability,test,test,156,"@geoffjentry Very nice, thanks for the link! Wish I did know this earlier... :+1: ; Could this file then be provided to `cromwell` when running integration test via `centaur`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519570433
https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519609887:61,Testability,test,test,61,"@likeanowl Yes - you just need to specify it in the centaur [test description](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/docker_alpine.test#L8), with a pointer to where the option file lives",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519609887
https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519609887:194,Testability,test,test,194,"@likeanowl Yes - you just need to specify it in the centaur [test description](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/docker_alpine.test#L8), with a pointer to where the option file lives",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519609887
https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-573350539:432,Testability,log,logs,432,"I like to report that this is also an issue when running CWL scripts via cromwell in `run` mode too: the `options` argument is ignored. . ```; $ java -jar ${crom} --version; cromwell 47; $; $ cat workflow.options.json; {; ""final_workflow_outputs_dir"": ""results.cromwell"",; ""use_relative_output_paths"": true; }; $; $ java -jar ${crom} run example.cwl -i inputs.yml --type cwl -o workflow.options.json; :; : # workflow runs normally, logs and other files in `cromwell-executions` folder as expected; : ; $ ls results.cromwell; $ # folder is empty; $. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-573350539
https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956:235,Availability,failure,failures,235,Hi @drkennetz - this seems quite similar in intent to an existing (opt-in) option for checking exit-code timeouts (https://cromwell.readthedocs.io/en/develop/backends/HPC/#exit-code-timeout). Since the timeout function would catch all failures (eg even if the script doesn't get a chance to trap the signal) I suspect it's more generally useful. What do you think?. cc @EvanTheB and @rhpvorderman since they probably know at least as much about why we went down the `exit-code-timeout` route as I do... 😄,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956
https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956:457,Availability,down,down,457,Hi @drkennetz - this seems quite similar in intent to an existing (opt-in) option for checking exit-code timeouts (https://cromwell.readthedocs.io/en/develop/backends/HPC/#exit-code-timeout). Since the timeout function would catch all failures (eg even if the script doesn't get a chance to trap the signal) I suspect it's more generally useful. What do you think?. cc @EvanTheB and @rhpvorderman since they probably know at least as much about why we went down the `exit-code-timeout` route as I do... 😄,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956
https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956:486,Integrability,rout,route,486,Hi @drkennetz - this seems quite similar in intent to an existing (opt-in) option for checking exit-code timeouts (https://cromwell.readthedocs.io/en/develop/backends/HPC/#exit-code-timeout). Since the timeout function would catch all failures (eg even if the script doesn't get a chance to trap the signal) I suspect it's more generally useful. What do you think?. cc @EvanTheB and @rhpvorderman since they probably know at least as much about why we went down the `exit-code-timeout` route as I do... 😄,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956
https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956:105,Safety,timeout,timeouts,105,Hi @drkennetz - this seems quite similar in intent to an existing (opt-in) option for checking exit-code timeouts (https://cromwell.readthedocs.io/en/develop/backends/HPC/#exit-code-timeout). Since the timeout function would catch all failures (eg even if the script doesn't get a chance to trap the signal) I suspect it's more generally useful. What do you think?. cc @EvanTheB and @rhpvorderman since they probably know at least as much about why we went down the `exit-code-timeout` route as I do... 😄,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956
https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956:182,Safety,timeout,timeout,182,Hi @drkennetz - this seems quite similar in intent to an existing (opt-in) option for checking exit-code timeouts (https://cromwell.readthedocs.io/en/develop/backends/HPC/#exit-code-timeout). Since the timeout function would catch all failures (eg even if the script doesn't get a chance to trap the signal) I suspect it's more generally useful. What do you think?. cc @EvanTheB and @rhpvorderman since they probably know at least as much about why we went down the `exit-code-timeout` route as I do... 😄,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956
https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956:202,Safety,timeout,timeout,202,Hi @drkennetz - this seems quite similar in intent to an existing (opt-in) option for checking exit-code timeouts (https://cromwell.readthedocs.io/en/develop/backends/HPC/#exit-code-timeout). Since the timeout function would catch all failures (eg even if the script doesn't get a chance to trap the signal) I suspect it's more generally useful. What do you think?. cc @EvanTheB and @rhpvorderman since they probably know at least as much about why we went down the `exit-code-timeout` route as I do... 😄,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956
https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956:477,Safety,timeout,timeout,477,Hi @drkennetz - this seems quite similar in intent to an existing (opt-in) option for checking exit-code timeouts (https://cromwell.readthedocs.io/en/develop/backends/HPC/#exit-code-timeout). Since the timeout function would catch all failures (eg even if the script doesn't get a chance to trap the signal) I suspect it's more generally useful. What do you think?. cc @EvanTheB and @rhpvorderman since they probably know at least as much about why we went down the `exit-code-timeout` route as I do... 😄,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956
https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519627465:7,Testability,test,tested,7,I have tested this locally and I think that is a great solution to my problem. I wasn't even aware that existed! Thanks for pointing me in the right direction.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519627465
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-520349943:51,Availability,error,error,51,"Reopened PR because last build failed with strange error and after triggering re-build on travis everything was OK, but here status wasn't updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-520349943
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-520349943:139,Deployability,update,updated,139,"Reopened PR because last build failed with strange error and after triggering re-build on travis everything was OK, but here status wasn't updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-520349943
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921:21,Deployability,update,updated,21,"Hi @wleepang !; I've updated a pull-request with two commits from my colleagues @TimurKustov and @SergeySdv . Their contribution includes an addition of `AWS` filesystem support for `fileSystemCheck` option in a `centaur` test cases files and also a refactoring of `CheckFiles`.; Because `AWS` is now supported in fileSystemCheck `centaur`'s option I've added some integration tests which are checking that copying of workflow results, workflow logs and call logs (options `final_workflow_outputs_dir`, `final_workflow_log_dir`, `final_call_logs_dir` from options.json file) is now correct on 3 backends (GCP, AWS and local backend). There are placeholders for paths in these test cases and options used by them, because I didn't came with any better options without creating a public buckets on GCP and AWS.; Hope that you can take a look on this soon!; Thanks in advance, best regards!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921:365,Deployability,integrat,integration,365,"Hi @wleepang !; I've updated a pull-request with two commits from my colleagues @TimurKustov and @SergeySdv . Their contribution includes an addition of `AWS` filesystem support for `fileSystemCheck` option in a `centaur` test cases files and also a refactoring of `CheckFiles`.; Because `AWS` is now supported in fileSystemCheck `centaur`'s option I've added some integration tests which are checking that copying of workflow results, workflow logs and call logs (options `final_workflow_outputs_dir`, `final_workflow_log_dir`, `final_call_logs_dir` from options.json file) is now correct on 3 backends (GCP, AWS and local backend). There are placeholders for paths in these test cases and options used by them, because I didn't came with any better options without creating a public buckets on GCP and AWS.; Hope that you can take a look on this soon!; Thanks in advance, best regards!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921:365,Integrability,integrat,integration,365,"Hi @wleepang !; I've updated a pull-request with two commits from my colleagues @TimurKustov and @SergeySdv . Their contribution includes an addition of `AWS` filesystem support for `fileSystemCheck` option in a `centaur` test cases files and also a refactoring of `CheckFiles`.; Because `AWS` is now supported in fileSystemCheck `centaur`'s option I've added some integration tests which are checking that copying of workflow results, workflow logs and call logs (options `final_workflow_outputs_dir`, `final_workflow_log_dir`, `final_call_logs_dir` from options.json file) is now correct on 3 backends (GCP, AWS and local backend). There are placeholders for paths in these test cases and options used by them, because I didn't came with any better options without creating a public buckets on GCP and AWS.; Hope that you can take a look on this soon!; Thanks in advance, best regards!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921:250,Modifiability,refactor,refactoring,250,"Hi @wleepang !; I've updated a pull-request with two commits from my colleagues @TimurKustov and @SergeySdv . Their contribution includes an addition of `AWS` filesystem support for `fileSystemCheck` option in a `centaur` test cases files and also a refactoring of `CheckFiles`.; Because `AWS` is now supported in fileSystemCheck `centaur`'s option I've added some integration tests which are checking that copying of workflow results, workflow logs and call logs (options `final_workflow_outputs_dir`, `final_workflow_log_dir`, `final_call_logs_dir` from options.json file) is now correct on 3 backends (GCP, AWS and local backend). There are placeholders for paths in these test cases and options used by them, because I didn't came with any better options without creating a public buckets on GCP and AWS.; Hope that you can take a look on this soon!; Thanks in advance, best regards!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921:222,Testability,test,test,222,"Hi @wleepang !; I've updated a pull-request with two commits from my colleagues @TimurKustov and @SergeySdv . Their contribution includes an addition of `AWS` filesystem support for `fileSystemCheck` option in a `centaur` test cases files and also a refactoring of `CheckFiles`.; Because `AWS` is now supported in fileSystemCheck `centaur`'s option I've added some integration tests which are checking that copying of workflow results, workflow logs and call logs (options `final_workflow_outputs_dir`, `final_workflow_log_dir`, `final_call_logs_dir` from options.json file) is now correct on 3 backends (GCP, AWS and local backend). There are placeholders for paths in these test cases and options used by them, because I didn't came with any better options without creating a public buckets on GCP and AWS.; Hope that you can take a look on this soon!; Thanks in advance, best regards!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921:377,Testability,test,tests,377,"Hi @wleepang !; I've updated a pull-request with two commits from my colleagues @TimurKustov and @SergeySdv . Their contribution includes an addition of `AWS` filesystem support for `fileSystemCheck` option in a `centaur` test cases files and also a refactoring of `CheckFiles`.; Because `AWS` is now supported in fileSystemCheck `centaur`'s option I've added some integration tests which are checking that copying of workflow results, workflow logs and call logs (options `final_workflow_outputs_dir`, `final_workflow_log_dir`, `final_call_logs_dir` from options.json file) is now correct on 3 backends (GCP, AWS and local backend). There are placeholders for paths in these test cases and options used by them, because I didn't came with any better options without creating a public buckets on GCP and AWS.; Hope that you can take a look on this soon!; Thanks in advance, best regards!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921:445,Testability,log,logs,445,"Hi @wleepang !; I've updated a pull-request with two commits from my colleagues @TimurKustov and @SergeySdv . Their contribution includes an addition of `AWS` filesystem support for `fileSystemCheck` option in a `centaur` test cases files and also a refactoring of `CheckFiles`.; Because `AWS` is now supported in fileSystemCheck `centaur`'s option I've added some integration tests which are checking that copying of workflow results, workflow logs and call logs (options `final_workflow_outputs_dir`, `final_workflow_log_dir`, `final_call_logs_dir` from options.json file) is now correct on 3 backends (GCP, AWS and local backend). There are placeholders for paths in these test cases and options used by them, because I didn't came with any better options without creating a public buckets on GCP and AWS.; Hope that you can take a look on this soon!; Thanks in advance, best regards!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921:459,Testability,log,logs,459,"Hi @wleepang !; I've updated a pull-request with two commits from my colleagues @TimurKustov and @SergeySdv . Their contribution includes an addition of `AWS` filesystem support for `fileSystemCheck` option in a `centaur` test cases files and also a refactoring of `CheckFiles`.; Because `AWS` is now supported in fileSystemCheck `centaur`'s option I've added some integration tests which are checking that copying of workflow results, workflow logs and call logs (options `final_workflow_outputs_dir`, `final_workflow_log_dir`, `final_call_logs_dir` from options.json file) is now correct on 3 backends (GCP, AWS and local backend). There are placeholders for paths in these test cases and options used by them, because I didn't came with any better options without creating a public buckets on GCP and AWS.; Hope that you can take a look on this soon!; Thanks in advance, best regards!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921:676,Testability,test,test,676,"Hi @wleepang !; I've updated a pull-request with two commits from my colleagues @TimurKustov and @SergeySdv . Their contribution includes an addition of `AWS` filesystem support for `fileSystemCheck` option in a `centaur` test cases files and also a refactoring of `CheckFiles`.; Because `AWS` is now supported in fileSystemCheck `centaur`'s option I've added some integration tests which are checking that copying of workflow results, workflow logs and call logs (options `final_workflow_outputs_dir`, `final_workflow_log_dir`, `final_call_logs_dir` from options.json file) is now correct on 3 backends (GCP, AWS and local backend). There are placeholders for paths in these test cases and options used by them, because I didn't came with any better options without creating a public buckets on GCP and AWS.; Hope that you can take a look on this soon!; Thanks in advance, best regards!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527659921
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527854752:55,Availability,error,errors,55,"There are again some issues with Travis build (strange errors) , I'll reopen PR to trigger it again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527854752
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527913772:30,Testability,test,tests,30,We recently fixed some flakey tests on `develop` so you should merge it in if you haven't already.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527913772
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527959017:359,Deployability,configurat,configuration,359,"@aednichols I've rebased on develop couple days ago.; There only one job fails (https://travis-ci.com/broadinstitute/cromwell/jobs/231053156), and the last lines of log are:; ```; No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.; Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-received; The build has been terminated; ```. Never saw this before, so I don't know how to fix this...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527959017
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527959017:359,Modifiability,config,configuration,359,"@aednichols I've rebased on develop couple days ago.; There only one job fails (https://travis-ci.com/broadinstitute/cromwell/jobs/231053156), and the last lines of log are:; ```; No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.; Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-received; The build has been terminated; ```. Never saw this before, so I don't know how to fix this...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527959017
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527959017:165,Testability,log,log,165,"@aednichols I've rebased on develop couple days ago.; There only one job fails (https://travis-ci.com/broadinstitute/cromwell/jobs/231053156), and the last lines of log are:; ```; No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.; Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-received; The build has been terminated; ```. Never saw this before, so I don't know how to fix this...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527959017
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-529430432:73,Deployability,update,updated,73,Hi @wleepang @cjllanwarne @aednichols !; Could you please take a look at updated changes?; Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-529430432
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-536672187:130,Testability,test,tests,130,"Hi @likeanowl, could you possibly rebase this PR against the latest develop? The good news is the clone of this PR [passed our CI tests](https://github.com/broadinstitute/cromwell/pull/5192) but a lot has changed on develop since this was originally opened. 🙂",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-536672187
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537142165:128,Deployability,update,updates,128,"I'm a little wary of introducing 2 different AWS sdk's into the project. Is the reason to use Transfer Manager? Besides logging updates what is the benefit of it?. Besides using transfer manager is there a need to pull in the ""old"" SDK? We were told the current 2.X series is the one to use.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537142165
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537142165:120,Testability,log,logging,120,"I'm a little wary of introducing 2 different AWS sdk's into the project. Is the reason to use Transfer Manager? Besides logging updates what is the benefit of it?. Besides using transfer manager is there a need to pull in the ""old"" SDK? We were told the current 2.X series is the one to use.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537142165
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537142572:102,Deployability,update,updates,102,Oh I think I see that the existing version doesn't support copying directories. Maybe there have been updates since this code was written though?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537142572
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537262399:153,Deployability,update,updates,153,"Hi @danbills !; > I'm a little wary of introducing 2 different AWS sdk's into the project.; > ; > Is the reason to use Transfer Manager? Besides logging updates what is the benefit of it?; > ; > Besides using transfer manager is there a need to pull in the ""old"" SDK? We were told the current 2.X series is the one to use. I rewrote files copying using `TransferManager` just because @wleepang adviced it's usage in original issue ([this](https://github.com/broadinstitute/cromwell/issues/4982)). > Oh I think I see that the existing version doesn't support copying directories. Maybe there have been updates since this code was written though?. If you are talking about [this comment](https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-530285103), I think that it is about old version, before my changes. Actually, I haven't tested my fix with ` Array[File]` type, only with `File`, will do it tomorrow. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537262399
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537262399:601,Deployability,update,updates,601,"Hi @danbills !; > I'm a little wary of introducing 2 different AWS sdk's into the project.; > ; > Is the reason to use Transfer Manager? Besides logging updates what is the benefit of it?; > ; > Besides using transfer manager is there a need to pull in the ""old"" SDK? We were told the current 2.X series is the one to use. I rewrote files copying using `TransferManager` just because @wleepang adviced it's usage in original issue ([this](https://github.com/broadinstitute/cromwell/issues/4982)). > Oh I think I see that the existing version doesn't support copying directories. Maybe there have been updates since this code was written though?. If you are talking about [this comment](https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-530285103), I think that it is about old version, before my changes. Actually, I haven't tested my fix with ` Array[File]` type, only with `File`, will do it tomorrow. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537262399
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537262399:145,Testability,log,logging,145,"Hi @danbills !; > I'm a little wary of introducing 2 different AWS sdk's into the project.; > ; > Is the reason to use Transfer Manager? Besides logging updates what is the benefit of it?; > ; > Besides using transfer manager is there a need to pull in the ""old"" SDK? We were told the current 2.X series is the one to use. I rewrote files copying using `TransferManager` just because @wleepang adviced it's usage in original issue ([this](https://github.com/broadinstitute/cromwell/issues/4982)). > Oh I think I see that the existing version doesn't support copying directories. Maybe there have been updates since this code was written though?. If you are talking about [this comment](https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-530285103), I think that it is about old version, before my changes. Actually, I haven't tested my fix with ` Array[File]` type, only with `File`, will do it tomorrow. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537262399
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537262399:843,Testability,test,tested,843,"Hi @danbills !; > I'm a little wary of introducing 2 different AWS sdk's into the project.; > ; > Is the reason to use Transfer Manager? Besides logging updates what is the benefit of it?; > ; > Besides using transfer manager is there a need to pull in the ""old"" SDK? We were told the current 2.X series is the one to use. I rewrote files copying using `TransferManager` just because @wleepang adviced it's usage in original issue ([this](https://github.com/broadinstitute/cromwell/issues/4982)). > Oh I think I see that the existing version doesn't support copying directories. Maybe there have been updates since this code was written though?. If you are talking about [this comment](https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-530285103), I think that it is about old version, before my changes. Actually, I haven't tested my fix with ` Array[File]` type, only with `File`, will do it tomorrow. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-537262399
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-543415388:46,Testability,test,tests,46,"Hi Sergey, we recently reenabled many Centaur tests for the AWS backend that had previously been excluded since we found they were actually passing on the current `develop` code. One of these Centaur tests is `space` which unfortunately does not pass with this PR. Here's a link to a failed Centaur run, please let us know if you need any more information: https://travis-ci.com/broadinstitute/cromwell/jobs/246824689",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-543415388
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-543415388:200,Testability,test,tests,200,"Hi Sergey, we recently reenabled many Centaur tests for the AWS backend that had previously been excluded since we found they were actually passing on the current `develop` code. One of these Centaur tests is `space` which unfortunately does not pass with this PR. Here's a link to a failed Centaur run, please let us know if you need any more information: https://travis-ci.com/broadinstitute/cromwell/jobs/246824689",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-543415388
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-543740380:39,Testability,test,test,39,"Hi!; Sure, I'll take a look at failing test during this weekend. :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-543740380
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-544924499:128,Security,Access,AccessDenied,128,"@mcovarr ; Is the bucket http://s3.amazonaws.com/cromwell-centaur-execution/ public? Because when I tried to open it, I've got `AccessDenied`. ; As far as I know, centaur do not share credentials with cromwell (at least if credentials were provided in `.conf` file, idk what about default auth mechanism), and therefore this exception may be caused by lack of credentials on centaur side.; Actually, in this PR a support for aws auth was added to centaur, so can you please try to run this test with aws credentials in `centaur/src/main/resources/reference.conf`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-544924499
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-544924499:490,Testability,test,test,490,"@mcovarr ; Is the bucket http://s3.amazonaws.com/cromwell-centaur-execution/ public? Because when I tried to open it, I've got `AccessDenied`. ; As far as I know, centaur do not share credentials with cromwell (at least if credentials were provided in `.conf` file, idk what about default auth mechanism), and therefore this exception may be caused by lack of credentials on centaur side.; Actually, in this PR a support for aws auth was added to centaur, so can you please try to run this test with aws credentials in `centaur/src/main/resources/reference.conf`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-544924499
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-545981425:145,Testability,test,tests,145,Hey @likeanowl -- can you please confirm if this workflow runs on your local branch of Cromwell on AWS? We're seeing it fail in the AWS specific tests. https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/space/space.wdl,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-545981425
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-547451533:15,Deployability,update,update,15,@likeanowl any update on the test case?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-547451533
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-547451533:29,Testability,test,test,29,@likeanowl any update on the test case?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-547451533
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-548577269:19,Testability,test,test,19,"Hi @ruchim !; I'll test it locally really soon... Just did not had time yet, sorry.; I'll write here about status, and also will rebase branch on the most recent `develop`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-548577269
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-555097592:122,Energy Efficiency,green,green,122,"Hi @cjllanwarne @ruchim @mcovarr !; I've finally rebased against the latest develop and resolved conflicts, and CI is all green. So it a good sign? :); I still could try to reproduce a `test with space` scenario, if it was turned off on CI. Should I?; Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-555097592
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-555097592:186,Testability,test,test,186,"Hi @cjllanwarne @ruchim @mcovarr !; I've finally rebased against the latest develop and resolved conflicts, and CI is all green. So it a good sign? :); I still could try to reproduce a `test with space` scenario, if it was turned off on CI. Should I?; Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-555097592
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-555218927:29,Usability,simpl,simple,29,Resolved conflicts (it was a simple union of branch changes and develop changes),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-555218927
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558154031:116,Energy Efficiency,green,green,116,"Hi @cjllanwarne @mcovarr @ruchim !; I've again rebased my branch against the latest develop and Travis jobs are all green. So I want to repeat my last question about failing test: **was it removed, or is everything OK now?**; Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558154031
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558154031:174,Testability,test,test,174,"Hi @cjllanwarne @mcovarr @ruchim !; I've again rebased my branch against the latest develop and Travis jobs are all green. So I want to repeat my last question about failing test: **was it removed, or is everything OK now?**; Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558154031
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558350731:86,Security,secur,security,86,"Hi Sergey,. Most of our Travis builds short-circuit for external contributions due to security issues; the `space` test is still around but Travis does not run it against your fork. . [This](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/space/space.wdl) is the `space` WDL which you could try running against your changes. The workflow should succeed and have outputs that look like [these metadata expectations](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/space.test) in the `.test` file. Please let us know if you have any questions. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558350731
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558350731:115,Testability,test,test,115,"Hi Sergey,. Most of our Travis builds short-circuit for external contributions due to security issues; the `space` test is still around but Travis does not run it against your fork. . [This](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/space/space.wdl) is the `space` WDL which you could try running against your changes. The workflow should succeed and have outputs that look like [these metadata expectations](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/space.test) in the `.test` file. Please let us know if you have any questions. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558350731
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558350731:575,Testability,test,test,575,"Hi Sergey,. Most of our Travis builds short-circuit for external contributions due to security issues; the `space` test is still around but Travis does not run it against your fork. . [This](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/space/space.wdl) is the `space` WDL which you could try running against your changes. The workflow should succeed and have outputs that look like [these metadata expectations](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/space.test) in the `.test` file. Please let us know if you have any questions. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558350731
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558350731:590,Testability,test,test,590,"Hi Sergey,. Most of our Travis builds short-circuit for external contributions due to security issues; the `space` test is still around but Travis does not run it against your fork. . [This](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/space/space.wdl) is the `space` WDL which you could try running against your changes. The workflow should succeed and have outputs that look like [these metadata expectations](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/space.test) in the `.test` file. Please let us know if you have any questions. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558350731
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-570351716:114,Testability,test,test,114,"Possibly unblocked by the merge of https://github.com/broadinstitute/cromwell/pull/5331 which removed the `space` test for AWS, though this still needs a rebase.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-570351716
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-574777216:104,Availability,reliab,reliability,104,"Hi @likeanowl, would you mind updating your branch from our latest develop? We have some important test reliability fixes that need picking up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-574777216
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-574777216:99,Testability,test,test,99,"Hi @likeanowl, would you mind updating your branch from our latest develop? We have some important test reliability fixes that need picking up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-574777216
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991:64,Availability,error,error,64,"As I can see, there are 2 jobs which failed on travis with same error:; ```; ""message"" : ""Task gpu_on_papi.task_with_gpu:1:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [\""docker\"" \""pull\"" \""gcr.io/google.com/cloudsdktool/cloud-sdk@sha256:0e3fc9aa87d01e7203a2ea90ba7b4d2f52ca28f09920f69765f8118a88681217\""]: exit status 1 (standard error: \""failed to register layer: Error processing tar file(exit status 1): write /usr/share/perl/5.28.1/Unicode/Collate/allkeys.txt: no space left on device\\n\"")""; ```; @aednichols @mcovarr could you please re-trigger travis PR build? (since it seems like there were no other errors yet)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991:187,Availability,error,error,187,"As I can see, there are 2 jobs which failed on travis with same error:; ```; ""message"" : ""Task gpu_on_papi.task_with_gpu:1:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [\""docker\"" \""pull\"" \""gcr.io/google.com/cloudsdktool/cloud-sdk@sha256:0e3fc9aa87d01e7203a2ea90ba7b4d2f52ca28f09920f69765f8118a88681217\""]: exit status 1 (standard error: \""failed to register layer: Error processing tar file(exit status 1): write /usr/share/perl/5.28.1/Unicode/Collate/allkeys.txt: no space left on device\\n\"")""; ```; @aednichols @mcovarr could you please re-trigger travis PR build? (since it seems like there were no other errors yet)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991:419,Availability,error,error,419,"As I can see, there are 2 jobs which failed on travis with same error:; ```; ""message"" : ""Task gpu_on_papi.task_with_gpu:1:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [\""docker\"" \""pull\"" \""gcr.io/google.com/cloudsdktool/cloud-sdk@sha256:0e3fc9aa87d01e7203a2ea90ba7b4d2f52ca28f09920f69765f8118a88681217\""]: exit status 1 (standard error: \""failed to register layer: Error processing tar file(exit status 1): write /usr/share/perl/5.28.1/Unicode/Collate/allkeys.txt: no space left on device\\n\"")""; ```; @aednichols @mcovarr could you please re-trigger travis PR build? (since it seems like there were no other errors yet)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991:454,Availability,Error,Error,454,"As I can see, there are 2 jobs which failed on travis with same error:; ```; ""message"" : ""Task gpu_on_papi.task_with_gpu:1:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [\""docker\"" \""pull\"" \""gcr.io/google.com/cloudsdktool/cloud-sdk@sha256:0e3fc9aa87d01e7203a2ea90ba7b4d2f52ca28f09920f69765f8118a88681217\""]: exit status 1 (standard error: \""failed to register layer: Error processing tar file(exit status 1): write /usr/share/perl/5.28.1/Unicode/Collate/allkeys.txt: no space left on device\\n\"")""; ```; @aednichols @mcovarr could you please re-trigger travis PR build? (since it seems like there were no other errors yet)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991:698,Availability,error,errors,698,"As I can see, there are 2 jobs which failed on travis with same error:; ```; ""message"" : ""Task gpu_on_papi.task_with_gpu:1:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [\""docker\"" \""pull\"" \""gcr.io/google.com/cloudsdktool/cloud-sdk@sha256:0e3fc9aa87d01e7203a2ea90ba7b4d2f52ca28f09920f69765f8118a88681217\""]: exit status 1 (standard error: \""failed to register layer: Error processing tar file(exit status 1): write /usr/share/perl/5.28.1/Unicode/Collate/allkeys.txt: no space left on device\\n\"")""; ```; @aednichols @mcovarr could you please re-trigger travis PR build? (since it seems like there were no other errors yet)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991:78,Integrability,message,message,78,"As I can see, there are 2 jobs which failed on travis with same error:; ```; ""message"" : ""Task gpu_on_papi.task_with_gpu:1:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [\""docker\"" \""pull\"" \""gcr.io/google.com/cloudsdktool/cloud-sdk@sha256:0e3fc9aa87d01e7203a2ea90ba7b4d2f52ca28f09920f69765f8118a88681217\""]: exit status 1 (standard error: \""failed to register layer: Error processing tar file(exit status 1): write /usr/share/perl/5.28.1/Unicode/Collate/allkeys.txt: no space left on device\\n\"")""; ```; @aednichols @mcovarr could you please re-trigger travis PR build? (since it seems like there were no other errors yet)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575098991
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575281798:128,Availability,Error,Error,128,"hmm the same test failed on all 3 PAPI builds with a root cause of ""no space left on device"" 🤔 ; ```; failed to register layer: Error processing tar file(exit status 1): write /usr/share/perl/5.28.1/Unicode/Collate/allkeys.txt: no space left on device; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575281798
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575281798:13,Testability,test,test,13,"hmm the same test failed on all 3 PAPI builds with a root cause of ""no space left on device"" 🤔 ; ```; failed to register layer: Error processing tar file(exit status 1): write /usr/share/perl/5.28.1/Unicode/Collate/allkeys.txt: no space left on device; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575281798
https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575293472:16,Availability,error,error,16,I'm seeing this error on my builds too so I don't think it's related to your changes. I'll keep investigating.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575293472
https://github.com/broadinstitute/cromwell/pull/5111#issuecomment-524391215:161,Availability,outage,outage,161,"Ran commit `34fcb6bf3b9e29557a7d9ac057e8ae07370180b9` on my laptop via `sbt clean && src/ci/bin/testCentaurBcs.sh` and tests passed. Dunno if there was a GitHub outage missing the event, but [the Travis CI test passed](https://travis-ci.com/broadinstitute/cromwell/builds/123676180). Merging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5111#issuecomment-524391215
https://github.com/broadinstitute/cromwell/pull/5111#issuecomment-524391215:96,Testability,test,testCentaurBcs,96,"Ran commit `34fcb6bf3b9e29557a7d9ac057e8ae07370180b9` on my laptop via `sbt clean && src/ci/bin/testCentaurBcs.sh` and tests passed. Dunno if there was a GitHub outage missing the event, but [the Travis CI test passed](https://travis-ci.com/broadinstitute/cromwell/builds/123676180). Merging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5111#issuecomment-524391215
https://github.com/broadinstitute/cromwell/pull/5111#issuecomment-524391215:119,Testability,test,tests,119,"Ran commit `34fcb6bf3b9e29557a7d9ac057e8ae07370180b9` on my laptop via `sbt clean && src/ci/bin/testCentaurBcs.sh` and tests passed. Dunno if there was a GitHub outage missing the event, but [the Travis CI test passed](https://travis-ci.com/broadinstitute/cromwell/builds/123676180). Merging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5111#issuecomment-524391215
https://github.com/broadinstitute/cromwell/pull/5111#issuecomment-524391215:206,Testability,test,test,206,"Ran commit `34fcb6bf3b9e29557a7d9ac057e8ae07370180b9` on my laptop via `sbt clean && src/ci/bin/testCentaurBcs.sh` and tests passed. Dunno if there was a GitHub outage missing the event, but [the Travis CI test passed](https://travis-ci.com/broadinstitute/cromwell/builds/123676180). Merging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5111#issuecomment-524391215
https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520597689:56,Performance,perform,performance,56,"Yes but we should be very, very careful with describing performance improvements so as not to create unreasonable expectations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520597689
https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520597918:7,Performance,perform,performance,7,"""Fixed performance""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520597918
https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520824354:74,Performance,perform,performance,74,@cjllanwarne BA-5904 is the JIRA peer to this PR which in no way improves performance. 😉 I agree that the JIRA peers to the PRs that actually do improve performance should get a User Impact like what you describe.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520824354
https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520824354:153,Performance,perform,performance,153,@cjllanwarne BA-5904 is the JIRA peer to this PR which in no way improves performance. 😉 I agree that the JIRA peers to the PRs that actually do improve performance should get a User Impact like what you describe.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5112#issuecomment-520824354
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521284310:48,Availability,error,errors,48,UPDATE: If we can make this logic retry all 500 errors but leave any other new `IOException`s un-retried I think that's the best way to go for now. Does that sounds feasible? Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521284310
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521284310:0,Deployability,UPDATE,UPDATE,0,UPDATE: If we can make this logic retry all 500 errors but leave any other new `IOException`s un-retried I think that's the best way to go for now. Does that sounds feasible? Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521284310
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521284310:28,Testability,log,logic,28,UPDATE: If we can make this logic retry all 500 errors but leave any other new `IOException`s un-retried I think that's the best way to go for now. Does that sounds feasible? Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521284310
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521653694:311,Availability,Error,Error,311,"Hi @cjllanwarne ; I'm not sure that I'm fully understood what you are wanting. > leave any other new IOExceptions un-retried. Do you mean that we should leave the existing case from PR #4272 as is? Instead of changing it, we should add a case for any throwable to check whether it contains ""500 Internal Server Error"", right?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521653694
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521654448:37,Availability,error,errors,37,It's also possible that by; >all 500 errors. @cjllanwarne means 5xx,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521654448
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521675399:368,Availability,Error,Error,368,"Hi @aednichols !. > Is it possible to add `500` to `AdditionalRetryableHttpCodes`?. It is possible, but it won't give the result we need.; These codes are used only for `StorageException`s, since other exceptions don't have `getCode` method. Therefore, if we add `500` to `AdditionalRetryableHttpCodes`, Cromwell won't retry IOException caused by `500 Internal Server Error`. > It's also possible that by all 500 errors @cjllanwarne means 5xx. We did not think about it :) I think you're right, but just in case we will wait for an answer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521675399
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521675399:413,Availability,error,errors,413,"Hi @aednichols !. > Is it possible to add `500` to `AdditionalRetryableHttpCodes`?. It is possible, but it won't give the result we need.; These codes are used only for `StorageException`s, since other exceptions don't have `getCode` method. Therefore, if we add `500` to `AdditionalRetryableHttpCodes`, Cromwell won't retry IOException caused by `500 Internal Server Error`. > It's also possible that by all 500 errors @cjllanwarne means 5xx. We did not think about it :) I think you're right, but just in case we will wait for an answer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521675399
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521676544:120,Availability,error,error,120,"~The values in `AdditionalRetryableHttpCodes` are evaluated against `gcs.getCode` not `gcs.getMessage`, so the fact the error copy is different shouldn't matter (so I _think_ it should work for you)~. nvm, obviously did not fully understand your message before replying 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521676544
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521676544:246,Integrability,message,message,246,"~The values in `AdditionalRetryableHttpCodes` are evaluated against `gcs.getCode` not `gcs.getMessage`, so the fact the error copy is different shouldn't matter (so I _think_ it should work for you)~. nvm, obviously did not fully understand your message before replying 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521676544
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:1908,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1908,0003 7020 -2983; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [...ne/src/main/scala/cromwell/engine/io/IoActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS9pby9Jb0FjdG9yLnNjYWxh) | `69.84% <100%> (-2.29%)` | :arrow_down: |; | [...ala/wdl/draft2/model/WdlSyntaxErrorFormatter.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL1dkbFN5bnRheEVycm9yRm9ybWF0dGVyLnNjYWxh) | `70.19% <0%> (-0.67%)` | :arrow_down: |; | [.../scala/cromwell/database/slick/SlickDatabase.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-ZGF0YWJhc2Uvc3FsL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2RhdGFiYXNlL3NsaWNrL1NsaWNrRGF0YWJhc2Uuc2NhbGE=) | `84.78% <0%> (-0.64%)` | :arrow_down: |; | [...a1/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvUGlwZWxpbmVzQXBpQXN5bmNCYWNrZW5kSm9iRXhlY3V0aW9uQWN0b3Iuc2NhbGE=) | `6.84% <0%> (-0.2%)` | :arrow_down: |; | [...d/google/pipelines/v2alpha1/api/Localization.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (ø)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ=,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:2296,Deployability,pipeline,pipelines,2296,%)` | :arrow_down: |; | [...ala/wdl/draft2/model/WdlSyntaxErrorFormatter.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL1dkbFN5bnRheEVycm9yRm9ybWF0dGVyLnNjYWxh) | `70.19% <0%> (-0.67%)` | :arrow_down: |; | [.../scala/cromwell/database/slick/SlickDatabase.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-ZGF0YWJhc2Uvc3FsL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2RhdGFiYXNlL3NsaWNrL1NsaWNrRGF0YWJhc2Uuc2NhbGE=) | `84.78% <0%> (-0.64%)` | :arrow_down: |; | [...a1/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvUGlwZWxpbmVzQXBpQXN5bmNCYWNrZW5kSm9iRXhlY3V0aW9uQWN0b3Iuc2NhbGE=) | `6.84% <0%> (-0.2%)` | :arrow_down: |; | [...d/google/pipelines/v2alpha1/api/Localization.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (ø)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (ø)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:2635,Deployability,pipeline,pipelines,2635,slick/SlickDatabase.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-ZGF0YWJhc2Uvc3FsL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2RhdGFiYXNlL3NsaWNrL1NsaWNrRGF0YWJhc2Uuc2NhbGE=) | `84.78% <0%> (-0.64%)` | :arrow_down: |; | [...a1/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvUGlwZWxpbmVzQXBpQXN5bmNCYWNrZW5kSm9iRXhlY3V0aW9uQWN0b3Iuc2NhbGE=) | `6.84% <0%> (-0.2%)` | :arrow_down: |; | [...d/google/pipelines/v2alpha1/api/Localization.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (ø)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (ø)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96% <0%> (+0.39%)` | :arrow_up: |; | [...n/scala/cromwell/core/path/BetterFileMethods.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9jb3JlL3BhdGgvQmV0dGVyRmlsZU1ldGhvZHMuc2NhbGE=) | `30.1% <0%> (+1.02%)` | :arrow_up: |; | [...wl/src/main/scala/cwl/ExpressionInter,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:2652,Deployability,Pipeline,PipelinesApiJobPaths,2652,slick/SlickDatabase.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-ZGF0YWJhc2Uvc3FsL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2RhdGFiYXNlL3NsaWNrL1NsaWNrRGF0YWJhc2Uuc2NhbGE=) | `84.78% <0%> (-0.64%)` | :arrow_down: |; | [...a1/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvUGlwZWxpbmVzQXBpQXN5bmNCYWNrZW5kSm9iRXhlY3V0aW9uQWN0b3Iuc2NhbGE=) | `6.84% <0%> (-0.2%)` | :arrow_down: |; | [...d/google/pipelines/v2alpha1/api/Localization.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (ø)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (ø)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96% <0%> (+0.39%)` | :arrow_up: |; | [...n/scala/cromwell/core/path/BetterFileMethods.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9jb3JlL3BhdGgvQmV0dGVyRmlsZU1ldGhvZHMuc2NhbGE=) | `30.1% <0%> (+1.02%)` | :arrow_up: |; | [...wl/src/main/scala/cwl/ExpressionInter,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:2974,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2974,/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvUGlwZWxpbmVzQXBpQXN5bmNCYWNrZW5kSm9iRXhlY3V0aW9uQWN0b3Iuc2NhbGE=) | `6.84% <0%> (-0.2%)` | :arrow_down: |; | [...d/google/pipelines/v2alpha1/api/Localization.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (ø)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (ø)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96% <0%> (+0.39%)` | :arrow_up: |; | [...n/scala/cromwell/core/path/BetterFileMethods.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9jb3JlL3BhdGgvQmV0dGVyRmlsZU1ldGhvZHMuc2NhbGE=) | `30.1% <0%> (+1.02%)` | :arrow_up: |; | [...wl/src/main/scala/cwl/ExpressionInterpolator.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | ... and [301 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree-more) | |. ------. [Continue to review f,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:4363,Deployability,update,update,4363,"JhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (ø)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (ø)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96% <0%> (+0.39%)` | :arrow_up: |; | [...n/scala/cromwell/core/path/BetterFileMethods.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9jb3JlL3BhdGgvQmV0dGVyRmlsZU1ldGhvZHMuc2NhbGE=) | `30.1% <0%> (+1.02%)` | :arrow_up: |; | [...wl/src/main/scala/cwl/ExpressionInterpolator.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | ... and [301 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=footer). Last update [242206f...e9ad3d7](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:4262,Energy Efficiency,Power,Powered,4262,"JhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (ø)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (ø)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96% <0%> (+0.39%)` | :arrow_up: |; | [...n/scala/cromwell/core/path/BetterFileMethods.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9jb3JlL3BhdGgvQmV0dGVyRmlsZU1ldGhvZHMuc2NhbGE=) | `30.1% <0%> (+1.02%)` | :arrow_up: |; | [...wl/src/main/scala/cwl/ExpressionInterpolator.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | ... and [301 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=footer). Last update [242206f...e9ad3d7](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:4125,Usability,learn,learn,4125,"JhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (ø)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (ø)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96% <0%> (+0.39%)` | :arrow_up: |; | [...n/scala/cromwell/core/path/BetterFileMethods.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9jb3JlL3BhdGgvQmV0dGVyRmlsZU1ldGhvZHMuc2NhbGE=) | `30.1% <0%> (+1.02%)` | :arrow_up: |; | [...wl/src/main/scala/cwl/ExpressionInterpolator.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | ... and [301 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=footer). Last update [242206f...e9ad3d7](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-522059635:33,Testability,test,test,33,I had to reopen the PR since one test failed due to external reasons and I couldn't find a button to restart a single test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-522059635
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-522059635:118,Testability,test,test,118,I had to reopen the PR since one test failed due to external reasons and I couldn't find a button to restart a single test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-522059635
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-523668341:155,Testability,test,tests,155,"Hi @myazinn, unfortunately we had to revert this PR because it causes our CI to hang forever and time out. It was our mistake that we did not properly run tests on the branch before merging. We are working on fixing this so external contributors get the same test coverage as internal PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-523668341
https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-523668341:259,Testability,test,test,259,"Hi @myazinn, unfortunately we had to revert this PR because it causes our CI to hang forever and time out. It was our mistake that we did not properly run tests on the branch before merging. We are working on fixing this so external contributors get the same test coverage as internal PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-523668341
https://github.com/broadinstitute/cromwell/pull/5115#issuecomment-521012487:259,Security,access,accessed,259,This seems functionally correct to me... but is it easy to add a test case for this? . I'm slightly worried that the `override def getScheme: String` function could be being used elsewhere in the system - and that might cause problems when `drs://` files are accessed later on.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5115#issuecomment-521012487
https://github.com/broadinstitute/cromwell/pull/5115#issuecomment-521012487:65,Testability,test,test,65,This seems functionally correct to me... but is it easy to add a test case for this? . I'm slightly worried that the `override def getScheme: String` function could be being used elsewhere in the system - and that might cause problems when `drs://` files are accessed later on.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5115#issuecomment-521012487
https://github.com/broadinstitute/cromwell/pull/5118#issuecomment-522753211:17,Testability,test,test,17,"We have a flakey test that we are looking into (it impacts all development, so is obviously a priority)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5118#issuecomment-522753211
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-523980184:227,Testability,test,test,227,"@cjllanwarne. * Created BA-5940 for the startsWith/contains discrepancy.; * I have similar concerns with large metadata and was thinking the same thing of decreasing window size and increasing frequency, but mostly I'd like to test in perf before merging to develop.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-523980184
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-523980462:57,Testability,test,tested,57,Applied the DO NOT MERGE label since this should be perf tested before merge.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-523980462
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-525016335:11,Testability,test,test,11,"One way to test this would be to temporarily take over alpha and run Gary's alpha test. I have collected screenshots of CPU utilization on the runners, summarizer, and database during the most recent run, so we have a reference.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-525016335
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-525016335:82,Testability,test,test,82,"One way to test this would be to temporarily take over alpha and run Gary's alpha test. I have collected screenshots of CPU utilization on the runners, summarizer, and database during the most recent run, so we have a reference.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-525016335
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-526285440:78,Deployability,release,release,78,I guess one way to test this that would go with the grain of the conventional release process would be to create a config option that's disabled by default and selectively enable it on alpha on-instance for testing. It could be removed once we're confident it works in prod. (I fully own that I have questioned the value of config options in the past; I think this is a bit different because it's designed to be temporary.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-526285440
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-526285440:115,Modifiability,config,config,115,I guess one way to test this that would go with the grain of the conventional release process would be to create a config option that's disabled by default and selectively enable it on alpha on-instance for testing. It could be removed once we're confident it works in prod. (I fully own that I have questioned the value of config options in the past; I think this is a bit different because it's designed to be temporary.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-526285440
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-526285440:324,Modifiability,config,config,324,I guess one way to test this that would go with the grain of the conventional release process would be to create a config option that's disabled by default and selectively enable it on alpha on-instance for testing. It could be removed once we're confident it works in prod. (I fully own that I have questioned the value of config options in the past; I think this is a bit different because it's designed to be temporary.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-526285440
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-526285440:19,Testability,test,test,19,I guess one way to test this that would go with the grain of the conventional release process would be to create a config option that's disabled by default and selectively enable it on alpha on-instance for testing. It could be removed once we're confident it works in prod. (I fully own that I have questioned the value of config options in the past; I think this is a bit different because it's designed to be temporary.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-526285440
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-526285440:207,Testability,test,testing,207,I guess one way to test this that would go with the grain of the conventional release process would be to create a config option that's disabled by default and selectively enable it on alpha on-instance for testing. It could be removed once we're confident it works in prod. (I fully own that I have questioned the value of config options in the past; I think this is a bit different because it's designed to be temporary.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-526285440
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474:1920,Modifiability,variab,variable,1920,"work bytes higher:. <img width=""1338"" alt=""Screen Shot 2019-09-13 at 8 13 00 PM"" src=""https://user-images.githubusercontent.com/1087943/64965213-93d73b80-d86a-11e9-9278-03b6f665c378.png"">. ---. Database page read/writes identical:. <img width=""1334"" alt=""Screen Shot 2019-09-13 at 8 13 39 PM"" src=""https://user-images.githubusercontent.com/1087943/64965215-93d73b80-d86a-11e9-9db5-4416f5e94719.png"">. ---. Database CPU identical:. <img width=""1332"" alt=""Screen Shot 2019-09-13 at 8 13 52 PM"" src=""https://user-images.githubusercontent.com/1087943/64965216-946fd200-d86a-11e9-9735-6d91a8b74a4d.png"">. ---. Database egress bytes modestly higher, consistent with higher inbound on summarizer:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 14 33 PM"" src=""https://user-images.githubusercontent.com/1087943/64965217-946fd200-d86a-11e9-8fb5-c11a49da15e4.png"">. ---. Not too surprising, but SQL query rate also identical:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 15 13 PM"" src=""https://user-images.githubusercontent.com/1087943/64965218-946fd200-d86a-11e9-9db7-3149df61c0e0.png"">. ---. I thought it was interesting that the only variable that showed much change is bytes over the network. Theory:. MySQL pages on disk are buckets of row values. MySQL accesses the disk at the granularity of a page, it can't fetch just a single value. Therefore, fetching some data from a page (MySQL filtering) versus all data from a page (client-side filtering) does not make a difference in the number of pages read. This is supported by the graph. It would also appear that filtering in memory, whether on client or server, does not have much of a CPU cost at all either for Cromwell nor for MySQL, because we do not see MySQL doing any less work nor Cromwell doing any more. I think this is because once a set of rows is already in memory (after reading a page or receiving the rows over the wire) choosing specific ones is trivial. For MySQL, finding and loading the pages into memory is the hard part.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474:221,Performance,Perform,PerformanceTest-against-Alpha,221,"I conducted a bunch of testing on alpha and I have Results!. The test runs clocked in as follows:; - Client-side filtering (this PR, `fabece5`); - 1 hour, 1 minute, ; https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/470/; - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/471/; - Filtering in MySQL (original, `f666098`); - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/472/; - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/473/. The graphs below show the two client-side filtering runs, followed by the two MySQL filtering results. ---. Summarizer CPU is identical, inbound network bytes higher:. <img width=""1338"" alt=""Screen Shot 2019-09-13 at 8 13 00 PM"" src=""https://user-images.githubusercontent.com/1087943/64965213-93d73b80-d86a-11e9-9278-03b6f665c378.png"">. ---. Database page read/writes identical:. <img width=""1334"" alt=""Screen Shot 2019-09-13 at 8 13 39 PM"" src=""https://user-images.githubusercontent.com/1087943/64965215-93d73b80-d86a-11e9-9db5-4416f5e94719.png"">. ---. Database CPU identical:. <img width=""1332"" alt=""Screen Shot 2019-09-13 at 8 13 52 PM"" src=""https://user-images.githubusercontent.com/1087943/64965216-946fd200-d86a-11e9-9735-6d91a8b74a4d.png"">. ---. Database egress bytes modestly higher, consistent with higher inbound on summarizer:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 14 33 PM"" src=""https://user-images.githubusercontent.com/1087943/64965217-946fd200-d86a-11e9-8fb5-c11a49da15e4.png"">. ---. Not too surprising, but SQL query rate also identical:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 15 13 PM"" src=""https://user-images.githubusercontent.com/1087943/64965218-946fd200-d86a-11e9-9db7-3149df61c0e0.png"">. ---. I thought it was interesting that the only variable that showed much change is bytes over the network. Theory:. MySQL pages ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474:332,Performance,Perform,PerformanceTest-against-Alpha,332,"I conducted a bunch of testing on alpha and I have Results!. The test runs clocked in as follows:; - Client-side filtering (this PR, `fabece5`); - 1 hour, 1 minute, ; https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/470/; - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/471/; - Filtering in MySQL (original, `f666098`); - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/472/; - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/473/. The graphs below show the two client-side filtering runs, followed by the two MySQL filtering results. ---. Summarizer CPU is identical, inbound network bytes higher:. <img width=""1338"" alt=""Screen Shot 2019-09-13 at 8 13 00 PM"" src=""https://user-images.githubusercontent.com/1087943/64965213-93d73b80-d86a-11e9-9278-03b6f665c378.png"">. ---. Database page read/writes identical:. <img width=""1334"" alt=""Screen Shot 2019-09-13 at 8 13 39 PM"" src=""https://user-images.githubusercontent.com/1087943/64965215-93d73b80-d86a-11e9-9db5-4416f5e94719.png"">. ---. Database CPU identical:. <img width=""1332"" alt=""Screen Shot 2019-09-13 at 8 13 52 PM"" src=""https://user-images.githubusercontent.com/1087943/64965216-946fd200-d86a-11e9-9735-6d91a8b74a4d.png"">. ---. Database egress bytes modestly higher, consistent with higher inbound on summarizer:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 14 33 PM"" src=""https://user-images.githubusercontent.com/1087943/64965217-946fd200-d86a-11e9-8fb5-c11a49da15e4.png"">. ---. Not too surprising, but SQL query rate also identical:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 15 13 PM"" src=""https://user-images.githubusercontent.com/1087943/64965218-946fd200-d86a-11e9-9db7-3149df61c0e0.png"">. ---. I thought it was interesting that the only variable that showed much change is bytes over the network. Theory:. MySQL pages ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474:487,Performance,Perform,PerformanceTest-against-Alpha,487,"I conducted a bunch of testing on alpha and I have Results!. The test runs clocked in as follows:; - Client-side filtering (this PR, `fabece5`); - 1 hour, 1 minute, ; https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/470/; - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/471/; - Filtering in MySQL (original, `f666098`); - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/472/; - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/473/. The graphs below show the two client-side filtering runs, followed by the two MySQL filtering results. ---. Summarizer CPU is identical, inbound network bytes higher:. <img width=""1338"" alt=""Screen Shot 2019-09-13 at 8 13 00 PM"" src=""https://user-images.githubusercontent.com/1087943/64965213-93d73b80-d86a-11e9-9278-03b6f665c378.png"">. ---. Database page read/writes identical:. <img width=""1334"" alt=""Screen Shot 2019-09-13 at 8 13 39 PM"" src=""https://user-images.githubusercontent.com/1087943/64965215-93d73b80-d86a-11e9-9db5-4416f5e94719.png"">. ---. Database CPU identical:. <img width=""1332"" alt=""Screen Shot 2019-09-13 at 8 13 52 PM"" src=""https://user-images.githubusercontent.com/1087943/64965216-946fd200-d86a-11e9-9735-6d91a8b74a4d.png"">. ---. Database egress bytes modestly higher, consistent with higher inbound on summarizer:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 14 33 PM"" src=""https://user-images.githubusercontent.com/1087943/64965217-946fd200-d86a-11e9-8fb5-c11a49da15e4.png"">. ---. Not too surprising, but SQL query rate also identical:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 15 13 PM"" src=""https://user-images.githubusercontent.com/1087943/64965218-946fd200-d86a-11e9-9db7-3149df61c0e0.png"">. ---. I thought it was interesting that the only variable that showed much change is bytes over the network. Theory:. MySQL pages ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474:598,Performance,Perform,PerformanceTest-against-Alpha,598,"I conducted a bunch of testing on alpha and I have Results!. The test runs clocked in as follows:; - Client-side filtering (this PR, `fabece5`); - 1 hour, 1 minute, ; https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/470/; - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/471/; - Filtering in MySQL (original, `f666098`); - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/472/; - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/473/. The graphs below show the two client-side filtering runs, followed by the two MySQL filtering results. ---. Summarizer CPU is identical, inbound network bytes higher:. <img width=""1338"" alt=""Screen Shot 2019-09-13 at 8 13 00 PM"" src=""https://user-images.githubusercontent.com/1087943/64965213-93d73b80-d86a-11e9-9278-03b6f665c378.png"">. ---. Database page read/writes identical:. <img width=""1334"" alt=""Screen Shot 2019-09-13 at 8 13 39 PM"" src=""https://user-images.githubusercontent.com/1087943/64965215-93d73b80-d86a-11e9-9db5-4416f5e94719.png"">. ---. Database CPU identical:. <img width=""1332"" alt=""Screen Shot 2019-09-13 at 8 13 52 PM"" src=""https://user-images.githubusercontent.com/1087943/64965216-946fd200-d86a-11e9-9735-6d91a8b74a4d.png"">. ---. Database egress bytes modestly higher, consistent with higher inbound on summarizer:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 14 33 PM"" src=""https://user-images.githubusercontent.com/1087943/64965217-946fd200-d86a-11e9-8fb5-c11a49da15e4.png"">. ---. Not too surprising, but SQL query rate also identical:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 15 13 PM"" src=""https://user-images.githubusercontent.com/1087943/64965218-946fd200-d86a-11e9-9db7-3149df61c0e0.png"">. ---. I thought it was interesting that the only variable that showed much change is bytes over the network. Theory:. MySQL pages ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474:2735,Performance,load,loading,2735,"work bytes higher:. <img width=""1338"" alt=""Screen Shot 2019-09-13 at 8 13 00 PM"" src=""https://user-images.githubusercontent.com/1087943/64965213-93d73b80-d86a-11e9-9278-03b6f665c378.png"">. ---. Database page read/writes identical:. <img width=""1334"" alt=""Screen Shot 2019-09-13 at 8 13 39 PM"" src=""https://user-images.githubusercontent.com/1087943/64965215-93d73b80-d86a-11e9-9db5-4416f5e94719.png"">. ---. Database CPU identical:. <img width=""1332"" alt=""Screen Shot 2019-09-13 at 8 13 52 PM"" src=""https://user-images.githubusercontent.com/1087943/64965216-946fd200-d86a-11e9-9735-6d91a8b74a4d.png"">. ---. Database egress bytes modestly higher, consistent with higher inbound on summarizer:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 14 33 PM"" src=""https://user-images.githubusercontent.com/1087943/64965217-946fd200-d86a-11e9-8fb5-c11a49da15e4.png"">. ---. Not too surprising, but SQL query rate also identical:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 15 13 PM"" src=""https://user-images.githubusercontent.com/1087943/64965218-946fd200-d86a-11e9-9db7-3149df61c0e0.png"">. ---. I thought it was interesting that the only variable that showed much change is bytes over the network. Theory:. MySQL pages on disk are buckets of row values. MySQL accesses the disk at the granularity of a page, it can't fetch just a single value. Therefore, fetching some data from a page (MySQL filtering) versus all data from a page (client-side filtering) does not make a difference in the number of pages read. This is supported by the graph. It would also appear that filtering in memory, whether on client or server, does not have much of a CPU cost at all either for Cromwell nor for MySQL, because we do not see MySQL doing any less work nor Cromwell doing any more. I think this is because once a set of rows is already in memory (after reading a page or receiving the rows over the wire) choosing specific ones is trivial. For MySQL, finding and loading the pages into memory is the hard part.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474:2042,Security,access,accesses,2042,"work bytes higher:. <img width=""1338"" alt=""Screen Shot 2019-09-13 at 8 13 00 PM"" src=""https://user-images.githubusercontent.com/1087943/64965213-93d73b80-d86a-11e9-9278-03b6f665c378.png"">. ---. Database page read/writes identical:. <img width=""1334"" alt=""Screen Shot 2019-09-13 at 8 13 39 PM"" src=""https://user-images.githubusercontent.com/1087943/64965215-93d73b80-d86a-11e9-9db5-4416f5e94719.png"">. ---. Database CPU identical:. <img width=""1332"" alt=""Screen Shot 2019-09-13 at 8 13 52 PM"" src=""https://user-images.githubusercontent.com/1087943/64965216-946fd200-d86a-11e9-9735-6d91a8b74a4d.png"">. ---. Database egress bytes modestly higher, consistent with higher inbound on summarizer:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 14 33 PM"" src=""https://user-images.githubusercontent.com/1087943/64965217-946fd200-d86a-11e9-8fb5-c11a49da15e4.png"">. ---. Not too surprising, but SQL query rate also identical:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 15 13 PM"" src=""https://user-images.githubusercontent.com/1087943/64965218-946fd200-d86a-11e9-9db7-3149df61c0e0.png"">. ---. I thought it was interesting that the only variable that showed much change is bytes over the network. Theory:. MySQL pages on disk are buckets of row values. MySQL accesses the disk at the granularity of a page, it can't fetch just a single value. Therefore, fetching some data from a page (MySQL filtering) versus all data from a page (client-side filtering) does not make a difference in the number of pages read. This is supported by the graph. It would also appear that filtering in memory, whether on client or server, does not have much of a CPU cost at all either for Cromwell nor for MySQL, because we do not see MySQL doing any less work nor Cromwell doing any more. I think this is because once a set of rows is already in memory (after reading a page or receiving the rows over the wire) choosing specific ones is trivial. For MySQL, finding and loading the pages into memory is the hard part.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474:23,Testability,test,testing,23,"I conducted a bunch of testing on alpha and I have Results!. The test runs clocked in as follows:; - Client-side filtering (this PR, `fabece5`); - 1 hour, 1 minute, ; https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/470/; - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/471/; - Filtering in MySQL (original, `f666098`); - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/472/; - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/473/. The graphs below show the two client-side filtering runs, followed by the two MySQL filtering results. ---. Summarizer CPU is identical, inbound network bytes higher:. <img width=""1338"" alt=""Screen Shot 2019-09-13 at 8 13 00 PM"" src=""https://user-images.githubusercontent.com/1087943/64965213-93d73b80-d86a-11e9-9278-03b6f665c378.png"">. ---. Database page read/writes identical:. <img width=""1334"" alt=""Screen Shot 2019-09-13 at 8 13 39 PM"" src=""https://user-images.githubusercontent.com/1087943/64965215-93d73b80-d86a-11e9-9db5-4416f5e94719.png"">. ---. Database CPU identical:. <img width=""1332"" alt=""Screen Shot 2019-09-13 at 8 13 52 PM"" src=""https://user-images.githubusercontent.com/1087943/64965216-946fd200-d86a-11e9-9735-6d91a8b74a4d.png"">. ---. Database egress bytes modestly higher, consistent with higher inbound on summarizer:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 14 33 PM"" src=""https://user-images.githubusercontent.com/1087943/64965217-946fd200-d86a-11e9-8fb5-c11a49da15e4.png"">. ---. Not too surprising, but SQL query rate also identical:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 15 13 PM"" src=""https://user-images.githubusercontent.com/1087943/64965218-946fd200-d86a-11e9-9db7-3149df61c0e0.png"">. ---. I thought it was interesting that the only variable that showed much change is bytes over the network. Theory:. MySQL pages ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474:65,Testability,test,test,65,"I conducted a bunch of testing on alpha and I have Results!. The test runs clocked in as follows:; - Client-side filtering (this PR, `fabece5`); - 1 hour, 1 minute, ; https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/470/; - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/471/; - Filtering in MySQL (original, `f666098`); - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/472/; - 1 hour, 2 minutes, https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/473/. The graphs below show the two client-side filtering runs, followed by the two MySQL filtering results. ---. Summarizer CPU is identical, inbound network bytes higher:. <img width=""1338"" alt=""Screen Shot 2019-09-13 at 8 13 00 PM"" src=""https://user-images.githubusercontent.com/1087943/64965213-93d73b80-d86a-11e9-9278-03b6f665c378.png"">. ---. Database page read/writes identical:. <img width=""1334"" alt=""Screen Shot 2019-09-13 at 8 13 39 PM"" src=""https://user-images.githubusercontent.com/1087943/64965215-93d73b80-d86a-11e9-9db5-4416f5e94719.png"">. ---. Database CPU identical:. <img width=""1332"" alt=""Screen Shot 2019-09-13 at 8 13 52 PM"" src=""https://user-images.githubusercontent.com/1087943/64965216-946fd200-d86a-11e9-9735-6d91a8b74a4d.png"">. ---. Database egress bytes modestly higher, consistent with higher inbound on summarizer:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 14 33 PM"" src=""https://user-images.githubusercontent.com/1087943/64965217-946fd200-d86a-11e9-8fb5-c11a49da15e4.png"">. ---. Not too surprising, but SQL query rate also identical:. <img width=""1336"" alt=""Screen Shot 2019-09-13 at 8 15 13 PM"" src=""https://user-images.githubusercontent.com/1087943/64965218-946fd200-d86a-11e9-9db7-3149df61c0e0.png"">. ---. I thought it was interesting that the only variable that showed much change is bytes over the network. Theory:. MySQL pages ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-531803474
https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-532329425:17,Testability,test,testing,17,Awesome job perf testing! 🏆 Agree with your theory for the similarity of client / server side filtering results as well.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125#issuecomment-532329425
https://github.com/broadinstitute/cromwell/pull/5135#issuecomment-524451332:30,Usability,feedback,feedback,30,"Hi @cjllanwarne !; Thanks for feedback!; My main point is that `BigDecimal` supports operations with `Float` (without rounding) ""out of the box"", while `Long` and `BigInteger` does not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5135#issuecomment-524451332
https://github.com/broadinstitute/cromwell/pull/5135#issuecomment-525469779:177,Usability,simpl,simply,177,"Oh! I thought the discussion was about using the Java type for implementation, not proposing it to literally be part of WDL. My $0.02 w/ my OpenWDL hat on is that I'd prefer to simply have fairly generic descriptions of a integer and float type and let implementations choose what they think is best. But that's best held for a discussion over there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5135#issuecomment-525469779
https://github.com/broadinstitute/cromwell/pull/5141#issuecomment-524990707:181,Availability,failure,failure,181,@cjllanwarne I do think the existing test suite should validate this sufficiently apart from the issues raised in the separate Google Doc regarding retries and the probabilities of failure with transferring multiple files.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5141#issuecomment-524990707
https://github.com/broadinstitute/cromwell/pull/5141#issuecomment-524990707:55,Security,validat,validate,55,@cjllanwarne I do think the existing test suite should validate this sufficiently apart from the issues raised in the separate Google Doc regarding retries and the probabilities of failure with transferring multiple files.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5141#issuecomment-524990707
https://github.com/broadinstitute/cromwell/pull/5141#issuecomment-524990707:37,Testability,test,test,37,@cjllanwarne I do think the existing test suite should validate this sufficiently apart from the issues raised in the separate Google Doc regarding retries and the probabilities of failure with transferring multiple files.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5141#issuecomment-524990707
https://github.com/broadinstitute/cromwell/pull/5141#issuecomment-526653083:13,Testability,test,tests,13,LGTM pending tests passing,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5141#issuecomment-526653083
https://github.com/broadinstitute/cromwell/pull/5142#issuecomment-525772335:32,Testability,log,logic,32,Discovered a bug in the Jenkins logic. Fixed it in the latest commit by removing `docker` from Jenkins as it didn't work. Tested here: https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv1/465,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5142#issuecomment-525772335
https://github.com/broadinstitute/cromwell/pull/5142#issuecomment-525772335:122,Testability,Test,Tested,122,Discovered a bug in the Jenkins logic. Fixed it in the latest commit by removing `docker` from Jenkins as it didn't work. Tested here: https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv1/465,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5142#issuecomment-525772335
https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525135684:114,Testability,test,test,114,"@tmooney I'm no longer a Cromwell team member so take this with a grain of salt, but adding that CWL as a centaur test would likely be a great path - including if we can cajole the team into folding that into the GCP tests :P",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525135684
https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525135684:217,Testability,test,tests,217,"@tmooney I'm no longer a Cromwell team member so take this with a grain of salt, but adding that CWL as a centaur test would likely be a great path - including if we can cajole the team into folding that into the GCP tests :P",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525135684
https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525769630:85,Testability,test,test,85,"I agree with @geoffjentry - if you could write up one of your examples as a centaur* test, that'd be awesome to (a) prove that the change is working on all backends, and (b) stop us regressing and re-introducing this as a bug in the future. Let us know if you need any directions on how to get started. Thanks!. *: Centaur is one of our CI tools which runs workflows in various environments and backend and asserts that the results we get are what we expected",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525769630
https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525769630:407,Testability,assert,asserts,407,"I agree with @geoffjentry - if you could write up one of your examples as a centaur* test, that'd be awesome to (a) prove that the change is working on all backends, and (b) stop us regressing and re-introducing this as a bug in the future. Let us know if you need any directions on how to get started. Thanks!. *: Centaur is one of our CI tools which runs workflows in various environments and backend and asserts that the results we get are what we expected",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525769630
https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525769856:107,Testability,test,test-cases,107,Docs on getting started on Centaur: https://cromwell.readthedocs.io/en/stable/developers/Centaur/#defining-test-cases,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525769856
https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176:101,Availability,failure,failure,101,Thanks for the link to the centaur docs! I'll work on adding a testcase for this. Looks like that CI failure is a timeout error? Not sure what to make of that... but I'll see what happens after adding the new testcase.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176
https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176:122,Availability,error,error,122,Thanks for the link to the centaur docs! I'll work on adding a testcase for this. Looks like that CI failure is a timeout error? Not sure what to make of that... but I'll see what happens after adding the new testcase.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176
https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176:114,Safety,timeout,timeout,114,Thanks for the link to the centaur docs! I'll work on adding a testcase for this. Looks like that CI failure is a timeout error? Not sure what to make of that... but I'll see what happens after adding the new testcase.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176
https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176:63,Testability,test,testcase,63,Thanks for the link to the centaur docs! I'll work on adding a testcase for this. Looks like that CI failure is a timeout error? Not sure what to make of that... but I'll see what happens after adding the new testcase.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176
https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176:209,Testability,test,testcase,209,Thanks for the link to the centaur docs! I'll work on adding a testcase for this. Looks like that CI failure is a timeout error? Not sure what to make of that... but I'll see what happens after adding the new testcase.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176
https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525940334:170,Safety,timeout,timeout,170,A new centaur test has been added based on an existing one (`cwl_prefix_for_array`) that was from a previous example CWL of mine 😄 . Looks like whatever was causing that timeout didn't recur this go around.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525940334
https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525940334:14,Testability,test,test,14,A new centaur test has been added based on an existing one (`cwl_prefix_for_array`) that was from a previous example CWL of mine 😄 . Looks like whatever was causing that timeout didn't recur this go around.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525940334
https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-527484860:14,Testability,test,testing,14,"I'm currently testing this in #5153 - assuming that all passes I'll ~merge~ review this - thanks!. EDIT: oops, it looks like the PR still needs reviews, so I guess I won't merge it _just_ yet... 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-527484860
https://github.com/broadinstitute/cromwell/pull/5145#issuecomment-525825381:25,Deployability,patch,patch,25,Merging despite `codecov/patch` in this case. No tests necessary.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5145#issuecomment-525825381
https://github.com/broadinstitute/cromwell/pull/5145#issuecomment-525825381:49,Testability,test,tests,49,Merging despite `codecov/patch` in this case. No tests necessary.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5145#issuecomment-525825381
https://github.com/broadinstitute/cromwell/pull/5146#issuecomment-525871365:55,Performance,cache,cache,55,Closing for now. Will re-open once I sort out the call cache diff endpoint,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5146#issuecomment-525871365
https://github.com/broadinstitute/cromwell/pull/5149#issuecomment-526644592:359,Testability,test,tests,359,"@cjllanwarne @salonishah11 . From my understanding -- When we send a URI to martha, be it ""dos://"" or ""drs://"" -- I think it just returns a response json that has a key `dos` in it, and that's the `DosObject` in our codebase. The object is to encapsulate the response json and not the URI, and I believe the URi may not be returned at all but Saloni did some tests. Might be worth posting example responses here for education.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5149#issuecomment-526644592
https://github.com/broadinstitute/cromwell/pull/5149#issuecomment-526649237:450,Security,checksum,checksum,450,"@ruchim @cjllanwarne that's correct. We deserialize the response from Martha into [these](https://github.com/broadinstitute/cromwell/blob/26c5bbf007f81ab9604109ce46063b85e3ac0586/cloud-nio/cloud-nio-impl-drs/src/main/scala/cloud/nio/impl/drs/DrsPathResolver.scala#L81-L90) case classes. Currently, even though Martha accepts `drs://` uuids, the response we get back from Martha contains a key called `dos: {...}` within which lies our gs urls, size, checksum, etc. Hence the object can still be called `DosObject`. For example if you curl to Martha with a uuid `drs://path-here` it would respond back with a response os structure to ; ```; {; ""dos"": {; ""data_object"": {; ""id"": ""...."",; ""urls"": [; {; ""url"": ""https://url""; },; {; ""url"": ""gs://url""; }; ],; ""size"": ""123"",; ""checksums"": [; {; ""checksum"": ""123"",; ""type"": ""sha256""; }; ],; ""aliases"": [; ""some-alieas""; ],; ""version"": ""2019-07-04T104122.106166Z"",; ""name"": ""name-of-file""; }; },; ""googleServiceAccount"": {; ......; }; }; ```; where the metadata information lies in `dos` key.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5149#issuecomment-526649237
https://github.com/broadinstitute/cromwell/pull/5149#issuecomment-526649237:772,Security,checksum,checksums,772,"@ruchim @cjllanwarne that's correct. We deserialize the response from Martha into [these](https://github.com/broadinstitute/cromwell/blob/26c5bbf007f81ab9604109ce46063b85e3ac0586/cloud-nio/cloud-nio-impl-drs/src/main/scala/cloud/nio/impl/drs/DrsPathResolver.scala#L81-L90) case classes. Currently, even though Martha accepts `drs://` uuids, the response we get back from Martha contains a key called `dos: {...}` within which lies our gs urls, size, checksum, etc. Hence the object can still be called `DosObject`. For example if you curl to Martha with a uuid `drs://path-here` it would respond back with a response os structure to ; ```; {; ""dos"": {; ""data_object"": {; ""id"": ""...."",; ""urls"": [; {; ""url"": ""https://url""; },; {; ""url"": ""gs://url""; }; ],; ""size"": ""123"",; ""checksums"": [; {; ""checksum"": ""123"",; ""type"": ""sha256""; }; ],; ""aliases"": [; ""some-alieas""; ],; ""version"": ""2019-07-04T104122.106166Z"",; ""name"": ""name-of-file""; }; },; ""googleServiceAccount"": {; ......; }; }; ```; where the metadata information lies in `dos` key.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5149#issuecomment-526649237
https://github.com/broadinstitute/cromwell/pull/5149#issuecomment-526649237:791,Security,checksum,checksum,791,"@ruchim @cjllanwarne that's correct. We deserialize the response from Martha into [these](https://github.com/broadinstitute/cromwell/blob/26c5bbf007f81ab9604109ce46063b85e3ac0586/cloud-nio/cloud-nio-impl-drs/src/main/scala/cloud/nio/impl/drs/DrsPathResolver.scala#L81-L90) case classes. Currently, even though Martha accepts `drs://` uuids, the response we get back from Martha contains a key called `dos: {...}` within which lies our gs urls, size, checksum, etc. Hence the object can still be called `DosObject`. For example if you curl to Martha with a uuid `drs://path-here` it would respond back with a response os structure to ; ```; {; ""dos"": {; ""data_object"": {; ""id"": ""...."",; ""urls"": [; {; ""url"": ""https://url""; },; {; ""url"": ""gs://url""; }; ],; ""size"": ""123"",; ""checksums"": [; {; ""checksum"": ""123"",; ""type"": ""sha256""; }; ],; ""aliases"": [; ""some-alieas""; ],; ""version"": ""2019-07-04T104122.106166Z"",; ""name"": ""name-of-file""; }; },; ""googleServiceAccount"": {; ......; }; }; ```; where the metadata information lies in `dos` key.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5149#issuecomment-526649237
https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-526379078:4,Availability,failure,failure,4,"SBT failure is another `No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.` for those keeping track, the `SingleWorkflowRunnerActor` also failed previous to that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-526379078
https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529971535:30,Modifiability,refactor,refactor,30,Performance tests against the refactor are highlighted by the blue boxes:. ![Screen Shot 2019-09-09 at 5 33 44 PM](https://user-images.githubusercontent.com/13006282/64624289-6dbc2200-d3b8-11e9-8fc9-f9be83cd9f36.png),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529971535
https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529971535:0,Performance,Perform,Performance,0,Performance tests against the refactor are highlighted by the blue boxes:. ![Screen Shot 2019-09-09 at 5 33 44 PM](https://user-images.githubusercontent.com/13006282/64624289-6dbc2200-d3b8-11e9-8fc9-f9be83cd9f36.png),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529971535
https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529971535:12,Testability,test,tests,12,Performance tests against the refactor are highlighted by the blue boxes:. ![Screen Shot 2019-09-09 at 5 33 44 PM](https://user-images.githubusercontent.com/13006282/64624289-6dbc2200-d3b8-11e9-8fc9-f9be83cd9f36.png),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529971535
https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529975832:104,Availability,degraded,degraded,104,"what does the Y axis represent? Maximum requests/s? What is the takeaway? . It _looks_ like performance degraded slightly, no? So the question is whether to move ahead despite this? If that is the case I vote yes, let's move ahead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529975832
https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529975832:92,Performance,perform,performance,92,"what does the Y axis represent? Maximum requests/s? What is the takeaway? . It _looks_ like performance degraded slightly, no? So the question is whether to move ahead despite this? If that is the case I vote yes, let's move ahead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-529975832
https://github.com/broadinstitute/cromwell/pull/5151#issuecomment-527502667:437,Availability,avail,available,437,"Hi @mepowers, thanks for updating this for us. Two quick comments:. 1. It looks like this is now a match to the example in [cromwell.example.backends/slurm.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/slurm.conf) but if you could double check that they really match, that'd be very helpful.; 2. I believe that `mem-per-cpu` is a slurm-instance-specific option (ie it's not necessarily globally available)? If so, I wonder if there's any way to indicate that alongside the examples?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5151#issuecomment-527502667
https://github.com/broadinstitute/cromwell/pull/5154#issuecomment-528087750:0,Testability,Test,Tests,0,Tests passed in https://github.com/broadinstitute/cromwell/pull/5154,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5154#issuecomment-528087750
https://github.com/broadinstitute/cromwell/pull/5161#issuecomment-529006581:36,Availability,reliab,reliability,36,This certainly hasn't improved test reliability and I have more pressing things to look at just now,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5161#issuecomment-529006581
https://github.com/broadinstitute/cromwell/pull/5161#issuecomment-529006581:31,Testability,test,test,31,This certainly hasn't improved test reliability and I have more pressing things to look at just now,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5161#issuecomment-529006581
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540112608:26,Deployability,update,update,26,I got the same issue when update to version 46. Any idea? Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540112608
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:79,Availability,ERROR,ERROR,79,"Logs:; 2019-10-09 14:05:52,263 cromwell-system-akka.actor.default-dispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:87,Availability,Failure,Failure,87,"Logs:; 2019-10-09 14:05:52,263 cromwell-system-akka.actor.default-dispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:146,Deployability,update,updates,146,"Logs:; 2019-10-09 14:05:52,263 cromwell-system-akka.actor.default-dispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:413,Deployability,pipeline,pipeline,413,"Logs:; 2019-10-09 14:05:52,263 cromwell-system-akka.actor.default-dispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:546,Deployability,pipeline,pipeline,546,"Logs:; 2019-10-09 14:05:52,263 cromwell-system-akka.actor.default-dispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:673,Deployability,pipeline,pipeline,673,"Logs:; 2019-10-09 14:05:52,263 cromwell-system-akka.actor.default-dispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:793,Deployability,pipeline,pipeline,793,"Logs:; 2019-10-09 14:05:52,263 cromwell-system-akka.actor.default-dispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:913,Deployability,pipeline,pipeline,913,"Logs:; 2019-10-09 14:05:52,263 cromwell-system-akka.actor.default-dispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1063,Deployability,pipeline,pipeline,1063,"ispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1213,Deployability,pipeline,pipeline,1213,"xception: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1363,Deployability,pipeline,pipeline,1363,	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1503,Deployability,pipeline,pipeline,1503,tware.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTracki,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1643,Deployability,pipeline,pipeline,1643,awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutT,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1770,Deployability,pipeline,pipeline,1770,core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCal,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1894,Deployability,pipeline,pipeline,1894,.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.R,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:2004,Deployability,pipeline,pipeline,2004,va:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.ama,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:2114,Deployability,pipeline,pipeline,2114,ute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPip,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:2472,Deployability,pipeline,pipeline,2472,.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:2617,Deployability,pipeline,pipeline,2617,dk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awss,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:2753,Deployability,pipeline,pipeline,2753,nternal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseS,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:2889,Deployability,pipeline,pipeline,2889,.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(B,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:3039,Deployability,pipeline,pipeline,3039,RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:73); 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:3189,Deployability,pipeline,pipeline,3189,pelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:73); 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); 	at software.amazon.awssdk.services.batch.DefaultBat,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:3349,Deployability,pipeline,pipeline,3349,.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:73); 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); 	at software.amazon.awssdk.services.batch.DefaultBatchClient.listJobs(DefaultBatchClient.java:675); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.findJobsInStatus$1(OccasionalStatusPollingActor.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:4428,Deployability,update,updateStatuses,4428,gStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:73); 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); 	at software.amazon.awssdk.services.batch.DefaultBatchClient.listJobs(DefaultBatchClient.java:675); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.findJobsInStatus$1(OccasionalStatusPollingActor.scala:88); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$6(OccasionalStatusPollingActor.scala:105); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.immutable.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.s,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:4907,Deployability,update,updateStatuses,4907,	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); 	at software.amazon.awssdk.services.batch.DefaultBatchClient.listJobs(DefaultBatchClient.java:675); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.findJobsInStatus$1(OccasionalStatusPollingActor.scala:88); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$6(OccasionalStatusPollingActor.scala:105); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.immutable.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingAct,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:5400,Deployability,update,updateStatuses,5400,.$anonfun$updateStatuses$6(OccasionalStatusPollingActor.scala:105); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.immutable.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfu,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:5635,Deployability,update,updateForStatusNames,5635,leLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.immutable.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$an,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:5814,Deployability,update,updateStatuses,5814,.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockConte,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:6097,Performance,concurren,concurrent,6097,cala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.sca,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:6259,Performance,concurren,concurrent,6259,cala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:6321,Performance,concurren,concurrent,6321,108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:6388,Performance,concurren,concurrent,6388,nonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(Fo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:6462,Performance,concurren,concurrent,6462,ionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:6787,Performance,concurren,concurrent,6787,ionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1379,Safety,Timeout,TimeoutExceptionHandlingStage,1379,core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execut,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1417,Safety,Timeout,TimeoutExceptionHandlingStage,1417,.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1519,Safety,Timeout,TimeoutExceptionHandlingStage,1519,ternal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(A,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1557,Safety,Timeout,TimeoutExceptionHandlingStage,1557,ResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:231,Security,secur,security,231,"Logs:; 2019-10-09 14:05:52,263 cromwell-system-akka.actor.default-dispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:0,Testability,Log,Logs,0,"Logs:; 2019-10-09 14:05:52,263 cromwell-system-akka.actor.default-dispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429:49,Availability,error,error,49,"Any progress on this issue? I'm getting the same error running a multi-sample workflow using Cromwell v47 in server mode using AWS batch. Interestingly, the multi-sample workflow is just a scatter wrapped around a single-sample sub workflow that runs fine when run by itself. Perhaps this has something to do with nested workflows? Also, once the Cromwell server reports this error, it basically just gets stuck logging the same error over and over and log files become massive. I'll try with v44 to see if the issue is indeed something introduced in v45 as stated above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429:376,Availability,error,error,376,"Any progress on this issue? I'm getting the same error running a multi-sample workflow using Cromwell v47 in server mode using AWS batch. Interestingly, the multi-sample workflow is just a scatter wrapped around a single-sample sub workflow that runs fine when run by itself. Perhaps this has something to do with nested workflows? Also, once the Cromwell server reports this error, it basically just gets stuck logging the same error over and over and log files become massive. I'll try with v44 to see if the issue is indeed something introduced in v45 as stated above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429:429,Availability,error,error,429,"Any progress on this issue? I'm getting the same error running a multi-sample workflow using Cromwell v47 in server mode using AWS batch. Interestingly, the multi-sample workflow is just a scatter wrapped around a single-sample sub workflow that runs fine when run by itself. Perhaps this has something to do with nested workflows? Also, once the Cromwell server reports this error, it basically just gets stuck logging the same error over and over and log files become massive. I'll try with v44 to see if the issue is indeed something introduced in v45 as stated above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429:197,Integrability,wrap,wrapped,197,"Any progress on this issue? I'm getting the same error running a multi-sample workflow using Cromwell v47 in server mode using AWS batch. Interestingly, the multi-sample workflow is just a scatter wrapped around a single-sample sub workflow that runs fine when run by itself. Perhaps this has something to do with nested workflows? Also, once the Cromwell server reports this error, it basically just gets stuck logging the same error over and over and log files become massive. I'll try with v44 to see if the issue is indeed something introduced in v45 as stated above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429:412,Testability,log,logging,412,"Any progress on this issue? I'm getting the same error running a multi-sample workflow using Cromwell v47 in server mode using AWS batch. Interestingly, the multi-sample workflow is just a scatter wrapped around a single-sample sub workflow that runs fine when run by itself. Perhaps this has something to do with nested workflows? Also, once the Cromwell server reports this error, it basically just gets stuck logging the same error over and over and log files become massive. I'll try with v44 to see if the issue is indeed something introduced in v45 as stated above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429:453,Testability,log,log,453,"Any progress on this issue? I'm getting the same error running a multi-sample workflow using Cromwell v47 in server mode using AWS batch. Interestingly, the multi-sample workflow is just a scatter wrapped around a single-sample sub workflow that runs fine when run by itself. Perhaps this has something to do with nested workflows? Also, once the Cromwell server reports this error, it basically just gets stuck logging the same error over and over and log files become massive. I'll try with v44 to see if the issue is indeed something introduced in v45 as stated above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572125429
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:120,Availability,ERROR,ERROR,120,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:128,Availability,Failure,Failure,128,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:187,Deployability,update,updates,187,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:454,Deployability,pipeline,pipeline,454,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:587,Deployability,pipeline,pipeline,587,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:714,Deployability,pipeline,pipeline,714,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:834,Deployability,pipeline,pipeline,834,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:954,Deployability,pipeline,pipeline,954,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:1104,Deployability,pipeline,pipeline,1104,"spatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:1254,Deployability,pipeline,pipeline,1254,"xception: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:1404,Deployability,pipeline,pipeline,1404,	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:1544,Deployability,pipeline,pipeline,1544,tware.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTracki,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:1684,Deployability,pipeline,pipeline,1684,awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutT,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:1811,Deployability,pipeline,pipeline,1811,core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCal,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:1935,Deployability,pipeline,pipeline,1935,.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.R,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:2045,Deployability,pipeline,pipeline,2045,va:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.ama,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:2155,Deployability,pipeline,pipeline,2155,ute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPip,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:2513,Deployability,pipeline,pipeline,2513,.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:2658,Deployability,pipeline,pipeline,2658,dk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awss,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:2794,Deployability,pipeline,pipeline,2794,nternal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseS,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:2930,Deployability,pipeline,pipeline,2930,.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(B,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:3080,Deployability,pipeline,pipeline,3080,RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:73); 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:3230,Deployability,pipeline,pipeline,3230,pelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:73); 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); 	at software.amazon.awssdk.services.batch.DefaultBat,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:3390,Deployability,pipeline,pipeline,3390,.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:73); 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); 	at software.amazon.awssdk.services.batch.DefaultBatchClient.listJobs(DefaultBatchClient.java:675); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.findJobsInStatus$1(OccasionalStatusPollingActor.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:4469,Deployability,update,updateStatuses,4469,gStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:73); 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); 	at software.amazon.awssdk.services.batch.DefaultBatchClient.listJobs(DefaultBatchClient.java:675); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.findJobsInStatus$1(OccasionalStatusPollingActor.scala:88); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$6(OccasionalStatusPollingActor.scala:105); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.immutable.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.s,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:4948,Deployability,update,updateStatuses,4948,	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); 	at software.amazon.awssdk.services.batch.DefaultBatchClient.listJobs(DefaultBatchClient.java:675); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.findJobsInStatus$1(OccasionalStatusPollingActor.scala:88); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$6(OccasionalStatusPollingActor.scala:105); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.immutable.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingAct,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:5441,Deployability,update,updateStatuses,5441,.$anonfun$updateStatuses$6(OccasionalStatusPollingActor.scala:105); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.immutable.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfu,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:5676,Deployability,update,updateForStatusNames,5676,leLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.immutable.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$an,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:5855,Deployability,update,updateStatuses,5855,.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockConte,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:4,Integrability,message,message,4,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:6138,Performance,concurren,concurrent,6138,cala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.sca,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:6300,Performance,concurren,concurrent,6300,cala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:6362,Performance,concurren,concurrent,6362,108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:6429,Performance,concurren,concurrent,6429,nonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(Fo,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:6503,Performance,concurren,concurrent,6503,ionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:6828,Performance,concurren,concurrent,6828,ionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:1420,Safety,Timeout,TimeoutExceptionHandlingStage,1420,core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execut,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:1458,Safety,Timeout,TimeoutExceptionHandlingStage,1458,.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:1560,Safety,Timeout,TimeoutExceptionHandlingStage,1560,ternal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(A,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:1598,Safety,Timeout,TimeoutExceptionHandlingStage,1598,ResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:272,Security,secur,security,272,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:0,Testability,Log,Log,0,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033
https://github.com/broadinstitute/cromwell/pull/5163#issuecomment-529031738:63,Testability,test,test,63,Not 💯 the `drs` field renaming is correct so hopefully there's test coverage on that? 🙏,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5163#issuecomment-529031738
https://github.com/broadinstitute/cromwell/pull/5163#issuecomment-530457793:24,Testability,test,test,24,Good news is we do have test coverage on JSON parsing. [This](https://github.com/broadinstitute/cromwell/compare/rm_dos2drs_dev...mlc_drs_equis?expand=1#diff-a7db4dd7bcbffbd05de73c3c777382ceR12) allows for [renaming](https://github.com/broadinstitute/cromwell/compare/rm_dos2drs_dev...mlc_drs_equis?expand=1#diff-a7db4dd7bcbffbd05de73c3c777382ceR23) the case class field to `drs`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5163#issuecomment-530457793
https://github.com/broadinstitute/cromwell/issues/5166#issuecomment-539587892:87,Testability,test,tested,87,>I'm pretty sure you just need to delete the quotation marks that are surrounding 4. I tested it before reporting the issue and it did not work both with and without quotes,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5166#issuecomment-539587892
https://github.com/broadinstitute/cromwell/pull/5168#issuecomment-529671972:4,Availability,failure,failure,4,The failure looks like a random TravisCI glitch.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5168#issuecomment-529671972
https://github.com/broadinstitute/cromwell/pull/5169#issuecomment-530511479:8,Testability,test,tested,8,Jenkins tested: https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv1/481/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5169#issuecomment-530511479
https://github.com/broadinstitute/cromwell/pull/5171#issuecomment-533183374:19,Modifiability,refactor,refactored,19,"@danbills @kshakir refactored to reflect the new shape of https://www.lucidchart.com/invitations/accept/495747cc-4eeb-4a49-97c2-5545d2411a93. In brief:. * The decider is outside of the HMSA itself, to preserve responsiveness while deciding ""where do I send this read request""; * There is a new regulator layer between the HMSA and the read decider (so that we only choose once per identical read request)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5171#issuecomment-533183374
https://github.com/broadinstitute/cromwell/pull/5171#issuecomment-533183374:210,Usability,responsiv,responsiveness,210,"@danbills @kshakir refactored to reflect the new shape of https://www.lucidchart.com/invitations/accept/495747cc-4eeb-4a49-97c2-5545d2411a93. In brief:. * The decider is outside of the HMSA itself, to preserve responsiveness while deciding ""where do I send this read request""; * There is a new regulator layer between the HMSA and the read decider (so that we only choose once per identical read request)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5171#issuecomment-533183374
https://github.com/broadinstitute/cromwell/issues/5174#issuecomment-530831145:26,Performance,cache,cache-results,26,"Won't the `invalidate-bad-cache-results` setting work for you? It's not a timeout, it just tells Cromwell to gracefully handle missing files when attempting to retrieve from the cache. Seems to work pretty well in my hands (we have a similar situation here so I actually wrote a test for this).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5174#issuecomment-530831145
https://github.com/broadinstitute/cromwell/issues/5174#issuecomment-530831145:178,Performance,cache,cache,178,"Won't the `invalidate-bad-cache-results` setting work for you? It's not a timeout, it just tells Cromwell to gracefully handle missing files when attempting to retrieve from the cache. Seems to work pretty well in my hands (we have a similar situation here so I actually wrote a test for this).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5174#issuecomment-530831145
https://github.com/broadinstitute/cromwell/issues/5174#issuecomment-530831145:74,Safety,timeout,timeout,74,"Won't the `invalidate-bad-cache-results` setting work for you? It's not a timeout, it just tells Cromwell to gracefully handle missing files when attempting to retrieve from the cache. Seems to work pretty well in my hands (we have a similar situation here so I actually wrote a test for this).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5174#issuecomment-530831145
https://github.com/broadinstitute/cromwell/issues/5174#issuecomment-530831145:279,Testability,test,test,279,"Won't the `invalidate-bad-cache-results` setting work for you? It's not a timeout, it just tells Cromwell to gracefully handle missing files when attempting to retrieve from the cache. Seems to work pretty well in my hands (we have a similar situation here so I actually wrote a test for this).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5174#issuecomment-530831145
https://github.com/broadinstitute/cromwell/pull/5176#issuecomment-531269072:67,Testability,test,tests,67,This version verifies that the changes still work on 11.x. New 9.x tests are run in the develop PR: #5175,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5176#issuecomment-531269072
https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-541952762:190,Modifiability,config,config,190,"Hi @nh13, not from Broad but have you tried turning the [docker-digest lookup off](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#docker-digests) with the following in your config:. ```; docker.hash-lookup.enabled = false; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-541952762
https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-541952762:211,Security,hash,hash-lookup,211,"Hi @nh13, not from Broad but have you tried turning the [docker-digest lookup off](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#docker-digests) with the following in your config:. ```; docker.hash-lookup.enabled = false; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-541952762
https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-1250348746:68,Security,hash,hash-lookup,68,"I'm having the same issue, is there any solution other than `docker.hash-lookup.enabled = false` because it may cause problems for call caching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-1250348746
https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-1262637378:52,Security,hash,hash,52,"I think call caching can't work on images without a hash/digest anyway, since the hash is taken into account to evaluable caching eligibility.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-1262637378
https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-1262637378:82,Security,hash,hash,82,"I think call caching can't work on images without a hash/digest anyway, since the hash is taken into account to evaluable caching eligibility.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178#issuecomment-1262637378
https://github.com/broadinstitute/cromwell/pull/5180#issuecomment-532245618:55,Modifiability,config,config,55,"@kshakir I had thought of making the multiplier 2 as a config option. But since this ticket had evolved to be a PoC, I kept it constant at 2. If we decide to not rush this PR, than I am all in for 'MoreMemory' as compared to 'DoubleMemory'.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5180#issuecomment-532245618
https://github.com/broadinstitute/cromwell/pull/5180#issuecomment-532245618:96,Modifiability,evolve,evolved,96,"@kshakir I had thought of making the multiplier 2 as a config option. But since this ticket had evolved to be a PoC, I kept it constant at 2. If we decide to not rush this PR, than I am all in for 'MoreMemory' as compared to 'DoubleMemory'.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5180#issuecomment-532245618
https://github.com/broadinstitute/cromwell/pull/5180#issuecomment-533261206:63,Modifiability,config,configurable,63,@kshakir @mcovarr I have changed the `multiplier` factor to be configurable and made things more generic instead of `...doubleMemory...`. It's now ready for re-review!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5180#issuecomment-533261206
https://github.com/broadinstitute/cromwell/pull/5181#issuecomment-533318387:275,Testability,test,test,275,One immediate thing I noticed before reviewing the whole thing - you might not need separate `parallel_composite_uploads_off.wdl` and `parallel_composite_uploads_on.wdl` files (if they are as identical as they look) - you can just reference the same file from two separate `.test` definitions,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5181#issuecomment-533318387
https://github.com/broadinstitute/cromwell/issues/5182#issuecomment-533318396:98,Deployability,update,updates,98,Jira issue: https://broadworkbench.atlassian.net/browse/BA-6006. Please reference that ticket for updates! Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182#issuecomment-533318396
https://github.com/broadinstitute/cromwell/pull/5183#issuecomment-534122855:146,Testability,test,tests,146,"@myazinn our current course of action is to revert your PR (in this PR) but you are encouraged to submit a revised version of #5104, we even have tests for the functionality that regressed so it will not be possible to break the same thing again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5183#issuecomment-534122855
https://github.com/broadinstitute/cromwell/pull/5183#issuecomment-542219663:145,Testability,test,tests,145,"That sounds great! It is a desirable feature and you should not feel discouraged. In particular, the sensitive functionality is now protected by tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5183#issuecomment-542219663
https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-534574382:273,Energy Efficiency,schedul,scheduler,273,"> Looks good to me, once @cjllanwarne's comments are addressed. Thank you!; > Out of curiosity, what kind of cluster/tooling/data processing work are you using at your site?. Actually, we're trying to run gene sequencing tasks using cromwell, meanwhile adopting Volcano as scheduler for our kubernetes clusters.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-534574382
https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-851089634:171,Modifiability,config,config,171,"@jiangkaihua I want to knwo how does cromwell generate a yaml file for volcano?; Cromwell now genrates a bash script file when `actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""`, while vcctl job run just accept a yaml file.; Do I need to implement a volcano backend?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-851089634
https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-851089634:178,Modifiability,Config,ConfigBackendLifecycleActorFactory,178,"@jiangkaihua I want to knwo how does cromwell generate a yaml file for volcano?; Cromwell now genrates a bash script file when `actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""`, while vcctl job run just accept a yaml file.; Do I need to implement a volcano backend?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-851089634
https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-927790031:175,Modifiability,config,config,175,"> @jiangkaihua I want to knwo how does cromwell generate a yaml file for volcano?; > Cromwell now genrates a bash script file when `actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""`, while vcctl job run just accept a yaml file.; > Do I need to implement a volcano backend?. Did you end up figuring this out? Has somebody implemented a Volcano backend?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-927790031
https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-927790031:182,Modifiability,Config,ConfigBackendLifecycleActorFactory,182,"> @jiangkaihua I want to knwo how does cromwell generate a yaml file for volcano?; > Cromwell now genrates a bash script file when `actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""`, while vcctl job run just accept a yaml file.; > Do I need to implement a volcano backend?. Did you end up figuring this out? Has somebody implemented a Volcano backend?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-927790031
https://github.com/broadinstitute/cromwell/pull/5196#issuecomment-535638146:3,Testability,test,testing,3,Am testing these manually on our `fc-jenkins` system,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5196#issuecomment-535638146
https://github.com/broadinstitute/cromwell/pull/5196#issuecomment-536575194:7,Testability,test,tests,7,manual tests succeded:; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-run-centaur/49/console; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-run-centaur/48/console,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5196#issuecomment-536575194
https://github.com/broadinstitute/cromwell/pull/5203#issuecomment-537214275:55,Integrability,message,messages,55,Those dropped bits are where I stick my steganographic messages to people,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5203#issuecomment-537214275
https://github.com/broadinstitute/cromwell/pull/5203#issuecomment-537985405:108,Deployability,update,update,108,"So two things are happening here:; * We can't create a new WDL type (without modifying the spec) but we can update the backend implementation in Cromwell to widen the supported values. My comment in the previous PR was making that case.; * We can only widen the supported values as far as the WDL spec allows.; * As of WDL draft-2 and 1.0, the supported width is unspecified; * As of WDL 2.0 it's defined thus: ; * `Float` is a finite 64-bit IEEE-754 floating point number.; * `Int` can be used to hold a signed Integer in the range `[-2^63, 2^63]`. . So I think the best thing we could do is support the data types specified in WDL 2.0. Even though we'd be technically compliant with WDL 1.0 by going wider, I think it's just going to cause us headaches later. I'd suggest starting by defining what we will and won't support by adding these test cases and trying to satisfy them:; * Support for the max value of a finite 64-bit IEEE-754 floating point number; * Disallowing the max value of a finite 64-bit IEEE-754 floating point number + 1; * Support for the min value of a finite 64-bit IEEE-754 floating point number; * Disallowing the min value of a finite 64-bit IEEE-754 floating point number - 1; * Support for the max value of a signed Integer in the range `[-2^63, 2^63]`; * Disallowing the max value of a signed Integer in the range `[-2^63, 2^63]` + 1; * Support for the min value of a signed Integer in the range `[-2^63, 2^63]`; * Disallowing the min value of a signed Integer in the range `[-2^63, 2^63]` - 1. By the way - I think if we do achieve this, there's a PR that's been sitting in the WDL spec for a while that could then merge as ""implemented"" so it'd be really cool to be able to achieve that!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5203#issuecomment-537985405
https://github.com/broadinstitute/cromwell/pull/5203#issuecomment-537985405:842,Testability,test,test,842,"So two things are happening here:; * We can't create a new WDL type (without modifying the spec) but we can update the backend implementation in Cromwell to widen the supported values. My comment in the previous PR was making that case.; * We can only widen the supported values as far as the WDL spec allows.; * As of WDL draft-2 and 1.0, the supported width is unspecified; * As of WDL 2.0 it's defined thus: ; * `Float` is a finite 64-bit IEEE-754 floating point number.; * `Int` can be used to hold a signed Integer in the range `[-2^63, 2^63]`. . So I think the best thing we could do is support the data types specified in WDL 2.0. Even though we'd be technically compliant with WDL 1.0 by going wider, I think it's just going to cause us headaches later. I'd suggest starting by defining what we will and won't support by adding these test cases and trying to satisfy them:; * Support for the max value of a finite 64-bit IEEE-754 floating point number; * Disallowing the max value of a finite 64-bit IEEE-754 floating point number + 1; * Support for the min value of a finite 64-bit IEEE-754 floating point number; * Disallowing the min value of a finite 64-bit IEEE-754 floating point number - 1; * Support for the max value of a signed Integer in the range `[-2^63, 2^63]`; * Disallowing the max value of a signed Integer in the range `[-2^63, 2^63]` + 1; * Support for the min value of a signed Integer in the range `[-2^63, 2^63]`; * Disallowing the min value of a signed Integer in the range `[-2^63, 2^63]` - 1. By the way - I think if we do achieve this, there's a PR that's been sitting in the WDL spec for a while that could then merge as ""implemented"" so it'd be really cool to be able to achieve that!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5203#issuecomment-537985405
https://github.com/broadinstitute/cromwell/pull/5203#issuecomment-543962574:80,Deployability,update,update,80,"Hi @SergeySdv, we have marked your PR as ""back with originator"" so that you can update it based on the discussion. Once it is ready for review just leave a comment and we will take a look.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5203#issuecomment-543962574
https://github.com/broadinstitute/cromwell/pull/5206#issuecomment-539718371:6,Testability,test,test,6,"Alpha test looks good, plan is using `METADATA_WORKFLOW_IDX` on `METADATA_ENTRY`. Actually deleting rows (on an alpha clone) completes in 100-200ms for small workflows. This is an `EXPLAIN` in IntelliJ, exported as an HTML table – apparently you can just paste HTML into Github and it works! . <!DOCTYPE html>; <html>; <body>; <table border=""1"" style=""border-collapse:collapse"">; <tr><th>id</th><th>select_type</th><th>table</th><th>type</th><th>possible_keys</th><th>key</th><th>key_len</th><th>ref</th><th>rows</th><th>Extra</th></tr>; <tr><td>1</td><td>SIMPLE</td><td>WORKFLOW_METADATA_SUMMARY_ENTRY</td><td>index_merge</td><td>UC_WORKFLOW_METADATA_SUMMARY_ENTRY_WEU,IX_WORKFLOW_METADATA_SUMMARY_ENTRY_RWEU</td><td>IX_WORKFLOW_METADATA_SUMMARY_ENTRY_RWEU,UC_WORKFLOW_METADATA_SUMMARY_ENTRY_WEU</td><td>303,302</td><td>NULL</td><td>2</td><td>Using union(IX_WORKFLOW_METADATA_SUMMARY_ENTRY_RWEU,UC_WORKFLOW_METADATA_SUMMARY_ENTRY_WEU); Using where</td></tr>; <tr><td>1</td><td>SIMPLE</td><td>METADATA_ENTRY</td><td>ref</td><td>METADATA_WORKFLOW_IDX</td><td>METADATA_WORKFLOW_IDX</td><td>767</td><td>cromwell.WORKFLOW_METADATA_SUMMARY_ENTRY.WORKFLOW_EXECUTION_UUID</td><td>73</td><td>Using where</td></tr></table>; </body>; </html>",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5206#issuecomment-539718371
https://github.com/broadinstitute/cromwell/pull/5206#issuecomment-539718371:556,Usability,SIMPL,SIMPLE,556,"Alpha test looks good, plan is using `METADATA_WORKFLOW_IDX` on `METADATA_ENTRY`. Actually deleting rows (on an alpha clone) completes in 100-200ms for small workflows. This is an `EXPLAIN` in IntelliJ, exported as an HTML table – apparently you can just paste HTML into Github and it works! . <!DOCTYPE html>; <html>; <body>; <table border=""1"" style=""border-collapse:collapse"">; <tr><th>id</th><th>select_type</th><th>table</th><th>type</th><th>possible_keys</th><th>key</th><th>key_len</th><th>ref</th><th>rows</th><th>Extra</th></tr>; <tr><td>1</td><td>SIMPLE</td><td>WORKFLOW_METADATA_SUMMARY_ENTRY</td><td>index_merge</td><td>UC_WORKFLOW_METADATA_SUMMARY_ENTRY_WEU,IX_WORKFLOW_METADATA_SUMMARY_ENTRY_RWEU</td><td>IX_WORKFLOW_METADATA_SUMMARY_ENTRY_RWEU,UC_WORKFLOW_METADATA_SUMMARY_ENTRY_WEU</td><td>303,302</td><td>NULL</td><td>2</td><td>Using union(IX_WORKFLOW_METADATA_SUMMARY_ENTRY_RWEU,UC_WORKFLOW_METADATA_SUMMARY_ENTRY_WEU); Using where</td></tr>; <tr><td>1</td><td>SIMPLE</td><td>METADATA_ENTRY</td><td>ref</td><td>METADATA_WORKFLOW_IDX</td><td>METADATA_WORKFLOW_IDX</td><td>767</td><td>cromwell.WORKFLOW_METADATA_SUMMARY_ENTRY.WORKFLOW_EXECUTION_UUID</td><td>73</td><td>Using where</td></tr></table>; </body>; </html>",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5206#issuecomment-539718371
https://github.com/broadinstitute/cromwell/pull/5206#issuecomment-539718371:978,Usability,SIMPL,SIMPLE,978,"Alpha test looks good, plan is using `METADATA_WORKFLOW_IDX` on `METADATA_ENTRY`. Actually deleting rows (on an alpha clone) completes in 100-200ms for small workflows. This is an `EXPLAIN` in IntelliJ, exported as an HTML table – apparently you can just paste HTML into Github and it works! . <!DOCTYPE html>; <html>; <body>; <table border=""1"" style=""border-collapse:collapse"">; <tr><th>id</th><th>select_type</th><th>table</th><th>type</th><th>possible_keys</th><th>key</th><th>key_len</th><th>ref</th><th>rows</th><th>Extra</th></tr>; <tr><td>1</td><td>SIMPLE</td><td>WORKFLOW_METADATA_SUMMARY_ENTRY</td><td>index_merge</td><td>UC_WORKFLOW_METADATA_SUMMARY_ENTRY_WEU,IX_WORKFLOW_METADATA_SUMMARY_ENTRY_RWEU</td><td>IX_WORKFLOW_METADATA_SUMMARY_ENTRY_RWEU,UC_WORKFLOW_METADATA_SUMMARY_ENTRY_WEU</td><td>303,302</td><td>NULL</td><td>2</td><td>Using union(IX_WORKFLOW_METADATA_SUMMARY_ENTRY_RWEU,UC_WORKFLOW_METADATA_SUMMARY_ENTRY_WEU); Using where</td></tr>; <tr><td>1</td><td>SIMPLE</td><td>METADATA_ENTRY</td><td>ref</td><td>METADATA_WORKFLOW_IDX</td><td>METADATA_WORKFLOW_IDX</td><td>767</td><td>cromwell.WORKFLOW_METADATA_SUMMARY_ENTRY.WORKFLOW_EXECUTION_UUID</td><td>73</td><td>Using where</td></tr></table>; </body>; </html>",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5206#issuecomment-539718371
https://github.com/broadinstitute/cromwell/pull/5211#issuecomment-540820505:100,Energy Efficiency,green,green,100,"Applying the ""do not merge"" label for now, just in case turning off the awkward tests makes this go green... 🤔",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5211#issuecomment-540820505
https://github.com/broadinstitute/cromwell/pull/5211#issuecomment-540820505:80,Testability,test,tests,80,"Applying the ""do not merge"" label for now, just in case turning off the awkward tests makes this go green... 🤔",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5211#issuecomment-540820505
https://github.com/broadinstitute/cromwell/pull/5211#issuecomment-541103425:102,Availability,down,down,102,"Theory: something in the new SBT is meaning that some background thread/process is no longer shutting down the same way as before. This means that:; * (a) we get a bunch of resource leaks when `-Dsbt.classloader.close=false` is not set in the SBT options; * (b) when `-Dsbt.classloader.close=false` *is* set in sbt options, the thread remains running in the background preventing the tests from exiting on completion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5211#issuecomment-541103425
https://github.com/broadinstitute/cromwell/pull/5211#issuecomment-541103425:384,Testability,test,tests,384,"Theory: something in the new SBT is meaning that some background thread/process is no longer shutting down the same way as before. This means that:; * (a) we get a bunch of resource leaks when `-Dsbt.classloader.close=false` is not set in the SBT options; * (b) when `-Dsbt.classloader.close=false` *is* set in sbt options, the thread remains running in the background preventing the tests from exiting on completion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5211#issuecomment-541103425
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-541121668:158,Testability,test,tests,158,"Hi Ben, thank you for the contribution! . Could you describe what behaviors are fixed with these changes? Our CI for the AWS backend intentionally skips some tests due to missing functionality; I'm wondering if we might now be able to un-skip any of these tests (plus confirm correctness / guard against regressions).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-541121668
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-541121668:256,Testability,test,tests,256,"Hi Ben, thank you for the contribution! . Could you describe what behaviors are fixed with these changes? Our CI for the AWS backend intentionally skips some tests due to missing functionality; I'm wondering if we might now be able to un-skip any of these tests (plus confirm correctness / guard against regressions).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-541121668
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-541138726:67,Testability,test,tests,67,Running CI with no AWS exclusions to see if any currently excluded tests pass with these changes. 🙂 https://travis-ci.com/broadinstitute/cromwell/builds/131574026,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-541138726
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-541321720:42,Testability,test,tests,42,"It doesn't look like any existing Centaur tests pass on AWS as a result of these changes. I'll revisit this again next week to see if that's expected, and if so what tests we might be able to add to exercise these changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-541321720
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-541321720:166,Testability,test,tests,166,"It doesn't look like any existing Centaur tests pass on AWS as a result of these changes. I'll revisit this again next week to see if that's expected, and if so what tests we might be able to add to exercise these changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-541321720
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-555551772:52,Testability,test,tests,52,"I asked because my colleague @mcovarr reported that tests were not passing; that was a month ago though, so created https://github.com/broadinstitute/cromwell/pull/5291 to give them a new run",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-555551772
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170:799,Deployability,integrat,integration,799,"Hi @aednichols , ; Sorry for the long delay in response. This PR definitely fixes an issue that we found running Cromwell in AWS (see my description [above](https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-543478047)). . I don't know if this scenario is covered in the existing tests. I had a quick look at the CI build and it looks to me as if the AWS CI Job is failing due to a job timeout, rather than actual tests failing. The last line in the log is:; > The job exceeded the maximum time limit for jobs, and has been terminated. The other successful jobs have the test results summary and an overall success message. Can you adjust the CI settings & run it again to see what the results are?. If this scenario is not actually covered by the existing tests, do I need to add an integration test in order to have the PR merged?. thanks; Ben",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170:630,Integrability,message,message,630,"Hi @aednichols , ; Sorry for the long delay in response. This PR definitely fixes an issue that we found running Cromwell in AWS (see my description [above](https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-543478047)). . I don't know if this scenario is covered in the existing tests. I had a quick look at the CI build and it looks to me as if the AWS CI Job is failing due to a job timeout, rather than actual tests failing. The last line in the log is:; > The job exceeded the maximum time limit for jobs, and has been terminated. The other successful jobs have the test results summary and an overall success message. Can you adjust the CI settings & run it again to see what the results are?. If this scenario is not actually covered by the existing tests, do I need to add an integration test in order to have the PR merged?. thanks; Ben",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170:799,Integrability,integrat,integration,799,"Hi @aednichols , ; Sorry for the long delay in response. This PR definitely fixes an issue that we found running Cromwell in AWS (see my description [above](https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-543478047)). . I don't know if this scenario is covered in the existing tests. I had a quick look at the CI build and it looks to me as if the AWS CI Job is failing due to a job timeout, rather than actual tests failing. The last line in the log is:; > The job exceeded the maximum time limit for jobs, and has been terminated. The other successful jobs have the test results summary and an overall success message. Can you adjust the CI settings & run it again to see what the results are?. If this scenario is not actually covered by the existing tests, do I need to add an integration test in order to have the PR merged?. thanks; Ben",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170:401,Safety,timeout,timeout,401,"Hi @aednichols , ; Sorry for the long delay in response. This PR definitely fixes an issue that we found running Cromwell in AWS (see my description [above](https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-543478047)). . I don't know if this scenario is covered in the existing tests. I had a quick look at the CI build and it looks to me as if the AWS CI Job is failing due to a job timeout, rather than actual tests failing. The last line in the log is:; > The job exceeded the maximum time limit for jobs, and has been terminated. The other successful jobs have the test results summary and an overall success message. Can you adjust the CI settings & run it again to see what the results are?. If this scenario is not actually covered by the existing tests, do I need to add an integration test in order to have the PR merged?. thanks; Ben",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170:295,Testability,test,tests,295,"Hi @aednichols , ; Sorry for the long delay in response. This PR definitely fixes an issue that we found running Cromwell in AWS (see my description [above](https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-543478047)). . I don't know if this scenario is covered in the existing tests. I had a quick look at the CI build and it looks to me as if the AWS CI Job is failing due to a job timeout, rather than actual tests failing. The last line in the log is:; > The job exceeded the maximum time limit for jobs, and has been terminated. The other successful jobs have the test results summary and an overall success message. Can you adjust the CI settings & run it again to see what the results are?. If this scenario is not actually covered by the existing tests, do I need to add an integration test in order to have the PR merged?. thanks; Ben",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170:429,Testability,test,tests,429,"Hi @aednichols , ; Sorry for the long delay in response. This PR definitely fixes an issue that we found running Cromwell in AWS (see my description [above](https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-543478047)). . I don't know if this scenario is covered in the existing tests. I had a quick look at the CI build and it looks to me as if the AWS CI Job is failing due to a job timeout, rather than actual tests failing. The last line in the log is:; > The job exceeded the maximum time limit for jobs, and has been terminated. The other successful jobs have the test results summary and an overall success message. Can you adjust the CI settings & run it again to see what the results are?. If this scenario is not actually covered by the existing tests, do I need to add an integration test in order to have the PR merged?. thanks; Ben",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170:465,Testability,log,log,465,"Hi @aednichols , ; Sorry for the long delay in response. This PR definitely fixes an issue that we found running Cromwell in AWS (see my description [above](https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-543478047)). . I don't know if this scenario is covered in the existing tests. I had a quick look at the CI build and it looks to me as if the AWS CI Job is failing due to a job timeout, rather than actual tests failing. The last line in the log is:; > The job exceeded the maximum time limit for jobs, and has been terminated. The other successful jobs have the test results summary and an overall success message. Can you adjust the CI settings & run it again to see what the results are?. If this scenario is not actually covered by the existing tests, do I need to add an integration test in order to have the PR merged?. thanks; Ben",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170:586,Testability,test,test,586,"Hi @aednichols , ; Sorry for the long delay in response. This PR definitely fixes an issue that we found running Cromwell in AWS (see my description [above](https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-543478047)). . I don't know if this scenario is covered in the existing tests. I had a quick look at the CI build and it looks to me as if the AWS CI Job is failing due to a job timeout, rather than actual tests failing. The last line in the log is:; > The job exceeded the maximum time limit for jobs, and has been terminated. The other successful jobs have the test results summary and an overall success message. Can you adjust the CI settings & run it again to see what the results are?. If this scenario is not actually covered by the existing tests, do I need to add an integration test in order to have the PR merged?. thanks; Ben",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170:772,Testability,test,tests,772,"Hi @aednichols , ; Sorry for the long delay in response. This PR definitely fixes an issue that we found running Cromwell in AWS (see my description [above](https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-543478047)). . I don't know if this scenario is covered in the existing tests. I had a quick look at the CI build and it looks to me as if the AWS CI Job is failing due to a job timeout, rather than actual tests failing. The last line in the log is:; > The job exceeded the maximum time limit for jobs, and has been terminated. The other successful jobs have the test results summary and an overall success message. Can you adjust the CI settings & run it again to see what the results are?. If this scenario is not actually covered by the existing tests, do I need to add an integration test in order to have the PR merged?. thanks; Ben",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170:811,Testability,test,test,811,"Hi @aednichols , ; Sorry for the long delay in response. This PR definitely fixes an issue that we found running Cromwell in AWS (see my description [above](https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-543478047)). . I don't know if this scenario is covered in the existing tests. I had a quick look at the CI build and it looks to me as if the AWS CI Job is failing due to a job timeout, rather than actual tests failing. The last line in the log is:; > The job exceeded the maximum time limit for jobs, and has been terminated. The other successful jobs have the test results summary and an overall success message. Can you adjust the CI settings & run it again to see what the results are?. If this scenario is not actually covered by the existing tests, do I need to add an integration test in order to have the PR merged?. thanks; Ben",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563012170
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563424439:207,Availability,failure,failures,207,"Hello @bpsommerville,. The CI has a hard limit of 180 minutes. Often when we see tests hitting the limit it is because a change has inadvertently introduced behavior wherein Cromwell retries forever. Random failures are certainly possible also. I have restarted the tests on your behalf. To set expectations, our bandwidth to help with AWS is limited, so it would be up to you to check that your PR is covered by existing tests, or to add a test yourself. Best,; Adam",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563424439
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563424439:81,Testability,test,tests,81,"Hello @bpsommerville,. The CI has a hard limit of 180 minutes. Often when we see tests hitting the limit it is because a change has inadvertently introduced behavior wherein Cromwell retries forever. Random failures are certainly possible also. I have restarted the tests on your behalf. To set expectations, our bandwidth to help with AWS is limited, so it would be up to you to check that your PR is covered by existing tests, or to add a test yourself. Best,; Adam",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563424439
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563424439:266,Testability,test,tests,266,"Hello @bpsommerville,. The CI has a hard limit of 180 minutes. Often when we see tests hitting the limit it is because a change has inadvertently introduced behavior wherein Cromwell retries forever. Random failures are certainly possible also. I have restarted the tests on your behalf. To set expectations, our bandwidth to help with AWS is limited, so it would be up to you to check that your PR is covered by existing tests, or to add a test yourself. Best,; Adam",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563424439
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563424439:422,Testability,test,tests,422,"Hello @bpsommerville,. The CI has a hard limit of 180 minutes. Often when we see tests hitting the limit it is because a change has inadvertently introduced behavior wherein Cromwell retries forever. Random failures are certainly possible also. I have restarted the tests on your behalf. To set expectations, our bandwidth to help with AWS is limited, so it would be up to you to check that your PR is covered by existing tests, or to add a test yourself. Best,; Adam",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563424439
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563424439:441,Testability,test,test,441,"Hello @bpsommerville,. The CI has a hard limit of 180 minutes. Often when we see tests hitting the limit it is because a change has inadvertently introduced behavior wherein Cromwell retries forever. Random failures are certainly possible also. I have restarted the tests on your behalf. To set expectations, our bandwidth to help with AWS is limited, so it would be up to you to check that your PR is covered by existing tests, or to add a test yourself. Best,; Adam",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563424439
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-566800420:31,Testability,test,test,31,@aednichols I have added a new test to cover the changes in this PR.; Please let me know if you need anything further,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-566800420
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-566812068:58,Deployability,patch,patch,58,Not sure why the build is showing no code coverage on the patch. I tagged the new test the same way as the existing tests (as AwsTest). It ran successfully in my local env when I enable that tag.; Does the CI not run any of the AwsTests?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-566812068
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-566812068:82,Testability,test,test,82,Not sure why the build is showing no code coverage on the patch. I tagged the new test the same way as the existing tests (as AwsTest). It ran successfully in my local env when I enable that tag.; Does the CI not run any of the AwsTests?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-566812068
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-566812068:116,Testability,test,tests,116,Not sure why the build is showing no code coverage on the patch. I tagged the new test the same way as the existing tests (as AwsTest). It ran successfully in my local env when I enable that tag.; Does the CI not run any of the AwsTests?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-566812068
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321:316,Integrability,depend,depend,316,"As far as I can tell, the [refreshed branch](https://github.com/broadinstitute/cromwell/pull/5291) has passed tests for this PR. You're right, the test suite doesn't (as far as I can tell) run Aws-tagged unit tests (the Unit testing config doesn't include any AWS connectivity). Question: Does the new test actually depend on connecting to external AWS resources? ; * If so, it probably would fail in our unit test runs (so not running would be correct!). ; * If not (or if you can mock things out so that it doesn't), you can probably remove the tag to see it run during our regular unit testing. Whichever way you answer the above question is fine, just let us know one way or the other and I'd be happy to give you a thumb. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321:233,Modifiability,config,config,233,"As far as I can tell, the [refreshed branch](https://github.com/broadinstitute/cromwell/pull/5291) has passed tests for this PR. You're right, the test suite doesn't (as far as I can tell) run Aws-tagged unit tests (the Unit testing config doesn't include any AWS connectivity). Question: Does the new test actually depend on connecting to external AWS resources? ; * If so, it probably would fail in our unit test runs (so not running would be correct!). ; * If not (or if you can mock things out so that it doesn't), you can probably remove the tag to see it run during our regular unit testing. Whichever way you answer the above question is fine, just let us know one way or the other and I'd be happy to give you a thumb. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321:110,Testability,test,tests,110,"As far as I can tell, the [refreshed branch](https://github.com/broadinstitute/cromwell/pull/5291) has passed tests for this PR. You're right, the test suite doesn't (as far as I can tell) run Aws-tagged unit tests (the Unit testing config doesn't include any AWS connectivity). Question: Does the new test actually depend on connecting to external AWS resources? ; * If so, it probably would fail in our unit test runs (so not running would be correct!). ; * If not (or if you can mock things out so that it doesn't), you can probably remove the tag to see it run during our regular unit testing. Whichever way you answer the above question is fine, just let us know one way or the other and I'd be happy to give you a thumb. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321:147,Testability,test,test,147,"As far as I can tell, the [refreshed branch](https://github.com/broadinstitute/cromwell/pull/5291) has passed tests for this PR. You're right, the test suite doesn't (as far as I can tell) run Aws-tagged unit tests (the Unit testing config doesn't include any AWS connectivity). Question: Does the new test actually depend on connecting to external AWS resources? ; * If so, it probably would fail in our unit test runs (so not running would be correct!). ; * If not (or if you can mock things out so that it doesn't), you can probably remove the tag to see it run during our regular unit testing. Whichever way you answer the above question is fine, just let us know one way or the other and I'd be happy to give you a thumb. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321:209,Testability,test,tests,209,"As far as I can tell, the [refreshed branch](https://github.com/broadinstitute/cromwell/pull/5291) has passed tests for this PR. You're right, the test suite doesn't (as far as I can tell) run Aws-tagged unit tests (the Unit testing config doesn't include any AWS connectivity). Question: Does the new test actually depend on connecting to external AWS resources? ; * If so, it probably would fail in our unit test runs (so not running would be correct!). ; * If not (or if you can mock things out so that it doesn't), you can probably remove the tag to see it run during our regular unit testing. Whichever way you answer the above question is fine, just let us know one way or the other and I'd be happy to give you a thumb. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321:225,Testability,test,testing,225,"As far as I can tell, the [refreshed branch](https://github.com/broadinstitute/cromwell/pull/5291) has passed tests for this PR. You're right, the test suite doesn't (as far as I can tell) run Aws-tagged unit tests (the Unit testing config doesn't include any AWS connectivity). Question: Does the new test actually depend on connecting to external AWS resources? ; * If so, it probably would fail in our unit test runs (so not running would be correct!). ; * If not (or if you can mock things out so that it doesn't), you can probably remove the tag to see it run during our regular unit testing. Whichever way you answer the above question is fine, just let us know one way or the other and I'd be happy to give you a thumb. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321:302,Testability,test,test,302,"As far as I can tell, the [refreshed branch](https://github.com/broadinstitute/cromwell/pull/5291) has passed tests for this PR. You're right, the test suite doesn't (as far as I can tell) run Aws-tagged unit tests (the Unit testing config doesn't include any AWS connectivity). Question: Does the new test actually depend on connecting to external AWS resources? ; * If so, it probably would fail in our unit test runs (so not running would be correct!). ; * If not (or if you can mock things out so that it doesn't), you can probably remove the tag to see it run during our regular unit testing. Whichever way you answer the above question is fine, just let us know one way or the other and I'd be happy to give you a thumb. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321:410,Testability,test,test,410,"As far as I can tell, the [refreshed branch](https://github.com/broadinstitute/cromwell/pull/5291) has passed tests for this PR. You're right, the test suite doesn't (as far as I can tell) run Aws-tagged unit tests (the Unit testing config doesn't include any AWS connectivity). Question: Does the new test actually depend on connecting to external AWS resources? ; * If so, it probably would fail in our unit test runs (so not running would be correct!). ; * If not (or if you can mock things out so that it doesn't), you can probably remove the tag to see it run during our regular unit testing. Whichever way you answer the above question is fine, just let us know one way or the other and I'd be happy to give you a thumb. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321:482,Testability,mock,mock,482,"As far as I can tell, the [refreshed branch](https://github.com/broadinstitute/cromwell/pull/5291) has passed tests for this PR. You're right, the test suite doesn't (as far as I can tell) run Aws-tagged unit tests (the Unit testing config doesn't include any AWS connectivity). Question: Does the new test actually depend on connecting to external AWS resources? ; * If so, it probably would fail in our unit test runs (so not running would be correct!). ; * If not (or if you can mock things out so that it doesn't), you can probably remove the tag to see it run during our regular unit testing. Whichever way you answer the above question is fine, just let us know one way or the other and I'd be happy to give you a thumb. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321:589,Testability,test,testing,589,"As far as I can tell, the [refreshed branch](https://github.com/broadinstitute/cromwell/pull/5291) has passed tests for this PR. You're right, the test suite doesn't (as far as I can tell) run Aws-tagged unit tests (the Unit testing config doesn't include any AWS connectivity). Question: Does the new test actually depend on connecting to external AWS resources? ; * If so, it probably would fail in our unit test runs (so not running would be correct!). ; * If not (or if you can mock things out so that it doesn't), you can probably remove the tag to see it run during our regular unit testing. Whichever way you answer the above question is fine, just let us know one way or the other and I'd be happy to give you a thumb. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-568504321
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:123,Integrability,depend,depend,123,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:208,Integrability,depend,depend,208,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:309,Modifiability,variab,variable,309,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:544,Modifiability,variab,variables,544,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:109,Testability,test,test,109,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:396,Testability,test,test,396,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509
https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:570,Usability,simpl,simplest,570,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509
https://github.com/broadinstitute/cromwell/pull/5217#issuecomment-547487521:73,Testability,test,tests,73,Have started https://github.com/broadinstitute/cromwell/pull/5251 to run tests,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5217#issuecomment-547487521
https://github.com/broadinstitute/cromwell/pull/5217#issuecomment-548115430:47,Testability,test,tests,47,"Hi @TimurKustov, this looks good to me and the tests are passing. I think we're just waiting on you removing that newline now (so that an unexpected file doesn't show up in the git changeset for this PR), and we should be good to go",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5217#issuecomment-548115430
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952:325,Availability,alive,alive,325,"Theory 1:; I've also seen this issue. I've set our `concurrent-job-limit` parameter in the backend to 150 and yet the number of concurrent jobs (including those pending) seems to stay at around 60-65. . Could the other 90 be from tasks that still need to be 'ticked off' as complete?. My suspicion on this is that the `check-alive` parameter in the config is set to `squeue -j ${job_id}`.; On our cluster we have around 3 hours to continue running commands like `scontrol show job` to see the metadata on the job and find logs. This is useful but, `squeue -j ${job_id}` still returns true well and truly after the job has completed/failed. Could you try massively increasing the job limit (to say 10000) and see if that changes anything?. Theory 2:; Your configuration file could need a scale up - it may be that the number of system io requests require increasing:. ```; system {; io {; number-of-requests = 100000; per = 100 seconds; number-of-attempts = 50; }; ```. Will allow your job to make 1000 requests per second. For some of those batch calling jobs with many vcf inputs, it may be taking some time for the server to set up the task?. Theory 3:; Your duplication-strategy is causing lag.; Can you confirm that in providers.slurm.filesystems that you have hard-link or soft-link as the top priority for the localization and caching settings, over copying?. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952:755,Deployability,configurat,configuration,755,"Theory 1:; I've also seen this issue. I've set our `concurrent-job-limit` parameter in the backend to 150 and yet the number of concurrent jobs (including those pending) seems to stay at around 60-65. . Could the other 90 be from tasks that still need to be 'ticked off' as complete?. My suspicion on this is that the `check-alive` parameter in the config is set to `squeue -j ${job_id}`.; On our cluster we have around 3 hours to continue running commands like `scontrol show job` to see the metadata on the job and find logs. This is useful but, `squeue -j ${job_id}` still returns true well and truly after the job has completed/failed. Could you try massively increasing the job limit (to say 10000) and see if that changes anything?. Theory 2:; Your configuration file could need a scale up - it may be that the number of system io requests require increasing:. ```; system {; io {; number-of-requests = 100000; per = 100 seconds; number-of-attempts = 50; }; ```. Will allow your job to make 1000 requests per second. For some of those batch calling jobs with many vcf inputs, it may be taking some time for the server to set up the task?. Theory 3:; Your duplication-strategy is causing lag.; Can you confirm that in providers.slurm.filesystems that you have hard-link or soft-link as the top priority for the localization and caching settings, over copying?. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952:349,Modifiability,config,config,349,"Theory 1:; I've also seen this issue. I've set our `concurrent-job-limit` parameter in the backend to 150 and yet the number of concurrent jobs (including those pending) seems to stay at around 60-65. . Could the other 90 be from tasks that still need to be 'ticked off' as complete?. My suspicion on this is that the `check-alive` parameter in the config is set to `squeue -j ${job_id}`.; On our cluster we have around 3 hours to continue running commands like `scontrol show job` to see the metadata on the job and find logs. This is useful but, `squeue -j ${job_id}` still returns true well and truly after the job has completed/failed. Could you try massively increasing the job limit (to say 10000) and see if that changes anything?. Theory 2:; Your configuration file could need a scale up - it may be that the number of system io requests require increasing:. ```; system {; io {; number-of-requests = 100000; per = 100 seconds; number-of-attempts = 50; }; ```. Will allow your job to make 1000 requests per second. For some of those batch calling jobs with many vcf inputs, it may be taking some time for the server to set up the task?. Theory 3:; Your duplication-strategy is causing lag.; Can you confirm that in providers.slurm.filesystems that you have hard-link or soft-link as the top priority for the localization and caching settings, over copying?. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952:755,Modifiability,config,configuration,755,"Theory 1:; I've also seen this issue. I've set our `concurrent-job-limit` parameter in the backend to 150 and yet the number of concurrent jobs (including those pending) seems to stay at around 60-65. . Could the other 90 be from tasks that still need to be 'ticked off' as complete?. My suspicion on this is that the `check-alive` parameter in the config is set to `squeue -j ${job_id}`.; On our cluster we have around 3 hours to continue running commands like `scontrol show job` to see the metadata on the job and find logs. This is useful but, `squeue -j ${job_id}` still returns true well and truly after the job has completed/failed. Could you try massively increasing the job limit (to say 10000) and see if that changes anything?. Theory 2:; Your configuration file could need a scale up - it may be that the number of system io requests require increasing:. ```; system {; io {; number-of-requests = 100000; per = 100 seconds; number-of-attempts = 50; }; ```. Will allow your job to make 1000 requests per second. For some of those batch calling jobs with many vcf inputs, it may be taking some time for the server to set up the task?. Theory 3:; Your duplication-strategy is causing lag.; Can you confirm that in providers.slurm.filesystems that you have hard-link or soft-link as the top priority for the localization and caching settings, over copying?. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952:52,Performance,concurren,concurrent-job-limit,52,"Theory 1:; I've also seen this issue. I've set our `concurrent-job-limit` parameter in the backend to 150 and yet the number of concurrent jobs (including those pending) seems to stay at around 60-65. . Could the other 90 be from tasks that still need to be 'ticked off' as complete?. My suspicion on this is that the `check-alive` parameter in the config is set to `squeue -j ${job_id}`.; On our cluster we have around 3 hours to continue running commands like `scontrol show job` to see the metadata on the job and find logs. This is useful but, `squeue -j ${job_id}` still returns true well and truly after the job has completed/failed. Could you try massively increasing the job limit (to say 10000) and see if that changes anything?. Theory 2:; Your configuration file could need a scale up - it may be that the number of system io requests require increasing:. ```; system {; io {; number-of-requests = 100000; per = 100 seconds; number-of-attempts = 50; }; ```. Will allow your job to make 1000 requests per second. For some of those batch calling jobs with many vcf inputs, it may be taking some time for the server to set up the task?. Theory 3:; Your duplication-strategy is causing lag.; Can you confirm that in providers.slurm.filesystems that you have hard-link or soft-link as the top priority for the localization and caching settings, over copying?. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952:128,Performance,concurren,concurrent,128,"Theory 1:; I've also seen this issue. I've set our `concurrent-job-limit` parameter in the backend to 150 and yet the number of concurrent jobs (including those pending) seems to stay at around 60-65. . Could the other 90 be from tasks that still need to be 'ticked off' as complete?. My suspicion on this is that the `check-alive` parameter in the config is set to `squeue -j ${job_id}`.; On our cluster we have around 3 hours to continue running commands like `scontrol show job` to see the metadata on the job and find logs. This is useful but, `squeue -j ${job_id}` still returns true well and truly after the job has completed/failed. Could you try massively increasing the job limit (to say 10000) and see if that changes anything?. Theory 2:; Your configuration file could need a scale up - it may be that the number of system io requests require increasing:. ```; system {; io {; number-of-requests = 100000; per = 100 seconds; number-of-attempts = 50; }; ```. Will allow your job to make 1000 requests per second. For some of those batch calling jobs with many vcf inputs, it may be taking some time for the server to set up the task?. Theory 3:; Your duplication-strategy is causing lag.; Can you confirm that in providers.slurm.filesystems that you have hard-link or soft-link as the top priority for the localization and caching settings, over copying?. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952:522,Testability,log,logs,522,"Theory 1:; I've also seen this issue. I've set our `concurrent-job-limit` parameter in the backend to 150 and yet the number of concurrent jobs (including those pending) seems to stay at around 60-65. . Could the other 90 be from tasks that still need to be 'ticked off' as complete?. My suspicion on this is that the `check-alive` parameter in the config is set to `squeue -j ${job_id}`.; On our cluster we have around 3 hours to continue running commands like `scontrol show job` to see the metadata on the job and find logs. This is useful but, `squeue -j ${job_id}` still returns true well and truly after the job has completed/failed. Could you try massively increasing the job limit (to say 10000) and see if that changes anything?. Theory 2:; Your configuration file could need a scale up - it may be that the number of system io requests require increasing:. ```; system {; io {; number-of-requests = 100000; per = 100 seconds; number-of-attempts = 50; }; ```. Will allow your job to make 1000 requests per second. For some of those batch calling jobs with many vcf inputs, it may be taking some time for the server to set up the task?. Theory 3:; Your duplication-strategy is causing lag.; Can you confirm that in providers.slurm.filesystems that you have hard-link or soft-link as the top priority for the localization and caching settings, over copying?. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651055688:143,Performance,concurren,concurrent,143,"We are facing a similar issue when using the SLURM backend. It appears as if cromwell is limiting the number of jobs to a particular number of concurrent jobs, which is below the number specified by the `concurrent-job-limit` parameter. For example, when running a scatter task with 25 jobs, only 6 are started and the rest are started after these jobs are complete (the resources on the server are sufficient to run all 25). ; I have looked at all the possible issues that come to mind - including:; 1. Using the less CPU intensive call caching strategy (""fingerprint""), which instantly checks for a match in call cache. ; 2. Assigning all jobs a default hog group value ""static"" and setting the hog factor to 1. ; 3. Setting the `concurrent-job-limit` to a very high value (2000 in our case).; 4. I think I/O is not a problem, as the jobs are run almost instantly (including call caching checks and hard-linking) and then cromwell waits with execution of the rest until the first ones are complete. The CPU and RAM utilization of cromwell are low at all times. ; 5. This is seen with single and multiple workflows so the `max-concurrent-workflows` does not appear to be the problematic setting here. . Despite trying everything, we still only see about 25% of jobs being run concurrently. Is there another setting I am missing? Any input is much appreaciated - thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651055688
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651055688:204,Performance,concurren,concurrent-job-limit,204,"We are facing a similar issue when using the SLURM backend. It appears as if cromwell is limiting the number of jobs to a particular number of concurrent jobs, which is below the number specified by the `concurrent-job-limit` parameter. For example, when running a scatter task with 25 jobs, only 6 are started and the rest are started after these jobs are complete (the resources on the server are sufficient to run all 25). ; I have looked at all the possible issues that come to mind - including:; 1. Using the less CPU intensive call caching strategy (""fingerprint""), which instantly checks for a match in call cache. ; 2. Assigning all jobs a default hog group value ""static"" and setting the hog factor to 1. ; 3. Setting the `concurrent-job-limit` to a very high value (2000 in our case).; 4. I think I/O is not a problem, as the jobs are run almost instantly (including call caching checks and hard-linking) and then cromwell waits with execution of the rest until the first ones are complete. The CPU and RAM utilization of cromwell are low at all times. ; 5. This is seen with single and multiple workflows so the `max-concurrent-workflows` does not appear to be the problematic setting here. . Despite trying everything, we still only see about 25% of jobs being run concurrently. Is there another setting I am missing? Any input is much appreaciated - thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651055688
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651055688:615,Performance,cache,cache,615,"We are facing a similar issue when using the SLURM backend. It appears as if cromwell is limiting the number of jobs to a particular number of concurrent jobs, which is below the number specified by the `concurrent-job-limit` parameter. For example, when running a scatter task with 25 jobs, only 6 are started and the rest are started after these jobs are complete (the resources on the server are sufficient to run all 25). ; I have looked at all the possible issues that come to mind - including:; 1. Using the less CPU intensive call caching strategy (""fingerprint""), which instantly checks for a match in call cache. ; 2. Assigning all jobs a default hog group value ""static"" and setting the hog factor to 1. ; 3. Setting the `concurrent-job-limit` to a very high value (2000 in our case).; 4. I think I/O is not a problem, as the jobs are run almost instantly (including call caching checks and hard-linking) and then cromwell waits with execution of the rest until the first ones are complete. The CPU and RAM utilization of cromwell are low at all times. ; 5. This is seen with single and multiple workflows so the `max-concurrent-workflows` does not appear to be the problematic setting here. . Despite trying everything, we still only see about 25% of jobs being run concurrently. Is there another setting I am missing? Any input is much appreaciated - thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651055688
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651055688:732,Performance,concurren,concurrent-job-limit,732,"We are facing a similar issue when using the SLURM backend. It appears as if cromwell is limiting the number of jobs to a particular number of concurrent jobs, which is below the number specified by the `concurrent-job-limit` parameter. For example, when running a scatter task with 25 jobs, only 6 are started and the rest are started after these jobs are complete (the resources on the server are sufficient to run all 25). ; I have looked at all the possible issues that come to mind - including:; 1. Using the less CPU intensive call caching strategy (""fingerprint""), which instantly checks for a match in call cache. ; 2. Assigning all jobs a default hog group value ""static"" and setting the hog factor to 1. ; 3. Setting the `concurrent-job-limit` to a very high value (2000 in our case).; 4. I think I/O is not a problem, as the jobs are run almost instantly (including call caching checks and hard-linking) and then cromwell waits with execution of the rest until the first ones are complete. The CPU and RAM utilization of cromwell are low at all times. ; 5. This is seen with single and multiple workflows so the `max-concurrent-workflows` does not appear to be the problematic setting here. . Despite trying everything, we still only see about 25% of jobs being run concurrently. Is there another setting I am missing? Any input is much appreaciated - thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651055688
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651055688:1128,Performance,concurren,concurrent-workflows,1128,"We are facing a similar issue when using the SLURM backend. It appears as if cromwell is limiting the number of jobs to a particular number of concurrent jobs, which is below the number specified by the `concurrent-job-limit` parameter. For example, when running a scatter task with 25 jobs, only 6 are started and the rest are started after these jobs are complete (the resources on the server are sufficient to run all 25). ; I have looked at all the possible issues that come to mind - including:; 1. Using the less CPU intensive call caching strategy (""fingerprint""), which instantly checks for a match in call cache. ; 2. Assigning all jobs a default hog group value ""static"" and setting the hog factor to 1. ; 3. Setting the `concurrent-job-limit` to a very high value (2000 in our case).; 4. I think I/O is not a problem, as the jobs are run almost instantly (including call caching checks and hard-linking) and then cromwell waits with execution of the rest until the first ones are complete. The CPU and RAM utilization of cromwell are low at all times. ; 5. This is seen with single and multiple workflows so the `max-concurrent-workflows` does not appear to be the problematic setting here. . Despite trying everything, we still only see about 25% of jobs being run concurrently. Is there another setting I am missing? Any input is much appreaciated - thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651055688
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651055688:1277,Performance,concurren,concurrently,1277,"We are facing a similar issue when using the SLURM backend. It appears as if cromwell is limiting the number of jobs to a particular number of concurrent jobs, which is below the number specified by the `concurrent-job-limit` parameter. For example, when running a scatter task with 25 jobs, only 6 are started and the rest are started after these jobs are complete (the resources on the server are sufficient to run all 25). ; I have looked at all the possible issues that come to mind - including:; 1. Using the less CPU intensive call caching strategy (""fingerprint""), which instantly checks for a match in call cache. ; 2. Assigning all jobs a default hog group value ""static"" and setting the hog factor to 1. ; 3. Setting the `concurrent-job-limit` to a very high value (2000 in our case).; 4. I think I/O is not a problem, as the jobs are run almost instantly (including call caching checks and hard-linking) and then cromwell waits with execution of the rest until the first ones are complete. The CPU and RAM utilization of cromwell are low at all times. ; 5. This is seen with single and multiple workflows so the `max-concurrent-workflows` does not appear to be the problematic setting here. . Despite trying everything, we still only see about 25% of jobs being run concurrently. Is there another setting I am missing? Any input is much appreaciated - thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651055688
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651084834:374,Performance,cache,cached-copy,374,"Hi Ales, in my case it was this bug here (that I've filed and is yet to be attended to):; https://broadworkbench.atlassian.net/browse/BA-6147. Run a `du -hs` on the directory and check the size, if it's ridiculously large, It could be that the reference files are being copied in the scatter gather rather than hard-linked or soft-linked. This is meant to be resolved with `cached-copy` for shared file systems but doesn't appear to work, particularly if the reference files are on a separate mount point to the working cromwell directory. If you do find that this is the case, a workaround that I found is that you can create a step of the workflow at the start that takes in each of these large files and merely runs a cp to their output. Then, rather than using the reference argument in the workflow, use the outputs from that first step that runs a cp. That will ensure that all of the reference files in the scatter gather are hard-linked rather than copied. Kind regards,; Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-651084834
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652434370:443,Performance,concurren,concurrently,443,"Dear Alexis,; many thanks for the suggestion. I have checked and the files seem to be correctly hard-linked in our case and the folder sizes are as they should be. We have about 1Tb of reference files and the `cromwell-executions` directory with several workflow folders is only about 1.2 Tb in size, so it appears that the files are not being copied, but correctly hard-linked. ; I have checked again and it seems that the cap is at about 35 concurrently submitted tasks, even when several workflows are submitted, each set to scatter about 20-30 jobs. When running the jobs outsite cromwell server mode, we usually have several jobs in the ""pending"" state on slurm, but cromwell never submits more than 35 at once and the jobs never get to the pending list. It is almost as if there is some kind of hard limit in the number of jobs submitted, not controlled by the `concurrent-job-limit`. ; Thanks again! Best,; Ales",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652434370
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652434370:868,Performance,concurren,concurrent-job-limit,868,"Dear Alexis,; many thanks for the suggestion. I have checked and the files seem to be correctly hard-linked in our case and the folder sizes are as they should be. We have about 1Tb of reference files and the `cromwell-executions` directory with several workflow folders is only about 1.2 Tb in size, so it appears that the files are not being copied, but correctly hard-linked. ; I have checked again and it seems that the cap is at about 35 concurrently submitted tasks, even when several workflows are submitted, each set to scatter about 20-30 jobs. When running the jobs outsite cromwell server mode, we usually have several jobs in the ""pending"" state on slurm, but cromwell never submits more than 35 at once and the jobs never get to the pending list. It is almost as if there is some kind of hard limit in the number of jobs submitted, not controlled by the `concurrent-job-limit`. ; Thanks again! Best,; Ales",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652434370
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652768986:447,Availability,avail,available,447,"Thanks for this Ales,. I wonder if it's a slurm thing?; https://slurm.schedmd.com/resource_limits.html. Some key things to look at from here:; https://slurm.schedmd.com/slurm.conf.html. MaxJobCount - Number of jobs in the active database - this includes recently finished jobs, new jobs won't go into pending.; MaxSubmitJobs - Can be set for a per user too. Could Cromwell be continuously trying to submit jobs and waiting until a new position is available?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652768986
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652768986:376,Deployability,continuous,continuously,376,"Thanks for this Ales,. I wonder if it's a slurm thing?; https://slurm.schedmd.com/resource_limits.html. Some key things to look at from here:; https://slurm.schedmd.com/slurm.conf.html. MaxJobCount - Number of jobs in the active database - this includes recently finished jobs, new jobs won't go into pending.; MaxSubmitJobs - Can be set for a per user too. Could Cromwell be continuously trying to submit jobs and waiting until a new position is available?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652768986
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-655622391:458,Availability,avail,available,458,"Dear Alexis,; many thanks - I really appreciate the very useful inputs!; I asked the team operating the slurm server and they say that no such limits are in effect. The number of permitted jobs is oddly specific, it stays at maximum 37 jobs (either pending or running). It really appears that cromwell is doing something to stop further submission of jobs (they are tracked as ""Running"" in cromwell but no call within a job takes place until there are slots available). ; Once the jobs submitted exceed this queue, then cromwell server only generates this log:; ```; 2020-07-08 17:13:15,328 INFO - MaterializeWorkflowDescriptorActor [UUID(9db83645)]: Parsing workflow as WDL 1.0; 2020-07-08 17:13:16,442 INFO - MaterializeWorkflowDescriptorActor [UUID(9db83645)]: Call-to-Backend assignments: FastqToVCF.VariantFiltrationSNP -> slurm; ```; And then waits in this state until another job is finished, without even checking if the call is cached or anything else. Please note that the number of permitted workflows is set to a value higher than the number of workflows we usually submit. . One interesting observation - if I stop the cromwell server process (Control-C), all the jobs that were not previously submitted to slurm, get submitted immediately (as if cromwell was constantly blocking the submission for an unclear reason). Any input is, again, very much valuable! Thanks a lot. Ps. I just wanted to add that adding a second server process and submitting tasks to it allows submitting more jobs, so it is very likely that the slurm system is not limiting submissions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-655622391
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-655622391:508,Performance,queue,queue,508,"Dear Alexis,; many thanks - I really appreciate the very useful inputs!; I asked the team operating the slurm server and they say that no such limits are in effect. The number of permitted jobs is oddly specific, it stays at maximum 37 jobs (either pending or running). It really appears that cromwell is doing something to stop further submission of jobs (they are tracked as ""Running"" in cromwell but no call within a job takes place until there are slots available). ; Once the jobs submitted exceed this queue, then cromwell server only generates this log:; ```; 2020-07-08 17:13:15,328 INFO - MaterializeWorkflowDescriptorActor [UUID(9db83645)]: Parsing workflow as WDL 1.0; 2020-07-08 17:13:16,442 INFO - MaterializeWorkflowDescriptorActor [UUID(9db83645)]: Call-to-Backend assignments: FastqToVCF.VariantFiltrationSNP -> slurm; ```; And then waits in this state until another job is finished, without even checking if the call is cached or anything else. Please note that the number of permitted workflows is set to a value higher than the number of workflows we usually submit. . One interesting observation - if I stop the cromwell server process (Control-C), all the jobs that were not previously submitted to slurm, get submitted immediately (as if cromwell was constantly blocking the submission for an unclear reason). Any input is, again, very much valuable! Thanks a lot. Ps. I just wanted to add that adding a second server process and submitting tasks to it allows submitting more jobs, so it is very likely that the slurm system is not limiting submissions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-655622391
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-655622391:937,Performance,cache,cached,937,"Dear Alexis,; many thanks - I really appreciate the very useful inputs!; I asked the team operating the slurm server and they say that no such limits are in effect. The number of permitted jobs is oddly specific, it stays at maximum 37 jobs (either pending or running). It really appears that cromwell is doing something to stop further submission of jobs (they are tracked as ""Running"" in cromwell but no call within a job takes place until there are slots available). ; Once the jobs submitted exceed this queue, then cromwell server only generates this log:; ```; 2020-07-08 17:13:15,328 INFO - MaterializeWorkflowDescriptorActor [UUID(9db83645)]: Parsing workflow as WDL 1.0; 2020-07-08 17:13:16,442 INFO - MaterializeWorkflowDescriptorActor [UUID(9db83645)]: Call-to-Backend assignments: FastqToVCF.VariantFiltrationSNP -> slurm; ```; And then waits in this state until another job is finished, without even checking if the call is cached or anything else. Please note that the number of permitted workflows is set to a value higher than the number of workflows we usually submit. . One interesting observation - if I stop the cromwell server process (Control-C), all the jobs that were not previously submitted to slurm, get submitted immediately (as if cromwell was constantly blocking the submission for an unclear reason). Any input is, again, very much valuable! Thanks a lot. Ps. I just wanted to add that adding a second server process and submitting tasks to it allows submitting more jobs, so it is very likely that the slurm system is not limiting submissions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-655622391
https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-655622391:556,Testability,log,log,556,"Dear Alexis,; many thanks - I really appreciate the very useful inputs!; I asked the team operating the slurm server and they say that no such limits are in effect. The number of permitted jobs is oddly specific, it stays at maximum 37 jobs (either pending or running). It really appears that cromwell is doing something to stop further submission of jobs (they are tracked as ""Running"" in cromwell but no call within a job takes place until there are slots available). ; Once the jobs submitted exceed this queue, then cromwell server only generates this log:; ```; 2020-07-08 17:13:15,328 INFO - MaterializeWorkflowDescriptorActor [UUID(9db83645)]: Parsing workflow as WDL 1.0; 2020-07-08 17:13:16,442 INFO - MaterializeWorkflowDescriptorActor [UUID(9db83645)]: Call-to-Backend assignments: FastqToVCF.VariantFiltrationSNP -> slurm; ```; And then waits in this state until another job is finished, without even checking if the call is cached or anything else. Please note that the number of permitted workflows is set to a value higher than the number of workflows we usually submit. . One interesting observation - if I stop the cromwell server process (Control-C), all the jobs that were not previously submitted to slurm, get submitted immediately (as if cromwell was constantly blocking the submission for an unclear reason). Any input is, again, very much valuable! Thanks a lot. Ps. I just wanted to add that adding a second server process and submitting tasks to it allows submitting more jobs, so it is very likely that the slurm system is not limiting submissions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-655622391
https://github.com/broadinstitute/cromwell/pull/5219#issuecomment-540534801:67,Modifiability,config,config,67,Travis appears to be legitimately unhappy with the hybrid metadata config,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5219#issuecomment-540534801
https://github.com/broadinstitute/cromwell/pull/5220#issuecomment-541143265:44,Availability,error,error,44,```; [info] - should check that we classify error code 10 as a preemption *** FAILED *** (183 milliseconds); [info] Preempted was not equal to Preempted (GetRequestHandlerSpec.scala:207); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at cromwell.backend.google.pipelines.v2alpha1.api.request.GetRequestHandlerSpec.$anonfun$new$2(GetRequestHandlerSpec.scala:207); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5220#issuecomment-541143265
https://github.com/broadinstitute/cromwell/pull/5220#issuecomment-541143265:288,Deployability,pipeline,pipelines,288,```; [info] - should check that we classify error code 10 as a preemption *** FAILED *** (183 milliseconds); [info] Preempted was not equal to Preempted (GetRequestHandlerSpec.scala:207); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at cromwell.backend.google.pipelines.v2alpha1.api.request.GetRequestHandlerSpec.$anonfun$new$2(GetRequestHandlerSpec.scala:207); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5220#issuecomment-541143265
https://github.com/broadinstitute/cromwell/pull/5220#issuecomment-541143265:220,Testability,Test,TestFailedException,220,```; [info] - should check that we classify error code 10 as a preemption *** FAILED *** (183 milliseconds); [info] Preempted was not equal to Preempted (GetRequestHandlerSpec.scala:207); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at cromwell.backend.google.pipelines.v2alpha1.api.request.GetRequestHandlerSpec.$anonfun$new$2(GetRequestHandlerSpec.scala:207); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5220#issuecomment-541143265
https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-540813650:114,Availability,down,downloaded,114,"I've found that this issue happens only when I use my custom reference files for assembly in BWA (mouse reference downloaded from Sanger ftp). I don't know if this is an issue with Cromwell or just the files I'm using. When I use the Broad's reference files downloaded from the google cloud storage, everything behaves as normal.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-540813650
https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-540813650:258,Availability,down,downloaded,258,"I've found that this issue happens only when I use my custom reference files for assembly in BWA (mouse reference downloaded from Sanger ftp). I don't know if this is an issue with Cromwell or just the files I'm using. When I use the Broad's reference files downloaded from the google cloud storage, everything behaves as normal.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-540813650
https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-541156513:14,Availability,down,down,14,"I narrowed it down to the fact that I don't have an alt contig for the reference file -- i was leaving that blank in the wdl input file. If i just fake it by using the human alt from the Broad's human genome reference in their pipeline, the weird nesting-copying doesnt happen. I'll leave this open because I don't know if this is expected behavior or not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-541156513
https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-541156513:227,Deployability,pipeline,pipeline,227,"I narrowed it down to the fact that I don't have an alt contig for the reference file -- i was leaving that blank in the wdl input file. If i just fake it by using the human alt from the Broad's human genome reference in their pipeline, the weird nesting-copying doesnt happen. I'll leave this open because I don't know if this is expected behavior or not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-541156513
https://github.com/broadinstitute/cromwell/pull/5224#issuecomment-542328679:58,Availability,fault,fault,58,This is failing travis but my guess is that that's not my fault?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5224#issuecomment-542328679
https://github.com/broadinstitute/cromwell/pull/5224#issuecomment-542372465:45,Availability,failure,failures,45,"@lbergelson right, these look like transient failures so I'l give the tests a quick nudge...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5224#issuecomment-542372465
https://github.com/broadinstitute/cromwell/pull/5224#issuecomment-542372465:70,Testability,test,tests,70,"@lbergelson right, these look like transient failures so I'l give the tests a quick nudge...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5224#issuecomment-542372465
https://github.com/broadinstitute/cromwell/issues/5226#issuecomment-602846091:217,Modifiability,config,config,217,"It should be declared in the conf file as follows (see the ""root"" tag). ```; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; **root = ""s3://my-cromwell-bucket-name/cromwell-execution""**; ...; ```. I recommend using the cloud formation template detailed in this page, it will set it all up for you. https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5226#issuecomment-602846091
https://github.com/broadinstitute/cromwell/pull/5229#issuecomment-542979621:395,Modifiability,config,config,395,"For testing purposes - I was running this as a local server using the following auths; ```; auths = [{; name = ""patto""; scheme = ""assume_role""; base-auth = ""pattocreds""; role-arn = ""arn:aws:iam::XXXXXXXXXXXX:role/OrganizationAccountAccessRole""; }, ; {; name = ""pattocreds""; scheme = ""default"". }]. .. AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; auth = ""patto""; filesystems { s3 { auth = ""patto"" } }; ...; }. ```; Submitting jobs to this local server would work for an hour - and then start failing on all AWS calls - in particular the status calls in OccasionalStatusPollingActor. The default credential expiry for an assume-role is 3600 seconds. With the changes in this PR - the local server in assume-role mode has lasted for more than an hour.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5229#issuecomment-542979621
https://github.com/broadinstitute/cromwell/pull/5229#issuecomment-542979621:4,Testability,test,testing,4,"For testing purposes - I was running this as a local server using the following auths; ```; auths = [{; name = ""patto""; scheme = ""assume_role""; base-auth = ""pattocreds""; role-arn = ""arn:aws:iam::XXXXXXXXXXXX:role/OrganizationAccountAccessRole""; }, ; {; name = ""pattocreds""; scheme = ""default"". }]. .. AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; auth = ""patto""; filesystems { s3 { auth = ""patto"" } }; ...; }. ```; Submitting jobs to this local server would work for an hour - and then start failing on all AWS calls - in particular the status calls in OccasionalStatusPollingActor. The default credential expiry for an assume-role is 3600 seconds. With the changes in this PR - the local server in assume-role mode has lasted for more than an hour.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5229#issuecomment-542979621
https://github.com/broadinstitute/cromwell/pull/5229#issuecomment-543945357:142,Testability,test,test,142,"Thanks! . So so you can track it, I've made #5238 to trigger the CI against this branch, to make sure it hasn't accidentally broken any other test cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5229#issuecomment-543945357
https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-544098095:5,Deployability,deploy,deploying,5,"When deploying Cromwell on our HPC which uses slurm as a scheduler, I use a wckey unique to a workflow and identical across tasks in that workflow - very handy for searching for failed jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-544098095
https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-544098095:57,Energy Efficiency,schedul,scheduler,57,"When deploying Cromwell on our HPC which uses slurm as a scheduler, I use a wckey unique to a workflow and identical across tasks in that workflow - very handy for searching for failed jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-544098095
https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-561687838:723,Availability,echo,echo,723,"Hi @natechols – this is not a request I've heard before (not to say that's conclusive in any way) but it seems reasonable. We don't have any current plans for work in this area so a PR would be helpful. In particular, it looks like a natural place could be the existing list of `export`s in the generated run script. ```; #!/bin/bash. cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution; tmpDir=$(mkdir -p ""/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/tmp.7e77d324"" && echo ""/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/tmp.7e77d324""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution. ); outb631bc66=""${tmpDir}/out.$$"" errb631bc66=""${tmpDir}/err.$$""; mkfifo ""$outb631bc66"" ""$errb631bc66""; trap 'rm ""$outb631bc66"" ""$errb631bc66""' EXIT; tee '/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/stdout' < ""$outb631bc66"" &; tee '/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/stderr' < ""$errb631bc66"" >&2 &; (; cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution. echo $(( 2 + 1 )); ) ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-561687838
https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-561687838:1980,Availability,echo,echo,1980,"_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution. ); outb631bc66=""${tmpDir}/out.$$"" errb631bc66=""${tmpDir}/err.$$""; mkfifo ""$outb631bc66"" ""$errb631bc66""; trap 'rm ""$outb631bc66"" ""$errb631bc66""' EXIT; tee '/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/stdout' < ""$outb631bc66"" &; tee '/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/stderr' < ""$errb631bc66"" >&2 &; (; cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution. echo $(( 2 + 1 )); ) > ""$outb631bc66"" 2> ""$errb631bc66""; echo $? > /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/rc.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution; sleep 5 && sync. ); mv /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/rc.tmp /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-81",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-561687838
https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-561687838:2037,Availability,echo,echo,2037,"_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution. ); outb631bc66=""${tmpDir}/out.$$"" errb631bc66=""${tmpDir}/err.$$""; mkfifo ""$outb631bc66"" ""$errb631bc66""; trap 'rm ""$outb631bc66"" ""$errb631bc66""' EXIT; tee '/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/stdout' < ""$outb631bc66"" &; tee '/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/stderr' < ""$errb631bc66"" >&2 &; (; cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution. echo $(( 2 + 1 )); ) > ""$outb631bc66"" 2> ""$errb631bc66""; echo $? > /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/rc.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution; sleep 5 && sync. ); mv /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/rc.tmp /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-81",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-561687838
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543828698:128,Safety,avoid,avoid,128,"Actually... @aednichols has some ""run a single workflow mode"" tests - it might be nice to add this situation to those so we can avoid any regressions here. Does that sounds feasible to you Adam (or is it a much bigger change)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543828698
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543828698:62,Testability,test,tests,62,"Actually... @aednichols has some ""run a single workflow mode"" tests - it might be nice to add this situation to those so we can avoid any regressions here. Does that sounds feasible to you Adam (or is it a much bigger change)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543828698
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543873275:27,Testability,test,test,27,"The single workflow runner test seems like a good place to test this, indeed. It lives at `src/ci/bin/testSingleWorkflowRunner.sh`. Following the examples of the other test cases in that file, it should be possible to assert the tool emits the expected output under the conditions described in the ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543873275
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543873275:59,Testability,test,test,59,"The single workflow runner test seems like a good place to test this, indeed. It lives at `src/ci/bin/testSingleWorkflowRunner.sh`. Following the examples of the other test cases in that file, it should be possible to assert the tool emits the expected output under the conditions described in the ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543873275
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543873275:102,Testability,test,testSingleWorkflowRunner,102,"The single workflow runner test seems like a good place to test this, indeed. It lives at `src/ci/bin/testSingleWorkflowRunner.sh`. Following the examples of the other test cases in that file, it should be possible to assert the tool emits the expected output under the conditions described in the ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543873275
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543873275:168,Testability,test,test,168,"The single workflow runner test seems like a good place to test this, indeed. It lives at `src/ci/bin/testSingleWorkflowRunner.sh`. Following the examples of the other test cases in that file, it should be possible to assert the tool emits the expected output under the conditions described in the ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543873275
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543873275:218,Testability,assert,assert,218,"The single workflow runner test seems like a good place to test this, indeed. It lives at `src/ci/bin/testSingleWorkflowRunner.sh`. Following the examples of the other test cases in that file, it should be possible to assert the tool emits the expected output under the conditions described in the ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543873275
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543922163:110,Availability,error,errors,110,"The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543922163
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543922163:246,Availability,error,errors,246,"The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543922163
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:112,Availability,error,errors,112,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:248,Availability,error,errors,248,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:376,Integrability,message,messages,376,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:389,Security,validat,validation,389,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:593,Security,validat,validateRunArguments,593,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:688,Security,validat,validateRunArguments,688,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:344,Testability,test,tests,344,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:372,Testability,log,log,372,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:432,Testability,log,logging,432,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:555,Testability,log,logback,555,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:720,Testability,log,logback,720,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556
https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:800,Testability,log,logback,800,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556
https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548045062:67,Energy Efficiency,green,green,67,Good news: I think if you rebase this on develop the builds may go green 🤞,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548045062
https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305:509,Deployability,upgrade,upgrade,509,Good news first: the `centaurPapiV2` build passed 🎉 ; Bad news: the other 4 PAPI v2 builds failed 😢 . The horicromtal builds are running with Cromwell configured to Carbonite but Centaur not configured to wait for Carboniting. While this might not have been intentional it's IMHO kind of appealing as a real-world scenario. I have no idea why the builds seem to hang until timeout as if some workflows were never completing. Conformance: `Unexpected failing tests: (6)`. No idea what happened with the engine upgrade test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305
https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305:151,Modifiability,config,configured,151,Good news first: the `centaurPapiV2` build passed 🎉 ; Bad news: the other 4 PAPI v2 builds failed 😢 . The horicromtal builds are running with Cromwell configured to Carbonite but Centaur not configured to wait for Carboniting. While this might not have been intentional it's IMHO kind of appealing as a real-world scenario. I have no idea why the builds seem to hang until timeout as if some workflows were never completing. Conformance: `Unexpected failing tests: (6)`. No idea what happened with the engine upgrade test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305
https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305:191,Modifiability,config,configured,191,Good news first: the `centaurPapiV2` build passed 🎉 ; Bad news: the other 4 PAPI v2 builds failed 😢 . The horicromtal builds are running with Cromwell configured to Carbonite but Centaur not configured to wait for Carboniting. While this might not have been intentional it's IMHO kind of appealing as a real-world scenario. I have no idea why the builds seem to hang until timeout as if some workflows were never completing. Conformance: `Unexpected failing tests: (6)`. No idea what happened with the engine upgrade test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305
https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305:373,Safety,timeout,timeout,373,Good news first: the `centaurPapiV2` build passed 🎉 ; Bad news: the other 4 PAPI v2 builds failed 😢 . The horicromtal builds are running with Cromwell configured to Carbonite but Centaur not configured to wait for Carboniting. While this might not have been intentional it's IMHO kind of appealing as a real-world scenario. I have no idea why the builds seem to hang until timeout as if some workflows were never completing. Conformance: `Unexpected failing tests: (6)`. No idea what happened with the engine upgrade test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305
https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305:458,Testability,test,tests,458,Good news first: the `centaurPapiV2` build passed 🎉 ; Bad news: the other 4 PAPI v2 builds failed 😢 . The horicromtal builds are running with Cromwell configured to Carbonite but Centaur not configured to wait for Carboniting. While this might not have been intentional it's IMHO kind of appealing as a real-world scenario. I have no idea why the builds seem to hang until timeout as if some workflows were never completing. Conformance: `Unexpected failing tests: (6)`. No idea what happened with the engine upgrade test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305
https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305:517,Testability,test,test,517,Good news first: the `centaurPapiV2` build passed 🎉 ; Bad news: the other 4 PAPI v2 builds failed 😢 . The horicromtal builds are running with Cromwell configured to Carbonite but Centaur not configured to wait for Carboniting. While this might not have been intentional it's IMHO kind of appealing as a real-world scenario. I have no idea why the builds seem to hang until timeout as if some workflows were never completing. Conformance: `Unexpected failing tests: (6)`. No idea what happened with the engine upgrade test.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548148305
https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:244,Deployability,integrat,integrations,244,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633
https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:244,Integrability,integrat,integrations,244,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633
https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:101,Testability,test,tests,101,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633
https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:198,Testability,test,tests,198,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633
https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:223,Testability,test,tests,223,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633
https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:257,Testability,test,testing,257,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633
https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:296,Testability,test,tests,296,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633
https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633:64,Usability,undo,undoing,64,"I don't believe the codecov in this case (check for yourself by undoing the changes and watching the tests fail). Also see the follow up branch `cjl_describe_then_run_centaur` which adds `describe` tests to all the centaur tests, so we'll have integrations testing of `/describe` as well as unit tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5244#issuecomment-547148633
https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-561689371:319,Deployability,release,releases,319,"The workflow options are not stored in metadata directly, but there is a record of how Cromwell behaved based on their values. For call caching, this is `allowResultReuse` and `effectiveCallCachingMode`. I _think_ we still record call caching info if the default is used. See https://github.com/broadinstitute/cromwell/releases/tag/25",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-561689371
https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-773541913:805,Modifiability,config,config,805,"Is there an example of how to set call caching to true and allowResultReuse to true in the `-o options` file when running a workflow. I am looking for examples and the documentation and I just keep guessing. Both ways of setting outside of `callCaching` and inside still have my `-m metadata` file showing below:; ```. ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. options.json file:; ```; {; 	""default_runtime_attributes"": {; 		""write_to_cache"": true,; 		""read_from_cache"": true,; 		""system.file-hash-cache"": true,; 		""allowResultReuse"" : true,; 		""callCaching"": {; 			""hit"": false,; 			""effectiveCallCachingMode"": ""ReadAndWriteCache"",; 			""result"": ""Cache Miss"",; 			""allowResultReuse"": true; 		}; 	}; }; ```. EDIT:; This only worked by creating a config file with lines:; ```; call-caching {; enabled = true; }; ```; If this is required, shouldn't this documentation [page](https://cromwell.readthedocs.io/en/stable/wf_options/Overview/) include that information under section ""Call Caching Options""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-773541913
https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-773541913:557,Performance,cache,cache,557,"Is there an example of how to set call caching to true and allowResultReuse to true in the `-o options` file when running a workflow. I am looking for examples and the documentation and I just keep guessing. Both ways of setting outside of `callCaching` and inside still have my `-m metadata` file showing below:; ```. ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. options.json file:; ```; {; 	""default_runtime_attributes"": {; 		""write_to_cache"": true,; 		""read_from_cache"": true,; 		""system.file-hash-cache"": true,; 		""allowResultReuse"" : true,; 		""callCaching"": {; 			""hit"": false,; 			""effectiveCallCachingMode"": ""ReadAndWriteCache"",; 			""result"": ""Cache Miss"",; 			""allowResultReuse"": true; 		}; 	}; }; ```. EDIT:; This only worked by creating a config file with lines:; ```; call-caching {; enabled = true; }; ```; If this is required, shouldn't this documentation [page](https://cromwell.readthedocs.io/en/stable/wf_options/Overview/) include that information under section ""Call Caching Options""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-773541913
https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-773541913:707,Performance,Cache,Cache,707,"Is there an example of how to set call caching to true and allowResultReuse to true in the `-o options` file when running a workflow. I am looking for examples and the documentation and I just keep guessing. Both ways of setting outside of `callCaching` and inside still have my `-m metadata` file showing below:; ```. ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. options.json file:; ```; {; 	""default_runtime_attributes"": {; 		""write_to_cache"": true,; 		""read_from_cache"": true,; 		""system.file-hash-cache"": true,; 		""allowResultReuse"" : true,; 		""callCaching"": {; 			""hit"": false,; 			""effectiveCallCachingMode"": ""ReadAndWriteCache"",; 			""result"": ""Cache Miss"",; 			""allowResultReuse"": true; 		}; 	}; }; ```. EDIT:; This only worked by creating a config file with lines:; ```; call-caching {; enabled = true; }; ```; If this is required, shouldn't this documentation [page](https://cromwell.readthedocs.io/en/stable/wf_options/Overview/) include that information under section ""Call Caching Options""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-773541913
https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-773541913:552,Security,hash,hash-cache,552,"Is there an example of how to set call caching to true and allowResultReuse to true in the `-o options` file when running a workflow. I am looking for examples and the documentation and I just keep guessing. Both ways of setting outside of `callCaching` and inside still have my `-m metadata` file showing below:; ```. ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. options.json file:; ```; {; 	""default_runtime_attributes"": {; 		""write_to_cache"": true,; 		""read_from_cache"": true,; 		""system.file-hash-cache"": true,; 		""allowResultReuse"" : true,; 		""callCaching"": {; 			""hit"": false,; 			""effectiveCallCachingMode"": ""ReadAndWriteCache"",; 			""result"": ""Cache Miss"",; 			""allowResultReuse"": true; 		}; 	}; }; ```. EDIT:; This only worked by creating a config file with lines:; ```; call-caching {; enabled = true; }; ```; If this is required, shouldn't this documentation [page](https://cromwell.readthedocs.io/en/stable/wf_options/Overview/) include that information under section ""Call Caching Options""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5246#issuecomment-773541913
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-547681039:40,Deployability,configurat,configuration,40,"Ah ok, I'm not familiar with how to add configuration options. Depending on how involved a process it is / if there is some example from an earlier pull request I could follow, I would be happy to give it a shot. If not, I'm also ok with shelving this for now and just using an ad-hoc build in the meantime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-547681039
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-547681039:63,Integrability,Depend,Depending,63,"Ah ok, I'm not familiar with how to add configuration options. Depending on how involved a process it is / if there is some example from an earlier pull request I could follow, I would be happy to give it a shot. If not, I'm also ok with shelving this for now and just using an ad-hoc build in the meantime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-547681039
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-547681039:40,Modifiability,config,configuration,40,"Ah ok, I'm not familiar with how to add configuration options. Depending on how involved a process it is / if there is some example from an earlier pull request I could follow, I would be happy to give it a shot. If not, I'm also ok with shelving this for now and just using an ad-hoc build in the meantime.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-547681039
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-548064730:61,Deployability,configurat,configuration,61,"It shouldn't be too involved - there's an example of backend configuration being used in the same file you're updating [here](https://github.com/broadinstitute/cromwell/blob/develop/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L132), which means it could be a per-backend option (see where the value could be set within a backend config [here](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf#L379)). . Does that give you what you needed?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-548064730
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-548064730:61,Modifiability,config,configuration,61,"It shouldn't be too involved - there's an example of backend configuration being used in the same file you're updating [here](https://github.com/broadinstitute/cromwell/blob/develop/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L132), which means it could be a per-backend option (see where the value could be set within a backend config [here](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf#L379)). . Does that give you what you needed?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-548064730
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-548064730:368,Modifiability,config,config,368,"It shouldn't be too involved - there's an example of backend configuration being used in the same file you're updating [here](https://github.com/broadinstitute/cromwell/blob/develop/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L132), which means it could be a per-backend option (see where the value could be set within a backend config [here](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf#L379)). . Does that give you what you needed?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-548064730
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:53,Deployability,configurat,configuration,53,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:107,Deployability,update,updated,107,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:119,Deployability,configurat,configuration,119,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:265,Deployability,update,updated,265,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:273,Deployability,configurat,configuration,273,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:53,Modifiability,config,configuration,53,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:119,Modifiability,config,configuration,119,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:273,Modifiability,config,configuration,273,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:100,Usability,simpl,simply,100,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892
https://github.com/broadinstitute/cromwell/pull/5254#issuecomment-548511548:0,Integrability,Depend,Depending,0,Depending on how many words you have left... is this the right level to explain where the dotted lined call-input expressions come from (/what they're for)?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5254#issuecomment-548511548
https://github.com/broadinstitute/cromwell/pull/5255#issuecomment-549571920:51,Integrability,message,messages,51,"@grsterin perhaps arrows showing which actors send messages to others? perhaps ""sends messages to""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5255#issuecomment-549571920
https://github.com/broadinstitute/cromwell/pull/5255#issuecomment-549571920:86,Integrability,message,messages,86,"@grsterin perhaps arrows showing which actors send messages to others? perhaps ""sends messages to""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5255#issuecomment-549571920
https://github.com/broadinstitute/cromwell/pull/5255#issuecomment-550390960:22,Deployability,update,update,22,@grsterin remember to update the PR base before merging.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5255#issuecomment-550390960
https://github.com/broadinstitute/cromwell/issues/5256#issuecomment-548825348:304,Availability,error,error,304,"Hi @DSLituiev, thanks for your question and welcome to our repo!. I think I know the problem – the inputs in a call do not need the type declarations (like `File` and `String`). Give this a try:; ```; call touBam.unMap {; input:; mapped_bam=mapped_bam,; unmapped_base=bam_base; }; ```; Unfortunately the error is pretty confusing because when a workflow doesn't conform to the grammar, the parser has a very hard time describing what's wrong in meaningful terms (it just kind of freaks out).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5256#issuecomment-548825348
https://github.com/broadinstitute/cromwell/pull/5259#issuecomment-555230954:68,Testability,test,tests,68,Created https://github.com/broadinstitute/cromwell/pull/5289 to run tests,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5259#issuecomment-555230954
https://github.com/broadinstitute/cromwell/pull/5260#issuecomment-550346347:59,Availability,error,error,59,hmm so the whole subworkflow thing was a red 🐟 because the error message was so bad? 😬,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260#issuecomment-550346347
https://github.com/broadinstitute/cromwell/pull/5260#issuecomment-550346347:65,Integrability,message,message,65,hmm so the whole subworkflow thing was a red 🐟 because the error message was so bad? 😬,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260#issuecomment-550346347
https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551243707:286,Performance,cache,cache,286,"#### Comment 1. Could you add a key to indicate what the various colors and line-types mean?. #### Comment 2. All of the actors are ultimately descended from the CromwellRootActor (or should be, except the server or cmdline actors which create it) - I don't see that shown for the call cache actors group. Is that genuinely true (scary!), or do they need a line of ancestry coming in from somewhere?. #### Comment 3 ; (this one is more TOL-y...). Is there an even higher level ""10,000 foot"" view with functional units shown linked together? Eg you've drawn some of the boxes in different colors, presumably those would be reasonable candidates for boxes on some higher-level view?. The reason I ask is, (a) this diagram is scarier than I realized! It'd be nice to have some higher-level context before seeing the whole thing in full... or (b) I had to zoom in really far in order to make out the words in the individual boxes - I wonder if one context-setting diagram would then let us have (say) 5 different 5,000 foot magnifications for individual subsystems, rather than a single all-in-one diagram? (although EDIT... that said, I do find the big-picture diagram kind of awesome, maybe we could get it printed out on giant paper somewhere... 🤔)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551243707
https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551264545:12,Energy Efficiency,green,green,12,"These light green ones don't seem to have any parents:. <img width=""881"" alt=""Screen Shot 2019-11-07 at 4 06 57 PM"" src=""https://user-images.githubusercontent.com/13006282/68427727-bbff5e80-0178-11ea-9f81-b47efeb616e9.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551264545
https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551306283:28,Deployability,update,updated,28,"@rsasch @cjllanwarne I have updated the diagram and added a key. Regarding (Chris's) Comment 3, we can surely go 5000ft higher and create a 10000ft diagram. I will take a look. Somehow 10000ft higher is harder to visualize! :) ; Update: Ticket for The 10000ft view of actor system has been created. JIRA ticket [WA-67](https://broadworkbench.atlassian.net/browse/WA-67)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551306283
https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551306283:229,Deployability,Update,Update,229,"@rsasch @cjllanwarne I have updated the diagram and added a key. Regarding (Chris's) Comment 3, we can surely go 5000ft higher and create a 10000ft diagram. I will take a look. Somehow 10000ft higher is harder to visualize! :) ; Update: Ticket for The 10000ft view of actor system has been created. JIRA ticket [WA-67](https://broadworkbench.atlassian.net/browse/WA-67)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551306283
https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554023387:278,Availability,error,error,278,I'd suggest to avoid overlapping `database interaction` arrows with arrows of other types on the diagram. Something like this:; ![image](https://user-images.githubusercontent.com/4853242/68885542-5d3f6500-06e3-11ea-992e-ed9b6d0f3578.png). Looks like something happened with the error between `ServiceRegistryActor` and `HybridMetadataServiceActor`:; ![image](https://user-images.githubusercontent.com/4853242/68885905-0c7c3c00-06e4-11ea-9840-ddb24591df85.png),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554023387
https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554023387:15,Safety,avoid,avoid,15,I'd suggest to avoid overlapping `database interaction` arrows with arrows of other types on the diagram. Something like this:; ![image](https://user-images.githubusercontent.com/4853242/68885542-5d3f6500-06e3-11ea-992e-ed9b6d0f3578.png). Looks like something happened with the error between `ServiceRegistryActor` and `HybridMetadataServiceActor`:; ![image](https://user-images.githubusercontent.com/4853242/68885905-0c7c3c00-06e4-11ea-9840-ddb24591df85.png),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554023387
https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554031085:288,Usability,clear,clear,288,"![image](https://user-images.githubusercontent.com/4853242/68887215-9b8a5380-06e6-11ea-9870-8fd701773573.png); I'm still not sure what are `Execution store` and `Value store` from the picture above. Color coding suggests that they are `connected components`, but the meaning is still not clear to me. Also, maybe `connected component` is not the best term in this case, since its strong ties with graph theory (unless this is exactly what it means here)?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554031085
https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554533453:17,Deployability,update,updated,17,@grsterin I have updated the key to replace 'Connected Components' with 'In-memory Storage'.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554533453
https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551280306:41,Modifiability,config,config,41,"BA-6088 mentions `carboniter-start-date` config field, however in this PR `minimum-summary-entry-id` numeric field is used instead. This is a little confusing. ; Also, what is the reason for using `minimum-summary-entry-id` in favor of date?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551280306
https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551305427:490,Usability,simpl,simplest,490,"@grsterin the change from date to ID came up during a tech talk - it's because in our production database with several million rows, a timestamp comparison based query was extremely slow (~15s) whereas an ID based query (which are ordered, and indexed) was extremely fast (~0.04s). There probably *are* clever ways to work around that, but since we'll be the ones setting this value, and since we'll only need to do the calculation once, this slight change in the feature was chosen as the simplest way forward.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551305427
https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551387140:567,Deployability,update,update,567,"> @grsterin the change from date to ID came up during a tech talk - it's because in our production database with several million rows, a timestamp comparison based query was extremely slow (~15s) whereas an ID based query (which are ordered, and indexed) was extremely fast (~0.04s).; > ; > There probably _are_ clever ways to work around that, but since we'll be the ones setting this value, and since we'll only need to do the calculation once, this slight change in the feature was chosen as the simplest way forward. Sounds good. But in this case, I'd suggest to update ticket description in order to eliminate discrepancy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551387140
https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551387140:499,Usability,simpl,simplest,499,"> @grsterin the change from date to ID came up during a tech talk - it's because in our production database with several million rows, a timestamp comparison based query was extremely slow (~15s) whereas an ID based query (which are ordered, and indexed) was extremely fast (~0.04s).; > ; > There probably _are_ clever ways to work around that, but since we'll be the ones setting this value, and since we'll only need to do the calculation once, this slight change in the feature was chosen as the simplest way forward. Sounds good. But in this case, I'd suggest to update ticket description in order to eliminate discrepancy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5267#issuecomment-551387140
https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551225460:74,Deployability,integrat,integrating,74,"> Also I'm not the ticket author but I thought that was intended to cover integrating ""compressed at rest"" writes into carboniting?. oops, accidentally reverted that part during testing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551225460
https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551225460:74,Integrability,integrat,integrating,74,"> Also I'm not the ticket author but I thought that was intended to cover integrating ""compressed at rest"" writes into carboniting?. oops, accidentally reverted that part during testing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551225460
https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551225460:178,Testability,test,testing,178,"> Also I'm not the ticket author but I thought that was intended to cover integrating ""compressed at rest"" writes into carboniting?. oops, accidentally reverted that part during testing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551225460
https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551232213:496,Integrability,bridg,bridge,496,"It's a theoretical possibility that we might write compressed to local disk, but unless I'm missing something, I don't think there's any way in Cromwell today that it could happen?. So my $0.02 would be that if we have an option in the IoActor to compress a file on write, then the least surprising outcome is that it gets compressed - regardless of FS. If for whatever reason in the future we want to write then read compressed files to local disk, we can always cross the decompression-on-read bridge when we come to it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551232213
https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551233754:62,Integrability,message,message,62,"Regarding testing, we could at the very least interrogate the message going to the IoActor in the `CarbonitingMetadataFreezerActorSpec` to make sure it's asking for the file to be compressed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551233754
https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551233754:10,Testability,test,testing,10,"Regarding testing, we could at the very least interrogate the message going to the IoActor in the `CarbonitingMetadataFreezerActorSpec` to make sure it's asking for the file to be compressed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551233754
https://github.com/broadinstitute/cromwell/pull/5270#issuecomment-583121715:29,Availability,failure,failure,29,"It looks like the current CI failure (below) is spurious and unrelated to the changes in this PR. The [CI clone of this PR](https://github.com/broadinstitute/cromwell/pull/5314) is successfully passing CI checks. . $ src/ci/bin/test.sh; src/ci/bin/test.inc.sh: line 645: /home/travis/build/broadinstitute/cromwell/src/ci/bin/testCentaurHoricromtalPapiV2.sh: No such file or directory; The command ""src/ci/bin/test.sh"" exited with 1.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270#issuecomment-583121715
https://github.com/broadinstitute/cromwell/pull/5270#issuecomment-583121715:228,Testability,test,test,228,"It looks like the current CI failure (below) is spurious and unrelated to the changes in this PR. The [CI clone of this PR](https://github.com/broadinstitute/cromwell/pull/5314) is successfully passing CI checks. . $ src/ci/bin/test.sh; src/ci/bin/test.inc.sh: line 645: /home/travis/build/broadinstitute/cromwell/src/ci/bin/testCentaurHoricromtalPapiV2.sh: No such file or directory; The command ""src/ci/bin/test.sh"" exited with 1.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270#issuecomment-583121715
https://github.com/broadinstitute/cromwell/pull/5270#issuecomment-583121715:248,Testability,test,test,248,"It looks like the current CI failure (below) is spurious and unrelated to the changes in this PR. The [CI clone of this PR](https://github.com/broadinstitute/cromwell/pull/5314) is successfully passing CI checks. . $ src/ci/bin/test.sh; src/ci/bin/test.inc.sh: line 645: /home/travis/build/broadinstitute/cromwell/src/ci/bin/testCentaurHoricromtalPapiV2.sh: No such file or directory; The command ""src/ci/bin/test.sh"" exited with 1.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270#issuecomment-583121715
https://github.com/broadinstitute/cromwell/pull/5270#issuecomment-583121715:409,Testability,test,test,409,"It looks like the current CI failure (below) is spurious and unrelated to the changes in this PR. The [CI clone of this PR](https://github.com/broadinstitute/cromwell/pull/5314) is successfully passing CI checks. . $ src/ci/bin/test.sh; src/ci/bin/test.inc.sh: line 645: /home/travis/build/broadinstitute/cromwell/src/ci/bin/testCentaurHoricromtalPapiV2.sh: No such file or directory; The command ""src/ci/bin/test.sh"" exited with 1.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270#issuecomment-583121715
https://github.com/broadinstitute/cromwell/pull/5270#issuecomment-583144930:10,Testability,test,test,10,"Yes, that test script was renamed. The CI clone PR was rebased on the latest develop where the problem was fixed, rebasing should fix the problem here as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270#issuecomment-583144930
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-561682355:316,Availability,avail,available,316,"Hello and thanks for your question. We do not have any current plans to support SQLite (though I personally think it's a fantastic product!). If you can help us understand the scenario in which you're running Cromwell we may be able to offer some advice. In particular, it's surprising that a MySQL database is ""not available or difficult to get""; the typical scenario we envision for running Cromwell with persistence is a Cromwell Docker plus a MySQL Docker.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-561682355
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002:1228,Availability,down,down,1228,"This is also something we would want for our cluster. Users are not allowed to run docker containers on our HPC (that would give them root access :scream:). So we use cromwell on the command line instead. We modified our cromwell configuration to run with singularity containers instead of docker containers. Setting up a MySQL server somewhere is not easy for the average user. Furthermore we used to set up a MySQL server for all users on the cluster, but that meant they had to share a database user and password (they were all using the same configuration). This caused a lot of issues. . Mostly cromwell is run project based. So the call-caching is only interesting for that particular project. Also using a file-based database will automatically ensure that only people with rights to the share the project is on will have access. This is also of importance in a shared cluster environment. At LUMC we currently implement this by using the HSQLDB in-memory database with a persistence file. This has some disadvantages:; 1. Cromwell needs more memory compared to using a MySQL server; 2. The HSQLDB persistence files are huge, badly compressed (when compression is used). 3 GB is normal. Tarring and zipping will get this down to under <50 mb... ; 3. It is slower than using a MySQL server. I think that SQLite will solve problem 1 and 2. (3 is inherent to using a file-based DB). . Having a database is better than to have nothing at all, which is why many users are pining for SQLite. When looking into it I found the option for the persistence file. Although SQLite will be much better, this does not require any extra effort from the Cromwell developers and can already help out a lot of users. I will document how we did this in the Cromwell documentation so everyone can use this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002:230,Deployability,configurat,configuration,230,"This is also something we would want for our cluster. Users are not allowed to run docker containers on our HPC (that would give them root access :scream:). So we use cromwell on the command line instead. We modified our cromwell configuration to run with singularity containers instead of docker containers. Setting up a MySQL server somewhere is not easy for the average user. Furthermore we used to set up a MySQL server for all users on the cluster, but that meant they had to share a database user and password (they were all using the same configuration). This caused a lot of issues. . Mostly cromwell is run project based. So the call-caching is only interesting for that particular project. Also using a file-based database will automatically ensure that only people with rights to the share the project is on will have access. This is also of importance in a shared cluster environment. At LUMC we currently implement this by using the HSQLDB in-memory database with a persistence file. This has some disadvantages:; 1. Cromwell needs more memory compared to using a MySQL server; 2. The HSQLDB persistence files are huge, badly compressed (when compression is used). 3 GB is normal. Tarring and zipping will get this down to under <50 mb... ; 3. It is slower than using a MySQL server. I think that SQLite will solve problem 1 and 2. (3 is inherent to using a file-based DB). . Having a database is better than to have nothing at all, which is why many users are pining for SQLite. When looking into it I found the option for the persistence file. Although SQLite will be much better, this does not require any extra effort from the Cromwell developers and can already help out a lot of users. I will document how we did this in the Cromwell documentation so everyone can use this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002:546,Deployability,configurat,configuration,546,"This is also something we would want for our cluster. Users are not allowed to run docker containers on our HPC (that would give them root access :scream:). So we use cromwell on the command line instead. We modified our cromwell configuration to run with singularity containers instead of docker containers. Setting up a MySQL server somewhere is not easy for the average user. Furthermore we used to set up a MySQL server for all users on the cluster, but that meant they had to share a database user and password (they were all using the same configuration). This caused a lot of issues. . Mostly cromwell is run project based. So the call-caching is only interesting for that particular project. Also using a file-based database will automatically ensure that only people with rights to the share the project is on will have access. This is also of importance in a shared cluster environment. At LUMC we currently implement this by using the HSQLDB in-memory database with a persistence file. This has some disadvantages:; 1. Cromwell needs more memory compared to using a MySQL server; 2. The HSQLDB persistence files are huge, badly compressed (when compression is used). 3 GB is normal. Tarring and zipping will get this down to under <50 mb... ; 3. It is slower than using a MySQL server. I think that SQLite will solve problem 1 and 2. (3 is inherent to using a file-based DB). . Having a database is better than to have nothing at all, which is why many users are pining for SQLite. When looking into it I found the option for the persistence file. Although SQLite will be much better, this does not require any extra effort from the Cromwell developers and can already help out a lot of users. I will document how we did this in the Cromwell documentation so everyone can use this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002:230,Modifiability,config,configuration,230,"This is also something we would want for our cluster. Users are not allowed to run docker containers on our HPC (that would give them root access :scream:). So we use cromwell on the command line instead. We modified our cromwell configuration to run with singularity containers instead of docker containers. Setting up a MySQL server somewhere is not easy for the average user. Furthermore we used to set up a MySQL server for all users on the cluster, but that meant they had to share a database user and password (they were all using the same configuration). This caused a lot of issues. . Mostly cromwell is run project based. So the call-caching is only interesting for that particular project. Also using a file-based database will automatically ensure that only people with rights to the share the project is on will have access. This is also of importance in a shared cluster environment. At LUMC we currently implement this by using the HSQLDB in-memory database with a persistence file. This has some disadvantages:; 1. Cromwell needs more memory compared to using a MySQL server; 2. The HSQLDB persistence files are huge, badly compressed (when compression is used). 3 GB is normal. Tarring and zipping will get this down to under <50 mb... ; 3. It is slower than using a MySQL server. I think that SQLite will solve problem 1 and 2. (3 is inherent to using a file-based DB). . Having a database is better than to have nothing at all, which is why many users are pining for SQLite. When looking into it I found the option for the persistence file. Although SQLite will be much better, this does not require any extra effort from the Cromwell developers and can already help out a lot of users. I will document how we did this in the Cromwell documentation so everyone can use this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002:546,Modifiability,config,configuration,546,"This is also something we would want for our cluster. Users are not allowed to run docker containers on our HPC (that would give them root access :scream:). So we use cromwell on the command line instead. We modified our cromwell configuration to run with singularity containers instead of docker containers. Setting up a MySQL server somewhere is not easy for the average user. Furthermore we used to set up a MySQL server for all users on the cluster, but that meant they had to share a database user and password (they were all using the same configuration). This caused a lot of issues. . Mostly cromwell is run project based. So the call-caching is only interesting for that particular project. Also using a file-based database will automatically ensure that only people with rights to the share the project is on will have access. This is also of importance in a shared cluster environment. At LUMC we currently implement this by using the HSQLDB in-memory database with a persistence file. This has some disadvantages:; 1. Cromwell needs more memory compared to using a MySQL server; 2. The HSQLDB persistence files are huge, badly compressed (when compression is used). 3 GB is normal. Tarring and zipping will get this down to under <50 mb... ; 3. It is slower than using a MySQL server. I think that SQLite will solve problem 1 and 2. (3 is inherent to using a file-based DB). . Having a database is better than to have nothing at all, which is why many users are pining for SQLite. When looking into it I found the option for the persistence file. Although SQLite will be much better, this does not require any extra effort from the Cromwell developers and can already help out a lot of users. I will document how we did this in the Cromwell documentation so everyone can use this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002:139,Security,access,access,139,"This is also something we would want for our cluster. Users are not allowed to run docker containers on our HPC (that would give them root access :scream:). So we use cromwell on the command line instead. We modified our cromwell configuration to run with singularity containers instead of docker containers. Setting up a MySQL server somewhere is not easy for the average user. Furthermore we used to set up a MySQL server for all users on the cluster, but that meant they had to share a database user and password (they were all using the same configuration). This caused a lot of issues. . Mostly cromwell is run project based. So the call-caching is only interesting for that particular project. Also using a file-based database will automatically ensure that only people with rights to the share the project is on will have access. This is also of importance in a shared cluster environment. At LUMC we currently implement this by using the HSQLDB in-memory database with a persistence file. This has some disadvantages:; 1. Cromwell needs more memory compared to using a MySQL server; 2. The HSQLDB persistence files are huge, badly compressed (when compression is used). 3 GB is normal. Tarring and zipping will get this down to under <50 mb... ; 3. It is slower than using a MySQL server. I think that SQLite will solve problem 1 and 2. (3 is inherent to using a file-based DB). . Having a database is better than to have nothing at all, which is why many users are pining for SQLite. When looking into it I found the option for the persistence file. Although SQLite will be much better, this does not require any extra effort from the Cromwell developers and can already help out a lot of users. I will document how we did this in the Cromwell documentation so everyone can use this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002:507,Security,password,password,507,"This is also something we would want for our cluster. Users are not allowed to run docker containers on our HPC (that would give them root access :scream:). So we use cromwell on the command line instead. We modified our cromwell configuration to run with singularity containers instead of docker containers. Setting up a MySQL server somewhere is not easy for the average user. Furthermore we used to set up a MySQL server for all users on the cluster, but that meant they had to share a database user and password (they were all using the same configuration). This caused a lot of issues. . Mostly cromwell is run project based. So the call-caching is only interesting for that particular project. Also using a file-based database will automatically ensure that only people with rights to the share the project is on will have access. This is also of importance in a shared cluster environment. At LUMC we currently implement this by using the HSQLDB in-memory database with a persistence file. This has some disadvantages:; 1. Cromwell needs more memory compared to using a MySQL server; 2. The HSQLDB persistence files are huge, badly compressed (when compression is used). 3 GB is normal. Tarring and zipping will get this down to under <50 mb... ; 3. It is slower than using a MySQL server. I think that SQLite will solve problem 1 and 2. (3 is inherent to using a file-based DB). . Having a database is better than to have nothing at all, which is why many users are pining for SQLite. When looking into it I found the option for the persistence file. Although SQLite will be much better, this does not require any extra effort from the Cromwell developers and can already help out a lot of users. I will document how we did this in the Cromwell documentation so everyone can use this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002:829,Security,access,access,829,"This is also something we would want for our cluster. Users are not allowed to run docker containers on our HPC (that would give them root access :scream:). So we use cromwell on the command line instead. We modified our cromwell configuration to run with singularity containers instead of docker containers. Setting up a MySQL server somewhere is not easy for the average user. Furthermore we used to set up a MySQL server for all users on the cluster, but that meant they had to share a database user and password (they were all using the same configuration). This caused a lot of issues. . Mostly cromwell is run project based. So the call-caching is only interesting for that particular project. Also using a file-based database will automatically ensure that only people with rights to the share the project is on will have access. This is also of importance in a shared cluster environment. At LUMC we currently implement this by using the HSQLDB in-memory database with a persistence file. This has some disadvantages:; 1. Cromwell needs more memory compared to using a MySQL server; 2. The HSQLDB persistence files are huge, badly compressed (when compression is used). 3 GB is normal. Tarring and zipping will get this down to under <50 mb... ; 3. It is slower than using a MySQL server. I think that SQLite will solve problem 1 and 2. (3 is inherent to using a file-based DB). . Having a database is better than to have nothing at all, which is why many users are pining for SQLite. When looking into it I found the option for the persistence file. Although SQLite will be much better, this does not require any extra effort from the Cromwell developers and can already help out a lot of users. I will document how we did this in the Cromwell documentation so everyone can use this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564721678:551,Deployability,update,updated,551,"@aednichols . Thanks for your response. We're in a University and the sys admins are worried that with users submitting thousands of jobs, depending on what cromwell actually sends MySQL there may be quite a bit of overhead. Do you have a link to what Cromwell stores in the MySQL database? That may assuage some of their concerns. Using SQLite would just be easier, users can create a local instance and be on with it. @rhpvorderman . That sounds like a workable option. That sounds exactly like our situation, it would be great if you could keep us updated! It would definitely be very useful for us!. Thanks,; Bobbie.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564721678
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564721678:139,Integrability,depend,depending,139,"@aednichols . Thanks for your response. We're in a University and the sys admins are worried that with users submitting thousands of jobs, depending on what cromwell actually sends MySQL there may be quite a bit of overhead. Do you have a link to what Cromwell stores in the MySQL database? That may assuage some of their concerns. Using SQLite would just be easier, users can create a local instance and be on with it. @rhpvorderman . That sounds like a workable option. That sounds exactly like our situation, it would be great if you could keep us updated! It would definitely be very useful for us!. Thanks,; Bobbie.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564721678
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564911759:93,Modifiability,Config,Configuring,93,"@ParkvilleData It is now in the development docs. https://cromwell.readthedocs.io/en/develop/Configuring/#database. The formatting is a bit off, but that will be fixed. It should be in the stable docs (the default cromwell.readthedocs.io) from version 48 onwards.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564911759
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-571506055:292,Testability,test,tested,292,"I just want to ask, is using the local file-based database only supported for call caching from version 48 onwards?; I'm running both cromwell 43 and 47, both versions appear to use the file-based database fine for running a workflow, but 43 does not seem to use it for call caching (haven't tested with 47 yet).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-571506055
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:183,Availability,error,error,183,"Hi, I've been running Cromwell with the file-based DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:1577,Availability,Error,Error,1577,"seActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:25",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:1721,Availability,ERROR,ERROR,1721,".$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:250); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:249); at slick.jd",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:1777,Availability,down,down,1777,"eduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:250); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:249); at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); at slick.basic",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:1858,Availability,avail,available,1858,"nbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:250); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:249); at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:275); at java.ba",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:3119,Availability,repair,repairing,3119,"uteTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:250); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:249); at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:275); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:834); ```. Does anyone have any advice for repairing the db, or working out more what's happened?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:754,Energy Efficiency,Schedul,Scheduler,754,"Hi, I've been running Cromwell with the file-based DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:776,Energy Efficiency,Schedul,Scheduler,776,"Hi, I've been running Cromwell with the file-based DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:368,Integrability,Message,Message,368,"Hi, I've been running Cromwell with the file-based DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:807,Performance,concurren,concurrent,807,"Hi, I've been running Cromwell with the file-based DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:896,Performance,concurren,concurrent,896,"Hi, I've been running Cromwell with the file-based DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:970,Performance,concurren,concurrent,970,"Hi, I've been running Cromwell with the file-based DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:1045,Performance,concurren,concurrent,1045," DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.jav",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:2865,Performance,concurren,concurrent,2865,"uteTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:250); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:249); at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:275); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:834); ```. Does anyone have any advice for repairing the db, or working out more what's happened?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:2959,Performance,concurren,concurrent,2959,"uteTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:250); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:249); at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:275); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:834); ```. Does anyone have any advice for repairing the db, or working out more what's happened?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:867,Availability,error,errors,867,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:1985,Deployability,pipeline,pipeline,1985,"ached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MySQL performs much better with cromwell, but is infeasible to use in a per-project, per-user fashion. [I am working on solving the problem using SQLite](https://github.com/broadinstitute/cromwell/issues/5490) but there is NO ETA, and I don't know if I will ever get it to work. Still it is worth trying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:432,Performance,cache,cached,432,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:501,Performance,cache,cache,501,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:1134,Performance,cache,cached,1134," files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MyS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:1638,Performance,perform,performing,1638,"ached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MySQL performs much better with cromwell, but is infeasible to use in a per-project, per-user fashion. [I am working on solving the problem using SQLite](https://github.com/broadinstitute/cromwell/issues/5490) but there is NO ETA, and I don't know if I will ever get it to work. Still it is worth trying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:2135,Performance,perform,performs,2135,"ached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MySQL performs much better with cromwell, but is infeasible to use in a per-project, per-user fashion. [I am working on solving the problem using SQLite](https://github.com/broadinstitute/cromwell/issues/5490) but there is NO ETA, and I don't know if I will ever get it to work. Still it is worth trying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:115,Safety,timeout,timeouts,115,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:1542,Safety,timeout,timeout,1542,"ached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MySQL performs much better with cromwell, but is infeasible to use in a per-project, per-user fashion. [I am working on solving the problem using SQLite](https://github.com/broadinstitute/cromwell/issues/5490) but there is NO ETA, and I don't know if I will ever get it to work. Still it is worth trying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:863,Testability,Log,Log,863,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:317,Usability,guid,guide,317,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:684,Usability,clear,clear,684,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:728,Usability,guid,guide,728,"Your DB has become too big. This means it will take too much time to open the database and you will get connection timeouts. (These files can be multiple GBs). Here is our database setup:; ```; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; # See http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html; # Shutdown=false. Cromwell will shutdown the database; # hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that ha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629961081:107,Safety,timeout,timeouts,107,"Hey @rhpvorderman, that worked great for a bit so thanks for the comment! . I sometimes seem to be getting timeouts on smaller databases (300 seconds for a 2GB file), I think this might be due to Cromwell terminating incorrectly and it not starting up again. I'm following the SQLite with fingers crossed, and if there's anything I can do I'm more than happy to help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629961081
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629968131:227,Performance,cache,cached,227,"> I sometimes seem to be getting timeouts on smaller databases (300 seconds for a 2GB file), I think this might be due to Cromwell terminating incorrectly and it not starting up again. If the database is on NFS it might not be cached locally. And with a 100mbit connection it might happen. But this is just speculation. Anyway, I hope my PR on liquibase gets merged soon so I can continue working on the problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629968131
https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629968131:33,Safety,timeout,timeouts,33,"> I sometimes seem to be getting timeouts on smaller databases (300 seconds for a 2GB file), I think this might be due to Cromwell terminating incorrectly and it not starting up again. If the database is on NFS it might not be cached locally. And with a 100mbit connection it might happen. But this is just speculation. Anyway, I hope my PR on liquibase gets merged soon so I can continue working on the problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-629968131
https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-555176373:237,Modifiability,config,config,237,">It'd be a shame for this PR if Google expected that people might want to lower this value, but didn't want anyone to raise it. Google confirmed to me that yes, it can be raised, up to 30 days – that might be a useful thing to note in a config comment. See https://broadworkbench.atlassian.net/browse/BA-6015",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-555176373
https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-555176923:114,Modifiability,config,configurable,114,"@hkeward welcome to our repo and thank you for your contribution. We were actually looking at making this timeout configurable ourselves, so you've done some of our work for us.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-555176923
https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-555176923:106,Safety,timeout,timeout,106,"@hkeward welcome to our repo and thank you for your contribution. We were actually looking at making this timeout configurable ourselves, so you've done some of our work for us.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-555176923
https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-574767372:104,Availability,reliab,reliability,104,"Hi @hkeward, would you mind updating your branch from our latest `develop`? We have some important test reliability fixes that need picking up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-574767372
https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-574767372:99,Testability,test,test,99,"Hi @hkeward, would you mind updating your branch from our latest `develop`? We have some important test reliability fixes that need picking up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-574767372
https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-552997986:216,Deployability,integrat,integration,216,"Thank you for looking into this! Since you encountered this problem in real life, is there any chance you have a minimal WDL / options that reproduces the issue? If so we can take care of turning that into a Centaur integration test so this doesn't regress.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-552997986
https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-552997986:216,Integrability,integrat,integration,216,"Thank you for looking into this! Since you encountered this problem in real life, is there any chance you have a minimal WDL / options that reproduces the issue? If so we can take care of turning that into a Centaur integration test so this doesn't regress.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-552997986
https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-552997986:228,Testability,test,test,228,"Thank you for looking into this! Since you encountered this problem in real life, is there any chance you have a minimal WDL / options that reproduces the issue? If so we can take care of turning that into a Centaur integration test so this doesn't regress.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-552997986
https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-553007232:35,Testability,test,test,35,"Thanks - I think there's already a test for `memory-retry` at least, but the reason why it hasn't been failing is because it doesn't have an `output {}` section I think:; https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/retry_with_more_memory/retry_with_more_memory.wdl. So if we just add that, I think it will properly capture the mechanics. Does that sound reasonable?. And btw, how can I run the test locally for this module?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-553007232
https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-553007232:451,Testability,test,test,451,"Thanks - I think there's already a test for `memory-retry` at least, but the reason why it hasn't been failing is because it doesn't have an `output {}` section I think:; https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/retry_with_more_memory/retry_with_more_memory.wdl. So if we just add that, I think it will properly capture the mechanics. Does that sound reasonable?. And btw, how can I run the test locally for this module?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-553007232
https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-553031754:215,Testability,test,testOnly,215,"If you want to try making those changes and running it locally, per the instructions in [`Centaur.md`](https://github.com/broadinstitute/cromwell/blame/develop/docs/developers/Centaur.md#L23):. ```; sbt ""centaur/it:testOnly * -- -n retry_with_more_memory""; ```. This assumes you have a Cromwell running locally and listening on the port Centaur expects.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-553031754
https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-553075595:122,Testability,log,logic,122,"I'd like to nominate @salonishah11 to have a look at this (if you have time!), since you probably know the `memory-retry` logic best",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5274#issuecomment-553075595
https://github.com/broadinstitute/cromwell/pull/5276#issuecomment-553168102:26,Energy Efficiency,charge,charge,26,"Ok thanks, would you take charge of that?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5276#issuecomment-553168102
https://github.com/broadinstitute/cromwell/pull/5278#issuecomment-558290125:31,Usability,pause,pause,31,"@cjllanwarne, @mcovarr, please pause further reviews for now. I don't like current implementation and I think that previous implementation actually may have been better. This needs additional discussion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5278#issuecomment-558290125
https://github.com/broadinstitute/cromwell/pull/5278#issuecomment-558475452:33,Usability,pause,pause,33,"> @cjllanwarne, @mcovarr, please pause further reviews for now. I don't like current implementation and I think that previous implementation actually may have been better. This needs additional discussion. Moved handling of `preemptible` and `maxRetries` attributes from Engine to backend's `StandardAsyncExecutionActor`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5278#issuecomment-558475452
https://github.com/broadinstitute/cromwell/pull/5279#issuecomment-556439136:4,Deployability,update,updated,4,Pic updated. FYI merging if/when tests pass.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5279#issuecomment-556439136
https://github.com/broadinstitute/cromwell/pull/5279#issuecomment-556439136:33,Testability,test,tests,33,Pic updated. FYI merging if/when tests pass.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5279#issuecomment-556439136
https://github.com/broadinstitute/cromwell/issues/5280#issuecomment-553954232:9,Deployability,update,update,9,"Hi,. any update here? I will be really interested in these answers, as well. Thanks a lot!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280#issuecomment-553954232
https://github.com/broadinstitute/cromwell/issues/5280#issuecomment-561684948:385,Performance,cache,cache,385,"1. I think call caching should work with the in-memory database, but I would think its utility is very limited – when you restart Cromwell and erase the DB, all of your call caching information is lost.; 2. Call caching operates at the task level and is independent of workflows. If a task has the same command, input files, docker images, and maybe some other stuff I don't remember, cache reading will take place.; 3. Maybe we do explicitly disable CC with HSQL for the reasons described in (1). In any case I strongly recommend setting up a persistent RDBMS if you intend to use Cromwell with call caching. Hope this helps,; Adam",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280#issuecomment-561684948
https://github.com/broadinstitute/cromwell/issues/5285#issuecomment-571342815:75,Availability,error,error,75,"Hey @sona1111, I took your workflow and was able to approximately get your error. Your issue is that you have trailing whitespace after the first line: `voila tsv \ `:. ![image](https://user-images.githubusercontent.com/22381693/71853318-17c17600-312f-11ea-9e6b-5b85ae692ce2.png). This is a bash thing. You can replicate this problem by just running the following command inside your container:. ```; voila tsv \ f1; ```. This returned me the error:. ```; voila tsv: error: argument files: cannot find ""/cromwell-executions/myWorkflow1/591dc02e-f9e4-48c6-8498-df17242fe217/call-task_voila_tsv/execution/ ""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5285#issuecomment-571342815
https://github.com/broadinstitute/cromwell/issues/5285#issuecomment-571342815:443,Availability,error,error,443,"Hey @sona1111, I took your workflow and was able to approximately get your error. Your issue is that you have trailing whitespace after the first line: `voila tsv \ `:. ![image](https://user-images.githubusercontent.com/22381693/71853318-17c17600-312f-11ea-9e6b-5b85ae692ce2.png). This is a bash thing. You can replicate this problem by just running the following command inside your container:. ```; voila tsv \ f1; ```. This returned me the error:. ```; voila tsv: error: argument files: cannot find ""/cromwell-executions/myWorkflow1/591dc02e-f9e4-48c6-8498-df17242fe217/call-task_voila_tsv/execution/ ""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5285#issuecomment-571342815
https://github.com/broadinstitute/cromwell/issues/5285#issuecomment-571342815:467,Availability,error,error,467,"Hey @sona1111, I took your workflow and was able to approximately get your error. Your issue is that you have trailing whitespace after the first line: `voila tsv \ `:. ![image](https://user-images.githubusercontent.com/22381693/71853318-17c17600-312f-11ea-9e6b-5b85ae692ce2.png). This is a bash thing. You can replicate this problem by just running the following command inside your container:. ```; voila tsv \ f1; ```. This returned me the error:. ```; voila tsv: error: argument files: cannot find ""/cromwell-executions/myWorkflow1/591dc02e-f9e4-48c6-8498-df17242fe217/call-task_voila_tsv/execution/ ""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5285#issuecomment-571342815
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555110419:34,Testability,test,tested,34,"@mcovarr @dinvlad - but _is_ this tested anywhere?. This seems reasonable to me, but are we sure this is correct?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555110419
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555113433:43,Testability,test,test,43,There is a `monitoring_log_papiv2` Centaur test which appears to be exercising this code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555113433
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555228005:71,Testability,test,tests,71,I'm not sure `monitoring_log_papiv2` is related. I haven't written any tests for this code FWIW..,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555228005
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082:24,Energy Efficiency,monitor,monitoring,24,"@dinvlad I just ran the monitoring script test using this ""doctered"" version of your function:; ```; def monitoringTerminationAction(): Action = {; val result = cloudSdkAction; .withCommand(s""/bin/sh"", ""-c"", s""kill -TERM -1 && sleep $monitoringTerminationGraceTime""); .withFlags(List(ActionFlag.AlwaysRun)); .setPidNamespace(monitoringPidNamespace). println(""result""). throw new Exception(result.toPrettyString); }; ```. ... and to my surprise... nothing got printed out and everything seemed to work fine. . Are you sure this function is actually being called from anywhere? If so, what are you doing that the test case is not? And can we engineer a test somehow to test this line of code?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082:105,Energy Efficiency,monitor,monitoringTerminationAction,105,"@dinvlad I just ran the monitoring script test using this ""doctered"" version of your function:; ```; def monitoringTerminationAction(): Action = {; val result = cloudSdkAction; .withCommand(s""/bin/sh"", ""-c"", s""kill -TERM -1 && sleep $monitoringTerminationGraceTime""); .withFlags(List(ActionFlag.AlwaysRun)); .setPidNamespace(monitoringPidNamespace). println(""result""). throw new Exception(result.toPrettyString); }; ```. ... and to my surprise... nothing got printed out and everything seemed to work fine. . Are you sure this function is actually being called from anywhere? If so, what are you doing that the test case is not? And can we engineer a test somehow to test this line of code?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082:234,Energy Efficiency,monitor,monitoringTerminationGraceTime,234,"@dinvlad I just ran the monitoring script test using this ""doctered"" version of your function:; ```; def monitoringTerminationAction(): Action = {; val result = cloudSdkAction; .withCommand(s""/bin/sh"", ""-c"", s""kill -TERM -1 && sleep $monitoringTerminationGraceTime""); .withFlags(List(ActionFlag.AlwaysRun)); .setPidNamespace(monitoringPidNamespace). println(""result""). throw new Exception(result.toPrettyString); }; ```. ... and to my surprise... nothing got printed out and everything seemed to work fine. . Are you sure this function is actually being called from anywhere? If so, what are you doing that the test case is not? And can we engineer a test somehow to test this line of code?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082:325,Energy Efficiency,monitor,monitoringPidNamespace,325,"@dinvlad I just ran the monitoring script test using this ""doctered"" version of your function:; ```; def monitoringTerminationAction(): Action = {; val result = cloudSdkAction; .withCommand(s""/bin/sh"", ""-c"", s""kill -TERM -1 && sleep $monitoringTerminationGraceTime""); .withFlags(List(ActionFlag.AlwaysRun)); .setPidNamespace(monitoringPidNamespace). println(""result""). throw new Exception(result.toPrettyString); }; ```. ... and to my surprise... nothing got printed out and everything seemed to work fine. . Are you sure this function is actually being called from anywhere? If so, what are you doing that the test case is not? And can we engineer a test somehow to test this line of code?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082:42,Testability,test,test,42,"@dinvlad I just ran the monitoring script test using this ""doctered"" version of your function:; ```; def monitoringTerminationAction(): Action = {; val result = cloudSdkAction; .withCommand(s""/bin/sh"", ""-c"", s""kill -TERM -1 && sleep $monitoringTerminationGraceTime""); .withFlags(List(ActionFlag.AlwaysRun)); .setPidNamespace(monitoringPidNamespace). println(""result""). throw new Exception(result.toPrettyString); }; ```. ... and to my surprise... nothing got printed out and everything seemed to work fine. . Are you sure this function is actually being called from anywhere? If so, what are you doing that the test case is not? And can we engineer a test somehow to test this line of code?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082:611,Testability,test,test,611,"@dinvlad I just ran the monitoring script test using this ""doctered"" version of your function:; ```; def monitoringTerminationAction(): Action = {; val result = cloudSdkAction; .withCommand(s""/bin/sh"", ""-c"", s""kill -TERM -1 && sleep $monitoringTerminationGraceTime""); .withFlags(List(ActionFlag.AlwaysRun)); .setPidNamespace(monitoringPidNamespace). println(""result""). throw new Exception(result.toPrettyString); }; ```. ... and to my surprise... nothing got printed out and everything seemed to work fine. . Are you sure this function is actually being called from anywhere? If so, what are you doing that the test case is not? And can we engineer a test somehow to test this line of code?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082:651,Testability,test,test,651,"@dinvlad I just ran the monitoring script test using this ""doctered"" version of your function:; ```; def monitoringTerminationAction(): Action = {; val result = cloudSdkAction; .withCommand(s""/bin/sh"", ""-c"", s""kill -TERM -1 && sleep $monitoringTerminationGraceTime""); .withFlags(List(ActionFlag.AlwaysRun)); .setPidNamespace(monitoringPidNamespace). println(""result""). throw new Exception(result.toPrettyString); }; ```. ... and to my surprise... nothing got printed out and everything seemed to work fine. . Are you sure this function is actually being called from anywhere? If so, what are you doing that the test case is not? And can we engineer a test somehow to test this line of code?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082:667,Testability,test,test,667,"@dinvlad I just ran the monitoring script test using this ""doctered"" version of your function:; ```; def monitoringTerminationAction(): Action = {; val result = cloudSdkAction; .withCommand(s""/bin/sh"", ""-c"", s""kill -TERM -1 && sleep $monitoringTerminationGraceTime""); .withFlags(List(ActionFlag.AlwaysRun)); .setPidNamespace(monitoringPidNamespace). println(""result""). throw new Exception(result.toPrettyString); }; ```. ... and to my surprise... nothing got printed out and everything seemed to work fine. . Are you sure this function is actually being called from anywhere? If so, what are you doing that the test case is not? And can we engineer a test somehow to test this line of code?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714119:91,Energy Efficiency,monitor,monitoring,91,"Hey @cjllanwarne yes, I regularly run it with `monitoring_image` and it does terminate the monitoring container gracefully every time, as can be seen via the PAPIv2 operation log. This is the whole reason for its existence - to send SIGTERM to the monitoring container, otherwise PAPIv2 sends a SIGKILL and the last time points don't get reported..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714119
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714119:248,Energy Efficiency,monitor,monitoring,248,"Hey @cjllanwarne yes, I regularly run it with `monitoring_image` and it does terminate the monitoring container gracefully every time, as can be seen via the PAPIv2 operation log. This is the whole reason for its existence - to send SIGTERM to the monitoring container, otherwise PAPIv2 sends a SIGKILL and the last time points don't get reported..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714119
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714119:175,Testability,log,log,175,"Hey @cjllanwarne yes, I regularly run it with `monitoring_image` and it does terminate the monitoring container gracefully every time, as can be seen via the PAPIv2 operation log. This is the whole reason for its existence - to send SIGTERM to the monitoring container, otherwise PAPIv2 sends a SIGKILL and the last time points don't get reported..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714119
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714712:250,Deployability,integrat,integration,250,"To re-iterate, this option is only used for `monitoring_image`, and not related to `monitoring_script` in any way.. I could add some tests, though I'm not sure how to go about it because I've never done any testing in Scala and not sure how to write integration tests for Cromwell. I'd need some help from you on that..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714712
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714712:250,Integrability,integrat,integration,250,"To re-iterate, this option is only used for `monitoring_image`, and not related to `monitoring_script` in any way.. I could add some tests, though I'm not sure how to go about it because I've never done any testing in Scala and not sure how to write integration tests for Cromwell. I'd need some help from you on that..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714712
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714712:133,Testability,test,tests,133,"To re-iterate, this option is only used for `monitoring_image`, and not related to `monitoring_script` in any way.. I could add some tests, though I'm not sure how to go about it because I've never done any testing in Scala and not sure how to write integration tests for Cromwell. I'd need some help from you on that..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714712
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714712:207,Testability,test,testing,207,"To re-iterate, this option is only used for `monitoring_image`, and not related to `monitoring_script` in any way.. I could add some tests, though I'm not sure how to go about it because I've never done any testing in Scala and not sure how to write integration tests for Cromwell. I'd need some help from you on that..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714712
https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714712:262,Testability,test,tests,262,"To re-iterate, this option is only used for `monitoring_image`, and not related to `monitoring_script` in any way.. I could add some tests, though I'm not sure how to go about it because I've never done any testing in Scala and not sure how to write integration tests for Cromwell. I'd need some help from you on that..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714712
https://github.com/broadinstitute/cromwell/pull/5288#issuecomment-556298892:10,Testability,test,test,10,I wrote a test for this but unfortunately even with some more fixes it does not yet pass. Still investigating.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5288#issuecomment-556298892
https://github.com/broadinstitute/cromwell/pull/5288#issuecomment-556300904:67,Testability,test,test,67,"Closing temporarily, this shouldn't be reviewed until the modified test is passing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5288#issuecomment-556300904
https://github.com/broadinstitute/cromwell/pull/5293#issuecomment-556289007:28,Testability,test,tests,28,I guess while I'm inventing tests in a PR comment for someone else to write... we should make sure `:`-delimited keys (representing json traversal) still work too...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5293#issuecomment-556289007
https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557643561:68,Testability,test,tests,68,"Created https://github.com/broadinstitute/cromwell/pull/5298 to run tests, I'm not sure what the original problem was and can't guess whether it would be resolved so we'll await results",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557643561
https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557645572:40,Testability,test,tests,40,"I think this PR was already running the tests, as I've created a branch under broadinstitute/cromwell",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557645572
https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557665188:16,Testability,test,tests,16,"Looks like some tests are failing with `The job exceeded the maximum time limit for jobs, and has been terminated.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557665188
https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557684483:147,Energy Efficiency,green,green,147,"oh nvm, that one was exceeding the maximum log length. in any case there are some systemic test issues that we are working on so this PR should go green – or show evidence of a PR-specific problem – in due time",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557684483
https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557684483:43,Testability,log,log,43,"oh nvm, that one was exceeding the maximum log length. in any case there are some systemic test issues that we are working on so this PR should go green – or show evidence of a PR-specific problem – in due time",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557684483
https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557684483:91,Testability,test,test,91,"oh nvm, that one was exceeding the maximum log length. in any case there are some systemic test issues that we are working on so this PR should go green – or show evidence of a PR-specific problem – in due time",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557684483
https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557696421:71,Availability,error,error,71,"Thanks - btw, looking at the code, it doesn't seem like our _specific_ error message - `IOException: Could not read from gs://<...>: 503 Service Unavailable Backend Error` - is not covered by the tests. I see a similar test for 500, should I add one for 503 as well?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557696421
https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557696421:165,Availability,Error,Error,165,"Thanks - btw, looking at the code, it doesn't seem like our _specific_ error message - `IOException: Could not read from gs://<...>: 503 Service Unavailable Backend Error` - is not covered by the tests. I see a similar test for 500, should I add one for 503 as well?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557696421
https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557696421:77,Integrability,message,message,77,"Thanks - btw, looking at the code, it doesn't seem like our _specific_ error message - `IOException: Could not read from gs://<...>: 503 Service Unavailable Backend Error` - is not covered by the tests. I see a similar test for 500, should I add one for 503 as well?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557696421
https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557696421:196,Testability,test,tests,196,"Thanks - btw, looking at the code, it doesn't seem like our _specific_ error message - `IOException: Could not read from gs://<...>: 503 Service Unavailable Backend Error` - is not covered by the tests. I see a similar test for 500, should I add one for 503 as well?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557696421
https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557696421:219,Testability,test,test,219,"Thanks - btw, looking at the code, it doesn't seem like our _specific_ error message - `IOException: Could not read from gs://<...>: 503 Service Unavailable Backend Error` - is not covered by the tests. I see a similar test for 500, should I add one for 503 as well?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557696421
https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-558852164:6,Testability,test,tests,6,"Alas, tests continue to time out, just like last week..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-558852164
https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-561234995:11,Testability,test,tests,11,"Hi Denis - tests timing out are exactly the problem we saw in the previous version of this PR, so my first thought is the problem may be specific to this code and not a general flakiness",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-561234995
https://github.com/broadinstitute/cromwell/pull/5299#issuecomment-562911235:45,Availability,error,errors,45,"@cjllanwarne Thanks for picking up those two errors, I fixed them. Should be all set now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5299#issuecomment-562911235
https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-558871640:578,Deployability,configurat,configurations,578,"If you back Cromwell with a database (see [persisting data between restarts](https://cromwell.readthedocs.io/en/stable/tutorials/PersistentServer/)), you are able to stop Cromwell and restart without job information loss. After setting up this database, you can enable [call-caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) which if you modify your workflow, tools or inputs, will use previously computed results where:. - The command line used to generate the files is exactly the same; - The computed files still exist; - Other [call-caching configurations](https://cromwell.readthedocs.io/en/stable/wf_options/Overview/#call-caching-options) are valid. Alternatively, you could always stop your workflow, and modify it to run only from the point you've executed from with your already computed files. Straight forward, but could be a little tedious.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-558871640
https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-558871640:578,Modifiability,config,configurations,578,"If you back Cromwell with a database (see [persisting data between restarts](https://cromwell.readthedocs.io/en/stable/tutorials/PersistentServer/)), you are able to stop Cromwell and restart without job information loss. After setting up this database, you can enable [call-caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) which if you modify your workflow, tools or inputs, will use previously computed results where:. - The command line used to generate the files is exactly the same; - The computed files still exist; - Other [call-caching configurations](https://cromwell.readthedocs.io/en/stable/wf_options/Overview/#call-caching-options) are valid. Alternatively, you could always stop your workflow, and modify it to run only from the point you've executed from with your already computed files. Straight forward, but could be a little tedious.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-558871640
https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-559094884:42,Deployability,pipeline,pipeline,42,"So can i post wdl and other inputs, start pipeline and after get any status about created any files - stop process, get some data, edit files that is cromwell contains and then rerun it again?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-559094884
https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-561673981:158,Performance,cache,cache,158,"Can you elaborate on why you want the functionality described? It is possible that call caching will help, but be aware that Cromwell will only read from the cache when absolutely everything matches - hashes of input files, WDL workflow, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-561673981
https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-561673981:201,Security,hash,hashes,201,"Can you elaborate on why you want the functionality described? It is possible that call caching will help, but be aware that Cromwell will only read from the cache when absolutely everything matches - hashes of input files, WDL workflow, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5300#issuecomment-561673981
https://github.com/broadinstitute/cromwell/pull/5301#issuecomment-558829131:61,Testability,test,test,61,I think it's actually only `checkPublish` that we would want test if the docs have changed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5301#issuecomment-558829131
https://github.com/broadinstitute/cromwell/pull/5302#issuecomment-560557658:440,Testability,test,test,440,"@kshakir re: _issue where String outputs will attempt to be deleted_, I think I have covered this case. String outputs are stored as `WomString` even if they contain a file path. And such outputs should be eliminated [here](https://github.com/broadinstitute/cromwell/blob/180c64ab265e763528b66b945a584191d886020f/engine/src/main/scala/cromwell/engine/workflow/lifecycle/deletion/DeleteWorkflowFilesActor.scala#L82-L89). I have covered that test case [here](https://github.com/broadinstitute/cromwell/blob/180c64ab265e763528b66b945a584191d886020f/engine/src/test/scala/cromwell/engine/workflow/lifecycle/deletion/DeleteWorkflowFilesActorSpec.scala#L172-L209). Am I missing some other scenario?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5302#issuecomment-560557658
https://github.com/broadinstitute/cromwell/pull/5302#issuecomment-560557658:557,Testability,test,test,557,"@kshakir re: _issue where String outputs will attempt to be deleted_, I think I have covered this case. String outputs are stored as `WomString` even if they contain a file path. And such outputs should be eliminated [here](https://github.com/broadinstitute/cromwell/blob/180c64ab265e763528b66b945a584191d886020f/engine/src/main/scala/cromwell/engine/workflow/lifecycle/deletion/DeleteWorkflowFilesActor.scala#L82-L89). I have covered that test case [here](https://github.com/broadinstitute/cromwell/blob/180c64ab265e763528b66b945a584191d886020f/engine/src/test/scala/cromwell/engine/workflow/lifecycle/deletion/DeleteWorkflowFilesActorSpec.scala#L172-L209). Am I missing some other scenario?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5302#issuecomment-560557658
https://github.com/broadinstitute/cromwell/pull/5312#issuecomment-575089532:61,Availability,error,error,61,The dockerScripts build seems to fail due to some connection error. Hopefully it will succeed after a restart. ; I am happy that upgrading the Betterfiles dependency to a new major version release did not cause any issues in the rest of cromwell.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5312#issuecomment-575089532
https://github.com/broadinstitute/cromwell/pull/5312#issuecomment-575089532:189,Deployability,release,release,189,The dockerScripts build seems to fail due to some connection error. Hopefully it will succeed after a restart. ; I am happy that upgrading the Betterfiles dependency to a new major version release did not cause any issues in the rest of cromwell.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5312#issuecomment-575089532
https://github.com/broadinstitute/cromwell/pull/5312#issuecomment-575089532:155,Integrability,depend,dependency,155,The dockerScripts build seems to fail due to some connection error. Hopefully it will succeed after a restart. ; I am happy that upgrading the Betterfiles dependency to a new major version release did not cause any issues in the rest of cromwell.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5312#issuecomment-575089532
https://github.com/broadinstitute/cromwell/pull/5314#issuecomment-588304145:58,Testability,test,tests,58,original community contribution PR has been merged as the tests here have passed and no changes made since the tests.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5314#issuecomment-588304145
https://github.com/broadinstitute/cromwell/pull/5314#issuecomment-588304145:111,Testability,test,tests,111,original community contribution PR has been merged as the tests here have passed and no changes made since the tests.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5314#issuecomment-588304145
https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176:1228,Deployability,release,release,1228,"@cjllanwarne Thanks for the clarification. I was already wondering why you would negate your own well-written namespace code with a single line... Anyway I created a pull request on the spec here: https://github.com/openwdl/wdl/pull/347, your feedback would be much appreciated. Here's to hoping that it gets unanimously approved :crossed_fingers: . @geoffjentry yes, the Cromwell team has a lot of influence on the spec by implementing or not implementing things. I can understand the temptation to use this for ""the greater good"" :wink: . But I am quite happy that the Cromwell developers chose to be in touch with the community and aggressively implement the development spec in the development version of Cromwell. This allows us to see how certain spec changes turn out *before* they get implemented in production. In this case I came across this when I was testing the code for #5312 and found that I could not set my resource requirements for BWA anymore (in BioWDL all tasks default to the least number of cores needed, and sometimes you want to override this for more power). Since BWA was nested in a subworkflow this turned out not to be possible. So now we can fix the spec and Cromwell before this ever gets into a release. I think it is great work by the Cromwell team. It can't always be easy to follow the spec that closely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176
https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176:1077,Energy Efficiency,power,power,1077,"@cjllanwarne Thanks for the clarification. I was already wondering why you would negate your own well-written namespace code with a single line... Anyway I created a pull request on the spec here: https://github.com/openwdl/wdl/pull/347, your feedback would be much appreciated. Here's to hoping that it gets unanimously approved :crossed_fingers: . @geoffjentry yes, the Cromwell team has a lot of influence on the spec by implementing or not implementing things. I can understand the temptation to use this for ""the greater good"" :wink: . But I am quite happy that the Cromwell developers chose to be in touch with the community and aggressively implement the development spec in the development version of Cromwell. This allows us to see how certain spec changes turn out *before* they get implemented in production. In this case I came across this when I was testing the code for #5312 and found that I could not set my resource requirements for BWA anymore (in BioWDL all tasks default to the least number of cores needed, and sometimes you want to override this for more power). Since BWA was nested in a subworkflow this turned out not to be possible. So now we can fix the spec and Cromwell before this ever gets into a release. I think it is great work by the Cromwell team. It can't always be easy to follow the spec that closely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176
https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176:863,Testability,test,testing,863,"@cjllanwarne Thanks for the clarification. I was already wondering why you would negate your own well-written namespace code with a single line... Anyway I created a pull request on the spec here: https://github.com/openwdl/wdl/pull/347, your feedback would be much appreciated. Here's to hoping that it gets unanimously approved :crossed_fingers: . @geoffjentry yes, the Cromwell team has a lot of influence on the spec by implementing or not implementing things. I can understand the temptation to use this for ""the greater good"" :wink: . But I am quite happy that the Cromwell developers chose to be in touch with the community and aggressively implement the development spec in the development version of Cromwell. This allows us to see how certain spec changes turn out *before* they get implemented in production. In this case I came across this when I was testing the code for #5312 and found that I could not set my resource requirements for BWA anymore (in BioWDL all tasks default to the least number of cores needed, and sometimes you want to override this for more power). Since BWA was nested in a subworkflow this turned out not to be possible. So now we can fix the spec and Cromwell before this ever gets into a release. I think it is great work by the Cromwell team. It can't always be easy to follow the spec that closely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176
https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176:243,Usability,feedback,feedback,243,"@cjllanwarne Thanks for the clarification. I was already wondering why you would negate your own well-written namespace code with a single line... Anyway I created a pull request on the spec here: https://github.com/openwdl/wdl/pull/347, your feedback would be much appreciated. Here's to hoping that it gets unanimously approved :crossed_fingers: . @geoffjentry yes, the Cromwell team has a lot of influence on the spec by implementing or not implementing things. I can understand the temptation to use this for ""the greater good"" :wink: . But I am quite happy that the Cromwell developers chose to be in touch with the community and aggressively implement the development spec in the development version of Cromwell. This allows us to see how certain spec changes turn out *before* they get implemented in production. In this case I came across this when I was testing the code for #5312 and found that I could not set my resource requirements for BWA anymore (in BioWDL all tasks default to the least number of cores needed, and sometimes you want to override this for more power). Since BWA was nested in a subworkflow this turned out not to be possible. So now we can fix the spec and Cromwell before this ever gets into a release. I think it is great work by the Cromwell team. It can't always be easy to follow the spec that closely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176
https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-566628876:89,Availability,error,error,89,">Does ""5xx HTTP Status Code"" literally appear in real life instead of the actual 500-511 error codes?. No but I think the idea is to make sure the regex that checks for ""5"" matches ""500"" but not ""5 apples""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-566628876
https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-566633999:60,Availability,error,errors,60,"Updated to be way more specific in targeting Denis-reported errors. I've found this area can get difficult to reason about when we throw in ranges and very broad regexes, so I chose to be incredibly specific at the cost of more code & slightly less functionality (but Denis's functionality is there).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-566633999
https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-566633999:0,Deployability,Update,Updated,0,"Updated to be way more specific in targeting Denis-reported errors. I've found this area can get difficult to reason about when we throw in ranges and very broad regexes, so I chose to be incredibly specific at the cost of more code & slightly less functionality (but Denis's functionality is there).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-566633999
https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-572676517:45,Availability,error,errors,45,Looks like we now have `504 Gateway Timeout` errors too! Should we perhaps add a new case for that?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-572676517
https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-572676517:36,Safety,Timeout,Timeout,36,Looks like we now have `504 Gateway Timeout` errors too! Should we perhaps add a new case for that?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-572676517
https://github.com/broadinstitute/cromwell/pull/5324#issuecomment-565247518:87,Testability,test,tests,87,"Temporarily closing, some of the nested stuff isn't working correctly and the existing tests are inadequate to demonstrate that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5324#issuecomment-565247518
https://github.com/broadinstitute/cromwell/pull/5324#issuecomment-566274833:21,Testability,test,tests,21,"Added a bunch of new tests which are now passing. Despite what GitHub says, `JsonEditor.scala` actually should get reviewed but the `.json` files are mostly uninteresting test data.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5324#issuecomment-566274833
https://github.com/broadinstitute/cromwell/pull/5324#issuecomment-566274833:171,Testability,test,test,171,"Added a bunch of new tests which are now passing. Despite what GitHub says, `JsonEditor.scala` actually should get reviewed but the `.json` files are mostly uninteresting test data.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5324#issuecomment-566274833
https://github.com/broadinstitute/cromwell/pull/5325#issuecomment-565204796:32,Testability,test,test,32,I believe it was intentional to test Cromwell restarts with many workflows running to make sure that works.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5325#issuecomment-565204796
https://github.com/broadinstitute/cromwell/pull/5325#issuecomment-565211025:113,Testability,test,test,113,"Yeah, I decided to try switching it off in order to check if those; unanticipated restarts somehow contribute to test flakiness (I had a; groundless feeling that they do). On Thu, Dec 12, 2019, 4:56 PM mcovarr <notifications@github.com> wrote:. > I believe it was intentional to test Cromwell restarts with many workflows; > running to make sure that works.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5325?email_source=notifications&email_token=ABFA36XAKDCT4ITI3H47GN3QYKXPLA5CNFSM4J2D5JEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGYFOPA#issuecomment-565204796>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABFA36VIKFXY7DYBIWGFWIDQYKXPLANCNFSM4J2D5JEA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5325#issuecomment-565211025
https://github.com/broadinstitute/cromwell/pull/5325#issuecomment-565211025:279,Testability,test,test,279,"Yeah, I decided to try switching it off in order to check if those; unanticipated restarts somehow contribute to test flakiness (I had a; groundless feeling that they do). On Thu, Dec 12, 2019, 4:56 PM mcovarr <notifications@github.com> wrote:. > I believe it was intentional to test Cromwell restarts with many workflows; > running to make sure that works.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5325?email_source=notifications&email_token=ABFA36XAKDCT4ITI3H47GN3QYKXPLA5CNFSM4J2D5JEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGYFOPA#issuecomment-565204796>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABFA36VIKFXY7DYBIWGFWIDQYKXPLANCNFSM4J2D5JEA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5325#issuecomment-565211025
https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623:482,Availability,failure,failure,482,"@mcovarr: ; > how do these changes enable WDL 1.0 support. It replaces the use of WDL draft 2 objects to build a graph with the use of WOM objects to build the graph. > or the tests confirm that support has been added?. The tests make sure that the examples in the `womtool validate` test suite (which includes WDL draft-2 and 1.0) also run to completion in `womtool graph`. It doesn't assert that the output is _correct_ per se, but it does check that the process exits with a non-failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623
https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623:274,Security,validat,validate,274,"@mcovarr: ; > how do these changes enable WDL 1.0 support. It replaces the use of WDL draft 2 objects to build a graph with the use of WOM objects to build the graph. > or the tests confirm that support has been added?. The tests make sure that the examples in the `womtool validate` test suite (which includes WDL draft-2 and 1.0) also run to completion in `womtool graph`. It doesn't assert that the output is _correct_ per se, but it does check that the process exits with a non-failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623
https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623:176,Testability,test,tests,176,"@mcovarr: ; > how do these changes enable WDL 1.0 support. It replaces the use of WDL draft 2 objects to build a graph with the use of WOM objects to build the graph. > or the tests confirm that support has been added?. The tests make sure that the examples in the `womtool validate` test suite (which includes WDL draft-2 and 1.0) also run to completion in `womtool graph`. It doesn't assert that the output is _correct_ per se, but it does check that the process exits with a non-failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623
https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623:224,Testability,test,tests,224,"@mcovarr: ; > how do these changes enable WDL 1.0 support. It replaces the use of WDL draft 2 objects to build a graph with the use of WOM objects to build the graph. > or the tests confirm that support has been added?. The tests make sure that the examples in the `womtool validate` test suite (which includes WDL draft-2 and 1.0) also run to completion in `womtool graph`. It doesn't assert that the output is _correct_ per se, but it does check that the process exits with a non-failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623
https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623:284,Testability,test,test,284,"@mcovarr: ; > how do these changes enable WDL 1.0 support. It replaces the use of WDL draft 2 objects to build a graph with the use of WOM objects to build the graph. > or the tests confirm that support has been added?. The tests make sure that the examples in the `womtool validate` test suite (which includes WDL draft-2 and 1.0) also run to completion in `womtool graph`. It doesn't assert that the output is _correct_ per se, but it does check that the process exits with a non-failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623
https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623:386,Testability,assert,assert,386,"@mcovarr: ; > how do these changes enable WDL 1.0 support. It replaces the use of WDL draft 2 objects to build a graph with the use of WOM objects to build the graph. > or the tests confirm that support has been added?. The tests make sure that the examples in the `womtool validate` test suite (which includes WDL draft-2 and 1.0) also run to completion in `womtool graph`. It doesn't assert that the output is _correct_ per se, but it does check that the process exits with a non-failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567208623
https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567523168:48,Deployability,release,released,48,Thanks everyone! FWIW I would need to have this released by mid-Jan probably so that we can have all the right versions etc show up in the relevant book chapters. Will that be possible?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5326#issuecomment-567523168
https://github.com/broadinstitute/cromwell/pull/5328#issuecomment-566167823:451,Integrability,contract,contract,451,"> It might be nice to doublecheck in the `newFileSystem` where the `put` happens instead of this one caller since that would make all code paths threadsafe, but I'm not sure if that would introduce any other issues. I agree. But the reason I didn't do this is that `newFileSystem` method has another logic for the case when filesystem with such key already exists - it throws exception instead of just returning the existing filesystem. And this is a contract stated in the core `FileSystemProvider` abstract class.; But I think I can do some refactoring to overcome this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5328#issuecomment-566167823
https://github.com/broadinstitute/cromwell/pull/5328#issuecomment-566167823:543,Modifiability,refactor,refactoring,543,"> It might be nice to doublecheck in the `newFileSystem` where the `put` happens instead of this one caller since that would make all code paths threadsafe, but I'm not sure if that would introduce any other issues. I agree. But the reason I didn't do this is that `newFileSystem` method has another logic for the case when filesystem with such key already exists - it throws exception instead of just returning the existing filesystem. And this is a contract stated in the core `FileSystemProvider` abstract class.; But I think I can do some refactoring to overcome this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5328#issuecomment-566167823
https://github.com/broadinstitute/cromwell/pull/5328#issuecomment-566167823:300,Testability,log,logic,300,"> It might be nice to doublecheck in the `newFileSystem` where the `put` happens instead of this one caller since that would make all code paths threadsafe, but I'm not sure if that would introduce any other issues. I agree. But the reason I didn't do this is that `newFileSystem` method has another logic for the case when filesystem with such key already exists - it throws exception instead of just returning the existing filesystem. And this is a contract stated in the core `FileSystemProvider` abstract class.; But I think I can do some refactoring to overcome this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5328#issuecomment-566167823
https://github.com/broadinstitute/cromwell/pull/5330#issuecomment-570748662:62,Performance,cache,caches,62,I was having trouble with my PR builds so I blew away all the caches; that seems to have had beneficial effects here too. 🙂,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5330#issuecomment-570748662
https://github.com/broadinstitute/cromwell/pull/5332#issuecomment-579463064:0,Performance,Perform,Performance,0,Performance comparison between old (production) and new (with slick bug fix) Cromwell versions here: https://docs.google.com/document/d/1bvO63-FAotUcV0NPAwXo591QAI2VYrqUp_sU8rE1-14,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5332#issuecomment-579463064
https://github.com/broadinstitute/cromwell/pull/5332#issuecomment-580869825:22,Availability,error,errors,22,seems to have compile errors...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5332#issuecomment-580869825
https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484:156,Availability,avail,available,156,"Hey, not part of the Cromwell team but thought I'd try to help out. To clarify, you've:; - Built a Docker container with SGE + mysql; - Where `qsub` is not available through your `$PATH`, but installed at `/opt/gridengine/bin/lx-amd64/qsub`; - A (_virtual?_) SGE cluster is running within the container; - Running Cromwell inside this container; - Asking the cluster inside your docker container to spin up another Docker container. If this is correct, I'm struggling to understand the motivations behind it, but a few pointers:. - What does intermittent errors mean?; - You should avoid running Docker-in-Docker (SO: [Is it ok to run docker from inside docker?](https://stackoverflow.com/questions/27879713/is-it-ok-to-run-docker-from-inside-docker)); - It might be more predictable add `qsub` to the docker's path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484
https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484:555,Availability,error,errors,555,"Hey, not part of the Cromwell team but thought I'd try to help out. To clarify, you've:; - Built a Docker container with SGE + mysql; - Where `qsub` is not available through your `$PATH`, but installed at `/opt/gridengine/bin/lx-amd64/qsub`; - A (_virtual?_) SGE cluster is running within the container; - Running Cromwell inside this container; - Asking the cluster inside your docker container to spin up another Docker container. If this is correct, I'm struggling to understand the motivations behind it, but a few pointers:. - What does intermittent errors mean?; - You should avoid running Docker-in-Docker (SO: [Is it ok to run docker from inside docker?](https://stackoverflow.com/questions/27879713/is-it-ok-to-run-docker-from-inside-docker)); - It might be more predictable add `qsub` to the docker's path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484
https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484:192,Deployability,install,installed,192,"Hey, not part of the Cromwell team but thought I'd try to help out. To clarify, you've:; - Built a Docker container with SGE + mysql; - Where `qsub` is not available through your `$PATH`, but installed at `/opt/gridengine/bin/lx-amd64/qsub`; - A (_virtual?_) SGE cluster is running within the container; - Running Cromwell inside this container; - Asking the cluster inside your docker container to spin up another Docker container. If this is correct, I'm struggling to understand the motivations behind it, but a few pointers:. - What does intermittent errors mean?; - You should avoid running Docker-in-Docker (SO: [Is it ok to run docker from inside docker?](https://stackoverflow.com/questions/27879713/is-it-ok-to-run-docker-from-inside-docker)); - It might be more predictable add `qsub` to the docker's path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484
https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484:582,Safety,avoid,avoid,582,"Hey, not part of the Cromwell team but thought I'd try to help out. To clarify, you've:; - Built a Docker container with SGE + mysql; - Where `qsub` is not available through your `$PATH`, but installed at `/opt/gridengine/bin/lx-amd64/qsub`; - A (_virtual?_) SGE cluster is running within the container; - Running Cromwell inside this container; - Asking the cluster inside your docker container to spin up another Docker container. If this is correct, I'm struggling to understand the motivations behind it, but a few pointers:. - What does intermittent errors mean?; - You should avoid running Docker-in-Docker (SO: [Is it ok to run docker from inside docker?](https://stackoverflow.com/questions/27879713/is-it-ok-to-run-docker-from-inside-docker)); - It might be more predictable add `qsub` to the docker's path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484
https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484:772,Safety,predict,predictable,772,"Hey, not part of the Cromwell team but thought I'd try to help out. To clarify, you've:; - Built a Docker container with SGE + mysql; - Where `qsub` is not available through your `$PATH`, but installed at `/opt/gridengine/bin/lx-amd64/qsub`; - A (_virtual?_) SGE cluster is running within the container; - Running Cromwell inside this container; - Asking the cluster inside your docker container to spin up another Docker container. If this is correct, I'm struggling to understand the motivations behind it, but a few pointers:. - What does intermittent errors mean?; - You should avoid running Docker-in-Docker (SO: [Is it ok to run docker from inside docker?](https://stackoverflow.com/questions/27879713/is-it-ok-to-run-docker-from-inside-docker)); - It might be more predictable add `qsub` to the docker's path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484
https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570559229:41,Safety,timeout,timeout,41,Looks like this PR didn't catch the test timeout increases for some reason 🤔,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570559229
https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570559229:36,Testability,test,test,36,Looks like this PR didn't catch the test timeout increases for some reason 🤔,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570559229
https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570604638:43,Safety,timeout,timeout,43,"> Looks like this PR didn't catch the test timeout increases for some reason 🤔. Yeah, that's strange. I think this has something to do with the fact that I initially created a draft PR for this branch. Then I closed the draft PR and created a new one, but looks like that didn't help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570604638
https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570604638:38,Testability,test,test,38,"> Looks like this PR didn't catch the test timeout increases for some reason 🤔. Yeah, that's strange. I think this has something to do with the fact that I initially created a draft PR for this branch. Then I closed the draft PR and created a new one, but looks like that didn't help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5337#issuecomment-570604638
https://github.com/broadinstitute/cromwell/pull/5340#issuecomment-571829738:137,Availability,error,errors,137,"Looks like Travis caches' deletion doesn't solve the problem with centaurHoricromtalEngineUpgradePapiV2 builds (BA-6164). There're still errors like `ERROR: for cromwell-summarizer-plus-backend Container ""dcdaaee217fb"" is unhealthy.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5340#issuecomment-571829738
https://github.com/broadinstitute/cromwell/pull/5340#issuecomment-571829738:150,Availability,ERROR,ERROR,150,"Looks like Travis caches' deletion doesn't solve the problem with centaurHoricromtalEngineUpgradePapiV2 builds (BA-6164). There're still errors like `ERROR: for cromwell-summarizer-plus-backend Container ""dcdaaee217fb"" is unhealthy.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5340#issuecomment-571829738
https://github.com/broadinstitute/cromwell/pull/5340#issuecomment-571829738:18,Performance,cache,caches,18,"Looks like Travis caches' deletion doesn't solve the problem with centaurHoricromtalEngineUpgradePapiV2 builds (BA-6164). There're still errors like `ERROR: for cromwell-summarizer-plus-backend Container ""dcdaaee217fb"" is unhealthy.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5340#issuecomment-571829738
https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988:89,Energy Efficiency,reduce,reduce,89,"Hello @ruchim! Thanks for looking into the issue. The idea behind the init script was to reduce code duplication between all Cromwell tasks that use [recently added](https://github.com/broadinstitute/cromwell/pull/5343) `enable_fuse` flag as much as possible. Otherwise mounts have to be manually configured for each and every Cromwell task in order to take advantage of the fuse capabilities. Definitely the decision either to use or not use such init script highly depends on a workflow. From my point of view if some of workflow tasks use fuse capabilities then most of them probably do the same. Therefore the usage of init script is not required but can be helpful in such cases. As an example let's look at the following workflow. It just calculates number of files in some of the mounted directories. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. task references {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/references/ | wc -l > number; >>>; }. task samples {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/samples/ | wc -l > number; >>>; }; ```. As long as we have some common initialization in both tasks we can extract it to the init script which will be executed right before each task command. If we perform such optimization then we have to upload `init_script.sh` to google cloud and enable it in the workflow properties. *init_script.sh*; ```bash; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ```. *workflow_options.json*; ```bash; {; ""init_script"": ""gs://storage/init_script.sh""; }; ```. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988
https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988:467,Integrability,depend,depends,467,"Hello @ruchim! Thanks for looking into the issue. The idea behind the init script was to reduce code duplication between all Cromwell tasks that use [recently added](https://github.com/broadinstitute/cromwell/pull/5343) `enable_fuse` flag as much as possible. Otherwise mounts have to be manually configured for each and every Cromwell task in order to take advantage of the fuse capabilities. Definitely the decision either to use or not use such init script highly depends on a workflow. From my point of view if some of workflow tasks use fuse capabilities then most of them probably do the same. Therefore the usage of init script is not required but can be helpful in such cases. As an example let's look at the following workflow. It just calculates number of files in some of the mounted directories. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. task references {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/references/ | wc -l > number; >>>; }. task samples {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/samples/ | wc -l > number; >>>; }; ```. As long as we have some common initialization in both tasks we can extract it to the init script which will be executed right before each task command. If we perform such optimization then we have to upload `init_script.sh` to google cloud and enable it in the workflow properties. *init_script.sh*; ```bash; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ```. *workflow_options.json*; ```bash; {; ""init_script"": ""gs://storage/init_script.sh""; }; ```. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988
https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988:297,Modifiability,config,configured,297,"Hello @ruchim! Thanks for looking into the issue. The idea behind the init script was to reduce code duplication between all Cromwell tasks that use [recently added](https://github.com/broadinstitute/cromwell/pull/5343) `enable_fuse` flag as much as possible. Otherwise mounts have to be manually configured for each and every Cromwell task in order to take advantage of the fuse capabilities. Definitely the decision either to use or not use such init script highly depends on a workflow. From my point of view if some of workflow tasks use fuse capabilities then most of them probably do the same. Therefore the usage of init script is not required but can be helpful in such cases. As an example let's look at the following workflow. It just calculates number of files in some of the mounted directories. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. task references {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/references/ | wc -l > number; >>>; }. task samples {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/samples/ | wc -l > number; >>>; }; ```. As long as we have some common initialization in both tasks we can extract it to the init script which will be executed right before each task command. If we perform such optimization then we have to upload `init_script.sh` to google cloud and enable it in the workflow properties. *init_script.sh*; ```bash; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ```. *workflow_options.json*; ```bash; {; ""init_script"": ""gs://storage/init_script.sh""; }; ```. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988
https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988:1524,Performance,perform,perform,1524,"mounts have to be manually configured for each and every Cromwell task in order to take advantage of the fuse capabilities. Definitely the decision either to use or not use such init script highly depends on a workflow. From my point of view if some of workflow tasks use fuse capabilities then most of them probably do the same. Therefore the usage of init script is not required but can be helpful in such cases. As an example let's look at the following workflow. It just calculates number of files in some of the mounted directories. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. task references {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/references/ | wc -l > number; >>>; }. task samples {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/samples/ | wc -l > number; >>>; }; ```. As long as we have some common initialization in both tasks we can extract it to the init script which will be executed right before each task command. If we perform such optimization then we have to upload `init_script.sh` to google cloud and enable it in the workflow properties. *init_script.sh*; ```bash; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ```. *workflow_options.json*; ```bash; {; ""init_script"": ""gs://storage/init_script.sh""; }; ```. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. task references {; output {; Int NUMBER = read_int(""number""); }. command <<<; ls -lh /mount-point/references/ | wc -l > number; >>>; }. task samples {; output {; Int NUMBER = read_int(""number""); }. command <<<; ls -lh /mount-point/samples/ | wc -l > number; >>>; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988
https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988:1537,Performance,optimiz,optimization,1537,"mounts have to be manually configured for each and every Cromwell task in order to take advantage of the fuse capabilities. Definitely the decision either to use or not use such init script highly depends on a workflow. From my point of view if some of workflow tasks use fuse capabilities then most of them probably do the same. Therefore the usage of init script is not required but can be helpful in such cases. As an example let's look at the following workflow. It just calculates number of files in some of the mounted directories. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. task references {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/references/ | wc -l > number; >>>; }. task samples {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/samples/ | wc -l > number; >>>; }; ```. As long as we have some common initialization in both tasks we can extract it to the init script which will be executed right before each task command. If we perform such optimization then we have to upload `init_script.sh` to google cloud and enable it in the workflow properties. *init_script.sh*; ```bash; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ```. *workflow_options.json*; ```bash; {; ""init_script"": ""gs://storage/init_script.sh""; }; ```. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. task references {; output {; Int NUMBER = read_int(""number""); }. command <<<; ls -lh /mount-point/references/ | wc -l > number; >>>; }. task samples {; output {; Int NUMBER = read_int(""number""); }. command <<<; ls -lh /mount-point/samples/ | wc -l > number; >>>; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988
https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597:1595,Deployability,integrat,integration,1595,"@tcibinan Thanks for the contribution! To expectation set, we'll aim to review your PR within a reasonable timeframe but we unfortunately can't make any firm guarantees of the timeline since we're always working to internal deadlines of our own!. My initial thought is:; 1. Since the filesystem is something that Cromwell cannot have any control over, a number of its assumptions might be invalidated (depending on how the workflow author uses the filesystem). That's probably fine, as long as both the author and the workflow runner are aware of it and can work around it.; 2. We probably want to have our own version of ""pre-computed inputs mounted by fuse"" with some more controls around the problems mentioned in (1.) to help keep our assumptions correct.; 3. None of those points are any worse than passing in `gs://` paths as strings and localizing them manually, so we shouldn't disallow people opting-in to fuse support like this if they really want to. Could you add some additional warning comments along these lines?; 1. Any inputs brought in via a Fuse filesystem will not be considered for call caching.; 1. Any outputs stored via a Fuse filesystem will not be recreated if a task is replayed from a call-cache hit.; 1. If the filesystem is writable, your job is potentially no longer idempotent - Cromwell may decide to retry your job for you, and you might get unforeseen file collisions or even incorrect results if that happens.; 1. This is a community contribution and not officially supported (at least not yet!) by the Cromwell team?. @ruchim I know you were looking at Fuse integration too. Do you have any thoughts on this?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597
https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597:402,Integrability,depend,depending,402,"@tcibinan Thanks for the contribution! To expectation set, we'll aim to review your PR within a reasonable timeframe but we unfortunately can't make any firm guarantees of the timeline since we're always working to internal deadlines of our own!. My initial thought is:; 1. Since the filesystem is something that Cromwell cannot have any control over, a number of its assumptions might be invalidated (depending on how the workflow author uses the filesystem). That's probably fine, as long as both the author and the workflow runner are aware of it and can work around it.; 2. We probably want to have our own version of ""pre-computed inputs mounted by fuse"" with some more controls around the problems mentioned in (1.) to help keep our assumptions correct.; 3. None of those points are any worse than passing in `gs://` paths as strings and localizing them manually, so we shouldn't disallow people opting-in to fuse support like this if they really want to. Could you add some additional warning comments along these lines?; 1. Any inputs brought in via a Fuse filesystem will not be considered for call caching.; 1. Any outputs stored via a Fuse filesystem will not be recreated if a task is replayed from a call-cache hit.; 1. If the filesystem is writable, your job is potentially no longer idempotent - Cromwell may decide to retry your job for you, and you might get unforeseen file collisions or even incorrect results if that happens.; 1. This is a community contribution and not officially supported (at least not yet!) by the Cromwell team?. @ruchim I know you were looking at Fuse integration too. Do you have any thoughts on this?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597
https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597:1595,Integrability,integrat,integration,1595,"@tcibinan Thanks for the contribution! To expectation set, we'll aim to review your PR within a reasonable timeframe but we unfortunately can't make any firm guarantees of the timeline since we're always working to internal deadlines of our own!. My initial thought is:; 1. Since the filesystem is something that Cromwell cannot have any control over, a number of its assumptions might be invalidated (depending on how the workflow author uses the filesystem). That's probably fine, as long as both the author and the workflow runner are aware of it and can work around it.; 2. We probably want to have our own version of ""pre-computed inputs mounted by fuse"" with some more controls around the problems mentioned in (1.) to help keep our assumptions correct.; 3. None of those points are any worse than passing in `gs://` paths as strings and localizing them manually, so we shouldn't disallow people opting-in to fuse support like this if they really want to. Could you add some additional warning comments along these lines?; 1. Any inputs brought in via a Fuse filesystem will not be considered for call caching.; 1. Any outputs stored via a Fuse filesystem will not be recreated if a task is replayed from a call-cache hit.; 1. If the filesystem is writable, your job is potentially no longer idempotent - Cromwell may decide to retry your job for you, and you might get unforeseen file collisions or even incorrect results if that happens.; 1. This is a community contribution and not officially supported (at least not yet!) by the Cromwell team?. @ruchim I know you were looking at Fuse integration too. Do you have any thoughts on this?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597
https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597:1218,Performance,cache,cache,1218,"@tcibinan Thanks for the contribution! To expectation set, we'll aim to review your PR within a reasonable timeframe but we unfortunately can't make any firm guarantees of the timeline since we're always working to internal deadlines of our own!. My initial thought is:; 1. Since the filesystem is something that Cromwell cannot have any control over, a number of its assumptions might be invalidated (depending on how the workflow author uses the filesystem). That's probably fine, as long as both the author and the workflow runner are aware of it and can work around it.; 2. We probably want to have our own version of ""pre-computed inputs mounted by fuse"" with some more controls around the problems mentioned in (1.) to help keep our assumptions correct.; 3. None of those points are any worse than passing in `gs://` paths as strings and localizing them manually, so we shouldn't disallow people opting-in to fuse support like this if they really want to. Could you add some additional warning comments along these lines?; 1. Any inputs brought in via a Fuse filesystem will not be considered for call caching.; 1. Any outputs stored via a Fuse filesystem will not be recreated if a task is replayed from a call-cache hit.; 1. If the filesystem is writable, your job is potentially no longer idempotent - Cromwell may decide to retry your job for you, and you might get unforeseen file collisions or even incorrect results if that happens.; 1. This is a community contribution and not officially supported (at least not yet!) by the Cromwell team?. @ruchim I know you were looking at Fuse integration too. Do you have any thoughts on this?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572896597
https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572980186:206,Deployability,integrat,integration,206,@cjllanwarne Thanks for the quick response!. I agreed that it seems reasonable to have built-in support for FUSE mounts in Cromwell. Nevertheless this PR can be a neat addition to the existing Google Cloud integration. I've updated the docs with the FUSE filesystem usage limitations as you asked. Looking forward for the review. Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572980186
https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572980186:224,Deployability,update,updated,224,@cjllanwarne Thanks for the quick response!. I agreed that it seems reasonable to have built-in support for FUSE mounts in Cromwell. Nevertheless this PR can be a neat addition to the existing Google Cloud integration. I've updated the docs with the FUSE filesystem usage limitations as you asked. Looking forward for the review. Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572980186
https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572980186:206,Integrability,integrat,integration,206,@cjllanwarne Thanks for the quick response!. I agreed that it seems reasonable to have built-in support for FUSE mounts in Cromwell. Nevertheless this PR can be a neat addition to the existing Google Cloud integration. I've updated the docs with the FUSE filesystem usage limitations as you asked. Looking forward for the review. Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572980186
https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-593587452:210,Testability,log,logic,210,"@cjllanwarne @tcibinan . A few thoughts -- as a user, I think I'd pair this flag with `localization_optional` set to true so that files don't actually get localized, and I can do something like my own mounting logic in the command section (which involves making new WDL tasks to take advantage of this feature). However, assuming the i/o of FUSE is something that is really helpful and a popular choice, you can imagine that the next iteration of this feature takes it to the next level and you can imagine any ""File"" type object is mounted rather than the mounting logic embedded directly into the command. This means more people can take advantage of this feature without modifying commands. Perhaps the third iteration would take into account call caching -- but that's a problem for the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-593587452
https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-593587452:566,Testability,log,logic,566,"@cjllanwarne @tcibinan . A few thoughts -- as a user, I think I'd pair this flag with `localization_optional` set to true so that files don't actually get localized, and I can do something like my own mounting logic in the command section (which involves making new WDL tasks to take advantage of this feature). However, assuming the i/o of FUSE is something that is really helpful and a popular choice, you can imagine that the next iteration of this feature takes it to the next level and you can imagine any ""File"" type object is mounted rather than the mounting logic embedded directly into the command. This means more people can take advantage of this feature without modifying commands. Perhaps the third iteration would take into account call caching -- but that's a problem for the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-593587452
https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-593987713:65,Testability,test,tests,65,"Thanks @tcibinan, I've created #5438 to run the full suite of CI tests against this branch.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-593987713
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957:199,Availability,error,errors,199,I have just had a run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API and which resulted in several `504 Gateway Timeout` errors while attempting to read `rc` and `stdout` output files. Is this something that should be looked into?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957:84,Deployability,pipeline,pipelines,84,I have just had a run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API and which resulted in several `504 Gateway Timeout` errors while attempting to read `rc` and `stdout` output files. Is this something that should be looked into?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957:101,Deployability,Pipeline,PipelinesApiLifecycleActorFactory,101,I have just had a run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API and which resulted in several `504 Gateway Timeout` errors while attempting to read `rc` and `stdout` output files. Is this something that should be looked into?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957:39,Modifiability,config,configured,39,I have just had a run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API and which resulted in several `504 Gateway Timeout` errors while attempting to read `rc` and `stdout` output files. Is this something that should be looked into?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957:190,Safety,Timeout,Timeout,190,I have just had a run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API and which resulted in several `504 Gateway Timeout` errors while attempting to read `rc` and `stdout` output files. Is this something that should be looked into?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760305422:96,Availability,error,error,96,@freeseek are you reporting a bug in Cromwell's 504 detection and retry logic?. Receiving a 504 error in the first place is a Google problem and we have no control.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760305422
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760305422:52,Safety,detect,detection,52,@freeseek are you reporting a bug in Cromwell's 504 detection and retry logic?. Receiving a 504 error in the first place is a Google problem and we have no control.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760305422
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760305422:72,Testability,log,logic,72,@freeseek are you reporting a bug in Cromwell's 504 detection and retry logic?. Receiving a 504 error in the first place is a Google problem and we have no control.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760305422
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:71,Availability,error,errors,71,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:219,Availability,down,download,219,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:613,Availability,down,download,613,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:1027,Availability,down,download,1027,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:43,Deployability,pipeline,pipeline,43,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:1239,Deployability,pipeline,pipeline,1239,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:152,Integrability,message,message,152,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:406,Integrability,message,message,406,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:783,Integrability,message,message,783,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:1197,Integrability,message,message,1197,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:175,Safety,Timeout,Timeout,175,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:569,Safety,Timeout,Timeout,569,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:983,Safety,Timeout,Timeout,983,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719
https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760314632:168,Availability,error,errors,168,"Please file a new bug report in the format ""expected result"" / ""actual result"", taking into account that Cromwell has no control over whether Google servers return 504 errors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760314632
https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236:124,Availability,failure,failures,124,@illusional . At our cluster we use both `cached-copy` and `path+modtime` and call-caching works fine. All our call-caching failures where related to how we implemented the tasks. Have you tried using `cromwell run -m metadata.json workflow.wdl`? In that case the call cache variables will be saved in the `metadata.json` file. It is very informative to see how these change between runs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236
https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236:275,Modifiability,variab,variables,275,@illusional . At our cluster we use both `cached-copy` and `path+modtime` and call-caching works fine. All our call-caching failures where related to how we implemented the tasks. Have you tried using `cromwell run -m metadata.json workflow.wdl`? In that case the call cache variables will be saved in the `metadata.json` file. It is very informative to see how these change between runs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236
https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236:42,Performance,cache,cached-copy,42,@illusional . At our cluster we use both `cached-copy` and `path+modtime` and call-caching works fine. All our call-caching failures where related to how we implemented the tasks. Have you tried using `cromwell run -m metadata.json workflow.wdl`? In that case the call cache variables will be saved in the `metadata.json` file. It is very informative to see how these change between runs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236
https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236:269,Performance,cache,cache,269,@illusional . At our cluster we use both `cached-copy` and `path+modtime` and call-caching works fine. All our call-caching failures where related to how we implemented the tasks. Have you tried using `cromwell run -m metadata.json workflow.wdl`? In that case the call cache variables will be saved in the `metadata.json` file. It is very informative to see how these change between runs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346#issuecomment-589548236
https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-572850228:74,Energy Efficiency,schedul,schedule,74,"It's unclear how your environment is set up:. 1. You're using Cromwell to schedule jobs to GoogleCloud.; 2. You're Cromwell on a Google cloud instance and not scheduling any jobs out. ## Cromwell scheduling to GCloud. - Cromwell runs out of memory and dies; - Your task is running out of memory, this would cause your workflow to fail. . ### Cromwell OOM. Use MySQL and connect it to Cromwell: https://cromwell.readthedocs.io/en/stable/Configuring/#database. Cromwell + MySQL uses less memory and you get durability:. ### Task. It seems that your task is running out of memory, and not Cromwell. . I'll direct you towards the WDL spec: https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#memory. But basically, within your task you need to place the block or with more memory. ```; task mytask {; ...other stuff; runtime {; memory: ""2GB""; }; ```. ## Running Cromwell on a GC instance. Restart your instance with more memory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-572850228
https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-572850228:159,Energy Efficiency,schedul,scheduling,159,"It's unclear how your environment is set up:. 1. You're using Cromwell to schedule jobs to GoogleCloud.; 2. You're Cromwell on a Google cloud instance and not scheduling any jobs out. ## Cromwell scheduling to GCloud. - Cromwell runs out of memory and dies; - Your task is running out of memory, this would cause your workflow to fail. . ### Cromwell OOM. Use MySQL and connect it to Cromwell: https://cromwell.readthedocs.io/en/stable/Configuring/#database. Cromwell + MySQL uses less memory and you get durability:. ### Task. It seems that your task is running out of memory, and not Cromwell. . I'll direct you towards the WDL spec: https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#memory. But basically, within your task you need to place the block or with more memory. ```; task mytask {; ...other stuff; runtime {; memory: ""2GB""; }; ```. ## Running Cromwell on a GC instance. Restart your instance with more memory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-572850228
https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-572850228:196,Energy Efficiency,schedul,scheduling,196,"It's unclear how your environment is set up:. 1. You're using Cromwell to schedule jobs to GoogleCloud.; 2. You're Cromwell on a Google cloud instance and not scheduling any jobs out. ## Cromwell scheduling to GCloud. - Cromwell runs out of memory and dies; - Your task is running out of memory, this would cause your workflow to fail. . ### Cromwell OOM. Use MySQL and connect it to Cromwell: https://cromwell.readthedocs.io/en/stable/Configuring/#database. Cromwell + MySQL uses less memory and you get durability:. ### Task. It seems that your task is running out of memory, and not Cromwell. . I'll direct you towards the WDL spec: https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#memory. But basically, within your task you need to place the block or with more memory. ```; task mytask {; ...other stuff; runtime {; memory: ""2GB""; }; ```. ## Running Cromwell on a GC instance. Restart your instance with more memory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-572850228
https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-572850228:436,Modifiability,Config,Configuring,436,"It's unclear how your environment is set up:. 1. You're using Cromwell to schedule jobs to GoogleCloud.; 2. You're Cromwell on a Google cloud instance and not scheduling any jobs out. ## Cromwell scheduling to GCloud. - Cromwell runs out of memory and dies; - Your task is running out of memory, this would cause your workflow to fail. . ### Cromwell OOM. Use MySQL and connect it to Cromwell: https://cromwell.readthedocs.io/en/stable/Configuring/#database. Cromwell + MySQL uses less memory and you get durability:. ### Task. It seems that your task is running out of memory, and not Cromwell. . I'll direct you towards the WDL spec: https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#memory. But basically, within your task you need to place the block or with more memory. ```; task mytask {; ...other stuff; runtime {; memory: ""2GB""; }; ```. ## Running Cromwell on a GC instance. Restart your instance with more memory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-572850228
https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-573160131:146,Modifiability,config,config,146,"Great increasing the memory was the solution. In case somebody else needs helping setting up a mysql database:. 1. Add this to the bottom of your config; ```; database { ; profile = ""slick.jdbc.MySQLProfile$"" ; db { ; driver = ""com.mysql.jdbc.Driver"" ; url = ""jdbc:mysql://localhost/DatabaseName?useSSL=false&allowPublicKeyRetrieval=true"" ; user = ""ChooseAName"" ; password = ""YourOtherPassword"" ; connectionTimeout = 5000 ; } ; }; ```; 2. Then set up the mysql database using docker; ```; sudo docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=YourPassword -e MYSQL_DATABASE=DatabaseName -e MYSQL_USER=ChooseAName -e MYSQL_PASSWORD=YourOtherPassword -d mysql; ```. 3. Then check that your docker container is running: ; ```; sudo docker ps -a; ```. 4. Then you should be all set. Most of this is from:; https://gatkforums.broadinstitute.org/wdl/discussion/comment/51170#Comment_51170",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-573160131
https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-573160131:364,Security,password,password,364,"Great increasing the memory was the solution. In case somebody else needs helping setting up a mysql database:. 1. Add this to the bottom of your config; ```; database { ; profile = ""slick.jdbc.MySQLProfile$"" ; db { ; driver = ""com.mysql.jdbc.Driver"" ; url = ""jdbc:mysql://localhost/DatabaseName?useSSL=false&allowPublicKeyRetrieval=true"" ; user = ""ChooseAName"" ; password = ""YourOtherPassword"" ; connectionTimeout = 5000 ; } ; }; ```; 2. Then set up the mysql database using docker; ```; sudo docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=YourPassword -e MYSQL_DATABASE=DatabaseName -e MYSQL_USER=ChooseAName -e MYSQL_PASSWORD=YourOtherPassword -d mysql; ```. 3. Then check that your docker container is running: ; ```; sudo docker ps -a; ```. 4. Then you should be all set. Most of this is from:; https://gatkforums.broadinstitute.org/wdl/discussion/comment/51170#Comment_51170",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-573160131
https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305:290,Availability,avail,available,290,"Not necessarily the problem, but even non-obvious GCP quotas can limit how many workers are scheduled. Specifically, some things to look out for:. - Preemtible specific resources, like CPUs and Memory (if you're running preemtibles); - If you are using preemtibles, there may not be enough available instances; - Local SSD (GB); - Internal IP addresses; - In-use IP addresses; - List requests per 100 seconds. I thought mine were high enough, but from this page (replace `$region` with your region) you can click the ""Current Usage"" to sort by in-demand resources:. - https://console.cloud.google.com/iam-admin/quotas?project=portable-pipeline-project&location=$region. ![image](https://user-images.githubusercontent.com/22381693/72295508-d2132900-36ab-11ea-8380-256c2ad381b4.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305
https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305:635,Deployability,pipeline,pipeline-project,635,"Not necessarily the problem, but even non-obvious GCP quotas can limit how many workers are scheduled. Specifically, some things to look out for:. - Preemtible specific resources, like CPUs and Memory (if you're running preemtibles); - If you are using preemtibles, there may not be enough available instances; - Local SSD (GB); - Internal IP addresses; - In-use IP addresses; - List requests per 100 seconds. I thought mine were high enough, but from this page (replace `$region` with your region) you can click the ""Current Usage"" to sort by in-demand resources:. - https://console.cloud.google.com/iam-admin/quotas?project=portable-pipeline-project&location=$region. ![image](https://user-images.githubusercontent.com/22381693/72295508-d2132900-36ab-11ea-8380-256c2ad381b4.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305
https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305:92,Energy Efficiency,schedul,scheduled,92,"Not necessarily the problem, but even non-obvious GCP quotas can limit how many workers are scheduled. Specifically, some things to look out for:. - Preemtible specific resources, like CPUs and Memory (if you're running preemtibles); - If you are using preemtibles, there may not be enough available instances; - Local SSD (GB); - Internal IP addresses; - In-use IP addresses; - List requests per 100 seconds. I thought mine were high enough, but from this page (replace `$region` with your region) you can click the ""Current Usage"" to sort by in-demand resources:. - https://console.cloud.google.com/iam-admin/quotas?project=portable-pipeline-project&location=$region. ![image](https://user-images.githubusercontent.com/22381693/72295508-d2132900-36ab-11ea-8380-256c2ad381b4.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305
https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305:626,Modifiability,portab,portable-pipeline-project,626,"Not necessarily the problem, but even non-obvious GCP quotas can limit how many workers are scheduled. Specifically, some things to look out for:. - Preemtible specific resources, like CPUs and Memory (if you're running preemtibles); - If you are using preemtibles, there may not be enough available instances; - Local SSD (GB); - Internal IP addresses; - In-use IP addresses; - List requests per 100 seconds. I thought mine were high enough, but from this page (replace `$region` with your region) you can click the ""Current Usage"" to sort by in-demand resources:. - https://console.cloud.google.com/iam-admin/quotas?project=portable-pipeline-project&location=$region. ![image](https://user-images.githubusercontent.com/22381693/72295508-d2132900-36ab-11ea-8380-256c2ad381b4.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305
https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577747339:79,Deployability,pipeline,pipelines,79,"Looks good to me on a first pass. . Just to confirm, did you look into how the pipelines tool implements this (https://github.com/googlegenomics/pipelines-tools#ssh-into-the-worker-machine)? In the back of my mind there's a question over whether there's an existing API switch on the pipelines API for this (vs rolling your own action)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577747339
https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577747339:145,Deployability,pipeline,pipelines-tools,145,"Looks good to me on a first pass. . Just to confirm, did you look into how the pipelines tool implements this (https://github.com/googlegenomics/pipelines-tools#ssh-into-the-worker-machine)? In the back of my mind there's a question over whether there's an existing API switch on the pipelines API for this (vs rolling your own action)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577747339
https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577747339:284,Deployability,pipeline,pipelines,284,"Looks good to me on a first pass. . Just to confirm, did you look into how the pipelines tool implements this (https://github.com/googlegenomics/pipelines-tools#ssh-into-the-worker-machine)? In the back of my mind there's a question over whether there's an existing API switch on the pipelines API for this (vs rolling your own action)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577747339
https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577747339:311,Deployability,rolling,rolling,311,"Looks good to me on a first pass. . Just to confirm, did you look into how the pipelines tool implements this (https://github.com/googlegenomics/pipelines-tools#ssh-into-the-worker-machine)? In the back of my mind there's a question over whether there's an existing API switch on the pipelines API for this (vs rolling your own action)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577747339
https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577771625:142,Deployability,pipeline,pipelines-tools,142,"Hi @cjllanwarne, thanks for taking a look!. Yeah the link to ""google's implementation"" in the [description](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L759-L766) is where the Action came from; its essentially a direct port into Scala. You can see [here](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L402) where that flag is handled, if its set to true, then the Action is added. . The `ssh-server` entrypoint is google's own implementation for the pipelines api, found [here](https://github.com/googlegenomics/pipelines-tools/tree/749315a73e6c3bd5277351e32a365f42198db1ae/ssh-server). As far as I'm aware there's no direct switch on the existing API to have it do that automatically.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577771625
https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577771625:204,Deployability,pipeline,pipelines,204,"Hi @cjllanwarne, thanks for taking a look!. Yeah the link to ""google's implementation"" in the [description](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L759-L766) is where the Action came from; its essentially a direct port into Scala. You can see [here](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L402) where that flag is handled, if its set to true, then the Action is added. . The `ssh-server` entrypoint is google's own implementation for the pipelines api, found [here](https://github.com/googlegenomics/pipelines-tools/tree/749315a73e6c3bd5277351e32a365f42198db1ae/ssh-server). As far as I'm aware there's no direct switch on the existing API to have it do that automatically.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577771625
https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577771625:380,Deployability,pipeline,pipelines-tools,380,"Hi @cjllanwarne, thanks for taking a look!. Yeah the link to ""google's implementation"" in the [description](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L759-L766) is where the Action came from; its essentially a direct port into Scala. You can see [here](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L402) where that flag is handled, if its set to true, then the Action is added. . The `ssh-server` entrypoint is google's own implementation for the pipelines api, found [here](https://github.com/googlegenomics/pipelines-tools/tree/749315a73e6c3bd5277351e32a365f42198db1ae/ssh-server). As far as I'm aware there's no direct switch on the existing API to have it do that automatically.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577771625
https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577771625:442,Deployability,pipeline,pipelines,442,"Hi @cjllanwarne, thanks for taking a look!. Yeah the link to ""google's implementation"" in the [description](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L759-L766) is where the Action came from; its essentially a direct port into Scala. You can see [here](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L402) where that flag is handled, if its set to true, then the Action is added. . The `ssh-server` entrypoint is google's own implementation for the pipelines api, found [here](https://github.com/googlegenomics/pipelines-tools/tree/749315a73e6c3bd5277351e32a365f42198db1ae/ssh-server). As far as I'm aware there's no direct switch on the existing API to have it do that automatically.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577771625
https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577771625:630,Deployability,pipeline,pipelines,630,"Hi @cjllanwarne, thanks for taking a look!. Yeah the link to ""google's implementation"" in the [description](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L759-L766) is where the Action came from; its essentially a direct port into Scala. You can see [here](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L402) where that flag is handled, if its set to true, then the Action is added. . The `ssh-server` entrypoint is google's own implementation for the pipelines api, found [here](https://github.com/googlegenomics/pipelines-tools/tree/749315a73e6c3bd5277351e32a365f42198db1ae/ssh-server). As far as I'm aware there's no direct switch on the existing API to have it do that automatically.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577771625
https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577771625:692,Deployability,pipeline,pipelines-tools,692,"Hi @cjllanwarne, thanks for taking a look!. Yeah the link to ""google's implementation"" in the [description](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L759-L766) is where the Action came from; its essentially a direct port into Scala. You can see [here](https://github.com/googlegenomics/pipelines-tools/blob/749315a73e6c3bd5277351e32a365f42198db1ae/pipelines/internal/commands/run/run.go#L402) where that flag is handled, if its set to true, then the Action is added. . The `ssh-server` entrypoint is google's own implementation for the pipelines api, found [here](https://github.com/googlegenomics/pipelines-tools/tree/749315a73e6c3bd5277351e32a365f42198db1ae/ssh-server). As far as I'm aware there's no direct switch on the existing API to have it do that automatically.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-577771625
https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-578289552:113,Testability,test,tests,113,"Thanks @acoffman, sorry for missing that comment. I've now made an internal clone of your branch to allow our CI tests run over your changes (as #5385).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-578289552
https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-582120445:32,Deployability,update,updated,32,"@grsterin added support for the updated backend and tested with the same workflow/inputs as above, but using the new backend and all seems to be well. Thanks for taking a look at this!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-582120445
https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-582120445:52,Testability,test,tested,52,"@grsterin added support for the updated backend and tested with the same workflow/inputs as above, but using the new backend and all seems to be well. Thanks for taking a look at this!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-582120445
https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-584887151:4,Testability,test,tests,4,"The tests in #5385 have passed despite codecov's unhappiness on this PR, and with two thumbs I think this is ready for merging. Thanks again @acoffman !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5353#issuecomment-584887151
https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-575139656:12,Modifiability,config,config,12,"I found the config file in this link [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf);. **BUT** when I set the `concurrent-job-limit = 2`, and run with `-Dconfig.file=cromwell.conf`，in `local` mode，but cromwell still forks **8** job, it seems the limit not working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-575139656
https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-575139656:194,Performance,concurren,concurrent-job-limit,194,"I found the config file in this link [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf);. **BUT** when I set the `concurrent-job-limit = 2`, and run with `-Dconfig.file=cromwell.conf`，in `local` mode，but cromwell still forks **8** job, it seems the limit not working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-575139656
https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302:584,Availability,down,downloaded,584,"I have the exact same issue. First of all, both the [Configuration examples](https://cromwell.readthedocs.io/en/stable/Configuring/#configuration-examples) and [Local](https://cromwell.readthedocs.io/en/stable/backends/Local/) sections of the documentation point to non-existing file https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf while they should point to https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf like lozybean pointed out. This is still not fixed in the documentation. But then I have downloaded the [cromwell.examples.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf) file and used it as follows:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```. And Cromwell on my laptop still spawned 23 job tasks simultaneously. What do I have to do to limit the number of concurrent jobs? This would be very convenient for me to be able to speed up development of my own WDL. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302
https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302:53,Deployability,Configurat,Configuration,53,"I have the exact same issue. First of all, both the [Configuration examples](https://cromwell.readthedocs.io/en/stable/Configuring/#configuration-examples) and [Local](https://cromwell.readthedocs.io/en/stable/backends/Local/) sections of the documentation point to non-existing file https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf while they should point to https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf like lozybean pointed out. This is still not fixed in the documentation. But then I have downloaded the [cromwell.examples.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf) file and used it as follows:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```. And Cromwell on my laptop still spawned 23 job tasks simultaneously. What do I have to do to limit the number of concurrent jobs? This would be very convenient for me to be able to speed up development of my own WDL. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302
https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302:132,Deployability,configurat,configuration-examples,132,"I have the exact same issue. First of all, both the [Configuration examples](https://cromwell.readthedocs.io/en/stable/Configuring/#configuration-examples) and [Local](https://cromwell.readthedocs.io/en/stable/backends/Local/) sections of the documentation point to non-existing file https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf while they should point to https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf like lozybean pointed out. This is still not fixed in the documentation. But then I have downloaded the [cromwell.examples.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf) file and used it as follows:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```. And Cromwell on my laptop still spawned 23 job tasks simultaneously. What do I have to do to limit the number of concurrent jobs? This would be very convenient for me to be able to speed up development of my own WDL. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302
https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302:53,Modifiability,Config,Configuration,53,"I have the exact same issue. First of all, both the [Configuration examples](https://cromwell.readthedocs.io/en/stable/Configuring/#configuration-examples) and [Local](https://cromwell.readthedocs.io/en/stable/backends/Local/) sections of the documentation point to non-existing file https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf while they should point to https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf like lozybean pointed out. This is still not fixed in the documentation. But then I have downloaded the [cromwell.examples.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf) file and used it as follows:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```. And Cromwell on my laptop still spawned 23 job tasks simultaneously. What do I have to do to limit the number of concurrent jobs? This would be very convenient for me to be able to speed up development of my own WDL. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302
https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302:119,Modifiability,Config,Configuring,119,"I have the exact same issue. First of all, both the [Configuration examples](https://cromwell.readthedocs.io/en/stable/Configuring/#configuration-examples) and [Local](https://cromwell.readthedocs.io/en/stable/backends/Local/) sections of the documentation point to non-existing file https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf while they should point to https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf like lozybean pointed out. This is still not fixed in the documentation. But then I have downloaded the [cromwell.examples.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf) file and used it as follows:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```. And Cromwell on my laptop still spawned 23 job tasks simultaneously. What do I have to do to limit the number of concurrent jobs? This would be very convenient for me to be able to speed up development of my own WDL. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302
https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302:132,Modifiability,config,configuration-examples,132,"I have the exact same issue. First of all, both the [Configuration examples](https://cromwell.readthedocs.io/en/stable/Configuring/#configuration-examples) and [Local](https://cromwell.readthedocs.io/en/stable/backends/Local/) sections of the documentation point to non-existing file https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf while they should point to https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf like lozybean pointed out. This is still not fixed in the documentation. But then I have downloaded the [cromwell.examples.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf) file and used it as follows:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```. And Cromwell on my laptop still spawned 23 job tasks simultaneously. What do I have to do to limit the number of concurrent jobs? This would be very convenient for me to be able to speed up development of my own WDL. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302
https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302:907,Performance,concurren,concurrent-job-limit,907,"I have the exact same issue. First of all, both the [Configuration examples](https://cromwell.readthedocs.io/en/stable/Configuring/#configuration-examples) and [Local](https://cromwell.readthedocs.io/en/stable/backends/Local/) sections of the documentation point to non-existing file https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf while they should point to https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf like lozybean pointed out. This is still not fixed in the documentation. But then I have downloaded the [cromwell.examples.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf) file and used it as follows:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```. And Cromwell on my laptop still spawned 23 job tasks simultaneously. What do I have to do to limit the number of concurrent jobs? This would be very convenient for me to be able to speed up development of my own WDL. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302
https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302:932,Performance,concurren,concurrent-job-limit,932,"I have the exact same issue. First of all, both the [Configuration examples](https://cromwell.readthedocs.io/en/stable/Configuring/#configuration-examples) and [Local](https://cromwell.readthedocs.io/en/stable/backends/Local/) sections of the documentation point to non-existing file https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf while they should point to https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf like lozybean pointed out. This is still not fixed in the documentation. But then I have downloaded the [cromwell.examples.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf) file and used it as follows:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```. And Cromwell on my laptop still spawned 23 job tasks simultaneously. What do I have to do to limit the number of concurrent jobs? This would be very convenient for me to be able to speed up development of my own WDL. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302
https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302:1173,Performance,concurren,concurrent,1173,"I have the exact same issue. First of all, both the [Configuration examples](https://cromwell.readthedocs.io/en/stable/Configuring/#configuration-examples) and [Local](https://cromwell.readthedocs.io/en/stable/backends/Local/) sections of the documentation point to non-existing file https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf while they should point to https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf like lozybean pointed out. This is still not fixed in the documentation. But then I have downloaded the [cromwell.examples.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf) file and used it as follows:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```. And Cromwell on my laptop still spawned 23 job tasks simultaneously. What do I have to do to limit the number of concurrent jobs? This would be very convenient for me to be able to speed up development of my own WDL. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302
https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649838138:321,Performance,concurren,concurrent-job-limit,321,"Figured out the issue thanks to @cjllanwarne as I actually needed to run the following instead:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#default = ""LocalExample""/default = ""LocalExample""/' cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649838138
https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649838138:346,Performance,concurren,concurrent-job-limit,346,"Figured out the issue thanks to @cjllanwarne as I actually needed to run the following instead:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#default = ""LocalExample""/default = ""LocalExample""/' cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649838138
https://github.com/broadinstitute/cromwell/issues/5370#issuecomment-575730947:76,Security,hash,hashing,76,"I beleive that I have worked out the reason for this.; I'm using the ""path"" hashing strategy, and at out org there is an ongoing data move that is leaving symlinks behind, so all my paths in my reference file are still correct, but the real path has changed. Changing the hashing strategy to file has worked in at least one test case, and I'm trying a larger test now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370#issuecomment-575730947
https://github.com/broadinstitute/cromwell/issues/5370#issuecomment-575730947:272,Security,hash,hashing,272,"I beleive that I have worked out the reason for this.; I'm using the ""path"" hashing strategy, and at out org there is an ongoing data move that is leaving symlinks behind, so all my paths in my reference file are still correct, but the real path has changed. Changing the hashing strategy to file has worked in at least one test case, and I'm trying a larger test now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370#issuecomment-575730947
https://github.com/broadinstitute/cromwell/issues/5370#issuecomment-575730947:324,Testability,test,test,324,"I beleive that I have worked out the reason for this.; I'm using the ""path"" hashing strategy, and at out org there is an ongoing data move that is leaving symlinks behind, so all my paths in my reference file are still correct, but the real path has changed. Changing the hashing strategy to file has worked in at least one test case, and I'm trying a larger test now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370#issuecomment-575730947
https://github.com/broadinstitute/cromwell/issues/5370#issuecomment-575730947:359,Testability,test,test,359,"I beleive that I have worked out the reason for this.; I'm using the ""path"" hashing strategy, and at out org there is an ongoing data move that is leaving symlinks behind, so all my paths in my reference file are still correct, but the real path has changed. Changing the hashing strategy to file has worked in at least one test case, and I'm trying a larger test now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370#issuecomment-575730947
https://github.com/broadinstitute/cromwell/pull/5377#issuecomment-578859908:49,Testability,test,tests,49,"Squashing into two commits, then will merge once tests pass.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5377#issuecomment-578859908
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:385,Availability,down,down,385,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:37,Deployability,Pipeline,Pipelines,37,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:787,Deployability,Pipeline,Pipelines,787,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:498,Modifiability,Config,Configuring,498,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:105,Performance,queue,queue,105,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:711,Performance,perform,perform,711,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:797,Performance,queue,queue,797,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:220,Safety,Abort,Abort,220,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:302,Safety,abort,abort,302,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:511,Safety,abort,abort,511,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:551,Safety,abort,abort,551,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:613,Safety,abort,abort,613,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898:226,Usability,guid,guide,226,"Cromwell may submit more jobs to the Pipelines API than is able to run at one time, so they're held in a queue by Google Cloud. As jobs finish, the next job is run. There are a few ways to terminate a workflow (see the [Abort guide](https://cromwell.readthedocs.io/en/stable/execution/ExecutionTwists/#abort) for more information). But essentially you need Cromwell to gracefully shut down the workflow:. - In `run` mode, you can issue [SIGINT or SIGTERM](https://cromwell.readthedocs.io/en/stable/Configuring/#abort) which asks Cromwell to issue the abort requests to GCP,; - In `server` mode, you can issue an `abort` through a POST request. By running `scancel`, you may not give Cromwell sufficient time to perform this graceful shutdown process, and hence your jobs held in the GCP Pipelines queue will still execute.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380#issuecomment-579527898
https://github.com/broadinstitute/cromwell/pull/5382#issuecomment-577410441:64,Usability,simpl,simpler,64,"Yeah, for once I talked myself out of a regex, this is just way simpler to assess correctness on",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5382#issuecomment-577410441
https://github.com/broadinstitute/cromwell/pull/5382#issuecomment-577750772:24,Testability,test,test,24,@dinvlad if you want to test in a dev environment it should be possible to do that immediately (ie before merging the branch),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5382#issuecomment-577750772
https://github.com/broadinstitute/cromwell/pull/5382#issuecomment-577760343:20,Energy Efficiency,power,powers,20,Not sure I have the powers to push a dev image to Docker Hub though (let's discuss offline).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5382#issuecomment-577760343
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715:71,Deployability,configurat,configuration,71,"> Should we include a backwards compatibility adapter so that existing configuration still work? It would be sad if someone upgraded Cromwell without reading the release noted and suddenly they have no (recognized) backends configured. I'm not sure about that, since with `v2beta` there was introduced an additional mandatory parameter: `location` (currently supported `us-central1` and `europe-west2`). If we decide to coerce configured v2alpha1 actor factory to v2beta, then we'll also have choose some default value for `location`, which I'd be cautious to do due to possible issues (e.g., someone runs Cromwell in Europe and doesn't want their data to leave Europe)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715:124,Deployability,upgrade,upgraded,124,"> Should we include a backwards compatibility adapter so that existing configuration still work? It would be sad if someone upgraded Cromwell without reading the release noted and suddenly they have no (recognized) backends configured. I'm not sure about that, since with `v2beta` there was introduced an additional mandatory parameter: `location` (currently supported `us-central1` and `europe-west2`). If we decide to coerce configured v2alpha1 actor factory to v2beta, then we'll also have choose some default value for `location`, which I'd be cautious to do due to possible issues (e.g., someone runs Cromwell in Europe and doesn't want their data to leave Europe)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715:162,Deployability,release,release,162,"> Should we include a backwards compatibility adapter so that existing configuration still work? It would be sad if someone upgraded Cromwell without reading the release noted and suddenly they have no (recognized) backends configured. I'm not sure about that, since with `v2beta` there was introduced an additional mandatory parameter: `location` (currently supported `us-central1` and `europe-west2`). If we decide to coerce configured v2alpha1 actor factory to v2beta, then we'll also have choose some default value for `location`, which I'd be cautious to do due to possible issues (e.g., someone runs Cromwell in Europe and doesn't want their data to leave Europe)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715:46,Energy Efficiency,adapt,adapter,46,"> Should we include a backwards compatibility adapter so that existing configuration still work? It would be sad if someone upgraded Cromwell without reading the release noted and suddenly they have no (recognized) backends configured. I'm not sure about that, since with `v2beta` there was introduced an additional mandatory parameter: `location` (currently supported `us-central1` and `europe-west2`). If we decide to coerce configured v2alpha1 actor factory to v2beta, then we'll also have choose some default value for `location`, which I'd be cautious to do due to possible issues (e.g., someone runs Cromwell in Europe and doesn't want their data to leave Europe)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715:46,Integrability,adapter,adapter,46,"> Should we include a backwards compatibility adapter so that existing configuration still work? It would be sad if someone upgraded Cromwell without reading the release noted and suddenly they have no (recognized) backends configured. I'm not sure about that, since with `v2beta` there was introduced an additional mandatory parameter: `location` (currently supported `us-central1` and `europe-west2`). If we decide to coerce configured v2alpha1 actor factory to v2beta, then we'll also have choose some default value for `location`, which I'd be cautious to do due to possible issues (e.g., someone runs Cromwell in Europe and doesn't want their data to leave Europe)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715:46,Modifiability,adapt,adapter,46,"> Should we include a backwards compatibility adapter so that existing configuration still work? It would be sad if someone upgraded Cromwell without reading the release noted and suddenly they have no (recognized) backends configured. I'm not sure about that, since with `v2beta` there was introduced an additional mandatory parameter: `location` (currently supported `us-central1` and `europe-west2`). If we decide to coerce configured v2alpha1 actor factory to v2beta, then we'll also have choose some default value for `location`, which I'd be cautious to do due to possible issues (e.g., someone runs Cromwell in Europe and doesn't want their data to leave Europe)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715:71,Modifiability,config,configuration,71,"> Should we include a backwards compatibility adapter so that existing configuration still work? It would be sad if someone upgraded Cromwell without reading the release noted and suddenly they have no (recognized) backends configured. I'm not sure about that, since with `v2beta` there was introduced an additional mandatory parameter: `location` (currently supported `us-central1` and `europe-west2`). If we decide to coerce configured v2alpha1 actor factory to v2beta, then we'll also have choose some default value for `location`, which I'd be cautious to do due to possible issues (e.g., someone runs Cromwell in Europe and doesn't want their data to leave Europe)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715:224,Modifiability,config,configured,224,"> Should we include a backwards compatibility adapter so that existing configuration still work? It would be sad if someone upgraded Cromwell without reading the release noted and suddenly they have no (recognized) backends configured. I'm not sure about that, since with `v2beta` there was introduced an additional mandatory parameter: `location` (currently supported `us-central1` and `europe-west2`). If we decide to coerce configured v2alpha1 actor factory to v2beta, then we'll also have choose some default value for `location`, which I'd be cautious to do due to possible issues (e.g., someone runs Cromwell in Europe and doesn't want their data to leave Europe)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715:427,Modifiability,config,configured,427,"> Should we include a backwards compatibility adapter so that existing configuration still work? It would be sad if someone upgraded Cromwell without reading the release noted and suddenly they have no (recognized) backends configured. I'm not sure about that, since with `v2beta` there was introduced an additional mandatory parameter: `location` (currently supported `us-central1` and `europe-west2`). If we decide to coerce configured v2alpha1 actor factory to v2beta, then we'll also have choose some default value for `location`, which I'd be cautious to do due to possible issues (e.g., someone runs Cromwell in Europe and doesn't want their data to leave Europe)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:218,Availability,error,errors,218,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:125,Deployability,update,update,125,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:32,Energy Efficiency,adapt,adapter,32,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:32,Integrability,adapter,adapter,32,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:32,Modifiability,adapt,adapter,32,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:53,Modifiability,config,config,53,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:137,Modifiability,config,config,137,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:74,Testability,stub,stub,74,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:220,Availability,error,errors,220,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:127,Deployability,update,update,127,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:34,Energy Efficiency,adapt,adapter,34,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:34,Integrability,adapter,adapter,34,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:34,Modifiability,adapt,adapter,34,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:55,Modifiability,config,config,55,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:139,Modifiability,config,config,139,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:76,Testability,stub,stub,76,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580354869:180,Modifiability,config,config,180,"I'm not a big fan of copy/pasting the entire backend - not least because now all edit history in git from the original files is lost. Is it possible to bring in alpha vs beta as a config option to the backend and just switch which API gets called at the relevant points in the code?. eg; ```; backends {; PAPIv2alpha {; class="".../papiv2backend""; config {; api_version: ""alpha""; }; }; PAPIv2alpha {; class="".../papiv2backend""; config {; api_version: ""beta""; }; }; }; ```. Alternatively, could the `class="".../papiv2beta""` backend just be a really thin extension of the existing papiv2alpha backend, which just overrides the api which gets called during submission and status polling?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580354869
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580354869:347,Modifiability,config,config,347,"I'm not a big fan of copy/pasting the entire backend - not least because now all edit history in git from the original files is lost. Is it possible to bring in alpha vs beta as a config option to the backend and just switch which API gets called at the relevant points in the code?. eg; ```; backends {; PAPIv2alpha {; class="".../papiv2backend""; config {; api_version: ""alpha""; }; }; PAPIv2alpha {; class="".../papiv2backend""; config {; api_version: ""beta""; }; }; }; ```. Alternatively, could the `class="".../papiv2beta""` backend just be a really thin extension of the existing papiv2alpha backend, which just overrides the api which gets called during submission and status polling?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580354869
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580354869:427,Modifiability,config,config,427,"I'm not a big fan of copy/pasting the entire backend - not least because now all edit history in git from the original files is lost. Is it possible to bring in alpha vs beta as a config option to the backend and just switch which API gets called at the relevant points in the code?. eg; ```; backends {; PAPIv2alpha {; class="".../papiv2backend""; config {; api_version: ""alpha""; }; }; PAPIv2alpha {; class="".../papiv2backend""; config {; api_version: ""beta""; }; }; }; ```. Alternatively, could the `class="".../papiv2beta""` backend just be a really thin extension of the existing papiv2alpha backend, which just overrides the api which gets called during submission and status polling?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580354869
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936:1005,Availability,error,error-prone,1005,"> I'm not a big fan of copy/pasting the entire backend - not least because now all edit history in git from the original files is lost.; > ; > Is it possible to bring in alpha vs beta as a config option to the backend and just switch which API gets called at the relevant points in the code?; > ; > eg; > ; > ```; > backends {; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""alpha""; > }; > }; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""beta""; > }; > }; > }; > ```; > ; > Alternatively, could the `class="".../papiv2beta""` backend just be a really thin extension of the existing papiv2alpha backend, which just overrides the api which gets called during submission and status polling?. I'm not a fan of copy-pasting the whole backend too, but in my opinion, in this particular case it's well justified:; 1. In future (I hope sooner than later) when we'll decide to remove v2aplha1, it'll be as easy as deleting a whole single package.; 2. It's less error-prone - we don't modify v2alpha1 implementation at all.; 3. Between the v2alpha1 and v2beta Google made changes not only in the URL and operation id format, but also in the data model (the most significant ones in `Event` and `Action` classes), so I'm afraid that thin extension would end up not so thin.; 4. Also, I don't think git history will be lost. I think git will consider that as files being renamed. At least this is what I'm seeing on `git log --follow -- /Users/gsterin/projects/cromwell/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/LifeSciencesFactory.scala`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936:1536,Deployability,pipeline,pipelines,1536,"> I'm not a big fan of copy/pasting the entire backend - not least because now all edit history in git from the original files is lost.; > ; > Is it possible to bring in alpha vs beta as a config option to the backend and just switch which API gets called at the relevant points in the code?; > ; > eg; > ; > ```; > backends {; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""alpha""; > }; > }; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""beta""; > }; > }; > }; > ```; > ; > Alternatively, could the `class="".../papiv2beta""` backend just be a really thin extension of the existing papiv2alpha backend, which just overrides the api which gets called during submission and status polling?. I'm not a fan of copy-pasting the whole backend too, but in my opinion, in this particular case it's well justified:; 1. In future (I hope sooner than later) when we'll decide to remove v2aplha1, it'll be as easy as deleting a whole single package.; 2. It's less error-prone - we don't modify v2alpha1 implementation at all.; 3. Between the v2alpha1 and v2beta Google made changes not only in the URL and operation id format, but also in the data model (the most significant ones in `Event` and `Action` classes), so I'm afraid that thin extension would end up not so thin.; 4. Also, I don't think git history will be lost. I think git will consider that as files being renamed. At least this is what I'm seeing on `git log --follow -- /Users/gsterin/projects/cromwell/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/LifeSciencesFactory.scala`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936:1592,Deployability,pipeline,pipelines,1592,"> I'm not a big fan of copy/pasting the entire backend - not least because now all edit history in git from the original files is lost.; > ; > Is it possible to bring in alpha vs beta as a config option to the backend and just switch which API gets called at the relevant points in the code?; > ; > eg; > ; > ```; > backends {; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""alpha""; > }; > }; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""beta""; > }; > }; > }; > ```; > ; > Alternatively, could the `class="".../papiv2beta""` backend just be a really thin extension of the existing papiv2alpha backend, which just overrides the api which gets called during submission and status polling?. I'm not a fan of copy-pasting the whole backend too, but in my opinion, in this particular case it's well justified:; 1. In future (I hope sooner than later) when we'll decide to remove v2aplha1, it'll be as easy as deleting a whole single package.; 2. It's less error-prone - we don't modify v2alpha1 implementation at all.; 3. Between the v2alpha1 and v2beta Google made changes not only in the URL and operation id format, but also in the data model (the most significant ones in `Event` and `Action` classes), so I'm afraid that thin extension would end up not so thin.; 4. Also, I don't think git history will be lost. I think git will consider that as files being renamed. At least this is what I'm seeing on `git log --follow -- /Users/gsterin/projects/cromwell/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/LifeSciencesFactory.scala`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936:189,Modifiability,config,config,189,"> I'm not a big fan of copy/pasting the entire backend - not least because now all edit history in git from the original files is lost.; > ; > Is it possible to bring in alpha vs beta as a config option to the backend and just switch which API gets called at the relevant points in the code?; > ; > eg; > ; > ```; > backends {; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""alpha""; > }; > }; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""beta""; > }; > }; > }; > ```; > ; > Alternatively, could the `class="".../papiv2beta""` backend just be a really thin extension of the existing papiv2alpha backend, which just overrides the api which gets called during submission and status polling?. I'm not a fan of copy-pasting the whole backend too, but in my opinion, in this particular case it's well justified:; 1. In future (I hope sooner than later) when we'll decide to remove v2aplha1, it'll be as easy as deleting a whole single package.; 2. It's less error-prone - we don't modify v2alpha1 implementation at all.; 3. Between the v2alpha1 and v2beta Google made changes not only in the URL and operation id format, but also in the data model (the most significant ones in `Event` and `Action` classes), so I'm afraid that thin extension would end up not so thin.; 4. Also, I don't think git history will be lost. I think git will consider that as files being renamed. At least this is what I'm seeing on `git log --follow -- /Users/gsterin/projects/cromwell/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/LifeSciencesFactory.scala`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936:376,Modifiability,config,config,376,"> I'm not a big fan of copy/pasting the entire backend - not least because now all edit history in git from the original files is lost.; > ; > Is it possible to bring in alpha vs beta as a config option to the backend and just switch which API gets called at the relevant points in the code?; > ; > eg; > ; > ```; > backends {; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""alpha""; > }; > }; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""beta""; > }; > }; > }; > ```; > ; > Alternatively, could the `class="".../papiv2beta""` backend just be a really thin extension of the existing papiv2alpha backend, which just overrides the api which gets called during submission and status polling?. I'm not a fan of copy-pasting the whole backend too, but in my opinion, in this particular case it's well justified:; 1. In future (I hope sooner than later) when we'll decide to remove v2aplha1, it'll be as easy as deleting a whole single package.; 2. It's less error-prone - we don't modify v2alpha1 implementation at all.; 3. Between the v2alpha1 and v2beta Google made changes not only in the URL and operation id format, but also in the data model (the most significant ones in `Event` and `Action` classes), so I'm afraid that thin extension would end up not so thin.; 4. Also, I don't think git history will be lost. I think git will consider that as files being renamed. At least this is what I'm seeing on `git log --follow -- /Users/gsterin/projects/cromwell/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/LifeSciencesFactory.scala`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936:468,Modifiability,config,config,468,"> I'm not a big fan of copy/pasting the entire backend - not least because now all edit history in git from the original files is lost.; > ; > Is it possible to bring in alpha vs beta as a config option to the backend and just switch which API gets called at the relevant points in the code?; > ; > eg; > ; > ```; > backends {; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""alpha""; > }; > }; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""beta""; > }; > }; > }; > ```; > ; > Alternatively, could the `class="".../papiv2beta""` backend just be a really thin extension of the existing papiv2alpha backend, which just overrides the api which gets called during submission and status polling?. I'm not a fan of copy-pasting the whole backend too, but in my opinion, in this particular case it's well justified:; 1. In future (I hope sooner than later) when we'll decide to remove v2aplha1, it'll be as easy as deleting a whole single package.; 2. It's less error-prone - we don't modify v2alpha1 implementation at all.; 3. Between the v2alpha1 and v2beta Google made changes not only in the URL and operation id format, but also in the data model (the most significant ones in `Event` and `Action` classes), so I'm afraid that thin extension would end up not so thin.; 4. Also, I don't think git history will be lost. I think git will consider that as files being renamed. At least this is what I'm seeing on `git log --follow -- /Users/gsterin/projects/cromwell/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/LifeSciencesFactory.scala`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936
https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936:1462,Testability,log,log,1462,"> I'm not a big fan of copy/pasting the entire backend - not least because now all edit history in git from the original files is lost.; > ; > Is it possible to bring in alpha vs beta as a config option to the backend and just switch which API gets called at the relevant points in the code?; > ; > eg; > ; > ```; > backends {; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""alpha""; > }; > }; > PAPIv2alpha {; > class="".../papiv2backend""; > config {; > api_version: ""beta""; > }; > }; > }; > ```; > ; > Alternatively, could the `class="".../papiv2beta""` backend just be a really thin extension of the existing papiv2alpha backend, which just overrides the api which gets called during submission and status polling?. I'm not a fan of copy-pasting the whole backend too, but in my opinion, in this particular case it's well justified:; 1. In future (I hope sooner than later) when we'll decide to remove v2aplha1, it'll be as easy as deleting a whole single package.; 2. It's less error-prone - we don't modify v2alpha1 implementation at all.; 3. Between the v2alpha1 and v2beta Google made changes not only in the URL and operation id format, but also in the data model (the most significant ones in `Event` and `Action` classes), so I'm afraid that thin extension would end up not so thin.; 4. Also, I don't think git history will be lost. I think git will consider that as files being renamed. At least this is what I'm seeing on `git log --follow -- /Users/gsterin/projects/cromwell/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/LifeSciencesFactory.scala`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580366936
https://github.com/broadinstitute/cromwell/pull/5389#issuecomment-578990336:23,Modifiability,layers,layers,23,https://hub.docker.com/layers/broadinstitute/cromwell/49-c58d88c-SNAP/images/sha256-e0cf331faa3486a9d4d6a19d26da0facc11c039dd2d2f6383ed3042e73187ca9,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5389#issuecomment-578990336
https://github.com/broadinstitute/cromwell/pull/5391#issuecomment-578991896:23,Modifiability,layers,layers,23,https://hub.docker.com/layers/broadinstitute/cromwell/49-aa5dd9b-SNAP/images/sha256-65a104b766da0ee6f96243d28579531114ea2fee8542ef34f62fc0ea09bf4e6a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5391#issuecomment-578991896
https://github.com/broadinstitute/cromwell/issues/5395#issuecomment-580084497:66,Testability,log,login,66,Closing as this appears to be an issue with memory restriction on login nodes and not a Cromwell issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395#issuecomment-580084497
https://github.com/broadinstitute/cromwell/pull/5402#issuecomment-581559909:66,Modifiability,config,config,66,"Thanks @dirkpetersen - I thought this sounded familiar!. Does the config line suggested in the PR you found fix your problem too? If so, I think I would suggest leaving the default as it is and letting you override the glob link command in your config?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5402#issuecomment-581559909
https://github.com/broadinstitute/cromwell/pull/5402#issuecomment-581559909:245,Modifiability,config,config,245,"Thanks @dirkpetersen - I thought this sounded familiar!. Does the config line suggested in the PR you found fix your problem too? If so, I think I would suggest leaving the default as it is and letting you override the glob link command in your config?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5402#issuecomment-581559909
https://github.com/broadinstitute/cromwell/pull/5402#issuecomment-585269138:86,Modifiability,config,config,86,"Hey @dirkpetersen - I'm going to close this PR for now since I think you identified a config option to achieve what you want without needing a code change. If you disagree, or the config option isn't what you needed, feel free to re-open the PR either as-is or slightly different.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5402#issuecomment-585269138
https://github.com/broadinstitute/cromwell/pull/5402#issuecomment-585269138:180,Modifiability,config,config,180,"Hey @dirkpetersen - I'm going to close this PR for now since I think you identified a config option to achieve what you want without needing a code change. If you disagree, or the config option isn't what you needed, feel free to re-open the PR either as-is or slightly different.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5402#issuecomment-585269138
https://github.com/broadinstitute/cromwell/pull/5402#issuecomment-585341098:92,Modifiability,config,config,92,"@dirkpetersen For the record, the next version of SMRT Link will have the modified Cromwell config too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5402#issuecomment-585341098
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-583216806:75,Performance,perform,performance,75,"Hey @cmarkello, unrelated to your initial problem, but how do you find the performance of the file-hash based caching for Cromwell? We've found it to be incredibly CPU / memory / network intensive for large (~250GB) input files so looking for alternatives (#5346).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-583216806
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-583216806:99,Security,hash,hash,99,"Hey @cmarkello, unrelated to your initial problem, but how do you find the performance of the file-hash based caching for Cromwell? We've found it to be incredibly CPU / memory / network intensive for large (~250GB) input files so looking for alternatives (#5346).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-583216806
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-589546514:111,Modifiability,Config,Configuring,111,@illusional. There is a path+modtime strategy [in the documentation](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options) . That is what we use on our cluster and it works fine.; @cmarkello have you tried running cromwell with the `-m` flag to capture metadata? I believe the call-caching values are stored in the metadata. These can be used to diagnose the problem.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-589546514
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-589857972:326,Performance,Cache,Cache,326,"@rhpvorderman I have tried running cromwell with the `--metadata-output` flag. That's indicated in the JIRA issue.; For the tasks that fail to activate call caching I get the following metadata entry:; ```; ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""hit"": false,; ""result"": ""Cache Miss""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-589857972
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-590205206:85,Performance,cache,cache,85,@cmarkello. I believe the metadata also shows the data it uses to evaluate whether a cache entry is the same. This can be used for debugging I believe.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-590205206
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047358094:96,Modifiability,config,configure,96,Is call-cache unavailable if you use the Singularity image file?. Is there a solution? How do I configure it?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047358094
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047358094:8,Performance,cache,cache,8,Is call-cache unavailable if you use the Singularity image file?. Is there a solution? How do I configure it?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047358094
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047359061:79,Testability,log,logs,79,"Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047359061
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370199:82,Modifiability,config,configure,82,"We use singularity images too, are you following the Cromwell Containers guide to configure singularity: https://cromwell.readthedocs.io/en/stable/tutorials/Containers/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370199
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370199:73,Usability,guid,guide,73,"We use singularity images too, are you following the Cromwell Containers guide to configure singularity: https://cromwell.readthedocs.io/en/stable/tutorials/Containers/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370199
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:527,Availability,alive,alive,527,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:609,Availability,alive,alive,609,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:743,Availability,alive,alive,743,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:1803,Availability,alive,alive,1803,"ey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; }; `",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:338,Modifiability,config,config,338,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:345,Modifiability,Config,ConfigBackendLifecycleActorFactory,345,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:382,Modifiability,config,config,382,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:415,Performance,concurren,concurrent,415,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:432,Performance,concurren,concurrent-job-limit,432,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:479,Safety,timeout,timeout-seconds,479,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:792,Safety,timeout,timeout-seconds,792,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680:81,Testability,log,logs,81,"> Hey @kevin-furant, we had success getting it working. Are you seeing any weird logs? Is your Cromwell instance correctly resolving the docker digest (so it's requesting an image like ""imageName@sha256:ad21[...]"")?. We cannot use Docker on our cluster, I use a Singularity image file; ` SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 300. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. runtime-attributes = """"""; Int cpu = 1; Float memory_gb = 1; String? docker_mount; String? docker; String? sge_queue = ""bc_b2c_rd.q,b2c_rd_s1.q""; String? sge_project = ""P18Z15000N0143""; """""". runtime-attributes-for-caching {; # singularity_image: true; }. submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; IMAGE=/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124/${docker}.sif; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb + ""g""} \; ${""-l p="" + cpu } \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; singularity exec --containall --bind ${docker_mount}:${docker_mount} --bind ${cwd}:${cwd} --bind ${cwd}:${docker_cwd} $IMAGE /bin/bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }; `; ` runtime {; docker: ""qc_align""; docker_mount: ""/zfsyt1/B2C_RD_P2/USER/fuxiangke/wgs_server_mode_0124""; cpu: cpu; memory: ""~{mem}GB"" ; };",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047370680
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047374011:349,Performance,cache,cache,349,"> 是的，很酷，当您指定一个 docker 时，Cromwell 必须解析映像摘要才能使调用缓存正常工作。; > ; > 如果 Cromwell 无法在线找到 docker，或者您的代理阻止了 Cromwell，或者该映像不在线，则映像摘要无法正确解析，并且调用缓存处于禁用状态。; > ; > 这里有一些进一步的上下文： #6140. I also learned this from the official documents. Our cluster individual cannot use Docker, nor can it be connected to the Internet, so we have to choose between mirroring and Call-cache. Thank you very much for your answer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047374011
https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047374011:176,Usability,learn,learned,176,"> 是的，很酷，当您指定一个 docker 时，Cromwell 必须解析映像摘要才能使调用缓存正常工作。; > ; > 如果 Cromwell 无法在线找到 docker，或者您的代理阻止了 Cromwell，或者该映像不在线，则映像摘要无法正确解析，并且调用缓存处于禁用状态。; > ; > 这里有一些进一步的上下文： #6140. I also learned this from the official documents. Our cluster individual cannot use Docker, nor can it be connected to the Internet, so we have to choose between mirroring and Call-cache. Thank you very much for your answer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5405#issuecomment-1047374011
https://github.com/broadinstitute/cromwell/pull/5406#issuecomment-584202060:43,Testability,test,test,43,I'd feel better about thumbing this with a test case,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5406#issuecomment-584202060
https://github.com/broadinstitute/cromwell/pull/5406#issuecomment-584758749:60,Testability,test,test,60,> Bunting on review because it looks like we are adding the test in #5408. ![](http://www.vintagebluebird.co.uk/images/srv/calendar/Ashbourne/Ashbourne_Bunting_view_to_Church.jpg),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5406#issuecomment-584758749
https://github.com/broadinstitute/cromwell/pull/5406#issuecomment-584766720:202,Performance,cache,cache,202,> Bunting on review because it looks like we are adding the test in #5408. Not the type of bird usually drawn to my PRs but I'll take it ![indigo bunting](https://pittsburghquarterly.com/media/k2/items/cache/ff0158c2594917cd6a9c4e297e8a8d7c_XL.jpg),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5406#issuecomment-584766720
https://github.com/broadinstitute/cromwell/pull/5406#issuecomment-584766720:60,Testability,test,test,60,> Bunting on review because it looks like we are adding the test in #5408. Not the type of bird usually drawn to my PRs but I'll take it ![indigo bunting](https://pittsburghquarterly.com/media/k2/items/cache/ff0158c2594917cd6a9c4e297e8a8d7c_XL.jpg),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5406#issuecomment-584766720
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:494,Deployability,update,updates,494,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:679,Energy Efficiency,efficient,efficient,679,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:730,Performance,queue,queue,730,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:937,Performance,perform,performance,937,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:116,Security,hash,hash,116,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:279,Security,hash,hash,279,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:28,Testability,test,tests,28,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:330,Usability,undo,undo,330,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584890956:53,Performance,perform,performance,53,"> I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria. But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:. 1. Writing the entries we know don't need to be summarized to the summarization queue.; 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584890956
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584890956:426,Performance,queue,queue,426,"> I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria. But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:. 1. Writing the entries we know don't need to be summarized to the summarization queue.; 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584890956
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584890956:338,Safety,avoid,avoid,338,"> I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria. But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:. 1. Writing the entries we know don't need to be summarized to the summarization queue.; 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584890956
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534:55,Performance,perform,performance,55,"> > I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.; > ; > But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:; > ; > 1. Writing the entries we know don't need to be summarized to the summarization queue.; > 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable. I'm still reluctant to do that:; 1. Yes, but writing a single additional number per entry to a summary table is not nearly a huge overhead, taking into attention that, as you said, in the same transaction we are writing some huge metadata entries into the `metadata_entry` table.; 2. Summarizer works asynchronously and though I agree that we should keep it's performance good enough, moving metadata key filtering logic to the ""write-metadata-entry"" side of things may affect performance of running workflows.; > Reading the non-summarizable metadata _value_s from the database into Cromwell. Also, if my understanding is correct, this is how summarizer works right now. There are other possible things which we can do to optimize summarizer performance, one of which would be to add additional `metadata_key` column to our new queue table, and then allow summarizer to decide if it wants to fetch data from `metadata_entry` table based on that key. But this is food for thought for future optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534:441,Performance,queue,queue,441,"> > I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.; > ; > But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:; > ; > 1. Writing the entries we know don't need to be summarized to the summarization queue.; > 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable. I'm still reluctant to do that:; 1. Yes, but writing a single additional number per entry to a summary table is not nearly a huge overhead, taking into attention that, as you said, in the same transaction we are writing some huge metadata entries into the `metadata_entry` table.; 2. Summarizer works asynchronously and though I agree that we should keep it's performance good enough, moving metadata key filtering logic to the ""write-metadata-entry"" side of things may affect performance of running workflows.; > Reading the non-summarizable metadata _value_s from the database into Cromwell. Also, if my understanding is correct, this is how summarizer works right now. There are other possible things which we can do to optimize summarizer performance, one of which would be to add additional `metadata_key` column to our new queue table, and then allow summarizer to decide if it wants to fetch data from `metadata_entry` table based on that key. But this is food for thought for future optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534:1007,Performance,perform,performance,1007,"> > I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.; > ; > But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:; > ; > 1. Writing the entries we know don't need to be summarized to the summarization queue.; > 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable. I'm still reluctant to do that:; 1. Yes, but writing a single additional number per entry to a summary table is not nearly a huge overhead, taking into attention that, as you said, in the same transaction we are writing some huge metadata entries into the `metadata_entry` table.; 2. Summarizer works asynchronously and though I agree that we should keep it's performance good enough, moving metadata key filtering logic to the ""write-metadata-entry"" side of things may affect performance of running workflows.; > Reading the non-summarizable metadata _value_s from the database into Cromwell. Also, if my understanding is correct, this is how summarizer works right now. There are other possible things which we can do to optimize summarizer performance, one of which would be to add additional `metadata_key` column to our new queue table, and then allow summarizer to decide if it wants to fetch data from `metadata_entry` table based on that key. But this is food for thought for future optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534:1124,Performance,perform,performance,1124,"> > I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.; > ; > But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:; > ; > 1. Writing the entries we know don't need to be summarized to the summarization queue.; > 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable. I'm still reluctant to do that:; 1. Yes, but writing a single additional number per entry to a summary table is not nearly a huge overhead, taking into attention that, as you said, in the same transaction we are writing some huge metadata entries into the `metadata_entry` table.; 2. Summarizer works asynchronously and though I agree that we should keep it's performance good enough, moving metadata key filtering logic to the ""write-metadata-entry"" side of things may affect performance of running workflows.; > Reading the non-summarizable metadata _value_s from the database into Cromwell. Also, if my understanding is correct, this is how summarizer works right now. There are other possible things which we can do to optimize summarizer performance, one of which would be to add additional `metadata_key` column to our new queue table, and then allow summarizer to decide if it wants to fetch data from `metadata_entry` table based on that key. But this is food for thought for future optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534:1370,Performance,optimiz,optimize,1370,"> > I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.; > ; > But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:; > ; > 1. Writing the entries we know don't need to be summarized to the summarization queue.; > 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable. I'm still reluctant to do that:; 1. Yes, but writing a single additional number per entry to a summary table is not nearly a huge overhead, taking into attention that, as you said, in the same transaction we are writing some huge metadata entries into the `metadata_entry` table.; 2. Summarizer works asynchronously and though I agree that we should keep it's performance good enough, moving metadata key filtering logic to the ""write-metadata-entry"" side of things may affect performance of running workflows.; > Reading the non-summarizable metadata _value_s from the database into Cromwell. Also, if my understanding is correct, this is how summarizer works right now. There are other possible things which we can do to optimize summarizer performance, one of which would be to add additional `metadata_key` column to our new queue table, and then allow summarizer to decide if it wants to fetch data from `metadata_entry` table based on that key. But this is food for thought for future optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534:1390,Performance,perform,performance,1390,"> > I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.; > ; > But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:; > ; > 1. Writing the entries we know don't need to be summarized to the summarization queue.; > 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable. I'm still reluctant to do that:; 1. Yes, but writing a single additional number per entry to a summary table is not nearly a huge overhead, taking into attention that, as you said, in the same transaction we are writing some huge metadata entries into the `metadata_entry` table.; 2. Summarizer works asynchronously and though I agree that we should keep it's performance good enough, moving metadata key filtering logic to the ""write-metadata-entry"" side of things may affect performance of running workflows.; > Reading the non-summarizable metadata _value_s from the database into Cromwell. Also, if my understanding is correct, this is how summarizer works right now. There are other possible things which we can do to optimize summarizer performance, one of which would be to add additional `metadata_key` column to our new queue table, and then allow summarizer to decide if it wants to fetch data from `metadata_entry` table based on that key. But this is food for thought for future optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534:1476,Performance,queue,queue,1476,"> > I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.; > ; > But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:; > ; > 1. Writing the entries we know don't need to be summarized to the summarization queue.; > 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable. I'm still reluctant to do that:; 1. Yes, but writing a single additional number per entry to a summary table is not nearly a huge overhead, taking into attention that, as you said, in the same transaction we are writing some huge metadata entries into the `metadata_entry` table.; 2. Summarizer works asynchronously and though I agree that we should keep it's performance good enough, moving metadata key filtering logic to the ""write-metadata-entry"" side of things may affect performance of running workflows.; > Reading the non-summarizable metadata _value_s from the database into Cromwell. Also, if my understanding is correct, this is how summarizer works right now. There are other possible things which we can do to optimize summarizer performance, one of which would be to add additional `metadata_key` column to our new queue table, and then allow summarizer to decide if it wants to fetch data from `metadata_entry` table based on that key. But this is food for thought for future optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534:1638,Performance,optimiz,optimizations,1638,"> > I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.; > ; > But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:; > ; > 1. Writing the entries we know don't need to be summarized to the summarization queue.; > 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable. I'm still reluctant to do that:; 1. Yes, but writing a single additional number per entry to a summary table is not nearly a huge overhead, taking into attention that, as you said, in the same transaction we are writing some huge metadata entries into the `metadata_entry` table.; 2. Summarizer works asynchronously and though I agree that we should keep it's performance good enough, moving metadata key filtering logic to the ""write-metadata-entry"" side of things may affect performance of running workflows.; > Reading the non-summarizable metadata _value_s from the database into Cromwell. Also, if my understanding is correct, this is how summarizer works right now. There are other possible things which we can do to optimize summarizer performance, one of which would be to add additional `metadata_key` column to our new queue table, and then allow summarizer to decide if it wants to fetch data from `metadata_entry` table based on that key. But this is food for thought for future optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534:347,Safety,avoid,avoid,347,"> > I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.; > ; > But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:; > ; > 1. Writing the entries we know don't need to be summarized to the summarization queue.; > 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable. I'm still reluctant to do that:; 1. Yes, but writing a single additional number per entry to a summary table is not nearly a huge overhead, taking into attention that, as you said, in the same transaction we are writing some huge metadata entries into the `metadata_entry` table.; 2. Summarizer works asynchronously and though I agree that we should keep it's performance good enough, moving metadata key filtering logic to the ""write-metadata-entry"" side of things may affect performance of running workflows.; > Reading the non-summarizable metadata _value_s from the database into Cromwell. Also, if my understanding is correct, this is how summarizer works right now. There are other possible things which we can do to optimize summarizer performance, one of which would be to add additional `metadata_key` column to our new queue table, and then allow summarizer to decide if it wants to fetch data from `metadata_entry` table based on that key. But this is food for thought for future optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534
https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534:1062,Testability,log,logic,1062,"> > I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.; > ; > But sooner or later we have to do that anyway on the summarization side (and probably on the order of 99% of metadata doesn't need summarizing). By doing it before we write to the DB, I think we could avoid:; > ; > 1. Writing the entries we know don't need to be summarized to the summarization queue.; > 2. Reading the non-summarizable metadata _value_s from the database into Cromwell (and some of them are pretty big) - only to discard them when we read the key and discover it's not summarizable. I'm still reluctant to do that:; 1. Yes, but writing a single additional number per entry to a summary table is not nearly a huge overhead, taking into attention that, as you said, in the same transaction we are writing some huge metadata entries into the `metadata_entry` table.; 2. Summarizer works asynchronously and though I agree that we should keep it's performance good enough, moving metadata key filtering logic to the ""write-metadata-entry"" side of things may affect performance of running workflows.; > Reading the non-summarizable metadata _value_s from the database into Cromwell. Also, if my understanding is correct, this is how summarizer works right now. There are other possible things which we can do to optimize summarizer performance, one of which would be to add additional `metadata_key` column to our new queue table, and then allow summarizer to decide if it wants to fetch data from `metadata_entry` table based on that key. But this is food for thought for future optimizations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-585322534
https://github.com/broadinstitute/cromwell/pull/5413#issuecomment-608971741:40,Deployability,release,released,40,"Thank you. When (time frame) will it be released after the merge?. On Fri, Apr 3, 2020 at 12:48 PM Chris Llanwarne <notifications@github.com>; wrote:. > I've made a clone of this PR (#5475; > <https://github.com/broadinstitute/cromwell/pull/5475>) to allow our CI; > to run against these changes. Assuming all looks good I think we'll be able; > to merge!; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5413#issuecomment-608626878>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABBCBVPS2R3XQKUFVN5NFV3RKY4RJANCNFSM4KUEM3YQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5413#issuecomment-608971741
https://github.com/broadinstitute/cromwell/issues/5414#issuecomment-599692822:41,Availability,error,errors,41,"Thanks for the clarification. I get same errors with `version development`.; ; > Since you specified version 1.0 it is not weird that it crashes on stuff that is not valid WDL 1.0. I find this a little surprising. Maybe it is a documentation issue. The 1.0 spec says object literal and then shows an example using map literal? . > Struct Assignment from Object Literal; > Structs can be assigned using an object literal. When Writing the object, all entries must conform or be coercible into the underlying type they are being assigned to; > ; > Person a = {""name"": ""John"",""age"": 30}. In any case I think I was confusing Object type with object literal. I took following to mean object notation was also being removed:. > Be careful when using Object. They are superceded by 'struct' in WDL 1.0 and are being removed outright in WDL 2.0. Can probably resolve this if this is intended usage.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414#issuecomment-599692822
https://github.com/broadinstitute/cromwell/pull/5418#issuecomment-590590276:4,Testability,test,tests,4,The tests in #5422 passed but indicated that the majority of the code is not actually exercised by tests. I'll let you decide whether that's alright for your backend feature or not. Just waiting for one more 👍 before we can merge this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5418#issuecomment-590590276
https://github.com/broadinstitute/cromwell/pull/5418#issuecomment-590590276:99,Testability,test,tests,99,The tests in #5422 passed but indicated that the majority of the code is not actually exercised by tests. I'll let you decide whether that's alright for your backend feature or not. Just waiting for one more 👍 before we can merge this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5418#issuecomment-590590276
https://github.com/broadinstitute/cromwell/pull/5418#issuecomment-593658202:15,Testability,test,tests,15,"Re-running the tests in #5422, then we should be good to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5418#issuecomment-593658202
https://github.com/broadinstitute/cromwell/pull/5420#issuecomment-620603339:107,Testability,test,tests,107,doesn't seem like a necessity as its been fully worked around on the client side where the WDL conformance tests are. Sorry for asking for help but not merging -- and keeping it open so long!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5420#issuecomment-620603339
https://github.com/broadinstitute/cromwell/issues/5421#issuecomment-589380724:67,Availability,down,download,67,"So, in my testing, this appears to only happen in the scatter is a download from s3 job. Is it possible that heavy network congestion could create this error? The error itself doesn't seem to be associated with or come from the download, but then again I'm not sure what it means.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421#issuecomment-589380724
https://github.com/broadinstitute/cromwell/issues/5421#issuecomment-589380724:152,Availability,error,error,152,"So, in my testing, this appears to only happen in the scatter is a download from s3 job. Is it possible that heavy network congestion could create this error? The error itself doesn't seem to be associated with or come from the download, but then again I'm not sure what it means.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421#issuecomment-589380724
https://github.com/broadinstitute/cromwell/issues/5421#issuecomment-589380724:163,Availability,error,error,163,"So, in my testing, this appears to only happen in the scatter is a download from s3 job. Is it possible that heavy network congestion could create this error? The error itself doesn't seem to be associated with or come from the download, but then again I'm not sure what it means.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421#issuecomment-589380724
https://github.com/broadinstitute/cromwell/issues/5421#issuecomment-589380724:228,Availability,down,download,228,"So, in my testing, this appears to only happen in the scatter is a download from s3 job. Is it possible that heavy network congestion could create this error? The error itself doesn't seem to be associated with or come from the download, but then again I'm not sure what it means.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421#issuecomment-589380724
https://github.com/broadinstitute/cromwell/issues/5421#issuecomment-589380724:10,Testability,test,testing,10,"So, in my testing, this appears to only happen in the scatter is a download from s3 job. Is it possible that heavy network congestion could create this error? The error itself doesn't seem to be associated with or come from the download, but then again I'm not sure what it means.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421#issuecomment-589380724
https://github.com/broadinstitute/cromwell/pull/5427#issuecomment-591014236:66,Usability,learn,learned,66,"Thank you! This is a nice contrib on its own, and in reading it I learned of Oliver which I did not know before.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5427#issuecomment-591014236
https://github.com/broadinstitute/cromwell/pull/5429#issuecomment-590992581:76,Performance,cache,cached,76,@mcovarr because sometimes they were getting `attempt-1`s and sometime call cached. I _think_ that the workflows must have been copy/pasted from elsewhere in the test suite and it was a coin toss on which was running first.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5429#issuecomment-590992581
https://github.com/broadinstitute/cromwell/pull/5429#issuecomment-590992581:162,Testability,test,test,162,@mcovarr because sometimes they were getting `attempt-1`s and sometime call cached. I _think_ that the workflows must have been copy/pasted from elsewhere in the test suite and it was a coin toss on which was running first.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5429#issuecomment-590992581
https://github.com/broadinstitute/cromwell/pull/5432#issuecomment-591472639:338,Usability,learn,learn,338,"At 10am...; > created 9 hours ago. 😮 !. This is awesome, I really like this (the making things better with a new-to-cromwell technology, not just the 1am coding!). Bravo!. Today looks pretty busy but maybe a cross team tech talk about what this framework is, how we can use it, and what this PR is doing would be appropriate? I'd love to learn more at a high level as well as at an ""on the ground, code details"" level.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5432#issuecomment-591472639
https://github.com/broadinstitute/cromwell/pull/5436#issuecomment-595402100:246,Security,validat,validation,246,"> Just a couple of questions about using `flatMetadata` at all (eg we have a way of comparing the real json results directly, why go through all that indirection which may or may not do the right thing?). I kinda like the concept of more DSL-ish validation with flat metadata, as opposed to comparison of raw jsons. But probably it's excessive here indeed, so I removed the `compareFlatMetadata`. method.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5436#issuecomment-595402100
https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836:374,Availability,error,error,374,"I fixed the regex. It turned out that this also fixed any issues. > Thanks for finding and fixing this!. Thank you for trusting me with push access on this repository. It makes it easier for me as all tests run immediately, also the tests that need private variables. Also I can restart jobs on travis now that looks like they are failed due to some intermittent connection error. I had to restart one for this PR, and it indeed turned green on the retry. This makes it easier for me to fix any bugs I find. The trust is much appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836
https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836:436,Energy Efficiency,green,green,436,"I fixed the regex. It turned out that this also fixed any issues. > Thanks for finding and fixing this!. Thank you for trusting me with push access on this repository. It makes it easier for me as all tests run immediately, also the tests that need private variables. Also I can restart jobs on travis now that looks like they are failed due to some intermittent connection error. I had to restart one for this PR, and it indeed turned green on the retry. This makes it easier for me to fix any bugs I find. The trust is much appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836
https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836:257,Modifiability,variab,variables,257,"I fixed the regex. It turned out that this also fixed any issues. > Thanks for finding and fixing this!. Thank you for trusting me with push access on this repository. It makes it easier for me as all tests run immediately, also the tests that need private variables. Also I can restart jobs on travis now that looks like they are failed due to some intermittent connection error. I had to restart one for this PR, and it indeed turned green on the retry. This makes it easier for me to fix any bugs I find. The trust is much appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836
https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836:141,Security,access,access,141,"I fixed the regex. It turned out that this also fixed any issues. > Thanks for finding and fixing this!. Thank you for trusting me with push access on this repository. It makes it easier for me as all tests run immediately, also the tests that need private variables. Also I can restart jobs on travis now that looks like they are failed due to some intermittent connection error. I had to restart one for this PR, and it indeed turned green on the retry. This makes it easier for me to fix any bugs I find. The trust is much appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836
https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836:201,Testability,test,tests,201,"I fixed the regex. It turned out that this also fixed any issues. > Thanks for finding and fixing this!. Thank you for trusting me with push access on this repository. It makes it easier for me as all tests run immediately, also the tests that need private variables. Also I can restart jobs on travis now that looks like they are failed due to some intermittent connection error. I had to restart one for this PR, and it indeed turned green on the retry. This makes it easier for me to fix any bugs I find. The trust is much appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836
https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836:233,Testability,test,tests,233,"I fixed the regex. It turned out that this also fixed any issues. > Thanks for finding and fixing this!. Thank you for trusting me with push access on this repository. It makes it easier for me as all tests run immediately, also the tests that need private variables. Also I can restart jobs on travis now that looks like they are failed due to some intermittent connection error. I had to restart one for this PR, and it indeed turned green on the retry. This makes it easier for me to fix any bugs I find. The trust is much appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836
https://github.com/broadinstitute/cromwell/pull/5441#issuecomment-596391179:231,Testability,test,test,231,"Thanks for the invite. Just in case this is in response to my comment on centaur in the original post, I would like to clarify that I didn't mean that centaur didn't run for me, but rather that I couldn't find any definitions of a test for the gzipped inputs feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5441#issuecomment-596391179
https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120:31,Availability,failure,failure,31,Merging despite the slurm test failure because:. * this is for a hotfix which is needed urgently; * the error is in a slurm test. Slurm is currently unavailable in Terra.; * the error is in a CWL test. CWL is currently unavailable in Terra.; * the test failure does not appear to be new in this PR (the same failure affects the otherwise unrelated build: https://travis-ci.com/broadinstitute/cromwell/builds/152505665),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120
https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120:104,Availability,error,error,104,Merging despite the slurm test failure because:. * this is for a hotfix which is needed urgently; * the error is in a slurm test. Slurm is currently unavailable in Terra.; * the error is in a CWL test. CWL is currently unavailable in Terra.; * the test failure does not appear to be new in this PR (the same failure affects the otherwise unrelated build: https://travis-ci.com/broadinstitute/cromwell/builds/152505665),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120
https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120:178,Availability,error,error,178,Merging despite the slurm test failure because:. * this is for a hotfix which is needed urgently; * the error is in a slurm test. Slurm is currently unavailable in Terra.; * the error is in a CWL test. CWL is currently unavailable in Terra.; * the test failure does not appear to be new in this PR (the same failure affects the otherwise unrelated build: https://travis-ci.com/broadinstitute/cromwell/builds/152505665),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120
https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120:253,Availability,failure,failure,253,Merging despite the slurm test failure because:. * this is for a hotfix which is needed urgently; * the error is in a slurm test. Slurm is currently unavailable in Terra.; * the error is in a CWL test. CWL is currently unavailable in Terra.; * the test failure does not appear to be new in this PR (the same failure affects the otherwise unrelated build: https://travis-ci.com/broadinstitute/cromwell/builds/152505665),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120
https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120:308,Availability,failure,failure,308,Merging despite the slurm test failure because:. * this is for a hotfix which is needed urgently; * the error is in a slurm test. Slurm is currently unavailable in Terra.; * the error is in a CWL test. CWL is currently unavailable in Terra.; * the test failure does not appear to be new in this PR (the same failure affects the otherwise unrelated build: https://travis-ci.com/broadinstitute/cromwell/builds/152505665),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120
https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120:65,Deployability,hotfix,hotfix,65,Merging despite the slurm test failure because:. * this is for a hotfix which is needed urgently; * the error is in a slurm test. Slurm is currently unavailable in Terra.; * the error is in a CWL test. CWL is currently unavailable in Terra.; * the test failure does not appear to be new in this PR (the same failure affects the otherwise unrelated build: https://travis-ci.com/broadinstitute/cromwell/builds/152505665),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120
https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120:26,Testability,test,test,26,Merging despite the slurm test failure because:. * this is for a hotfix which is needed urgently; * the error is in a slurm test. Slurm is currently unavailable in Terra.; * the error is in a CWL test. CWL is currently unavailable in Terra.; * the test failure does not appear to be new in this PR (the same failure affects the otherwise unrelated build: https://travis-ci.com/broadinstitute/cromwell/builds/152505665),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120
https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120:124,Testability,test,test,124,Merging despite the slurm test failure because:. * this is for a hotfix which is needed urgently; * the error is in a slurm test. Slurm is currently unavailable in Terra.; * the error is in a CWL test. CWL is currently unavailable in Terra.; * the test failure does not appear to be new in this PR (the same failure affects the otherwise unrelated build: https://travis-ci.com/broadinstitute/cromwell/builds/152505665),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120
https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120:196,Testability,test,test,196,Merging despite the slurm test failure because:. * this is for a hotfix which is needed urgently; * the error is in a slurm test. Slurm is currently unavailable in Terra.; * the error is in a CWL test. CWL is currently unavailable in Terra.; * the test failure does not appear to be new in this PR (the same failure affects the otherwise unrelated build: https://travis-ci.com/broadinstitute/cromwell/builds/152505665),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120
https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120:248,Testability,test,test,248,Merging despite the slurm test failure because:. * this is for a hotfix which is needed urgently; * the error is in a slurm test. Slurm is currently unavailable in Terra.; * the error is in a CWL test. CWL is currently unavailable in Terra.; * the test failure does not appear to be new in this PR (the same failure affects the otherwise unrelated build: https://travis-ci.com/broadinstitute/cromwell/builds/152505665),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120
https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139719:32,Availability,failure,failure,32,>Merging despite the slurm test failure because:. I have considered and endorse this decision.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139719
https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139719:27,Testability,test,test,27,>Merging despite the slurm test failure because:. I have considered and endorse this decision.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139719
https://github.com/broadinstitute/cromwell/pull/5446#issuecomment-597169266:36,Deployability,hotfix,hotfix,36,"Reminder: could you also make a non-hotfix version of this PR (leaving out the metrics change for now, since that's going to be covered separately/properly in https://broadworkbench.atlassian.net/browse/BA-6307)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5446#issuecomment-597169266
https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-626967088:118,Availability,redundant,redundant,118,I am working on code in the branch `issue\5004` that will remove the need for the proxy container and might make this redundant. It would be good to discuss and see if there is a way to kill two birds with one stone.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-626967088
https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-626967088:118,Safety,redund,redundant,118,I am working on code in the branch `issue\5004` that will remove the need for the proxy container and might make this redundant. It would be good to discuss and see if there is a way to kill two birds with one stone.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-626967088
https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-627183681:26,Availability,redundant,redundant,26,"Feel free to make this PR redundant 😛 If your changes remove the need to put the outputs in a container override or does it in some different way that allows for larger values, then indeed this PR won't be needed anymore.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-627183681
https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-627183681:26,Safety,redund,redundant,26,"Feel free to make this PR redundant 😛 If your changes remove the need to put the outputs in a container override or does it in some different way that allows for larger values, then indeed this PR won't be needed anymore.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-627183681
https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-655510540:75,Availability,redundant,redundant,75,Closing this as #5468 changes the underlying code and might have made this redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-655510540
https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-655510540:75,Safety,redund,redundant,75,Closing this as #5468 changes the underlying code and might have made this redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-655510540
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-598752757:135,Deployability,release,released,135,"The PR is ready for review. The tests seem to work (on my PC locally at least). . I think a thorough test is in order before this gets released into the wild. I have added the ""DO NOT MERGE"" label because I want to test this change on our research cluster first. I will do this next week. I have a 100 sample run that needs to be reanalyzed using a stable version of the RNA-seq pipeline. This spawns some 2000ish jobs. So I can really put the new strategy trough its paces and check if it works correctly. I will report on this next week and remove the ""DO NOT MERGE"" label if everything turns out to be okay.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-598752757
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-598752757:379,Deployability,pipeline,pipeline,379,"The PR is ready for review. The tests seem to work (on my PC locally at least). . I think a thorough test is in order before this gets released into the wild. I have added the ""DO NOT MERGE"" label because I want to test this change on our research cluster first. I will do this next week. I have a 100 sample run that needs to be reanalyzed using a stable version of the RNA-seq pipeline. This spawns some 2000ish jobs. So I can really put the new strategy trough its paces and check if it works correctly. I will report on this next week and remove the ""DO NOT MERGE"" label if everything turns out to be okay.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-598752757
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-598752757:32,Testability,test,tests,32,"The PR is ready for review. The tests seem to work (on my PC locally at least). . I think a thorough test is in order before this gets released into the wild. I have added the ""DO NOT MERGE"" label because I want to test this change on our research cluster first. I will do this next week. I have a 100 sample run that needs to be reanalyzed using a stable version of the RNA-seq pipeline. This spawns some 2000ish jobs. So I can really put the new strategy trough its paces and check if it works correctly. I will report on this next week and remove the ""DO NOT MERGE"" label if everything turns out to be okay.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-598752757
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-598752757:101,Testability,test,test,101,"The PR is ready for review. The tests seem to work (on my PC locally at least). . I think a thorough test is in order before this gets released into the wild. I have added the ""DO NOT MERGE"" label because I want to test this change on our research cluster first. I will do this next week. I have a 100 sample run that needs to be reanalyzed using a stable version of the RNA-seq pipeline. This spawns some 2000ish jobs. So I can really put the new strategy trough its paces and check if it works correctly. I will report on this next week and remove the ""DO NOT MERGE"" label if everything turns out to be okay.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-598752757
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-598752757:215,Testability,test,test,215,"The PR is ready for review. The tests seem to work (on my PC locally at least). . I think a thorough test is in order before this gets released into the wild. I have added the ""DO NOT MERGE"" label because I want to test this change on our research cluster first. I will do this next week. I have a 100 sample run that needs to be reanalyzed using a stable version of the RNA-seq pipeline. This spawns some 2000ish jobs. So I can really put the new strategy trough its paces and check if it works correctly. I will report on this next week and remove the ""DO NOT MERGE"" label if everything turns out to be okay.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-598752757
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599207733:678,Availability,ping,pinging,678,"I thought I was going crazy! Thanks @rhpvorderman, I think this is a great PR, and really looking forward to being able to test this on our HPC. I'm still a little concerned with Cromwell requesting the whole file (up to 200GB) and it's impact on our network. But this is definitely a step in the write direction. I was thinking of maybe adding something similar, but would check the file size, and then perform `xxh64` on the first 1MB or some region of the file (eg: `size+xxh64-partial`). I'm happy to produce some documentation (whether a part of this PR, or linked to it), about call-caching on Cromwell and to include this new material once I can test it okay. Thanks for pinging me @rhpvorderman, excited to try this out!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599207733
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599207733:404,Performance,perform,perform,404,"I thought I was going crazy! Thanks @rhpvorderman, I think this is a great PR, and really looking forward to being able to test this on our HPC. I'm still a little concerned with Cromwell requesting the whole file (up to 200GB) and it's impact on our network. But this is definitely a step in the write direction. I was thinking of maybe adding something similar, but would check the file size, and then perform `xxh64` on the first 1MB or some region of the file (eg: `size+xxh64-partial`). I'm happy to produce some documentation (whether a part of this PR, or linked to it), about call-caching on Cromwell and to include this new material once I can test it okay. Thanks for pinging me @rhpvorderman, excited to try this out!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599207733
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599207733:123,Testability,test,test,123,"I thought I was going crazy! Thanks @rhpvorderman, I think this is a great PR, and really looking forward to being able to test this on our HPC. I'm still a little concerned with Cromwell requesting the whole file (up to 200GB) and it's impact on our network. But this is definitely a step in the write direction. I was thinking of maybe adding something similar, but would check the file size, and then perform `xxh64` on the first 1MB or some region of the file (eg: `size+xxh64-partial`). I'm happy to produce some documentation (whether a part of this PR, or linked to it), about call-caching on Cromwell and to include this new material once I can test it okay. Thanks for pinging me @rhpvorderman, excited to try this out!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599207733
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599207733:653,Testability,test,test,653,"I thought I was going crazy! Thanks @rhpvorderman, I think this is a great PR, and really looking forward to being able to test this on our HPC. I'm still a little concerned with Cromwell requesting the whole file (up to 200GB) and it's impact on our network. But this is definitely a step in the write direction. I was thinking of maybe adding something similar, but would check the file size, and then perform `xxh64` on the first 1MB or some region of the file (eg: `size+xxh64-partial`). I'm happy to produce some documentation (whether a part of this PR, or linked to it), about call-caching on Cromwell and to include this new material once I can test it okay. Thanks for pinging me @rhpvorderman, excited to try this out!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599207733
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301:55,Availability,down,downloading,55,"@illusional . A partial hash is a great idea. I am now downloading a 82 GB bam file in order to check the speed of the algorithm, but unfortunately my network connection is a 100mbits on this PC. It will take 2 hours. Even with a 1000mbit perfect connection it would take at least 12 minutes. So computation time is indeed not the limit here. We need to thinks this through though. Some files are more similar at the beginning (VCF headers come to mind) than at the end. So only hashing the beginning carries with it some major concerns. Using the size is indeed a good thing, and I think we should also include the modification time.; In that case `size+modtime+xx64hsum of first 10mb` should create a unique enough identifier for each file while negating any network performance issues. I think this strategy should be called `hpc`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301:769,Performance,perform,performance,769,"@illusional . A partial hash is a great idea. I am now downloading a 82 GB bam file in order to check the speed of the algorithm, but unfortunately my network connection is a 100mbits on this PC. It will take 2 hours. Even with a 1000mbit perfect connection it would take at least 12 minutes. So computation time is indeed not the limit here. We need to thinks this through though. Some files are more similar at the beginning (VCF headers come to mind) than at the end. So only hashing the beginning carries with it some major concerns. Using the size is indeed a good thing, and I think we should also include the modification time.; In that case `size+modtime+xx64hsum of first 10mb` should create a unique enough identifier for each file while negating any network performance issues. I think this strategy should be called `hpc`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301:24,Security,hash,hash,24,"@illusional . A partial hash is a great idea. I am now downloading a 82 GB bam file in order to check the speed of the algorithm, but unfortunately my network connection is a 100mbits on this PC. It will take 2 hours. Even with a 1000mbit perfect connection it would take at least 12 minutes. So computation time is indeed not the limit here. We need to thinks this through though. Some files are more similar at the beginning (VCF headers come to mind) than at the end. So only hashing the beginning carries with it some major concerns. Using the size is indeed a good thing, and I think we should also include the modification time.; In that case `size+modtime+xx64hsum of first 10mb` should create a unique enough identifier for each file while negating any network performance issues. I think this strategy should be called `hpc`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301:479,Security,hash,hashing,479,"@illusional . A partial hash is a great idea. I am now downloading a 82 GB bam file in order to check the speed of the algorithm, but unfortunately my network connection is a 100mbits on this PC. It will take 2 hours. Even with a 1000mbit perfect connection it would take at least 12 minutes. So computation time is indeed not the limit here. We need to thinks this through though. Some files are more similar at the beginning (VCF headers come to mind) than at the end. So only hashing the beginning carries with it some major concerns. Using the size is indeed a good thing, and I think we should also include the modification time.; In that case `size+modtime+xx64hsum of first 10mb` should create a unique enough identifier for each file while negating any network performance issues. I think this strategy should be called `hpc`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599448301
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599516129:460,Performance,bottleneck,bottleneck,460,"@illusional. I did some benchmarking on xxh64sum vs md5sum on a 35GB file. Results:; * Just reading the file with `cat <file> /dev/null` took 18 seconds. Virtually no CPU time; * xxh64sum took 24 seconds of which 3.6 seconds cpu time; * md5sum took 53 seconds, of which 48 seconds cpu time. Md5sum was cpu limited. So CPU was 100% all the time. xxh64sum was limited by the transfer speed of the disk (nvme ssd), so cpu usage never exceeded 20%. This means the bottleneck becomes I/O based, and for 200 GB files on NFS this can indeed be a big problem. I have added a `hpc` strategy` that takes the last modified time, size, and the xxh64sum of the first 10 megabytes of the file to alleviate this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599516129
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599516129:24,Testability,benchmark,benchmarking,24,"@illusional. I did some benchmarking on xxh64sum vs md5sum on a 35GB file. Results:; * Just reading the file with `cat <file> /dev/null` took 18 seconds. Virtually no CPU time; * xxh64sum took 24 seconds of which 3.6 seconds cpu time; * md5sum took 53 seconds, of which 48 seconds cpu time. Md5sum was cpu limited. So CPU was 100% all the time. xxh64sum was limited by the transfer speed of the disk (nvme ssd), so cpu usage never exceeded 20%. This means the bottleneck becomes I/O based, and for 200 GB files on NFS this can indeed be a big problem. I have added a `hpc` strategy` that takes the last modified time, size, and the xxh64sum of the first 10 megabytes of the file to alleviate this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599516129
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599532702:170,Testability,test,test,170,"You're awesome @rhpvorderman, can't wait to try this out. I'm not the best crash hot at Scala, but is it enough for me to check out your branch, `sbt` to get the jar and test this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599532702
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599539307:239,Availability,reliab,reliable,239,"@illusional. I renamed the strategy `fingerprint` because I think it can be used in a general case. Also because it is called ""fingerprint"" it does carry with it the sense that it only tests a small part of the file, and is therefore less reliable than a strategy that hashes the entire file. (Even though it should be reliable enough). To build a new jar, check out the [documentation](https://cromwell.readthedocs.io/en/stable/developers/Building/). It is as easy indeed as checking out the branch and running `sbt assembly`. It might take a while though. If you run out of memory I believe sbt has a `-mem` flag to set the memory. @cjllanwarne I fully agree with your comments on the documentation part, so I trimmed the changelog and moved the information to the documentation. I hope the documentation is adequate and well-explained enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599539307
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599539307:319,Availability,reliab,reliable,319,"@illusional. I renamed the strategy `fingerprint` because I think it can be used in a general case. Also because it is called ""fingerprint"" it does carry with it the sense that it only tests a small part of the file, and is therefore less reliable than a strategy that hashes the entire file. (Even though it should be reliable enough). To build a new jar, check out the [documentation](https://cromwell.readthedocs.io/en/stable/developers/Building/). It is as easy indeed as checking out the branch and running `sbt assembly`. It might take a while though. If you run out of memory I believe sbt has a `-mem` flag to set the memory. @cjllanwarne I fully agree with your comments on the documentation part, so I trimmed the changelog and moved the information to the documentation. I hope the documentation is adequate and well-explained enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599539307
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599539307:269,Security,hash,hashes,269,"@illusional. I renamed the strategy `fingerprint` because I think it can be used in a general case. Also because it is called ""fingerprint"" it does carry with it the sense that it only tests a small part of the file, and is therefore less reliable than a strategy that hashes the entire file. (Even though it should be reliable enough). To build a new jar, check out the [documentation](https://cromwell.readthedocs.io/en/stable/developers/Building/). It is as easy indeed as checking out the branch and running `sbt assembly`. It might take a while though. If you run out of memory I believe sbt has a `-mem` flag to set the memory. @cjllanwarne I fully agree with your comments on the documentation part, so I trimmed the changelog and moved the information to the documentation. I hope the documentation is adequate and well-explained enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599539307
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599539307:185,Testability,test,tests,185,"@illusional. I renamed the strategy `fingerprint` because I think it can be used in a general case. Also because it is called ""fingerprint"" it does carry with it the sense that it only tests a small part of the file, and is therefore less reliable than a strategy that hashes the entire file. (Even though it should be reliable enough). To build a new jar, check out the [documentation](https://cromwell.readthedocs.io/en/stable/developers/Building/). It is as easy indeed as checking out the branch and running `sbt assembly`. It might take a while though. If you run out of memory I believe sbt has a `-mem` flag to set the memory. @cjllanwarne I fully agree with your comments on the documentation part, so I trimmed the changelog and moved the information to the documentation. I hope the documentation is adequate and well-explained enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599539307
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-600018995:85,Deployability,release,releases,85,@illusional I have also provided jars here: https://github.com/rhpvorderman/cromwell/releases/tag/50-dev-lumctest,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-600018995
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438:537,Deployability,configurat,configurations,537,"I have run both strategies with a workflow that generates around about 2000 jobs (100 samples) using GATK best practices for RNA variant calling. . ### Method. The Cromwell instance ran with a SLURM cluster backend. All jobs were run using singularity containers. The cromwell process was limited to 3 akka threads and 1 GC thread (by default it grabs al threads on the login node, and this is not fair to other users). The HSQLDB memory database with persistance file was used. Said SLURM cluster has its storage connected via NFS. Two configurations of cromwell were used. One with the xxh64 strategy, and one with the fingerprint strategy. Each cromwell instance was executed in its own directory, with its own database and own cromwell-executions folder. The [BioWDL RNA-seq](https://github.com/biowdl/rna-seq) workflow was run. After running, the workflow was run again to see if the call-caching worked correctly. ### Results; Both `xxh64` and `fingerprint` strategies were able to rerun the workflow with a 100% Cache hit. The fingerprint strategy however was much quicker:; `time` results for fingerprint; ```; real 23m26.269s; user 15m31.229s; sys 2m43.406s; ```; `time` results for xxh64; ```; real 69m12.478s; user 56m7.371s; sys 52m6.262s; ```. ### Conclusion; Using xxh64 as a strategy requires some calculation but one hour for 100 samples on 2000 jobs is quite acceptable. What is obvious is that the system IO (`sys` time) takes a lot of time as well. This cluster has very fast optimized ISILON storage, but on clusters without this, any hashing strategy can be quite slow because of this. The fingerprint works very well for HPC environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438:537,Modifiability,config,configurations,537,"I have run both strategies with a workflow that generates around about 2000 jobs (100 samples) using GATK best practices for RNA variant calling. . ### Method. The Cromwell instance ran with a SLURM cluster backend. All jobs were run using singularity containers. The cromwell process was limited to 3 akka threads and 1 GC thread (by default it grabs al threads on the login node, and this is not fair to other users). The HSQLDB memory database with persistance file was used. Said SLURM cluster has its storage connected via NFS. Two configurations of cromwell were used. One with the xxh64 strategy, and one with the fingerprint strategy. Each cromwell instance was executed in its own directory, with its own database and own cromwell-executions folder. The [BioWDL RNA-seq](https://github.com/biowdl/rna-seq) workflow was run. After running, the workflow was run again to see if the call-caching worked correctly. ### Results; Both `xxh64` and `fingerprint` strategies were able to rerun the workflow with a 100% Cache hit. The fingerprint strategy however was much quicker:; `time` results for fingerprint; ```; real 23m26.269s; user 15m31.229s; sys 2m43.406s; ```; `time` results for xxh64; ```; real 69m12.478s; user 56m7.371s; sys 52m6.262s; ```. ### Conclusion; Using xxh64 as a strategy requires some calculation but one hour for 100 samples on 2000 jobs is quite acceptable. What is obvious is that the system IO (`sys` time) takes a lot of time as well. This cluster has very fast optimized ISILON storage, but on clusters without this, any hashing strategy can be quite slow because of this. The fingerprint works very well for HPC environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438:1019,Performance,Cache,Cache,1019,"I have run both strategies with a workflow that generates around about 2000 jobs (100 samples) using GATK best practices for RNA variant calling. . ### Method. The Cromwell instance ran with a SLURM cluster backend. All jobs were run using singularity containers. The cromwell process was limited to 3 akka threads and 1 GC thread (by default it grabs al threads on the login node, and this is not fair to other users). The HSQLDB memory database with persistance file was used. Said SLURM cluster has its storage connected via NFS. Two configurations of cromwell were used. One with the xxh64 strategy, and one with the fingerprint strategy. Each cromwell instance was executed in its own directory, with its own database and own cromwell-executions folder. The [BioWDL RNA-seq](https://github.com/biowdl/rna-seq) workflow was run. After running, the workflow was run again to see if the call-caching worked correctly. ### Results; Both `xxh64` and `fingerprint` strategies were able to rerun the workflow with a 100% Cache hit. The fingerprint strategy however was much quicker:; `time` results for fingerprint; ```; real 23m26.269s; user 15m31.229s; sys 2m43.406s; ```; `time` results for xxh64; ```; real 69m12.478s; user 56m7.371s; sys 52m6.262s; ```. ### Conclusion; Using xxh64 as a strategy requires some calculation but one hour for 100 samples on 2000 jobs is quite acceptable. What is obvious is that the system IO (`sys` time) takes a lot of time as well. This cluster has very fast optimized ISILON storage, but on clusters without this, any hashing strategy can be quite slow because of this. The fingerprint works very well for HPC environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438:1495,Performance,optimiz,optimized,1495,"I have run both strategies with a workflow that generates around about 2000 jobs (100 samples) using GATK best practices for RNA variant calling. . ### Method. The Cromwell instance ran with a SLURM cluster backend. All jobs were run using singularity containers. The cromwell process was limited to 3 akka threads and 1 GC thread (by default it grabs al threads on the login node, and this is not fair to other users). The HSQLDB memory database with persistance file was used. Said SLURM cluster has its storage connected via NFS. Two configurations of cromwell were used. One with the xxh64 strategy, and one with the fingerprint strategy. Each cromwell instance was executed in its own directory, with its own database and own cromwell-executions folder. The [BioWDL RNA-seq](https://github.com/biowdl/rna-seq) workflow was run. After running, the workflow was run again to see if the call-caching worked correctly. ### Results; Both `xxh64` and `fingerprint` strategies were able to rerun the workflow with a 100% Cache hit. The fingerprint strategy however was much quicker:; `time` results for fingerprint; ```; real 23m26.269s; user 15m31.229s; sys 2m43.406s; ```; `time` results for xxh64; ```; real 69m12.478s; user 56m7.371s; sys 52m6.262s; ```. ### Conclusion; Using xxh64 as a strategy requires some calculation but one hour for 100 samples on 2000 jobs is quite acceptable. What is obvious is that the system IO (`sys` time) takes a lot of time as well. This cluster has very fast optimized ISILON storage, but on clusters without this, any hashing strategy can be quite slow because of this. The fingerprint works very well for HPC environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438:1555,Security,hash,hashing,1555,"I have run both strategies with a workflow that generates around about 2000 jobs (100 samples) using GATK best practices for RNA variant calling. . ### Method. The Cromwell instance ran with a SLURM cluster backend. All jobs were run using singularity containers. The cromwell process was limited to 3 akka threads and 1 GC thread (by default it grabs al threads on the login node, and this is not fair to other users). The HSQLDB memory database with persistance file was used. Said SLURM cluster has its storage connected via NFS. Two configurations of cromwell were used. One with the xxh64 strategy, and one with the fingerprint strategy. Each cromwell instance was executed in its own directory, with its own database and own cromwell-executions folder. The [BioWDL RNA-seq](https://github.com/biowdl/rna-seq) workflow was run. After running, the workflow was run again to see if the call-caching worked correctly. ### Results; Both `xxh64` and `fingerprint` strategies were able to rerun the workflow with a 100% Cache hit. The fingerprint strategy however was much quicker:; `time` results for fingerprint; ```; real 23m26.269s; user 15m31.229s; sys 2m43.406s; ```; `time` results for xxh64; ```; real 69m12.478s; user 56m7.371s; sys 52m6.262s; ```. ### Conclusion; Using xxh64 as a strategy requires some calculation but one hour for 100 samples on 2000 jobs is quite acceptable. What is obvious is that the system IO (`sys` time) takes a lot of time as well. This cluster has very fast optimized ISILON storage, but on clusters without this, any hashing strategy can be quite slow because of this. The fingerprint works very well for HPC environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438:370,Testability,log,login,370,"I have run both strategies with a workflow that generates around about 2000 jobs (100 samples) using GATK best practices for RNA variant calling. . ### Method. The Cromwell instance ran with a SLURM cluster backend. All jobs were run using singularity containers. The cromwell process was limited to 3 akka threads and 1 GC thread (by default it grabs al threads on the login node, and this is not fair to other users). The HSQLDB memory database with persistance file was used. Said SLURM cluster has its storage connected via NFS. Two configurations of cromwell were used. One with the xxh64 strategy, and one with the fingerprint strategy. Each cromwell instance was executed in its own directory, with its own database and own cromwell-executions folder. The [BioWDL RNA-seq](https://github.com/biowdl/rna-seq) workflow was run. After running, the workflow was run again to see if the call-caching worked correctly. ### Results; Both `xxh64` and `fingerprint` strategies were able to rerun the workflow with a 100% Cache hit. The fingerprint strategy however was much quicker:; `time` results for fingerprint; ```; real 23m26.269s; user 15m31.229s; sys 2m43.406s; ```; `time` results for xxh64; ```; real 69m12.478s; user 56m7.371s; sys 52m6.262s; ```. ### Conclusion; Using xxh64 as a strategy requires some calculation but one hour for 100 samples on 2000 jobs is quite acceptable. What is obvious is that the system IO (`sys` time) takes a lot of time as well. This cluster has very fast optimized ISILON storage, but on clusters without this, any hashing strategy can be quite slow because of this. The fingerprint works very well for HPC environments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601604438
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601606406:20,Deployability,update,updated,20,"@cjllanwarne I have updated the documentation and the only test on Travis that fails is related to relative imports on CWL, so not something in this PR. I have tested the strategies in real life and found no problems. I see no ""ready for review"" label, and the ""on-deck for review label"" prioritizes the PR (which I feel I am not in a position to do). What is the usual process for declaring the PR ready?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601606406
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601606406:59,Testability,test,test,59,"@cjllanwarne I have updated the documentation and the only test on Travis that fails is related to relative imports on CWL, so not something in this PR. I have tested the strategies in real life and found no problems. I see no ""ready for review"" label, and the ""on-deck for review label"" prioritizes the PR (which I feel I am not in a position to do). What is the usual process for declaring the PR ready?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601606406
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601606406:160,Testability,test,tested,160,"@cjllanwarne I have updated the documentation and the only test on Travis that fails is related to relative imports on CWL, so not something in this PR. I have tested the strategies in real life and found no problems. I see no ""ready for review"" label, and the ""on-deck for review label"" prioritizes the PR (which I feel I am not in a position to do). What is the usual process for declaring the PR ready?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-601606406
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-602988562:22,Testability,test,testing,22,"Hey @rhpvorderman, my testing isn't as comprehensive as yours but just wanted to weigh in and mention that the `fingerprint` method seems to be working locally, and on our Slurm cluster. I intend to do a bit more testing over the next few weeks but really appreciative of your work here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-602988562
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-602988562:213,Testability,test,testing,213,"Hey @rhpvorderman, my testing isn't as comprehensive as yours but just wanted to weigh in and mention that the `fingerprint` method seems to be working locally, and on our Slurm cluster. I intend to do a bit more testing over the next few weeks but really appreciative of your work here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-602988562
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-610339380:103,Performance,cache,cache,103,Tested the newer fingerprint hashing strategy (using a hash of equal size). Happy to report about 1000 cache hits (100%) correctly on our cluster when I needed to restart a workflow.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-610339380
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-610339380:29,Security,hash,hashing,29,Tested the newer fingerprint hashing strategy (using a hash of equal size). Happy to report about 1000 cache hits (100%) correctly on our cluster when I needed to restart a workflow.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-610339380
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-610339380:55,Security,hash,hash,55,Tested the newer fingerprint hashing strategy (using a hash of equal size). Happy to report about 1000 cache hits (100%) correctly on our cluster when I needed to restart a workflow.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-610339380
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-610339380:0,Testability,Test,Tested,0,Tested the newer fingerprint hashing strategy (using a hash of equal size). Happy to report about 1000 cache hits (100%) correctly on our cluster when I needed to restart a workflow.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-610339380
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-610770544:13,Testability,test,testing,13,I've started testing this using the filebased db and it seems to be working as well. We're starting to scale to more users with `fingerprint` and seems to be working okay so far.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-610770544
https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-623283470:24,Usability,feedback,feedback,24,Thanks everyone for the feedback and the merging!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-623283470
https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-599309808:337,Deployability,install,installation,337,"> Why not use something like miniwdl?; > ; > run mode was originally created for **cromwell** development purposes, although for most of time there wasn't really an alternative for workflow development. Hi Geoff thanks for your suggestion. I have checked miniwdl but it has a docker dependency which does not fit my need for a sudo-less installation. The devs will certainly benefit from a quick REPL for WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-599309808
https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-599309808:283,Integrability,depend,dependency,283,"> Why not use something like miniwdl?; > ; > run mode was originally created for **cromwell** development purposes, although for most of time there wasn't really an alternative for workflow development. Hi Geoff thanks for your suggestion. I have checked miniwdl but it has a docker dependency which does not fit my need for a sudo-less installation. The devs will certainly benefit from a quick REPL for WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-599309808
https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-785271100:178,Usability,clear,clear,178,"Hi all,. unless I'm wrong, I've got the same problem ( > 5 minutes for a Hello-World ) https://gist.github.com/lindenb/d89e69f31e1bc5d390dc043b48d651d9. I'm new to WDL. It's not clear to me if the `server` mode is the only 'true' way to run a WDL workflow (?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-785271100
https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-785281065:226,Availability,mainten,maintenance,226,"Server mode is the intended, first-class usage scenario for Cromwell. It's really [no harder to use](https://cromwell.readthedocs.io/en/stable/tutorials/ServerMode/) than run mode and enables way more features. Run mode is in maintenance and we do not anticipate making enhancements.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-785281065
https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-785281065:270,Modifiability,enhance,enhancements,270,"Server mode is the intended, first-class usage scenario for Cromwell. It's really [no harder to use](https://cromwell.readthedocs.io/en/stable/tutorials/ServerMode/) than run mode and enables way more features. Run mode is in maintenance and we do not anticipate making enhancements.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-785281065
https://github.com/broadinstitute/cromwell/pull/5453#issuecomment-603682699:77,Testability,test,testing,77,"I found the issue. A new version of liquibase (3.8.8) solves the issues with testing. This means that I can debug the problem further. I will be continuing work on my own fork, so not to unnessecarily waste test resources.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453#issuecomment-603682699
https://github.com/broadinstitute/cromwell/pull/5453#issuecomment-603682699:207,Testability,test,test,207,"I found the issue. A new version of liquibase (3.8.8) solves the issues with testing. This means that I can debug the problem further. I will be continuing work on my own fork, so not to unnessecarily waste test resources.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5453#issuecomment-603682699
https://github.com/broadinstitute/cromwell/pull/5454#issuecomment-676540387:31,Testability,test,tests,31,Hi @apmasell - I've added some tests on your behalf in a new branch and made a new PR with those tests included to replace this one: https://github.com/broadinstitute/cromwell/pull/5780. Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5454#issuecomment-676540387
https://github.com/broadinstitute/cromwell/pull/5454#issuecomment-676540387:97,Testability,test,tests,97,Hi @apmasell - I've added some tests on your behalf in a new branch and made a new PR with those tests included to replace this one: https://github.com/broadinstitute/cromwell/pull/5780. Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5454#issuecomment-676540387
https://github.com/broadinstitute/cromwell/pull/5455#issuecomment-602869790:35,Availability,avail,available,35,"TOL: Is this intended to be a user-available script or just for jenkins to run? If the latter, is the cromwell repo the best place for it? And should we have ""don't use this yourself from the command line!!"" warnings on the script?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5455#issuecomment-602869790
https://github.com/broadinstitute/cromwell/pull/5456#issuecomment-616442782:81,Deployability,update,update,81,This has been tested in production now for quite some time and it seems that the update catches all folders. (At least on the sfsBackend.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5456#issuecomment-616442782
https://github.com/broadinstitute/cromwell/pull/5456#issuecomment-616442782:14,Testability,test,tested,14,This has been tested in production now for quite some time and it seems that the update catches all folders. (At least on the sfsBackend.),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5456#issuecomment-616442782
https://github.com/broadinstitute/cromwell/pull/5456#issuecomment-617607902:40,Deployability,release,released,40,"It is a bit unfortunate cromwell 50 was released before this was merged. It will break all testing everywhere since there is no way of running cromwell and knowing beforehand where the outputs will end up.; In biowdl all testing is already pinned to cromwell-48 to ensure continued operation, we were hoping we could unpin this with 50, but it seems we have to wait a little longer. EDIT: I do understand though with COVID-19 raging across the world that some other stuff deservedly gets priority.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5456#issuecomment-617607902
https://github.com/broadinstitute/cromwell/pull/5456#issuecomment-617607902:91,Testability,test,testing,91,"It is a bit unfortunate cromwell 50 was released before this was merged. It will break all testing everywhere since there is no way of running cromwell and knowing beforehand where the outputs will end up.; In biowdl all testing is already pinned to cromwell-48 to ensure continued operation, we were hoping we could unpin this with 50, but it seems we have to wait a little longer. EDIT: I do understand though with COVID-19 raging across the world that some other stuff deservedly gets priority.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5456#issuecomment-617607902
https://github.com/broadinstitute/cromwell/pull/5456#issuecomment-617607902:221,Testability,test,testing,221,"It is a bit unfortunate cromwell 50 was released before this was merged. It will break all testing everywhere since there is no way of running cromwell and knowing beforehand where the outputs will end up.; In biowdl all testing is already pinned to cromwell-48 to ensure continued operation, we were hoping we could unpin this with 50, but it seems we have to wait a little longer. EDIT: I do understand though with COVID-19 raging across the world that some other stuff deservedly gets priority.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5456#issuecomment-617607902
https://github.com/broadinstitute/cromwell/pull/5458#issuecomment-611659610:417,Deployability,Update,Update,417,"This PR has been merged. There is a [commit](https://github.com/broadinstitute/cromwell/commit/0f549a9fef716d8988c3269c8f42ec830e2f625c) in develop as well. But since this was merged when Github was having issues, it has been left in a weird state. Even after being merged it shows it is Open. I had filled a support ticket with GitHub with no response. As there is an option to close the PR, I am going to close it. Update: Well it's weird. As soon as I closed it, it changed the status to Merged 🤷‍♀️",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5458#issuecomment-611659610
https://github.com/broadinstitute/cromwell/pull/5462#issuecomment-605335824:112,Testability,benchmark,benchmarking,112,"We should check with product because the ticket doesn't say, but I believe the point of this spike was to allow benchmarking n2 and n2d machine types with the same workflows that were used for the v2 standard machine type spike. If I'm right about that then this code would need to use an algorithm like v1 does to pick standard machine types or else those workflows would not run. If I'm wrong about that then please ignore this whole comment. 🙂",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5462#issuecomment-605335824
https://github.com/broadinstitute/cromwell/pull/5462#issuecomment-606112682:0,Testability,Test,Tested,0,Tested that it works and that type of instantiated VMs is actually n2. Closing this PR.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5462#issuecomment-606112682
https://github.com/broadinstitute/cromwell/pull/5463#issuecomment-606112814:0,Testability,Test,Tested,0,Tested that it works and that type of instantiated VMs is actually n2d. Closing this PR.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5463#issuecomment-606112814
https://github.com/broadinstitute/cromwell/pull/5464#issuecomment-605742278:13,Testability,test,test,13,"I've added a test `string_interpolation_optional` that succeeds when I ran:. ```bash; sbt ""centaur/it:testOnly * -- -n interpolation""; ```. I've begun the process for getting this added as a conformance test for WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5464#issuecomment-605742278
https://github.com/broadinstitute/cromwell/pull/5464#issuecomment-605742278:102,Testability,test,testOnly,102,"I've added a test `string_interpolation_optional` that succeeds when I ran:. ```bash; sbt ""centaur/it:testOnly * -- -n interpolation""; ```. I've begun the process for getting this added as a conformance test for WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5464#issuecomment-605742278
https://github.com/broadinstitute/cromwell/pull/5464#issuecomment-605742278:203,Testability,test,test,203,"I've added a test `string_interpolation_optional` that succeeds when I ran:. ```bash; sbt ""centaur/it:testOnly * -- -n interpolation""; ```. I've begun the process for getting this added as a conformance test for WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5464#issuecomment-605742278
https://github.com/broadinstitute/cromwell/pull/5464#issuecomment-661918359:40,Energy Efficiency,schedul,schedule,40,"Hi @illusional, we put this PR into our schedule to review, could you please make a version of it inside the repo as you did for https://github.com/broadinstitute/cromwell/pull/5573?. Thanks,; Adam",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5464#issuecomment-661918359
https://github.com/broadinstitute/cromwell/pull/5465#issuecomment-606092621:13,Testability,test,tested,13,Is this also tested for a workflow with no tasks at all?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5465#issuecomment-606092621
https://github.com/broadinstitute/cromwell/pull/5465#issuecomment-606683493:54,Testability,test,test,54,@cjllanwarne I have added the `workflow-with-no-task` test in the [develop](https://github.com/broadinstitute/cromwell/pull/5458) version of this PR.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5465#issuecomment-606683493
https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606117531:3,Deployability,hotfix,hotfix,3,49 hotfix here: #5466,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606117531
https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666:293,Availability,error,error,293,@ysp0606 FYI we've had to disable our Alibaba tests for Cromwell while we wait for our OSS access to be restored. More info in https://broadworkbench.atlassian.net/browse/BA-6345. On our last update we gave Alibaba support a temporary access key from our account so they can try and debug the error code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666
https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666:192,Deployability,update,update,192,@ysp0606 FYI we've had to disable our Alibaba tests for Cromwell while we wait for our OSS access to be restored. More info in https://broadworkbench.atlassian.net/browse/BA-6345. On our last update we gave Alibaba support a temporary access key from our account so they can try and debug the error code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666
https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666:91,Security,access,access,91,@ysp0606 FYI we've had to disable our Alibaba tests for Cromwell while we wait for our OSS access to be restored. More info in https://broadworkbench.atlassian.net/browse/BA-6345. On our last update we gave Alibaba support a temporary access key from our account so they can try and debug the error code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666
https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666:235,Security,access,access,235,@ysp0606 FYI we've had to disable our Alibaba tests for Cromwell while we wait for our OSS access to be restored. More info in https://broadworkbench.atlassian.net/browse/BA-6345. On our last update we gave Alibaba support a temporary access key from our account so they can try and debug the error code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666
https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666:46,Testability,test,tests,46,@ysp0606 FYI we've had to disable our Alibaba tests for Cromwell while we wait for our OSS access to be restored. More info in https://broadworkbench.atlassian.net/browse/BA-6345. On our last update we gave Alibaba support a temporary access key from our account so they can try and debug the error code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606119666
https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606695991:25,Testability,test,tests,25,"Closing while I check on tests. This may not be needed anymore in `develop`. `49_hotfix` did merge this change though, which may be rolled back if we insist.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606695991
https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606724219:0,Testability,Test,Tests,0,"Tests are now passing again. https://travis-ci.com/github/broadinstitute/cromwell/builds/157239575. I believe the solution was that while our Alibaba Master Account was fully paid, the Alibaba Sub Account used to run tests had run out of credits. With help from Alibaba we fixed the funding for the Sub Account. Thanks all!; cc: @ysp0606",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606724219
https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606724219:217,Testability,test,tests,217,"Tests are now passing again. https://travis-ci.com/github/broadinstitute/cromwell/builds/157239575. I believe the solution was that while our Alibaba Master Account was fully paid, the Alibaba Sub Account used to run tests had run out of credits. With help from Alibaba we fixed the funding for the Sub Account. Thanks all!; cc: @ysp0606",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5467#issuecomment-606724219
https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-660381076:698,Availability,down,down,698,"@markjschreiber Thank you for this PR. I had a quick look at it and it looks pretty good. I just had a few questions. > 7. Set up /var/lib/docker/docker to auto-expand as inputs are now read directly into the container. Is this the only documentation on this requirement? Also, are you saying that that directory is now being auto-expanded in the underlying ECS image, or that a client needs to create an AMI to auto-expand that directory instead of `/cromwell_root`? Also, is it `/var/lib/docker/docker` or `/var/lib/docker/containers`? . EDIT:. The README.md references a LaunchTemplate which provides a UserData script to an underlying AMI, but it's not linked anywhere. I think I've tracked it down to the document here: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/aws-genomics-launch-template.template.yaml",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-660381076
https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662080007:246,Performance,Optimiz,Optimized,246,"@markjschreiber . > The EC2 workers contain a script that automatically expands that mount; users don't need to set that up. No custom AMI is required, in theory any; AMI that can work with ECS could be used. Is this a new feature of all new ECS Optimized Amazon Linux instances?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662080007
https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662153993:40,Energy Efficiency,monitor,monitors,40,"No. The feature is due to a script that monitors disk storage and mounts; new disks into a btrfs filesystem. This isn’t standard for ECS. On Tue, Jul 21, 2020 at 4:08 PM Richard Davison <notifications@github.com>; wrote:. > @markjschreiber <https://github.com/markjschreiber>; >; > The EC2 workers contain a script that automatically expands that mount; > users don't need to set that up. No custom AMI is required, in theory any; > AMI that can work with ECS could be used.; >; > Is this a new feature of all new ECS Optimized Amazon Linux instances?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662080007>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EM4MI3WWXI2AFZAK2LR4XYUHANCNFSM4LW5UP3A>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662153993
https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662153993:518,Performance,Optimiz,Optimized,518,"No. The feature is due to a script that monitors disk storage and mounts; new disks into a btrfs filesystem. This isn’t standard for ECS. On Tue, Jul 21, 2020 at 4:08 PM Richard Davison <notifications@github.com>; wrote:. > @markjschreiber <https://github.com/markjschreiber>; >; > The EC2 workers contain a script that automatically expands that mount; > users don't need to set that up. No custom AMI is required, in theory any; > AMI that can work with ECS could be used.; >; > Is this a new feature of all new ECS Optimized Amazon Linux instances?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662080007>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EM4MI3WWXI2AFZAK2LR4XYUHANCNFSM4LW5UP3A>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662153993
https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662751419:221,Availability,down,downloading,221,"When you submit a job to Cromwell with AWS Batch configured as the backend; AWS Batch will deploy workers via ECS. Those workers will be deployed; according to a launch template that configures the EC2 workers including; downloading and starting the EBS auto expander on the EC2 as part of its; startup. So it’s standard for the ECS cluster/ Batch compute environment used by; Cromwell but not standard for any other ECS node. On Wed, Jul 22, 2020 at 4:04 PM Richard Davison <notifications@github.com>; wrote:. > @markjschreiber <https://github.com/markjschreiber> : Oh! I think maybe I; > understand where the misunderstanding is. Could it be you're implying that; > the Cromwell *server* is correctly provisioning the worker nodes on; > behalf of the user?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662668064>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EKOXJKQMVJWYBPRYK3R45A6FANCNFSM4LW5UP3A>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662751419
https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662751419:91,Deployability,deploy,deploy,91,"When you submit a job to Cromwell with AWS Batch configured as the backend; AWS Batch will deploy workers via ECS. Those workers will be deployed; according to a launch template that configures the EC2 workers including; downloading and starting the EBS auto expander on the EC2 as part of its; startup. So it’s standard for the ECS cluster/ Batch compute environment used by; Cromwell but not standard for any other ECS node. On Wed, Jul 22, 2020 at 4:04 PM Richard Davison <notifications@github.com>; wrote:. > @markjschreiber <https://github.com/markjschreiber> : Oh! I think maybe I; > understand where the misunderstanding is. Could it be you're implying that; > the Cromwell *server* is correctly provisioning the worker nodes on; > behalf of the user?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662668064>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EKOXJKQMVJWYBPRYK3R45A6FANCNFSM4LW5UP3A>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662751419
https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662751419:137,Deployability,deploy,deployed,137,"When you submit a job to Cromwell with AWS Batch configured as the backend; AWS Batch will deploy workers via ECS. Those workers will be deployed; according to a launch template that configures the EC2 workers including; downloading and starting the EBS auto expander on the EC2 as part of its; startup. So it’s standard for the ECS cluster/ Batch compute environment used by; Cromwell but not standard for any other ECS node. On Wed, Jul 22, 2020 at 4:04 PM Richard Davison <notifications@github.com>; wrote:. > @markjschreiber <https://github.com/markjschreiber> : Oh! I think maybe I; > understand where the misunderstanding is. Could it be you're implying that; > the Cromwell *server* is correctly provisioning the worker nodes on; > behalf of the user?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662668064>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EKOXJKQMVJWYBPRYK3R45A6FANCNFSM4LW5UP3A>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662751419
https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662751419:49,Modifiability,config,configured,49,"When you submit a job to Cromwell with AWS Batch configured as the backend; AWS Batch will deploy workers via ECS. Those workers will be deployed; according to a launch template that configures the EC2 workers including; downloading and starting the EBS auto expander on the EC2 as part of its; startup. So it’s standard for the ECS cluster/ Batch compute environment used by; Cromwell but not standard for any other ECS node. On Wed, Jul 22, 2020 at 4:04 PM Richard Davison <notifications@github.com>; wrote:. > @markjschreiber <https://github.com/markjschreiber> : Oh! I think maybe I; > understand where the misunderstanding is. Could it be you're implying that; > the Cromwell *server* is correctly provisioning the worker nodes on; > behalf of the user?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662668064>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EKOXJKQMVJWYBPRYK3R45A6FANCNFSM4LW5UP3A>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662751419
https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662751419:183,Modifiability,config,configures,183,"When you submit a job to Cromwell with AWS Batch configured as the backend; AWS Batch will deploy workers via ECS. Those workers will be deployed; according to a launch template that configures the EC2 workers including; downloading and starting the EBS auto expander on the EC2 as part of its; startup. So it’s standard for the ECS cluster/ Batch compute environment used by; Cromwell but not standard for any other ECS node. On Wed, Jul 22, 2020 at 4:04 PM Richard Davison <notifications@github.com>; wrote:. > @markjschreiber <https://github.com/markjschreiber> : Oh! I think maybe I; > understand where the misunderstanding is. Could it be you're implying that; > the Cromwell *server* is correctly provisioning the worker nodes on; > behalf of the user?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662668064>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EKOXJKQMVJWYBPRYK3R45A6FANCNFSM4LW5UP3A>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662751419
https://github.com/broadinstitute/cromwell/pull/5469#issuecomment-608951967:225,Modifiability,config,config,225,"This was bad naming from the past that only popped up during centaur's auto-retry tests on a PostgreSQL database. The symptoms were centaur was unable to find tables such as `'cromwell_test'.'JOB_KEY_VALUE_ENTRY'`. I named a config key as `db.schema` and was using it to figure out the name of the cromwell database, that are all `cromwell_test` btw. This `db.schema` setting works fine on MariaDB, MySQL, and HsqlDB. Except on PostgreSQL the `schema` is usually `public`: https://www.postgresql.org/docs/12/ddl-schemas.html#DDL-SCHEMAS-PUBLIC. What was happening was that the config path `db` is fed by Slick to HikariCP. https://github.com/slick/slick/blob/v3.3.2/slick/src/main/scala/slick/basic/DatabaseConfig.scala#L102-L103. And the sub-config `schema` is used by HikariCP to feed to the underlying connection as the JDBC schema name. https://github.com/slick/slick/blob/v3.3.2/slick-hikaricp/src/main/scala/slick/jdbc/hikaricp/HikariCPJdbcDataSource.scala#L90. So, centaur setting `db.schema = cromwell_test` was connecting as normal to the `cromwell_test` PostgreSQL database, then HikariCP was also setting the schema to `cromwell_test`, effectively executing `select * from 'cromwell_test'.'SOME_TABLE'` instead of `select * from 'public'.'SOME_TABLE'`. Again, all of this was only in centaur, and only popped up when centaur was trying to retry a failed test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5469#issuecomment-608951967
https://github.com/broadinstitute/cromwell/pull/5469#issuecomment-608951967:577,Modifiability,config,config,577,"This was bad naming from the past that only popped up during centaur's auto-retry tests on a PostgreSQL database. The symptoms were centaur was unable to find tables such as `'cromwell_test'.'JOB_KEY_VALUE_ENTRY'`. I named a config key as `db.schema` and was using it to figure out the name of the cromwell database, that are all `cromwell_test` btw. This `db.schema` setting works fine on MariaDB, MySQL, and HsqlDB. Except on PostgreSQL the `schema` is usually `public`: https://www.postgresql.org/docs/12/ddl-schemas.html#DDL-SCHEMAS-PUBLIC. What was happening was that the config path `db` is fed by Slick to HikariCP. https://github.com/slick/slick/blob/v3.3.2/slick/src/main/scala/slick/basic/DatabaseConfig.scala#L102-L103. And the sub-config `schema` is used by HikariCP to feed to the underlying connection as the JDBC schema name. https://github.com/slick/slick/blob/v3.3.2/slick-hikaricp/src/main/scala/slick/jdbc/hikaricp/HikariCPJdbcDataSource.scala#L90. So, centaur setting `db.schema = cromwell_test` was connecting as normal to the `cromwell_test` PostgreSQL database, then HikariCP was also setting the schema to `cromwell_test`, effectively executing `select * from 'cromwell_test'.'SOME_TABLE'` instead of `select * from 'public'.'SOME_TABLE'`. Again, all of this was only in centaur, and only popped up when centaur was trying to retry a failed test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5469#issuecomment-608951967
https://github.com/broadinstitute/cromwell/pull/5469#issuecomment-608951967:743,Modifiability,config,config,743,"This was bad naming from the past that only popped up during centaur's auto-retry tests on a PostgreSQL database. The symptoms were centaur was unable to find tables such as `'cromwell_test'.'JOB_KEY_VALUE_ENTRY'`. I named a config key as `db.schema` and was using it to figure out the name of the cromwell database, that are all `cromwell_test` btw. This `db.schema` setting works fine on MariaDB, MySQL, and HsqlDB. Except on PostgreSQL the `schema` is usually `public`: https://www.postgresql.org/docs/12/ddl-schemas.html#DDL-SCHEMAS-PUBLIC. What was happening was that the config path `db` is fed by Slick to HikariCP. https://github.com/slick/slick/blob/v3.3.2/slick/src/main/scala/slick/basic/DatabaseConfig.scala#L102-L103. And the sub-config `schema` is used by HikariCP to feed to the underlying connection as the JDBC schema name. https://github.com/slick/slick/blob/v3.3.2/slick-hikaricp/src/main/scala/slick/jdbc/hikaricp/HikariCPJdbcDataSource.scala#L90. So, centaur setting `db.schema = cromwell_test` was connecting as normal to the `cromwell_test` PostgreSQL database, then HikariCP was also setting the schema to `cromwell_test`, effectively executing `select * from 'cromwell_test'.'SOME_TABLE'` instead of `select * from 'public'.'SOME_TABLE'`. Again, all of this was only in centaur, and only popped up when centaur was trying to retry a failed test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5469#issuecomment-608951967
https://github.com/broadinstitute/cromwell/pull/5469#issuecomment-608951967:82,Testability,test,tests,82,"This was bad naming from the past that only popped up during centaur's auto-retry tests on a PostgreSQL database. The symptoms were centaur was unable to find tables such as `'cromwell_test'.'JOB_KEY_VALUE_ENTRY'`. I named a config key as `db.schema` and was using it to figure out the name of the cromwell database, that are all `cromwell_test` btw. This `db.schema` setting works fine on MariaDB, MySQL, and HsqlDB. Except on PostgreSQL the `schema` is usually `public`: https://www.postgresql.org/docs/12/ddl-schemas.html#DDL-SCHEMAS-PUBLIC. What was happening was that the config path `db` is fed by Slick to HikariCP. https://github.com/slick/slick/blob/v3.3.2/slick/src/main/scala/slick/basic/DatabaseConfig.scala#L102-L103. And the sub-config `schema` is used by HikariCP to feed to the underlying connection as the JDBC schema name. https://github.com/slick/slick/blob/v3.3.2/slick-hikaricp/src/main/scala/slick/jdbc/hikaricp/HikariCPJdbcDataSource.scala#L90. So, centaur setting `db.schema = cromwell_test` was connecting as normal to the `cromwell_test` PostgreSQL database, then HikariCP was also setting the schema to `cromwell_test`, effectively executing `select * from 'cromwell_test'.'SOME_TABLE'` instead of `select * from 'public'.'SOME_TABLE'`. Again, all of this was only in centaur, and only popped up when centaur was trying to retry a failed test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5469#issuecomment-608951967
https://github.com/broadinstitute/cromwell/pull/5469#issuecomment-608951967:1365,Testability,test,test,1365,"This was bad naming from the past that only popped up during centaur's auto-retry tests on a PostgreSQL database. The symptoms were centaur was unable to find tables such as `'cromwell_test'.'JOB_KEY_VALUE_ENTRY'`. I named a config key as `db.schema` and was using it to figure out the name of the cromwell database, that are all `cromwell_test` btw. This `db.schema` setting works fine on MariaDB, MySQL, and HsqlDB. Except on PostgreSQL the `schema` is usually `public`: https://www.postgresql.org/docs/12/ddl-schemas.html#DDL-SCHEMAS-PUBLIC. What was happening was that the config path `db` is fed by Slick to HikariCP. https://github.com/slick/slick/blob/v3.3.2/slick/src/main/scala/slick/basic/DatabaseConfig.scala#L102-L103. And the sub-config `schema` is used by HikariCP to feed to the underlying connection as the JDBC schema name. https://github.com/slick/slick/blob/v3.3.2/slick-hikaricp/src/main/scala/slick/jdbc/hikaricp/HikariCPJdbcDataSource.scala#L90. So, centaur setting `db.schema = cromwell_test` was connecting as normal to the `cromwell_test` PostgreSQL database, then HikariCP was also setting the schema to `cromwell_test`, effectively executing `select * from 'cromwell_test'.'SOME_TABLE'` instead of `select * from 'public'.'SOME_TABLE'`. Again, all of this was only in centaur, and only popped up when centaur was trying to retry a failed test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5469#issuecomment-608951967
https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-606900517:48,Testability,test,test,48,@markjschreiber looks like we found your flakey test,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-606900517
https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-606907242:116,Availability,down,download,116,"> Did we update the akka http library in the last few days?. FYI, the import resolver, that during centaur tries to download the non-existent file from GitHub, [doesn't use akka-http](https://github.com/broadinstitute/cromwell/blob/49/languageFactories/language-factory-core/src/main/scala/cromwell/languages/util/ImportResolver.scala#L191) like most of cromwell/cromiam/centaur/etc. It uses yet another jvm http client called [sttp](https://github.com/softwaremill/sttp#readme). Still, I think what you're running into is that GitHub changed their 404 response. See `curl -i https://raw.githubusercontent.com/broadinstitute/cromwell/develop/my_workflow`. I don't know how stable the change is either... they may change the body of the 404 response again [without notice](https://xkcd.com/1172/).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-606907242
https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-606907242:9,Deployability,update,update,9,"> Did we update the akka http library in the last few days?. FYI, the import resolver, that during centaur tries to download the non-existent file from GitHub, [doesn't use akka-http](https://github.com/broadinstitute/cromwell/blob/49/languageFactories/language-factory-core/src/main/scala/cromwell/languages/util/ImportResolver.scala#L191) like most of cromwell/cromiam/centaur/etc. It uses yet another jvm http client called [sttp](https://github.com/softwaremill/sttp#readme). Still, I think what you're running into is that GitHub changed their 404 response. See `curl -i https://raw.githubusercontent.com/broadinstitute/cromwell/develop/my_workflow`. I don't know how stable the change is either... they may change the body of the 404 response again [without notice](https://xkcd.com/1172/).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-606907242
https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-607263047:50,Testability,test,test,50,"> @markjschreiber looks like we found your flakey test. Yes, this one has been failing for me a lot",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5471#issuecomment-607263047
https://github.com/broadinstitute/cromwell/issues/5476#issuecomment-1189554651:240,Availability,avail,available,240,"@AlexMTX @rhpvorderman Am I able to specify the following in a task using draft-2?; ```; meta {; volatile: ""true""; } ; ```. I found that call-caching still occurred for the task with this volatile: ""true"" included in the meta. Is this only available for version 1.0 or am I misunderstanding your issue Alex?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5476#issuecomment-1189554651
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-610961168:91,Testability,test,test,91,~~Hmm it turns out this should be fixed for other backends as well.~~; Edit: I changed the test only to run for sfsBackend.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-610961168
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735:489,Deployability,release,release,489,"@aednichols Thanks again for allowing me push access to the repo. I can not test all the backends manually so it is good that I can access the CI environment and see if the bug fix turned out well. What is the formal process of getting this bug under the Cromwell team's attention? I have made a JIRA issue. Should I put it on the sprint? I also ask this for #5456 which is a really simple fix. I am not in great haste getting a review, but I want to ensure these fixes end up in the next release of Cromwell. These bugs are now actively blocking BioWDL development as our CI always uses a mainline version of Cromwell. (Usually the latest, but we are already actively excluding 49 because of the relative outputs bug). . By no means I want to push the Cromwell team in reviewing these fixes right now, but if you could give me some procedure that would make sure these are reviewed before the next release is out, that would give me some peace of mind. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735:899,Deployability,release,release,899,"@aednichols Thanks again for allowing me push access to the repo. I can not test all the backends manually so it is good that I can access the CI environment and see if the bug fix turned out well. What is the formal process of getting this bug under the Cromwell team's attention? I have made a JIRA issue. Should I put it on the sprint? I also ask this for #5456 which is a really simple fix. I am not in great haste getting a review, but I want to ensure these fixes end up in the next release of Cromwell. These bugs are now actively blocking BioWDL development as our CI always uses a mainline version of Cromwell. (Usually the latest, but we are already actively excluding 49 because of the relative outputs bug). . By no means I want to push the Cromwell team in reviewing these fixes right now, but if you could give me some procedure that would make sure these are reviewed before the next release is out, that would give me some peace of mind. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735:46,Security,access,access,46,"@aednichols Thanks again for allowing me push access to the repo. I can not test all the backends manually so it is good that I can access the CI environment and see if the bug fix turned out well. What is the formal process of getting this bug under the Cromwell team's attention? I have made a JIRA issue. Should I put it on the sprint? I also ask this for #5456 which is a really simple fix. I am not in great haste getting a review, but I want to ensure these fixes end up in the next release of Cromwell. These bugs are now actively blocking BioWDL development as our CI always uses a mainline version of Cromwell. (Usually the latest, but we are already actively excluding 49 because of the relative outputs bug). . By no means I want to push the Cromwell team in reviewing these fixes right now, but if you could give me some procedure that would make sure these are reviewed before the next release is out, that would give me some peace of mind. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735:132,Security,access,access,132,"@aednichols Thanks again for allowing me push access to the repo. I can not test all the backends manually so it is good that I can access the CI environment and see if the bug fix turned out well. What is the formal process of getting this bug under the Cromwell team's attention? I have made a JIRA issue. Should I put it on the sprint? I also ask this for #5456 which is a really simple fix. I am not in great haste getting a review, but I want to ensure these fixes end up in the next release of Cromwell. These bugs are now actively blocking BioWDL development as our CI always uses a mainline version of Cromwell. (Usually the latest, but we are already actively excluding 49 because of the relative outputs bug). . By no means I want to push the Cromwell team in reviewing these fixes right now, but if you could give me some procedure that would make sure these are reviewed before the next release is out, that would give me some peace of mind. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735:76,Testability,test,test,76,"@aednichols Thanks again for allowing me push access to the repo. I can not test all the backends manually so it is good that I can access the CI environment and see if the bug fix turned out well. What is the formal process of getting this bug under the Cromwell team's attention? I have made a JIRA issue. Should I put it on the sprint? I also ask this for #5456 which is a really simple fix. I am not in great haste getting a review, but I want to ensure these fixes end up in the next release of Cromwell. These bugs are now actively blocking BioWDL development as our CI always uses a mainline version of Cromwell. (Usually the latest, but we are already actively excluding 49 because of the relative outputs bug). . By no means I want to push the Cromwell team in reviewing these fixes right now, but if you could give me some procedure that would make sure these are reviewed before the next release is out, that would give me some peace of mind. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735:383,Usability,simpl,simple,383,"@aednichols Thanks again for allowing me push access to the repo. I can not test all the backends manually so it is good that I can access the CI environment and see if the bug fix turned out well. What is the formal process of getting this bug under the Cromwell team's attention? I have made a JIRA issue. Should I put it on the sprint? I also ask this for #5456 which is a really simple fix. I am not in great haste getting a review, but I want to ensure these fixes end up in the next release of Cromwell. These bugs are now actively blocking BioWDL development as our CI always uses a mainline version of Cromwell. (Usually the latest, but we are already actively excluding 49 because of the relative outputs bug). . By no means I want to push the Cromwell team in reviewing these fixes right now, but if you could give me some procedure that would make sure these are reviewed before the next release is out, that would give me some peace of mind. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-611900735
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618685855:377,Performance,perform,performing,377,"This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. The `ValidatedWomNamespace` produced as part of workflow materialization contains a `womValueInputs` field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618685855
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618685855:255,Security,Validat,ValidatedWomNamespace,255,"This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. The `ValidatedWomNamespace` produced as part of workflow materialization contains a `womValueInputs` field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618685855
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618685855:426,Security,validat,validated,426,"This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. The `ValidatedWomNamespace` produced as part of workflow materialization contains a `womValueInputs` field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618685855
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618835746:567,Availability,down,down,567,"> I can follow how `forInput` is passed around, but I can't seem to discern where it is actually set - i.e. where the default value is overridden. Can you help me wrap my head around this?. [Here](https://github.com/broadinstitute/cromwell/blob/9bd4e90fdf999abc76d0ba8e801ad24eb99140b5/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/preparation/JobPreparationActor.scala#L62) in the JobPreparationActor. I agree that it is confusing. This is mostly because of the structure of the code. It has to be set in the top level class and then passed down through multiple layers. During JobPreparation the evaluation must be different. After that evaluation of the expressions in the outputs is handled, and there the default works well. I had to give the SFSBackend some info that it was being used in this way, so I had to set it all the way at toplevel and add all this code. It would be nice if there was a more straightforward way of doing this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618835746
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618835746:163,Integrability,wrap,wrap,163,"> I can follow how `forInput` is passed around, but I can't seem to discern where it is actually set - i.e. where the default value is overridden. Can you help me wrap my head around this?. [Here](https://github.com/broadinstitute/cromwell/blob/9bd4e90fdf999abc76d0ba8e801ad24eb99140b5/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/preparation/JobPreparationActor.scala#L62) in the JobPreparationActor. I agree that it is confusing. This is mostly because of the structure of the code. It has to be set in the top level class and then passed down through multiple layers. During JobPreparation the evaluation must be different. After that evaluation of the expressions in the outputs is handled, and there the default works well. I had to give the SFSBackend some info that it was being used in this way, so I had to set it all the way at toplevel and add all this code. It would be nice if there was a more straightforward way of doing this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618835746
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618835746:589,Modifiability,layers,layers,589,"> I can follow how `forInput` is passed around, but I can't seem to discern where it is actually set - i.e. where the default value is overridden. Can you help me wrap my head around this?. [Here](https://github.com/broadinstitute/cromwell/blob/9bd4e90fdf999abc76d0ba8e801ad24eb99140b5/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/preparation/JobPreparationActor.scala#L62) in the JobPreparationActor. I agree that it is confusing. This is mostly because of the structure of the code. It has to be set in the top level class and then passed down through multiple layers. During JobPreparation the evaluation must be different. After that evaluation of the expressions in the outputs is handled, and there the default works well. I had to give the SFSBackend some info that it was being used in this way, so I had to set it all the way at toplevel and add all this code. It would be nice if there was a more straightforward way of doing this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618835746
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024:977,Availability,down,down,977,"> This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. > The ValidatedWomNamespace produced as part of workflow materialization contains a womValueInputs field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?. Great suggestion. I will take a look at this. I can checkout the test case on a new branch and try to hack there. One of the catches will be that this resolving will be backend dependent. In the current situation the input expressions are evaluated first, and after that the inputs are resolved. (This makes sense because input can also be something like `baseDir + ""/my_file.txt""`, which needs to be evaluated). But indeed this could be bypassed by doing this already at the workflow level, before it gets passed down to the task level. I will take a look at this. If it does not work, (or work easily) then I will report back here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024:640,Integrability,depend,dependent,640,"> This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. > The ValidatedWomNamespace produced as part of workflow materialization contains a womValueInputs field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?. Great suggestion. I will take a look at this. I can checkout the test case on a new branch and try to hack there. One of the catches will be that this resolving will be backend dependent. In the current situation the input expressions are evaluated first, and after that the inputs are resolved. (This makes sense because input can also be something like `baseDir + ""/my_file.txt""`, which needs to be evaluated). But indeed this could be bypassed by doing this already at the workflow level, before it gets passed down to the task level. I will take a look at this. If it does not work, (or work easily) then I will report back here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024:377,Performance,perform,performing,377,"> This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. > The ValidatedWomNamespace produced as part of workflow materialization contains a womValueInputs field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?. Great suggestion. I will take a look at this. I can checkout the test case on a new branch and try to hack there. One of the catches will be that this resolving will be backend dependent. In the current situation the input expressions are evaluated first, and after that the inputs are resolved. (This makes sense because input can also be something like `baseDir + ""/my_file.txt""`, which needs to be evaluated). But indeed this could be bypassed by doing this already at the workflow level, before it gets passed down to the task level. I will take a look at this. If it does not work, (or work easily) then I will report back here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024:258,Security,Validat,ValidatedWomNamespace,258,"> This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. > The ValidatedWomNamespace produced as part of workflow materialization contains a womValueInputs field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?. Great suggestion. I will take a look at this. I can checkout the test case on a new branch and try to hack there. One of the catches will be that this resolving will be backend dependent. In the current situation the input expressions are evaluated first, and after that the inputs are resolved. (This makes sense because input can also be something like `baseDir + ""/my_file.txt""`, which needs to be evaluated). But indeed this could be bypassed by doing this already at the workflow level, before it gets passed down to the task level. I will take a look at this. If it does not work, (or work easily) then I will report back here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024:426,Security,validat,validated,426,"> This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. > The ValidatedWomNamespace produced as part of workflow materialization contains a womValueInputs field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?. Great suggestion. I will take a look at this. I can checkout the test case on a new branch and try to hack there. One of the catches will be that this resolving will be backend dependent. In the current situation the input expressions are evaluated first, and after that the inputs are resolved. (This makes sense because input can also be something like `baseDir + ""/my_file.txt""`, which needs to be evaluated). But indeed this could be bypassed by doing this already at the workflow level, before it gets passed down to the task level. I will take a look at this. If it does not work, (or work easily) then I will report back here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024:528,Testability,test,test,528,"> This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. > The ValidatedWomNamespace produced as part of workflow materialization contains a womValueInputs field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?. Great suggestion. I will take a look at this. I can checkout the test case on a new branch and try to hack there. One of the catches will be that this resolving will be backend dependent. In the current situation the input expressions are evaluated first, and after that the inputs are resolved. (This makes sense because input can also be something like `baseDir + ""/my_file.txt""`, which needs to be evaluated). But indeed this could be bypassed by doing this already at the workflow level, before it gets passed down to the task level. I will take a look at this. If it does not work, (or work easily) then I will report back here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618941883:257,Security,access,access,257,"@cjllanwarne I have had a look bit it is quite non-trivial to find how to evaluate paths at the womValueInputs stage, as cromwell has no knowledge of the backend at this point. This ""relative path"" business only makes sense for the sfsBackend which handles access to local systems. For cloud systems relative paths make no sense at all. ; Evaluating it at the JobPreparationActor is the correct spot, although the implementation ends up to be a bit messy :confused: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618941883
https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618975226:64,Testability,test,tests,64,"@aednichols @cjllanwarne I made a copy of this PR with the same tests and changelog, but with a less intrusive code change at #5495 .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618975226
https://github.com/broadinstitute/cromwell/issues/5479#issuecomment-618341433:172,Testability,log,logical,172,"I am not from the Cromwell core team but what I can say is that in the latest openwdl hackathon that the consensus was `no output section -> no output`. Which is much more logical than `no output section -> all outputs`. This will be codified in WDL 2.0. This does not answer your question, but I hope it gives some additional context.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5479#issuecomment-618341433
https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618343197:221,Testability,test,tested,221,workaround:. In biowdl we solve this like this: https://github.com/biowdl/tasks/blob/fd60df60a8c704811c46954e60417a2a081b6693/samtools.wdl#L60. Additionally there are a lot of other tasks in biowdl tasks that are heavily tested with cromwell.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618343197
https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618714864:515,Deployability,pipeline,pipelines,515,"@rhpvorderman I usually solve it by:; ```; ln -s ~{bam} ~{basename(bam)}; ```; However, even though I have workarounds I think it is a big flow of Cromwell that it allows the tools to mutate the Input folder instead of writing to executions. I believe Inputs should be immutable by design. Many new Cromwell users can waste hours (like I did) before untile they realize that some tools produce output but write in Inputs instead of execution. P.S. Also, I am a big fun of BioWDL and I get inspiration from it in my pipelines. However, as the workflows of our lab are different I do not git-clone them but prefer to implement my own versions. I am also open to contribute the tasks for the tools that are not in BioWDL but which I wrote in my pipelines and which may be interesting for many users",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618714864
https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618714864:742,Deployability,pipeline,pipelines,742,"@rhpvorderman I usually solve it by:; ```; ln -s ~{bam} ~{basename(bam)}; ```; However, even though I have workarounds I think it is a big flow of Cromwell that it allows the tools to mutate the Input folder instead of writing to executions. I believe Inputs should be immutable by design. Many new Cromwell users can waste hours (like I did) before untile they realize that some tools produce output but write in Inputs instead of execution. P.S. Also, I am a big fun of BioWDL and I get inspiration from it in my pipelines. However, as the workflows of our lab are different I do not git-clone them but prefer to implement my own versions. I am also open to contribute the tasks for the tools that are not in BioWDL but which I wrote in my pipelines and which may be interesting for many users",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618714864
https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618842438:241,Availability,down,down,241,"@antonkulaga We always welcome PRs! It's not obligated though, even if you use BioWDL :wink: . On-topic: Yes, I think Cromwell could modify the input folder and its contents to be read-only . But that might have some unforeseen consequences down the line. This would need to be tested.; DISCLAIMER: I am not of the cromwell team. So I will not implement this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618842438
https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618842438:278,Testability,test,tested,278,"@antonkulaga We always welcome PRs! It's not obligated though, even if you use BioWDL :wink: . On-topic: Yes, I think Cromwell could modify the input folder and its contents to be read-only . But that might have some unforeseen consequences down the line. This would need to be tested.; DISCLAIMER: I am not of the cromwell team. So I will not implement this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5482#issuecomment-618842438
https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617305109:45,Availability,failure,failures,45,I didn't drill in but that is a lot of build failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617305109
https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617315302:72,Availability,error,errors,72,"I'd be surprised if a one-line gitignore addition would cause all those errors, which from an n of 1, seem mostly like bad travis weather... 🤞 a restart will fix these",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617315302
https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617366702:43,Performance,cache,cache,43,restarts didn't seem to do the trick but a cache blast + restart is looking better,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617366702
https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617369958:4,Performance,cache,cache,4,> a cache blast + restart is looking better. thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617369958
https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527:724,Availability,error,errors,724,"### Progress update . I made a fix in liquibase and it was pulled. As a result I can now successfully create a correct cromwell database in SQLite. (At least I think so). There are some problems where SQLite has different types. (I.e. a timestamp is a text field, there is no separate TIMESTAMP column type only TEXT). This causes issues mainly in the testing, but I should be able to resolve this. There are some issues during the testing where Slick (?) or some other part does not seem to recognize foreign keys, primary keys and unique constraints, despite these being clearly there when I look at them with sqlitebrowser. This will require some more digging. As of yet running cromwell in server mode still spawns some errors when using a sqlite database, so I guess it will take some time before I have everything figured out. . Pinging @aednichols so at least someone in the Cromwell team knows this is ongoing :slightly_smiling_face: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527
https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527:835,Availability,Ping,Pinging,835,"### Progress update . I made a fix in liquibase and it was pulled. As a result I can now successfully create a correct cromwell database in SQLite. (At least I think so). There are some problems where SQLite has different types. (I.e. a timestamp is a text field, there is no separate TIMESTAMP column type only TEXT). This causes issues mainly in the testing, but I should be able to resolve this. There are some issues during the testing where Slick (?) or some other part does not seem to recognize foreign keys, primary keys and unique constraints, despite these being clearly there when I look at them with sqlitebrowser. This will require some more digging. As of yet running cromwell in server mode still spawns some errors when using a sqlite database, so I guess it will take some time before I have everything figured out. . Pinging @aednichols so at least someone in the Cromwell team knows this is ongoing :slightly_smiling_face: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527
https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527:13,Deployability,update,update,13,"### Progress update . I made a fix in liquibase and it was pulled. As a result I can now successfully create a correct cromwell database in SQLite. (At least I think so). There are some problems where SQLite has different types. (I.e. a timestamp is a text field, there is no separate TIMESTAMP column type only TEXT). This causes issues mainly in the testing, but I should be able to resolve this. There are some issues during the testing where Slick (?) or some other part does not seem to recognize foreign keys, primary keys and unique constraints, despite these being clearly there when I look at them with sqlitebrowser. This will require some more digging. As of yet running cromwell in server mode still spawns some errors when using a sqlite database, so I guess it will take some time before I have everything figured out. . Pinging @aednichols so at least someone in the Cromwell team knows this is ongoing :slightly_smiling_face: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527
https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527:352,Testability,test,testing,352,"### Progress update . I made a fix in liquibase and it was pulled. As a result I can now successfully create a correct cromwell database in SQLite. (At least I think so). There are some problems where SQLite has different types. (I.e. a timestamp is a text field, there is no separate TIMESTAMP column type only TEXT). This causes issues mainly in the testing, but I should be able to resolve this. There are some issues during the testing where Slick (?) or some other part does not seem to recognize foreign keys, primary keys and unique constraints, despite these being clearly there when I look at them with sqlitebrowser. This will require some more digging. As of yet running cromwell in server mode still spawns some errors when using a sqlite database, so I guess it will take some time before I have everything figured out. . Pinging @aednichols so at least someone in the Cromwell team knows this is ongoing :slightly_smiling_face: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527
https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527:432,Testability,test,testing,432,"### Progress update . I made a fix in liquibase and it was pulled. As a result I can now successfully create a correct cromwell database in SQLite. (At least I think so). There are some problems where SQLite has different types. (I.e. a timestamp is a text field, there is no separate TIMESTAMP column type only TEXT). This causes issues mainly in the testing, but I should be able to resolve this. There are some issues during the testing where Slick (?) or some other part does not seem to recognize foreign keys, primary keys and unique constraints, despite these being clearly there when I look at them with sqlitebrowser. This will require some more digging. As of yet running cromwell in server mode still spawns some errors when using a sqlite database, so I guess it will take some time before I have everything figured out. . Pinging @aednichols so at least someone in the Cromwell team knows this is ongoing :slightly_smiling_face: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527
https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527:573,Usability,clear,clearly,573,"### Progress update . I made a fix in liquibase and it was pulled. As a result I can now successfully create a correct cromwell database in SQLite. (At least I think so). There are some problems where SQLite has different types. (I.e. a timestamp is a text field, there is no separate TIMESTAMP column type only TEXT). This causes issues mainly in the testing, but I should be able to resolve this. There are some issues during the testing where Slick (?) or some other part does not seem to recognize foreign keys, primary keys and unique constraints, despite these being clearly there when I look at them with sqlitebrowser. This will require some more digging. As of yet running cromwell in server mode still spawns some errors when using a sqlite database, so I guess it will take some time before I have everything figured out. . Pinging @aednichols so at least someone in the Cromwell team knows this is ongoing :slightly_smiling_face: .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5490#issuecomment-654896527
https://github.com/broadinstitute/cromwell/pull/5493#issuecomment-619068749:90,Usability,feedback,feedback,90,"@rhpvorderman very much appreciated, thank you!. We're not python experts so this kind of feedback is extremely useful!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5493#issuecomment-619068749
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-619251813:172,Testability,test,tests,172,"Thanks @illusional!. I ended up making a PR against your PR branch back in the `illusional/cromwell` repo with a few changes... I fixed your compile issues and added a few tests. It looks like your type evaluator is expecting a map input to the function for some reason, and doesn't like the string/array, and the value evaluator has the order of the two parameters back to front, but otherwise I think it looks fairly solid. An end to end centaur test would probably also be a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-619251813
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-619251813:448,Testability,test,test,448,"Thanks @illusional!. I ended up making a PR against your PR branch back in the `illusional/cromwell` repo with a few changes... I fixed your compile issues and added a few tests. It looks like your type evaluator is expecting a map input to the function for some reason, and doesn't like the string/array, and the value evaluator has the order of the two parameters back to front, but otherwise I think it looks fairly solid. An end to end centaur test would probably also be a good idea.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-619251813
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-620324215:446,Testability,test,tests,446,"Hi @cjllanwarne, to get this change closer to the (now correctly referenced WDL card), will I need to remove the option for interpolators to work for the development / Biscayne spec. Technically, it's not against the spec for Cromwell to accept it, but it might be confusing for users if it does. If so, I assume changing the grammar might be sufficient, but this would come from WDL and not Cromwell. If I don't, I think this PR just needs more tests from me then it might be ready for review. Edit: The ANTLR grammar has already made to reflect this change, so I don't think it'll need to be done as part of this Cromwell PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-620324215
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308:736,Availability,avail,available,736,"I think this is ready for review, passes my basic workflows okay. Can I request the WDL Biscayne and general WDL labels for this PR?. ### Testing . Alrighty, so I did some digging and looks like similar PRs don't include tests. This doesn't mean I should blindly follow, but I can't see a great way to add a test and check its output, so I've only added validation tests (293256180ea8f6f5398866110ba8b727fd4c148e). I'm not sure if this should make it into the CHANGELOG, but I've added some text here which I'll add into the . > ### New sep function for joining an array of strings; >; > Per [OpenWDL #229](https://github.com/openwdl/wdl/pull/229), we've replaced the `sep=` string interpolator option with a new `sep` engine function, available in the WDL development (Biscayne) specification.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308:354,Security,validat,validation,354,"I think this is ready for review, passes my basic workflows okay. Can I request the WDL Biscayne and general WDL labels for this PR?. ### Testing . Alrighty, so I did some digging and looks like similar PRs don't include tests. This doesn't mean I should blindly follow, but I can't see a great way to add a test and check its output, so I've only added validation tests (293256180ea8f6f5398866110ba8b727fd4c148e). I'm not sure if this should make it into the CHANGELOG, but I've added some text here which I'll add into the . > ### New sep function for joining an array of strings; >; > Per [OpenWDL #229](https://github.com/openwdl/wdl/pull/229), we've replaced the `sep=` string interpolator option with a new `sep` engine function, available in the WDL development (Biscayne) specification.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308:138,Testability,Test,Testing,138,"I think this is ready for review, passes my basic workflows okay. Can I request the WDL Biscayne and general WDL labels for this PR?. ### Testing . Alrighty, so I did some digging and looks like similar PRs don't include tests. This doesn't mean I should blindly follow, but I can't see a great way to add a test and check its output, so I've only added validation tests (293256180ea8f6f5398866110ba8b727fd4c148e). I'm not sure if this should make it into the CHANGELOG, but I've added some text here which I'll add into the . > ### New sep function for joining an array of strings; >; > Per [OpenWDL #229](https://github.com/openwdl/wdl/pull/229), we've replaced the `sep=` string interpolator option with a new `sep` engine function, available in the WDL development (Biscayne) specification.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308:221,Testability,test,tests,221,"I think this is ready for review, passes my basic workflows okay. Can I request the WDL Biscayne and general WDL labels for this PR?. ### Testing . Alrighty, so I did some digging and looks like similar PRs don't include tests. This doesn't mean I should blindly follow, but I can't see a great way to add a test and check its output, so I've only added validation tests (293256180ea8f6f5398866110ba8b727fd4c148e). I'm not sure if this should make it into the CHANGELOG, but I've added some text here which I'll add into the . > ### New sep function for joining an array of strings; >; > Per [OpenWDL #229](https://github.com/openwdl/wdl/pull/229), we've replaced the `sep=` string interpolator option with a new `sep` engine function, available in the WDL development (Biscayne) specification.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308:308,Testability,test,test,308,"I think this is ready for review, passes my basic workflows okay. Can I request the WDL Biscayne and general WDL labels for this PR?. ### Testing . Alrighty, so I did some digging and looks like similar PRs don't include tests. This doesn't mean I should blindly follow, but I can't see a great way to add a test and check its output, so I've only added validation tests (293256180ea8f6f5398866110ba8b727fd4c148e). I'm not sure if this should make it into the CHANGELOG, but I've added some text here which I'll add into the . > ### New sep function for joining an array of strings; >; > Per [OpenWDL #229](https://github.com/openwdl/wdl/pull/229), we've replaced the `sep=` string interpolator option with a new `sep` engine function, available in the WDL development (Biscayne) specification.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308:365,Testability,test,tests,365,"I think this is ready for review, passes my basic workflows okay. Can I request the WDL Biscayne and general WDL labels for this PR?. ### Testing . Alrighty, so I did some digging and looks like similar PRs don't include tests. This doesn't mean I should blindly follow, but I can't see a great way to add a test and check its output, so I've only added validation tests (293256180ea8f6f5398866110ba8b727fd4c148e). I'm not sure if this should make it into the CHANGELOG, but I've added some text here which I'll add into the . > ### New sep function for joining an array of strings; >; > Per [OpenWDL #229](https://github.com/openwdl/wdl/pull/229), we've replaced the `sep=` string interpolator option with a new `sep` engine function, available in the WDL development (Biscayne) specification.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247:109,Availability,error,errors,109,"Hey @aednichols, I tested this to make sure, and as expected, running the test with `version 1.0` fails with errors:. ```; Failed to read task definition at line 3 column 6 (reason 1 of 1): Failed to read outputs section (reason 1 of 1): Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; Failed to read task definition at line 13 column 6 (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; ```. I've fixed the corresponding tests in https://github.com/broadinstitute/cromwell/pull/5494/commits/40851b7423de68bda7ff9aaa47a37fbf7a0a70a3. So this should still be good to merge!. Edit: I see the test is failing, but looks like an unrelated error:. ```; [error] java.lang.RuntimeException: The vault token file ""/home/travis/.vault-token"" does not exist. Be sure to login using the instructions on https://hub.docker.com/r/broadinstitute/dsde-toolbox/ under ""Authenticating to vault"".; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247:749,Availability,error,error,749,"Hey @aednichols, I tested this to make sure, and as expected, running the test with `version 1.0` fails with errors:. ```; Failed to read task definition at line 3 column 6 (reason 1 of 1): Failed to read outputs section (reason 1 of 1): Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; Failed to read task definition at line 13 column 6 (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; ```. I've fixed the corresponding tests in https://github.com/broadinstitute/cromwell/pull/5494/commits/40851b7423de68bda7ff9aaa47a37fbf7a0a70a3. So this should still be good to merge!. Edit: I see the test is failing, but looks like an unrelated error:. ```; [error] java.lang.RuntimeException: The vault token file ""/home/travis/.vault-token"" does not exist. Be sure to login using the instructions on https://hub.docker.com/r/broadinstitute/dsde-toolbox/ under ""Authenticating to vault"".; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247:763,Availability,error,error,763,"Hey @aednichols, I tested this to make sure, and as expected, running the test with `version 1.0` fails with errors:. ```; Failed to read task definition at line 3 column 6 (reason 1 of 1): Failed to read outputs section (reason 1 of 1): Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; Failed to read task definition at line 13 column 6 (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; ```. I've fixed the corresponding tests in https://github.com/broadinstitute/cromwell/pull/5494/commits/40851b7423de68bda7ff9aaa47a37fbf7a0a70a3. So this should still be good to merge!. Edit: I see the test is failing, but looks like an unrelated error:. ```; [error] java.lang.RuntimeException: The vault token file ""/home/travis/.vault-token"" does not exist. Be sure to login using the instructions on https://hub.docker.com/r/broadinstitute/dsde-toolbox/ under ""Authenticating to vault"".; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247:967,Security,Authenticat,Authenticating,967,"Hey @aednichols, I tested this to make sure, and as expected, running the test with `version 1.0` fails with errors:. ```; Failed to read task definition at line 3 column 6 (reason 1 of 1): Failed to read outputs section (reason 1 of 1): Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; Failed to read task definition at line 13 column 6 (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; ```. I've fixed the corresponding tests in https://github.com/broadinstitute/cromwell/pull/5494/commits/40851b7423de68bda7ff9aaa47a37fbf7a0a70a3. So this should still be good to merge!. Edit: I see the test is failing, but looks like an unrelated error:. ```; [error] java.lang.RuntimeException: The vault token file ""/home/travis/.vault-token"" does not exist. Be sure to login using the instructions on https://hub.docker.com/r/broadinstitute/dsde-toolbox/ under ""Authenticating to vault"".; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247:19,Testability,test,tested,19,"Hey @aednichols, I tested this to make sure, and as expected, running the test with `version 1.0` fails with errors:. ```; Failed to read task definition at line 3 column 6 (reason 1 of 1): Failed to read outputs section (reason 1 of 1): Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; Failed to read task definition at line 13 column 6 (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; ```. I've fixed the corresponding tests in https://github.com/broadinstitute/cromwell/pull/5494/commits/40851b7423de68bda7ff9aaa47a37fbf7a0a70a3. So this should still be good to merge!. Edit: I see the test is failing, but looks like an unrelated error:. ```; [error] java.lang.RuntimeException: The vault token file ""/home/travis/.vault-token"" does not exist. Be sure to login using the instructions on https://hub.docker.com/r/broadinstitute/dsde-toolbox/ under ""Authenticating to vault"".; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247:74,Testability,test,test,74,"Hey @aednichols, I tested this to make sure, and as expected, running the test with `version 1.0` fails with errors:. ```; Failed to read task definition at line 3 column 6 (reason 1 of 1): Failed to read outputs section (reason 1 of 1): Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; Failed to read task definition at line 13 column 6 (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; ```. I've fixed the corresponding tests in https://github.com/broadinstitute/cromwell/pull/5494/commits/40851b7423de68bda7ff9aaa47a37fbf7a0a70a3. So this should still be good to merge!. Edit: I see the test is failing, but looks like an unrelated error:. ```; [error] java.lang.RuntimeException: The vault token file ""/home/travis/.vault-token"" does not exist. Be sure to login using the instructions on https://hub.docker.com/r/broadinstitute/dsde-toolbox/ under ""Authenticating to vault"".; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247:536,Testability,test,tests,536,"Hey @aednichols, I tested this to make sure, and as expected, running the test with `version 1.0` fails with errors:. ```; Failed to read task definition at line 3 column 6 (reason 1 of 1): Failed to read outputs section (reason 1 of 1): Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; Failed to read task definition at line 13 column 6 (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; ```. I've fixed the corresponding tests in https://github.com/broadinstitute/cromwell/pull/5494/commits/40851b7423de68bda7ff9aaa47a37fbf7a0a70a3. So this should still be good to merge!. Edit: I see the test is failing, but looks like an unrelated error:. ```; [error] java.lang.RuntimeException: The vault token file ""/home/travis/.vault-token"" does not exist. Be sure to login using the instructions on https://hub.docker.com/r/broadinstitute/dsde-toolbox/ under ""Authenticating to vault"".; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247:704,Testability,test,test,704,"Hey @aednichols, I tested this to make sure, and as expected, running the test with `version 1.0` fails with errors:. ```; Failed to read task definition at line 3 column 6 (reason 1 of 1): Failed to read outputs section (reason 1 of 1): Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; Failed to read task definition at line 13 column 6 (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; ```. I've fixed the corresponding tests in https://github.com/broadinstitute/cromwell/pull/5494/commits/40851b7423de68bda7ff9aaa47a37fbf7a0a70a3. So this should still be good to merge!. Edit: I see the test is failing, but looks like an unrelated error:. ```; [error] java.lang.RuntimeException: The vault token file ""/home/travis/.vault-token"" does not exist. Be sure to login using the instructions on https://hub.docker.com/r/broadinstitute/dsde-toolbox/ under ""Authenticating to vault"".; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247:874,Testability,log,login,874,"Hey @aednichols, I tested this to make sure, and as expected, running the test with `version 1.0` fails with errors:. ```; Failed to read task definition at line 3 column 6 (reason 1 of 1): Failed to read outputs section (reason 1 of 1): Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; Failed to read task definition at line 13 column 6 (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; ```. I've fixed the corresponding tests in https://github.com/broadinstitute/cromwell/pull/5494/commits/40851b7423de68bda7ff9aaa47a37fbf7a0a70a3. So this should still be good to merge!. Edit: I see the test is failing, but looks like an unrelated error:. ```; [error] java.lang.RuntimeException: The vault token file ""/home/travis/.vault-token"" does not exist. Be sure to login using the instructions on https://hub.docker.com/r/broadinstitute/dsde-toolbox/ under ""Authenticating to vault"".; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-655750203:76,Availability,error,error,76,"@cjllanwarne @aednichols Looks like the tests didn’t complete with the same error, I gave merging from develop another crack without success. Hopefully it’s all good though 😬",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-655750203
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-655750203:40,Testability,test,tests,40,"@cjllanwarne @aednichols Looks like the tests didn’t complete with the same error, I gave merging from develop another crack without success. Hopefully it’s all good though 😬",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-655750203
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656432981:18,Availability,error,error,18,"SBT fails with an error that looks very related; ```; should evaluate a sep expression containing a sub-call to prefix correctly *** FAILED *** (50 milliseconds). [info] EvaluatedValue(WomString(-i a -i b -i c),List()) was not equal to EvaluatedValue(WomString(a b c),List()) (ErrorOrAssertions.scala:11). [info] org.scalatest.exceptions.TestFailedException:; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656432981
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656432981:277,Availability,Error,ErrorOrAssertions,277,"SBT fails with an error that looks very related; ```; should evaluate a sep expression containing a sub-call to prefix correctly *** FAILED *** (50 milliseconds). [info] EvaluatedValue(WomString(-i a -i b -i c),List()) was not equal to EvaluatedValue(WomString(a b c),List()) (ErrorOrAssertions.scala:11). [info] org.scalatest.exceptions.TestFailedException:; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656432981
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656432981:338,Testability,Test,TestFailedException,338,"SBT fails with an error that looks very related; ```; should evaluate a sep expression containing a sub-call to prefix correctly *** FAILED *** (50 milliseconds). [info] EvaluatedValue(WomString(-i a -i b -i c),List()) was not equal to EvaluatedValue(WomString(a b c),List()) (ErrorOrAssertions.scala:11). [info] org.scalatest.exceptions.TestFailedException:; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656432981
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553:47,Availability,down,down,47,"Thanks @aednichols, looks like I didn't scroll down enough - I think @cjllanwarne added this test at the very start so wasn't on my horizon to check. I've pushed a fix, and will confirm when it's fixed. Edit: Initial logs look like it's passing. Edit 2: Looks like `womtool/src/test/resources/validate/biscayne/valid/*` must have a workflow to validate - fixed and running tests again. ```; [info] - should be able to output a graph for biscayne workflow: 'sep_function' *** FAILED *** (59 milliseconds); [info] In WDL not in Graph: Set(); In Graph not in WDL: Set(SepTestInInterpolatorBlock) Set() was not equal to Set(""SepTestInInterpolatorBlock"") (WomtoolValidateSpec.scala:84); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at womtool.WomtoolValidateSpec.$anonfun$new$14(WomtoolValidateSpec.scala:84); [info] at org.scalatest.Assertions.withClue(Assertions.scala:1221); [info] at org.scalatest.Assertions.withClue$(Assertions.scala:1208); [info] at org.scalatest.FlatSpec.withClue(FlatSpec.scala:1685); [info] at womtool.WomtoolValidateSpec.$anonfun$new$12(WomtoolValidateSpec.scala:84); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553:293,Security,validat,validate,293,"Thanks @aednichols, looks like I didn't scroll down enough - I think @cjllanwarne added this test at the very start so wasn't on my horizon to check. I've pushed a fix, and will confirm when it's fixed. Edit: Initial logs look like it's passing. Edit 2: Looks like `womtool/src/test/resources/validate/biscayne/valid/*` must have a workflow to validate - fixed and running tests again. ```; [info] - should be able to output a graph for biscayne workflow: 'sep_function' *** FAILED *** (59 milliseconds); [info] In WDL not in Graph: Set(); In Graph not in WDL: Set(SepTestInInterpolatorBlock) Set() was not equal to Set(""SepTestInInterpolatorBlock"") (WomtoolValidateSpec.scala:84); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at womtool.WomtoolValidateSpec.$anonfun$new$14(WomtoolValidateSpec.scala:84); [info] at org.scalatest.Assertions.withClue(Assertions.scala:1221); [info] at org.scalatest.Assertions.withClue$(Assertions.scala:1208); [info] at org.scalatest.FlatSpec.withClue(FlatSpec.scala:1685); [info] at womtool.WomtoolValidateSpec.$anonfun$new$12(WomtoolValidateSpec.scala:84); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553:344,Security,validat,validate,344,"Thanks @aednichols, looks like I didn't scroll down enough - I think @cjllanwarne added this test at the very start so wasn't on my horizon to check. I've pushed a fix, and will confirm when it's fixed. Edit: Initial logs look like it's passing. Edit 2: Looks like `womtool/src/test/resources/validate/biscayne/valid/*` must have a workflow to validate - fixed and running tests again. ```; [info] - should be able to output a graph for biscayne workflow: 'sep_function' *** FAILED *** (59 milliseconds); [info] In WDL not in Graph: Set(); In Graph not in WDL: Set(SepTestInInterpolatorBlock) Set() was not equal to Set(""SepTestInInterpolatorBlock"") (WomtoolValidateSpec.scala:84); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at womtool.WomtoolValidateSpec.$anonfun$new$14(WomtoolValidateSpec.scala:84); [info] at org.scalatest.Assertions.withClue(Assertions.scala:1221); [info] at org.scalatest.Assertions.withClue$(Assertions.scala:1208); [info] at org.scalatest.FlatSpec.withClue(FlatSpec.scala:1685); [info] at womtool.WomtoolValidateSpec.$anonfun$new$12(WomtoolValidateSpec.scala:84); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553:93,Testability,test,test,93,"Thanks @aednichols, looks like I didn't scroll down enough - I think @cjllanwarne added this test at the very start so wasn't on my horizon to check. I've pushed a fix, and will confirm when it's fixed. Edit: Initial logs look like it's passing. Edit 2: Looks like `womtool/src/test/resources/validate/biscayne/valid/*` must have a workflow to validate - fixed and running tests again. ```; [info] - should be able to output a graph for biscayne workflow: 'sep_function' *** FAILED *** (59 milliseconds); [info] In WDL not in Graph: Set(); In Graph not in WDL: Set(SepTestInInterpolatorBlock) Set() was not equal to Set(""SepTestInInterpolatorBlock"") (WomtoolValidateSpec.scala:84); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at womtool.WomtoolValidateSpec.$anonfun$new$14(WomtoolValidateSpec.scala:84); [info] at org.scalatest.Assertions.withClue(Assertions.scala:1221); [info] at org.scalatest.Assertions.withClue$(Assertions.scala:1208); [info] at org.scalatest.FlatSpec.withClue(FlatSpec.scala:1685); [info] at womtool.WomtoolValidateSpec.$anonfun$new$12(WomtoolValidateSpec.scala:84); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553:217,Testability,log,logs,217,"Thanks @aednichols, looks like I didn't scroll down enough - I think @cjllanwarne added this test at the very start so wasn't on my horizon to check. I've pushed a fix, and will confirm when it's fixed. Edit: Initial logs look like it's passing. Edit 2: Looks like `womtool/src/test/resources/validate/biscayne/valid/*` must have a workflow to validate - fixed and running tests again. ```; [info] - should be able to output a graph for biscayne workflow: 'sep_function' *** FAILED *** (59 milliseconds); [info] In WDL not in Graph: Set(); In Graph not in WDL: Set(SepTestInInterpolatorBlock) Set() was not equal to Set(""SepTestInInterpolatorBlock"") (WomtoolValidateSpec.scala:84); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at womtool.WomtoolValidateSpec.$anonfun$new$14(WomtoolValidateSpec.scala:84); [info] at org.scalatest.Assertions.withClue(Assertions.scala:1221); [info] at org.scalatest.Assertions.withClue$(Assertions.scala:1208); [info] at org.scalatest.FlatSpec.withClue(FlatSpec.scala:1685); [info] at womtool.WomtoolValidateSpec.$anonfun$new$12(WomtoolValidateSpec.scala:84); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553:278,Testability,test,test,278,"Thanks @aednichols, looks like I didn't scroll down enough - I think @cjllanwarne added this test at the very start so wasn't on my horizon to check. I've pushed a fix, and will confirm when it's fixed. Edit: Initial logs look like it's passing. Edit 2: Looks like `womtool/src/test/resources/validate/biscayne/valid/*` must have a workflow to validate - fixed and running tests again. ```; [info] - should be able to output a graph for biscayne workflow: 'sep_function' *** FAILED *** (59 milliseconds); [info] In WDL not in Graph: Set(); In Graph not in WDL: Set(SepTestInInterpolatorBlock) Set() was not equal to Set(""SepTestInInterpolatorBlock"") (WomtoolValidateSpec.scala:84); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at womtool.WomtoolValidateSpec.$anonfun$new$14(WomtoolValidateSpec.scala:84); [info] at org.scalatest.Assertions.withClue(Assertions.scala:1221); [info] at org.scalatest.Assertions.withClue$(Assertions.scala:1208); [info] at org.scalatest.FlatSpec.withClue(FlatSpec.scala:1685); [info] at womtool.WomtoolValidateSpec.$anonfun$new$12(WomtoolValidateSpec.scala:84); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553:373,Testability,test,tests,373,"Thanks @aednichols, looks like I didn't scroll down enough - I think @cjllanwarne added this test at the very start so wasn't on my horizon to check. I've pushed a fix, and will confirm when it's fixed. Edit: Initial logs look like it's passing. Edit 2: Looks like `womtool/src/test/resources/validate/biscayne/valid/*` must have a workflow to validate - fixed and running tests again. ```; [info] - should be able to output a graph for biscayne workflow: 'sep_function' *** FAILED *** (59 milliseconds); [info] In WDL not in Graph: Set(); In Graph not in WDL: Set(SepTestInInterpolatorBlock) Set() was not equal to Set(""SepTestInInterpolatorBlock"") (WomtoolValidateSpec.scala:84); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at womtool.WomtoolValidateSpec.$anonfun$new$14(WomtoolValidateSpec.scala:84); [info] at org.scalatest.Assertions.withClue(Assertions.scala:1221); [info] at org.scalatest.Assertions.withClue$(Assertions.scala:1208); [info] at org.scalatest.FlatSpec.withClue(FlatSpec.scala:1685); [info] at womtool.WomtoolValidateSpec.$anonfun$new$12(WomtoolValidateSpec.scala:84); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553:714,Testability,Test,TestFailedException,714,"Thanks @aednichols, looks like I didn't scroll down enough - I think @cjllanwarne added this test at the very start so wasn't on my horizon to check. I've pushed a fix, and will confirm when it's fixed. Edit: Initial logs look like it's passing. Edit 2: Looks like `womtool/src/test/resources/validate/biscayne/valid/*` must have a workflow to validate - fixed and running tests again. ```; [info] - should be able to output a graph for biscayne workflow: 'sep_function' *** FAILED *** (59 milliseconds); [info] In WDL not in Graph: Set(); In Graph not in WDL: Set(SepTestInInterpolatorBlock) Set() was not equal to Set(""SepTestInInterpolatorBlock"") (WomtoolValidateSpec.scala:84); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at womtool.WomtoolValidateSpec.$anonfun$new$14(WomtoolValidateSpec.scala:84); [info] at org.scalatest.Assertions.withClue(Assertions.scala:1221); [info] at org.scalatest.Assertions.withClue$(Assertions.scala:1208); [info] at org.scalatest.FlatSpec.withClue(FlatSpec.scala:1685); [info] at womtool.WomtoolValidateSpec.$anonfun$new$12(WomtoolValidateSpec.scala:84); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553:857,Testability,Assert,Assertions,857,"Thanks @aednichols, looks like I didn't scroll down enough - I think @cjllanwarne added this test at the very start so wasn't on my horizon to check. I've pushed a fix, and will confirm when it's fixed. Edit: Initial logs look like it's passing. Edit 2: Looks like `womtool/src/test/resources/validate/biscayne/valid/*` must have a workflow to validate - fixed and running tests again. ```; [info] - should be able to output a graph for biscayne workflow: 'sep_function' *** FAILED *** (59 milliseconds); [info] In WDL not in Graph: Set(); In Graph not in WDL: Set(SepTestInInterpolatorBlock) Set() was not equal to Set(""SepTestInInterpolatorBlock"") (WomtoolValidateSpec.scala:84); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at womtool.WomtoolValidateSpec.$anonfun$new$14(WomtoolValidateSpec.scala:84); [info] at org.scalatest.Assertions.withClue(Assertions.scala:1221); [info] at org.scalatest.Assertions.withClue$(Assertions.scala:1208); [info] at org.scalatest.FlatSpec.withClue(FlatSpec.scala:1685); [info] at womtool.WomtoolValidateSpec.$anonfun$new$12(WomtoolValidateSpec.scala:84); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553:877,Testability,Assert,Assertions,877,"Thanks @aednichols, looks like I didn't scroll down enough - I think @cjllanwarne added this test at the very start so wasn't on my horizon to check. I've pushed a fix, and will confirm when it's fixed. Edit: Initial logs look like it's passing. Edit 2: Looks like `womtool/src/test/resources/validate/biscayne/valid/*` must have a workflow to validate - fixed and running tests again. ```; [info] - should be able to output a graph for biscayne workflow: 'sep_function' *** FAILED *** (59 milliseconds); [info] In WDL not in Graph: Set(); In Graph not in WDL: Set(SepTestInInterpolatorBlock) Set() was not equal to Set(""SepTestInInterpolatorBlock"") (WomtoolValidateSpec.scala:84); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at womtool.WomtoolValidateSpec.$anonfun$new$14(WomtoolValidateSpec.scala:84); [info] at org.scalatest.Assertions.withClue(Assertions.scala:1221); [info] at org.scalatest.Assertions.withClue$(Assertions.scala:1208); [info] at org.scalatest.FlatSpec.withClue(FlatSpec.scala:1685); [info] at womtool.WomtoolValidateSpec.$anonfun$new$12(WomtoolValidateSpec.scala:84); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553:925,Testability,Assert,Assertions,925,"Thanks @aednichols, looks like I didn't scroll down enough - I think @cjllanwarne added this test at the very start so wasn't on my horizon to check. I've pushed a fix, and will confirm when it's fixed. Edit: Initial logs look like it's passing. Edit 2: Looks like `womtool/src/test/resources/validate/biscayne/valid/*` must have a workflow to validate - fixed and running tests again. ```; [info] - should be able to output a graph for biscayne workflow: 'sep_function' *** FAILED *** (59 milliseconds); [info] In WDL not in Graph: Set(); In Graph not in WDL: Set(SepTestInInterpolatorBlock) Set() was not equal to Set(""SepTestInInterpolatorBlock"") (WomtoolValidateSpec.scala:84); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at womtool.WomtoolValidateSpec.$anonfun$new$14(WomtoolValidateSpec.scala:84); [info] at org.scalatest.Assertions.withClue(Assertions.scala:1221); [info] at org.scalatest.Assertions.withClue$(Assertions.scala:1208); [info] at org.scalatest.FlatSpec.withClue(FlatSpec.scala:1685); [info] at womtool.WomtoolValidateSpec.$anonfun$new$12(WomtoolValidateSpec.scala:84); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553:946,Testability,Assert,Assertions,946,"Thanks @aednichols, looks like I didn't scroll down enough - I think @cjllanwarne added this test at the very start so wasn't on my horizon to check. I've pushed a fix, and will confirm when it's fixed. Edit: Initial logs look like it's passing. Edit 2: Looks like `womtool/src/test/resources/validate/biscayne/valid/*` must have a workflow to validate - fixed and running tests again. ```; [info] - should be able to output a graph for biscayne workflow: 'sep_function' *** FAILED *** (59 milliseconds); [info] In WDL not in Graph: Set(); In Graph not in WDL: Set(SepTestInInterpolatorBlock) Set() was not equal to Set(""SepTestInInterpolatorBlock"") (WomtoolValidateSpec.scala:84); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at womtool.WomtoolValidateSpec.$anonfun$new$14(WomtoolValidateSpec.scala:84); [info] at org.scalatest.Assertions.withClue(Assertions.scala:1221); [info] at org.scalatest.Assertions.withClue$(Assertions.scala:1208); [info] at org.scalatest.FlatSpec.withClue(FlatSpec.scala:1685); [info] at womtool.WomtoolValidateSpec.$anonfun$new$12(WomtoolValidateSpec.scala:84); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553
https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656470426:20,Energy Efficiency,green,green,20,"Thanks @aednichols, green tick against SBT now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656470426
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492:397,Availability,redundant,redundant,397,"@cjllanwarne @aednichols Sorry for my late reaction, I was on holiday last week. I have removed the forInput variable entirely thanks to @cjllanwarne's feedback. Instead I created the `makeInputSpecificFunctions` in the `IoFunctionSet` trait so every backend can use it. I then overrided it in the sfsBackend to return another class with a different postmapping. This made the `forInput` variable redundant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492:109,Modifiability,variab,variable,109,"@cjllanwarne @aednichols Sorry for my late reaction, I was on holiday last week. I have removed the forInput variable entirely thanks to @cjllanwarne's feedback. Instead I created the `makeInputSpecificFunctions` in the `IoFunctionSet` trait so every backend can use it. I then overrided it in the sfsBackend to return another class with a different postmapping. This made the `forInput` variable redundant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492:388,Modifiability,variab,variable,388,"@cjllanwarne @aednichols Sorry for my late reaction, I was on holiday last week. I have removed the forInput variable entirely thanks to @cjllanwarne's feedback. Instead I created the `makeInputSpecificFunctions` in the `IoFunctionSet` trait so every backend can use it. I then overrided it in the sfsBackend to return another class with a different postmapping. This made the `forInput` variable redundant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492:397,Safety,redund,redundant,397,"@cjllanwarne @aednichols Sorry for my late reaction, I was on holiday last week. I have removed the forInput variable entirely thanks to @cjllanwarne's feedback. Instead I created the `makeInputSpecificFunctions` in the `IoFunctionSet` trait so every backend can use it. I then overrided it in the sfsBackend to return another class with a different postmapping. This made the `forInput` variable redundant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492:152,Usability,feedback,feedback,152,"@cjllanwarne @aednichols Sorry for my late reaction, I was on holiday last week. I have removed the forInput variable entirely thanks to @cjllanwarne's feedback. Instead I created the `makeInputSpecificFunctions` in the `IoFunctionSet` trait so every backend can use it. I then overrided it in the sfsBackend to return another class with a different postmapping. This made the `forInput` variable redundant.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-623374492
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015:31,Availability,error,errors,31,"@cjllanwarne I have seen these errors before. It happens when you use a java version that is higher than 8. (My OS has 11 installed by default, and I use a conda environment to use OpenJDK 8 on intellij). So it may be an update of travis CI's default image. . EDIT: Hmm I checked the `.travis.yml` and the openjdk8 is explicitly specified. Really weird that a higher version of java is used. EDIT2: And the compilation works again. Sometimes it is best to let a restart do the work :wink:. The errors that occur now is because quay.io is down, and related tests fail. https://status.quay.io/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015:494,Availability,error,errors,494,"@cjllanwarne I have seen these errors before. It happens when you use a java version that is higher than 8. (My OS has 11 installed by default, and I use a conda environment to use OpenJDK 8 on intellij). So it may be an update of travis CI's default image. . EDIT: Hmm I checked the `.travis.yml` and the openjdk8 is explicitly specified. Really weird that a higher version of java is used. EDIT2: And the compilation works again. Sometimes it is best to let a restart do the work :wink:. The errors that occur now is because quay.io is down, and related tests fail. https://status.quay.io/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015:538,Availability,down,down,538,"@cjllanwarne I have seen these errors before. It happens when you use a java version that is higher than 8. (My OS has 11 installed by default, and I use a conda environment to use OpenJDK 8 on intellij). So it may be an update of travis CI's default image. . EDIT: Hmm I checked the `.travis.yml` and the openjdk8 is explicitly specified. Really weird that a higher version of java is used. EDIT2: And the compilation works again. Sometimes it is best to let a restart do the work :wink:. The errors that occur now is because quay.io is down, and related tests fail. https://status.quay.io/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015:122,Deployability,install,installed,122,"@cjllanwarne I have seen these errors before. It happens when you use a java version that is higher than 8. (My OS has 11 installed by default, and I use a conda environment to use OpenJDK 8 on intellij). So it may be an update of travis CI's default image. . EDIT: Hmm I checked the `.travis.yml` and the openjdk8 is explicitly specified. Really weird that a higher version of java is used. EDIT2: And the compilation works again. Sometimes it is best to let a restart do the work :wink:. The errors that occur now is because quay.io is down, and related tests fail. https://status.quay.io/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015:221,Deployability,update,update,221,"@cjllanwarne I have seen these errors before. It happens when you use a java version that is higher than 8. (My OS has 11 installed by default, and I use a conda environment to use OpenJDK 8 on intellij). So it may be an update of travis CI's default image. . EDIT: Hmm I checked the `.travis.yml` and the openjdk8 is explicitly specified. Really weird that a higher version of java is used. EDIT2: And the compilation works again. Sometimes it is best to let a restart do the work :wink:. The errors that occur now is because quay.io is down, and related tests fail. https://status.quay.io/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015:556,Testability,test,tests,556,"@cjllanwarne I have seen these errors before. It happens when you use a java version that is higher than 8. (My OS has 11 installed by default, and I use a conda environment to use OpenJDK 8 on intellij). So it may be an update of travis CI's default image. . EDIT: Hmm I checked the `.travis.yml` and the openjdk8 is explicitly specified. Really weird that a higher version of java is used. EDIT2: And the compilation works again. Sometimes it is best to let a restart do the work :wink:. The errors that occur now is because quay.io is down, and related tests fail. https://status.quay.io/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-630613015
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-632128115:225,Availability,outage,outage,225,"Yeah, travis had to roll back a commit which was breaking different java versions (I found bug reports from people who were having similar issues with trying to use Java 14). A couple of your tests also bumped into a quay.io outage so I've restarted those and 🤞 we don't have any more transient failures on those!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-632128115
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-632128115:295,Availability,failure,failures,295,"Yeah, travis had to roll back a commit which was breaking different java versions (I found bug reports from people who were having similar issues with trying to use Java 14). A couple of your tests also bumped into a quay.io outage so I've restarted those and 🤞 we don't have any more transient failures on those!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-632128115
https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-632128115:192,Testability,test,tests,192,"Yeah, travis had to roll back a commit which was breaking different java versions (I found bug reports from people who were having similar issues with trying to use Java 14). A couple of your tests also bumped into a quay.io outage so I've restarted those and 🤞 we don't have any more transient failures on those!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5495#issuecomment-632128115
https://github.com/broadinstitute/cromwell/pull/5500#issuecomment-627550121:169,Deployability,update,updated,169,"TOL: You might want to add the `src/ci/bin/testMetadataComparisonPython.sh` file to the ""scripts only"" filter so that it only needs to run scripts tests if that file is updated",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5500#issuecomment-627550121
https://github.com/broadinstitute/cromwell/pull/5500#issuecomment-627550121:43,Testability,test,testMetadataComparisonPython,43,"TOL: You might want to add the `src/ci/bin/testMetadataComparisonPython.sh` file to the ""scripts only"" filter so that it only needs to run scripts tests if that file is updated",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5500#issuecomment-627550121
https://github.com/broadinstitute/cromwell/pull/5500#issuecomment-627550121:147,Testability,test,tests,147,"TOL: You might want to add the `src/ci/bin/testMetadataComparisonPython.sh` file to the ""scripts only"" filter so that it only needs to run scripts tests if that file is updated",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5500#issuecomment-627550121
https://github.com/broadinstitute/cromwell/pull/5500#issuecomment-627551228:71,Testability,test,test,71,cf: https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/test.inc.sh#L145,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5500#issuecomment-627551228
https://github.com/broadinstitute/cromwell/issues/5503#issuecomment-622644685:95,Performance,concurren,concurrent-workflows-and-max-concurrent-jobs,95,found the info here:; https://gatkforums.broadinstitute.org/wdl/discussion/9998/using-both-max-concurrent-workflows-and-max-concurrent-jobs. phew...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5503#issuecomment-622644685
https://github.com/broadinstitute/cromwell/pull/5509#issuecomment-624744933:36,Performance,queue,queues,36,"TOL:; * Is it worth having separate queues for summarizable vs non-summarizable metadata, and separate writers drawing from those two separate queues at separate rates?; * Is it worth assigning ""summarizable"" vs ""non-summarizable"" at metadata generation time rather than working it out again just before writing based on string matching (eg `SummarizableMetadataEvent` and `NonSummarizableMetadataEvent` subtypes of a `MetadataEvent` trait)?; * It'd be nice to be able to separate metadata write metrics between summarizable and non-summarizable metadata processing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5509#issuecomment-624744933
https://github.com/broadinstitute/cromwell/pull/5509#issuecomment-624744933:143,Performance,queue,queues,143,"TOL:; * Is it worth having separate queues for summarizable vs non-summarizable metadata, and separate writers drawing from those two separate queues at separate rates?; * Is it worth assigning ""summarizable"" vs ""non-summarizable"" at metadata generation time rather than working it out again just before writing based on string matching (eg `SummarizableMetadataEvent` and `NonSummarizableMetadataEvent` subtypes of a `MetadataEvent` trait)?; * It'd be nice to be able to separate metadata write metrics between summarizable and non-summarizable metadata processing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5509#issuecomment-624744933
https://github.com/broadinstitute/cromwell/pull/5509#issuecomment-624746718:43,Deployability,hotfix,hotfix,43,"also, once it's looking good, we'll want a hotfix edition of this changeset",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5509#issuecomment-624746718
https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371:66,Deployability,hotfix,hotfix,66,"@cjllanwarne I certainly don't think this needs a unit test for a hotfix. And as there was no unit test added for the introduction of the queue, and many unit and virtually all integration tests exercise this indirectly I'm not sure I see the necessity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371
https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371:177,Deployability,integrat,integration,177,"@cjllanwarne I certainly don't think this needs a unit test for a hotfix. And as there was no unit test added for the introduction of the queue, and many unit and virtually all integration tests exercise this indirectly I'm not sure I see the necessity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371
https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371:177,Integrability,integrat,integration,177,"@cjllanwarne I certainly don't think this needs a unit test for a hotfix. And as there was no unit test added for the introduction of the queue, and many unit and virtually all integration tests exercise this indirectly I'm not sure I see the necessity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371
https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371:138,Performance,queue,queue,138,"@cjllanwarne I certainly don't think this needs a unit test for a hotfix. And as there was no unit test added for the introduction of the queue, and many unit and virtually all integration tests exercise this indirectly I'm not sure I see the necessity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371
https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371:55,Testability,test,test,55,"@cjllanwarne I certainly don't think this needs a unit test for a hotfix. And as there was no unit test added for the introduction of the queue, and many unit and virtually all integration tests exercise this indirectly I'm not sure I see the necessity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371
https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371:99,Testability,test,test,99,"@cjllanwarne I certainly don't think this needs a unit test for a hotfix. And as there was no unit test added for the introduction of the queue, and many unit and virtually all integration tests exercise this indirectly I'm not sure I see the necessity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371
https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371:189,Testability,test,tests,189,"@cjllanwarne I certainly don't think this needs a unit test for a hotfix. And as there was no unit test added for the introduction of the queue, and many unit and virtually all integration tests exercise this indirectly I'm not sure I see the necessity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371
https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-632668244:404,Energy Efficiency,monitor,monitor,404,"If my question isnt clear enough, let me expand some. I need to set AWS_BATCH_JOB_ATTEMPTS because AWS will terminate my jobs in the middle of them if my spot instance request is outbid (usually the only higher bids are on demand prices). AWS_BATCH_JOB_ATTEMPTS will allow me to tell aws that when I job is stopped for that reason, it will restart the job automatically without me needing to continually monitor it, Is there any way to do this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-632668244
https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-632668244:20,Usability,clear,clear,20,"If my question isnt clear enough, let me expand some. I need to set AWS_BATCH_JOB_ATTEMPTS because AWS will terminate my jobs in the middle of them if my spot instance request is outbid (usually the only higher bids are on demand prices). AWS_BATCH_JOB_ATTEMPTS will allow me to tell aws that when I job is stopped for that reason, it will restart the job automatically without me needing to continually monitor it, Is there any way to do this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-632668244
https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-675797428:474,Deployability,configurat,configuration,474,"Hello, the cromwell team use a [JIRA](https://broadworkbench.atlassian.net/projects/BA/issues) now to triage issues - I'm as much of a fan of it as you are. 👎. My suggestion would be to use the the workflow options json to configure your workflow at runtime.; See here: https://cromwell.readthedocs.io/en/stable/wf_options/Overview/. This would extend your curl command with; `-F ""workflowOptions=@options.json""`. This may be a parameter however that needs to be set in the configuration file that is read by the server when it is launched. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-675797428
https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-675797428:223,Modifiability,config,configure,223,"Hello, the cromwell team use a [JIRA](https://broadworkbench.atlassian.net/projects/BA/issues) now to triage issues - I'm as much of a fan of it as you are. 👎. My suggestion would be to use the the workflow options json to configure your workflow at runtime.; See here: https://cromwell.readthedocs.io/en/stable/wf_options/Overview/. This would extend your curl command with; `-F ""workflowOptions=@options.json""`. This may be a parameter however that needs to be set in the configuration file that is read by the server when it is launched. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-675797428
https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-675797428:345,Modifiability,extend,extend,345,"Hello, the cromwell team use a [JIRA](https://broadworkbench.atlassian.net/projects/BA/issues) now to triage issues - I'm as much of a fan of it as you are. 👎. My suggestion would be to use the the workflow options json to configure your workflow at runtime.; See here: https://cromwell.readthedocs.io/en/stable/wf_options/Overview/. This would extend your curl command with; `-F ""workflowOptions=@options.json""`. This may be a parameter however that needs to be set in the configuration file that is read by the server when it is launched. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-675797428
https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-675797428:474,Modifiability,config,configuration,474,"Hello, the cromwell team use a [JIRA](https://broadworkbench.atlassian.net/projects/BA/issues) now to triage issues - I'm as much of a fan of it as you are. 👎. My suggestion would be to use the the workflow options json to configure your workflow at runtime.; See here: https://cromwell.readthedocs.io/en/stable/wf_options/Overview/. This would extend your curl command with; `-F ""workflowOptions=@options.json""`. This may be a parameter however that needs to be set in the configuration file that is read by the server when it is launched. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-675797428
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628065940:44,Testability,test,tests,44,"@rhpvorderman we can't merge with cancelled tests, but if you only change documentation, our CI should be able to spot that and only run a minimal set (see https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/test.inc.sh#L144... we might need to add `CHANGELOG.MD` to that pattern match)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628065940
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628065940:223,Testability,test,test,223,"@rhpvorderman we can't merge with cancelled tests, but if you only change documentation, our CI should be able to spot that and only run a minimal set (see https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/test.inc.sh#L144... we might need to add `CHANGELOG.MD` to that pattern match)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628065940
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628453342:423,Performance,load,load,423,"@vsoch and @illusional I have processed your comments. I made it more clear why the `--containall` flag is so important and I dropped the list of stuff that Singularity does without the flag. Instead I focused on the way Singularity affects reproducibility and how this can be prevented by the containall flag. I have also removed any references to a particular version of Singularity, and provided alternatives for module load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628453342
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628453342:70,Usability,clear,clear,70,"@vsoch and @illusional I have processed your comments. I made it more clear why the `--containall` flag is so important and I dropped the list of stuff that Singularity does without the flag. Instead I focused on the way Singularity affects reproducibility and how this can be prevented by the containall flag. I have also removed any references to a particular version of Singularity, and provided alternatives for module load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628453342
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628462648:35,Testability,test,tests,35,@cjllanwarne It still runs all the tests! . ... And then I realized that altering `src/ci/bin/test.sh` is not included in the regex... Oh well I solved this problem for the next documentation PR then :wink:.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628462648
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628462648:94,Testability,test,test,94,@cjllanwarne It still runs all the tests! . ... And then I realized that altering `src/ci/bin/test.sh` is not included in the regex... Oh well I solved this problem for the next documentation PR then :wink:.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-628462648
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630620299:231,Deployability,configurat,configurations,231,"Hey @rhpvorderman, your changes look fine to me. One thing I noticed is that we've used `${script}` when I believe it should actually be `${docker_script}` to get the container relevant path for the script. Can you confirm in your configurations?. Otherwise I'm happy to click approve from my side (if I'm allowed to do that?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630620299
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630620299:231,Modifiability,config,configurations,231,"Hey @rhpvorderman, your changes look fine to me. One thing I noticed is that we've used `${script}` when I believe it should actually be `${docker_script}` to get the container relevant path for the script. Can you confirm in your configurations?. Otherwise I'm happy to click approve from my side (if I'm allowed to do that?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630620299
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630625499:58,Security,access,access,58,"@illusional Thanks! Well, I do not know if you have write access. But I am sure the Cromwell team is happy to know that this PR is approved by other people in the HPC space. As regards to `docker_script`. It seems that `script` is what gets generated from the WDL task, and it works flawlessly. I don't know what `docker_script` does. It seems to be mentioned only in some corners of the code base and it is not mentioned in the container documentation. My guess would be that this is a now obsolete remnant of some design choices that have been made in the past. `script` works and it is used in all the examples on the container page, so let's be consistent and use that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630625499
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630626951:13,Security,access,access,13,"I have write access so technically I can approve, but I’ll leave it to you and @cjllanwarne to decide if my review is enough to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630626951
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630780758:12,Modifiability,config,config,12,@vsoch This config already does this by using exec. This way the binary image is created in the singularity cache dir.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630780758
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630780758:108,Performance,cache,cache,108,@vsoch This config already does this by using exec. This way the binary image is created in the singularity cache dir.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630780758
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541:147,Availability,echo,echo,147,"For further clarification. There are two `exec` statements in the config. The `exec` statement **before** the submit command will execute a simple echo command. If singularity notices the image is not there it will pull it to `SINGULARITY_CACHEDIR`. ; Then the job is submitted and the `exec` statement **in** the submit command is executed. If `SINGULARITY_CACHEDIR` is on a shared filesystem the image will already be present. The image will *not* be pulled by the execution node and the job executes right away. Using `exec` instead of `pull` means that Singularity will decide where the image goes, and not the user. This is quite useful as Singularity has all the functionality to make a functional image cache already built-in. This way we don't have to hack it together in bash, which is always the less preferable option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541:66,Modifiability,config,config,66,"For further clarification. There are two `exec` statements in the config. The `exec` statement **before** the submit command will execute a simple echo command. If singularity notices the image is not there it will pull it to `SINGULARITY_CACHEDIR`. ; Then the job is submitted and the `exec` statement **in** the submit command is executed. If `SINGULARITY_CACHEDIR` is on a shared filesystem the image will already be present. The image will *not* be pulled by the execution node and the job executes right away. Using `exec` instead of `pull` means that Singularity will decide where the image goes, and not the user. This is quite useful as Singularity has all the functionality to make a functional image cache already built-in. This way we don't have to hack it together in bash, which is always the less preferable option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541:710,Performance,cache,cache,710,"For further clarification. There are two `exec` statements in the config. The `exec` statement **before** the submit command will execute a simple echo command. If singularity notices the image is not there it will pull it to `SINGULARITY_CACHEDIR`. ; Then the job is submitted and the `exec` statement **in** the submit command is executed. If `SINGULARITY_CACHEDIR` is on a shared filesystem the image will already be present. The image will *not* be pulled by the execution node and the job executes right away. Using `exec` instead of `pull` means that Singularity will decide where the image goes, and not the user. This is quite useful as Singularity has all the functionality to make a functional image cache already built-in. This way we don't have to hack it together in bash, which is always the less preferable option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541:140,Usability,simpl,simple,140,"For further clarification. There are two `exec` statements in the config. The `exec` statement **before** the submit command will execute a simple echo command. If singularity notices the image is not there it will pull it to `SINGULARITY_CACHEDIR`. ; Then the job is submitted and the `exec` statement **in** the submit command is executed. If `SINGULARITY_CACHEDIR` is on a shared filesystem the image will already be present. The image will *not* be pulled by the execution node and the job executes right away. Using `exec` instead of `pull` means that Singularity will decide where the image goes, and not the user. This is quite useful as Singularity has all the functionality to make a functional image cache already built-in. This way we don't have to hack it together in bash, which is always the less preferable option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-630879541
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631177760:48,Availability,outage,outage,48,"A follow-up question (prompted by the `quay.io` outage today), does `singularity exec` check the internet to see if it has the most up to date version of the tag? What happens if I gave it a docker with digest instead, would it still poll the internet?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631177760
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995:55,Performance,cache,cache,55,"I think it has to, in order to see if the image in the cache hash matches the current pull (or other) request, here is an example: https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/library/pull.go#L55. I’m not super familiar with the code, but looks like the hash is retrieved here https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/oci/pull.go#L36 and then needs to get a manifest https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/build/oci/oci.go#L133 which I’d suspect does just that. https://github.com/containers/image/blob/175bf8b8f9ad897bdc10761e11b466d00f516a63/types/types.go#L238. If a user doesn’t have internet access, or has limited, or there is need to query the registry, might run into trouble. I just tried running an exec to a Docker uri, first of course with Internet to make sure that the images in my cache, and then I disabled my wireless. Without wireless, of course, I couldn’t run anything. This issue could be avoided if the user pulled an image first and then use that image instead of this Docker uri.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995:973,Performance,cache,cache,973,"I think it has to, in order to see if the image in the cache hash matches the current pull (or other) request, here is an example: https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/library/pull.go#L55. I’m not super familiar with the code, but looks like the hash is retrieved here https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/oci/pull.go#L36 and then needs to get a manifest https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/build/oci/oci.go#L133 which I’d suspect does just that. https://github.com/containers/image/blob/175bf8b8f9ad897bdc10761e11b466d00f516a63/types/types.go#L238. If a user doesn’t have internet access, or has limited, or there is need to query the registry, might run into trouble. I just tried running an exec to a Docker uri, first of course with Internet to make sure that the images in my cache, and then I disabled my wireless. Without wireless, of course, I couldn’t run anything. This issue could be avoided if the user pulled an image first and then use that image instead of this Docker uri.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995:1087,Safety,avoid,avoided,1087,"I think it has to, in order to see if the image in the cache hash matches the current pull (or other) request, here is an example: https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/library/pull.go#L55. I’m not super familiar with the code, but looks like the hash is retrieved here https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/oci/pull.go#L36 and then needs to get a manifest https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/build/oci/oci.go#L133 which I’d suspect does just that. https://github.com/containers/image/blob/175bf8b8f9ad897bdc10761e11b466d00f516a63/types/types.go#L238. If a user doesn’t have internet access, or has limited, or there is need to query the registry, might run into trouble. I just tried running an exec to a Docker uri, first of course with Internet to make sure that the images in my cache, and then I disabled my wireless. Without wireless, of course, I couldn’t run anything. This issue could be avoided if the user pulled an image first and then use that image instead of this Docker uri.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995:61,Security,hash,hash,61,"I think it has to, in order to see if the image in the cache hash matches the current pull (or other) request, here is an example: https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/library/pull.go#L55. I’m not super familiar with the code, but looks like the hash is retrieved here https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/oci/pull.go#L36 and then needs to get a manifest https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/build/oci/oci.go#L133 which I’d suspect does just that. https://github.com/containers/image/blob/175bf8b8f9ad897bdc10761e11b466d00f516a63/types/types.go#L238. If a user doesn’t have internet access, or has limited, or there is need to query the registry, might run into trouble. I just tried running an exec to a Docker uri, first of course with Internet to make sure that the images in my cache, and then I disabled my wireless. Without wireless, of course, I couldn’t run anything. This issue could be avoided if the user pulled an image first and then use that image instead of this Docker uri.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995:312,Security,hash,hash,312,"I think it has to, in order to see if the image in the cache hash matches the current pull (or other) request, here is an example: https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/library/pull.go#L55. I’m not super familiar with the code, but looks like the hash is retrieved here https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/oci/pull.go#L36 and then needs to get a manifest https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/build/oci/oci.go#L133 which I’d suspect does just that. https://github.com/containers/image/blob/175bf8b8f9ad897bdc10761e11b466d00f516a63/types/types.go#L238. If a user doesn’t have internet access, or has limited, or there is need to query the registry, might run into trouble. I just tried running an exec to a Docker uri, first of course with Internet to make sure that the images in my cache, and then I disabled my wireless. Without wireless, of course, I couldn’t run anything. This issue could be avoided if the user pulled an image first and then use that image instead of this Docker uri.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995:774,Security,access,access,774,"I think it has to, in order to see if the image in the cache hash matches the current pull (or other) request, here is an example: https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/library/pull.go#L55. I’m not super familiar with the code, but looks like the hash is retrieved here https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/client/oci/pull.go#L36 and then needs to get a manifest https://github.com/hpcng/singularity/blob/2c6cf59870cf0172d61099a3198def8334c94827/internal/pkg/build/oci/oci.go#L133 which I’d suspect does just that. https://github.com/containers/image/blob/175bf8b8f9ad897bdc10761e11b466d00f516a63/types/types.go#L238. If a user doesn’t have internet access, or has limited, or there is need to query the registry, might run into trouble. I just tried running an exec to a Docker uri, first of course with Internet to make sure that the images in my cache, and then I disabled my wireless. Without wireless, of course, I couldn’t run anything. This issue could be avoided if the user pulled an image first and then use that image instead of this Docker uri.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631184995
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046:553,Availability,error,error,553,"Here is the terminal output, for posterity:. The first command with internet, confirms we are using a cached image:; ```bash; $ singularity exec docker://busybox ls; INFO: Using cached SIF image; CHANGELOG.md LICENSE README.md dist paper qme.egg-info setup.py; Dockerfile MANIFEST.in build docs qme setup.cfg tests; ```; then I took off my wireless :scream: and ran the same - we know the image is in the cache:. ```bash; $ singularity exec docker://busybox ls; FATAL: Unable to handle docker://busybox uri: failed to get checksum for docker://busybox: error pinging docker registry registry-1.docker.io: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io: no such host; ```; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046:559,Availability,ping,pinging,559,"Here is the terminal output, for posterity:. The first command with internet, confirms we are using a cached image:; ```bash; $ singularity exec docker://busybox ls; INFO: Using cached SIF image; CHANGELOG.md LICENSE README.md dist paper qme.egg-info setup.py; Dockerfile MANIFEST.in build docs qme setup.cfg tests; ```; then I took off my wireless :scream: and ran the same - we know the image is in the cache:. ```bash; $ singularity exec docker://busybox ls; FATAL: Unable to handle docker://busybox uri: failed to get checksum for docker://busybox: error pinging docker registry registry-1.docker.io: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io: no such host; ```; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046:102,Performance,cache,cached,102,"Here is the terminal output, for posterity:. The first command with internet, confirms we are using a cached image:; ```bash; $ singularity exec docker://busybox ls; INFO: Using cached SIF image; CHANGELOG.md LICENSE README.md dist paper qme.egg-info setup.py; Dockerfile MANIFEST.in build docs qme setup.cfg tests; ```; then I took off my wireless :scream: and ran the same - we know the image is in the cache:. ```bash; $ singularity exec docker://busybox ls; FATAL: Unable to handle docker://busybox uri: failed to get checksum for docker://busybox: error pinging docker registry registry-1.docker.io: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io: no such host; ```; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046:178,Performance,cache,cached,178,"Here is the terminal output, for posterity:. The first command with internet, confirms we are using a cached image:; ```bash; $ singularity exec docker://busybox ls; INFO: Using cached SIF image; CHANGELOG.md LICENSE README.md dist paper qme.egg-info setup.py; Dockerfile MANIFEST.in build docs qme setup.cfg tests; ```; then I took off my wireless :scream: and ran the same - we know the image is in the cache:. ```bash; $ singularity exec docker://busybox ls; FATAL: Unable to handle docker://busybox uri: failed to get checksum for docker://busybox: error pinging docker registry registry-1.docker.io: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io: no such host; ```; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046:405,Performance,cache,cache,405,"Here is the terminal output, for posterity:. The first command with internet, confirms we are using a cached image:; ```bash; $ singularity exec docker://busybox ls; INFO: Using cached SIF image; CHANGELOG.md LICENSE README.md dist paper qme.egg-info setup.py; Dockerfile MANIFEST.in build docs qme setup.cfg tests; ```; then I took off my wireless :scream: and ran the same - we know the image is in the cache:. ```bash; $ singularity exec docker://busybox ls; FATAL: Unable to handle docker://busybox uri: failed to get checksum for docker://busybox: error pinging docker registry registry-1.docker.io: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io: no such host; ```; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046:522,Security,checksum,checksum,522,"Here is the terminal output, for posterity:. The first command with internet, confirms we are using a cached image:; ```bash; $ singularity exec docker://busybox ls; INFO: Using cached SIF image; CHANGELOG.md LICENSE README.md dist paper qme.egg-info setup.py; Dockerfile MANIFEST.in build docs qme setup.cfg tests; ```; then I took off my wireless :scream: and ran the same - we know the image is in the cache:. ```bash; $ singularity exec docker://busybox ls; FATAL: Unable to handle docker://busybox uri: failed to get checksum for docker://busybox: error pinging docker registry registry-1.docker.io: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io: no such host; ```; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046:309,Testability,test,tests,309,"Here is the terminal output, for posterity:. The first command with internet, confirms we are using a cached image:; ```bash; $ singularity exec docker://busybox ls; INFO: Using cached SIF image; CHANGELOG.md LICENSE README.md dist paper qme.egg-info setup.py; Dockerfile MANIFEST.in build docs qme setup.cfg tests; ```; then I took off my wireless :scream: and ran the same - we know the image is in the cache:. ```bash; $ singularity exec docker://busybox ls; FATAL: Unable to handle docker://busybox uri: failed to get checksum for docker://busybox: error pinging docker registry registry-1.docker.io: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io: no such host; ```; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498:649,Availability,robust,robust,649,"@vsoch Thanks for testing this! This was indeed a major oversight on my behalf. . I did some further testing:. + It does not matter if you use a hashed container. Singularity will still look it up on the internet.; + The singularity cache does not store the file in an easily retrievable way:. ```; ~/.singularity/cache/oci-tmp/7c7e798af52365c2fa3c1c4606dcf8c1e2d5e502f49f1185d774655648175308$ ls; fastqc_0.11.9--0.sif fastqc@sha256_319b8d4eca0fc0367d192941f221f7fcd29a6b96996c63cbf8931dbb66e53348.sif; ```; You would have to hack with find etc. Dammit, this means this solution only works for fully connected nodes. And it means an alternate (more robust) solution needs to be hacked together in bash :scream: . On the other hand, I feel this could be fixed easily by singularity having a `--use-cache-first` flag, so it checks the cache first instead of checking the internet. I will investigate what is possible upstream.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498:233,Performance,cache,cache,233,"@vsoch Thanks for testing this! This was indeed a major oversight on my behalf. . I did some further testing:. + It does not matter if you use a hashed container. Singularity will still look it up on the internet.; + The singularity cache does not store the file in an easily retrievable way:. ```; ~/.singularity/cache/oci-tmp/7c7e798af52365c2fa3c1c4606dcf8c1e2d5e502f49f1185d774655648175308$ ls; fastqc_0.11.9--0.sif fastqc@sha256_319b8d4eca0fc0367d192941f221f7fcd29a6b96996c63cbf8931dbb66e53348.sif; ```; You would have to hack with find etc. Dammit, this means this solution only works for fully connected nodes. And it means an alternate (more robust) solution needs to be hacked together in bash :scream: . On the other hand, I feel this could be fixed easily by singularity having a `--use-cache-first` flag, so it checks the cache first instead of checking the internet. I will investigate what is possible upstream.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498:314,Performance,cache,cache,314,"@vsoch Thanks for testing this! This was indeed a major oversight on my behalf. . I did some further testing:. + It does not matter if you use a hashed container. Singularity will still look it up on the internet.; + The singularity cache does not store the file in an easily retrievable way:. ```; ~/.singularity/cache/oci-tmp/7c7e798af52365c2fa3c1c4606dcf8c1e2d5e502f49f1185d774655648175308$ ls; fastqc_0.11.9--0.sif fastqc@sha256_319b8d4eca0fc0367d192941f221f7fcd29a6b96996c63cbf8931dbb66e53348.sif; ```; You would have to hack with find etc. Dammit, this means this solution only works for fully connected nodes. And it means an alternate (more robust) solution needs to be hacked together in bash :scream: . On the other hand, I feel this could be fixed easily by singularity having a `--use-cache-first` flag, so it checks the cache first instead of checking the internet. I will investigate what is possible upstream.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498:797,Performance,cache,cache-first,797,"@vsoch Thanks for testing this! This was indeed a major oversight on my behalf. . I did some further testing:. + It does not matter if you use a hashed container. Singularity will still look it up on the internet.; + The singularity cache does not store the file in an easily retrievable way:. ```; ~/.singularity/cache/oci-tmp/7c7e798af52365c2fa3c1c4606dcf8c1e2d5e502f49f1185d774655648175308$ ls; fastqc_0.11.9--0.sif fastqc@sha256_319b8d4eca0fc0367d192941f221f7fcd29a6b96996c63cbf8931dbb66e53348.sif; ```; You would have to hack with find etc. Dammit, this means this solution only works for fully connected nodes. And it means an alternate (more robust) solution needs to be hacked together in bash :scream: . On the other hand, I feel this could be fixed easily by singularity having a `--use-cache-first` flag, so it checks the cache first instead of checking the internet. I will investigate what is possible upstream.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498:833,Performance,cache,cache,833,"@vsoch Thanks for testing this! This was indeed a major oversight on my behalf. . I did some further testing:. + It does not matter if you use a hashed container. Singularity will still look it up on the internet.; + The singularity cache does not store the file in an easily retrievable way:. ```; ~/.singularity/cache/oci-tmp/7c7e798af52365c2fa3c1c4606dcf8c1e2d5e502f49f1185d774655648175308$ ls; fastqc_0.11.9--0.sif fastqc@sha256_319b8d4eca0fc0367d192941f221f7fcd29a6b96996c63cbf8931dbb66e53348.sif; ```; You would have to hack with find etc. Dammit, this means this solution only works for fully connected nodes. And it means an alternate (more robust) solution needs to be hacked together in bash :scream: . On the other hand, I feel this could be fixed easily by singularity having a `--use-cache-first` flag, so it checks the cache first instead of checking the internet. I will investigate what is possible upstream.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498:145,Security,hash,hashed,145,"@vsoch Thanks for testing this! This was indeed a major oversight on my behalf. . I did some further testing:. + It does not matter if you use a hashed container. Singularity will still look it up on the internet.; + The singularity cache does not store the file in an easily retrievable way:. ```; ~/.singularity/cache/oci-tmp/7c7e798af52365c2fa3c1c4606dcf8c1e2d5e502f49f1185d774655648175308$ ls; fastqc_0.11.9--0.sif fastqc@sha256_319b8d4eca0fc0367d192941f221f7fcd29a6b96996c63cbf8931dbb66e53348.sif; ```; You would have to hack with find etc. Dammit, this means this solution only works for fully connected nodes. And it means an alternate (more robust) solution needs to be hacked together in bash :scream: . On the other hand, I feel this could be fixed easily by singularity having a `--use-cache-first` flag, so it checks the cache first instead of checking the internet. I will investigate what is possible upstream.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498:18,Testability,test,testing,18,"@vsoch Thanks for testing this! This was indeed a major oversight on my behalf. . I did some further testing:. + It does not matter if you use a hashed container. Singularity will still look it up on the internet.; + The singularity cache does not store the file in an easily retrievable way:. ```; ~/.singularity/cache/oci-tmp/7c7e798af52365c2fa3c1c4606dcf8c1e2d5e502f49f1185d774655648175308$ ls; fastqc_0.11.9--0.sif fastqc@sha256_319b8d4eca0fc0367d192941f221f7fcd29a6b96996c63cbf8931dbb66e53348.sif; ```; You would have to hack with find etc. Dammit, this means this solution only works for fully connected nodes. And it means an alternate (more robust) solution needs to be hacked together in bash :scream: . On the other hand, I feel this could be fixed easily by singularity having a `--use-cache-first` flag, so it checks the cache first instead of checking the internet. I will investigate what is possible upstream.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498:101,Testability,test,testing,101,"@vsoch Thanks for testing this! This was indeed a major oversight on my behalf. . I did some further testing:. + It does not matter if you use a hashed container. Singularity will still look it up on the internet.; + The singularity cache does not store the file in an easily retrievable way:. ```; ~/.singularity/cache/oci-tmp/7c7e798af52365c2fa3c1c4606dcf8c1e2d5e502f49f1185d774655648175308$ ls; fastqc_0.11.9--0.sif fastqc@sha256_319b8d4eca0fc0367d192941f221f7fcd29a6b96996c63cbf8931dbb66e53348.sif; ```; You would have to hack with find etc. Dammit, this means this solution only works for fully connected nodes. And it means an alternate (more robust) solution needs to be hacked together in bash :scream: . On the other hand, I feel this could be fixed easily by singularity having a `--use-cache-first` flag, so it checks the cache first instead of checking the internet. I will investigate what is possible upstream.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631329498
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:234,Availability,down,downloading,234,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:506,Availability,down,downloading,506,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:809,Availability,outage,outages,809,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:136,Performance,cache,cache,136,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:440,Performance,cache,cache,440,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:482,Performance,cache,cache,482,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:594,Performance,cache,cache,594,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:778,Performance,cache,cache-first,778,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:228,Safety,avoid,avoid,228,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:491,Safety,avoid,avoid,491,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:539,Safety,safe,safe,539,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:61,Availability,down,down,61,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:862,Integrability,depend,dependencies,862,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:125,Performance,cache,cache,125,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:368,Performance,cache,cache,368,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:638,Performance,cache,cache,638,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:808,Performance,cache,cache,808,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:398,Usability,feedback,feedback,398,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-636151836:214,Performance,cache,cache,214,"Hello - @vsoch has pointed me over here, and reminded me of https://github.com/hpcng/singularity/issues/5309 which I'll respond to shortly. It may be useful to note here that in the forthcoming singularity 3.6 the cache functionality is rewritten to drop the additional directory structure and stuff that caused race conditions. We now fetch cache entries to a tmp name, and rename - so assuming the filesystem the the cache dir is on supports atomic rename there shouldn't be concurrency issues. This will not prevent the need to work around it here for older singularity versions, though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-636151836
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-636151836:312,Performance,race condition,race conditions,312,"Hello - @vsoch has pointed me over here, and reminded me of https://github.com/hpcng/singularity/issues/5309 which I'll respond to shortly. It may be useful to note here that in the forthcoming singularity 3.6 the cache functionality is rewritten to drop the additional directory structure and stuff that caused race conditions. We now fetch cache entries to a tmp name, and rename - so assuming the filesystem the the cache dir is on supports atomic rename there shouldn't be concurrency issues. This will not prevent the need to work around it here for older singularity versions, though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-636151836
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-636151836:342,Performance,cache,cache,342,"Hello - @vsoch has pointed me over here, and reminded me of https://github.com/hpcng/singularity/issues/5309 which I'll respond to shortly. It may be useful to note here that in the forthcoming singularity 3.6 the cache functionality is rewritten to drop the additional directory structure and stuff that caused race conditions. We now fetch cache entries to a tmp name, and rename - so assuming the filesystem the the cache dir is on supports atomic rename there shouldn't be concurrency issues. This will not prevent the need to work around it here for older singularity versions, though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-636151836
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-636151836:419,Performance,cache,cache,419,"Hello - @vsoch has pointed me over here, and reminded me of https://github.com/hpcng/singularity/issues/5309 which I'll respond to shortly. It may be useful to note here that in the forthcoming singularity 3.6 the cache functionality is rewritten to drop the additional directory structure and stuff that caused race conditions. We now fetch cache entries to a tmp name, and rename - so assuming the filesystem the the cache dir is on supports atomic rename there shouldn't be concurrency issues. This will not prevent the need to work around it here for older singularity versions, though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-636151836
https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-636151836:477,Performance,concurren,concurrency,477,"Hello - @vsoch has pointed me over here, and reminded me of https://github.com/hpcng/singularity/issues/5309 which I'll respond to shortly. It may be useful to note here that in the forthcoming singularity 3.6 the cache functionality is rewritten to drop the additional directory structure and stuff that caused race conditions. We now fetch cache entries to a tmp name, and rename - so assuming the filesystem the the cache dir is on supports atomic rename there shouldn't be concurrency issues. This will not prevent the need to work around it here for older singularity versions, though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-636151836
https://github.com/broadinstitute/cromwell/pull/5516#issuecomment-628714704:1108,Testability,log,log,1108,"@vanajasmy Everyone does `git` things a little differently and there are probably many, many ways to do this. The way I'd approach this would be:. 1. Make a feature branch for your changes, instead of using your `develop`. That will let you keep your changes even when you re-sync your `develop` branch with ours. You can do that with `git checkout -b new_branch_name`; 2. So now you can re-sync _your_ `develop` branch to match the head of _our_ `develop` branch. First switch back to develop (`git checkout develop`) and do something like these [sample instructions](https://gist.github.com/glennblock/1974465) (but replace `master` with `develop`!).; 3. You should now have an up to date copy of `develop` and your own changes on a feature branch. But your feature branch will include all changes made since the last time you sync'd, including your previous changeset. So now you'll want to rebase your feature branch onto the new head of your `develop` (`git rebase -i develop new_branch_name`). You'll be asked to pick which commits you want to keep on your branch. Delete any old lines from the commit log apart from the new ones you want in this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5516#issuecomment-628714704
https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-630561563:13,Testability,test,test,13,"This was the test I managed to run on `dev`: . - Run 101 instances of a 100-output task; - Delete all files in the submission directories; - Run a further 1000 instances of the 100-output task. Left is `develop`, right is `24ecda6`. And the bump in the middle was me starting the test on the wrong commit 🤦‍♂️:. ![image](https://user-images.githubusercontent.com/13006282/82282248-60046680-9961-11ea-9510-32df84bdcfc2.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-630561563
https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-630561563:280,Testability,test,test,280,"This was the test I managed to run on `dev`: . - Run 101 instances of a 100-output task; - Delete all files in the submission directories; - Run a further 1000 instances of the 100-output task. Left is `develop`, right is `24ecda6`. And the bump in the middle was me starting the test on the wrong commit 🤦‍♂️:. ![image](https://user-images.githubusercontent.com/13006282/82282248-60046680-9961-11ea-9510-32df84bdcfc2.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-630561563
https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441:63,Availability,down,down,63,Results from running this again with proper configuration: 75m down to 25. The entire run on the correct configuration appears to line up with the initial plateau from the `develop` results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441
https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441:44,Deployability,configurat,configuration,44,Results from running this again with proper configuration: 75m down to 25. The entire run on the correct configuration appears to line up with the initial plateau from the `develop` results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441
https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441:105,Deployability,configurat,configuration,105,Results from running this again with proper configuration: 75m down to 25. The entire run on the correct configuration appears to line up with the initial plateau from the `develop` results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441
https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441:44,Modifiability,config,configuration,44,Results from running this again with proper configuration: 75m down to 25. The entire run on the correct configuration appears to line up with the initial plateau from the `develop` results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441
https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441:105,Modifiability,config,configuration,105,Results from running this again with proper configuration: 75m down to 25. The entire run on the correct configuration appears to line up with the initial plateau from the `develop` results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441
https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-635284404:70,Deployability,Pipeline,PipelinesApiBackendCacheHitCopyingActorSpec,70,"Despite the Github eliding, `StandardCacheHitCopyingActor.scala` and `PipelinesApiBackendCacheHitCopyingActorSpec` do warrant review. 🙂",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-635284404
https://github.com/broadinstitute/cromwell/pull/5522#issuecomment-665306474:31,Deployability,update,update,31,"Hi @cjllanwarne Thanks for the update. I looked at #5468 and #5554 changes made. They all S3 related changes/enghancements. This fix is EFS specific and hence not addressed by the above two. So this change is needed. ; Thanks,; Vanaja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5522#issuecomment-665306474
https://github.com/broadinstitute/cromwell/pull/5523#issuecomment-634099775:38,Security,validat,validation,38,"I have written the necessary code for validation. However, optional inputs still show up in womtool inputs even if `meta {allowNestedInputs: false}` so I need to do some more coding to fix that. Will do that tomorrow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5523#issuecomment-634099775
https://github.com/broadinstitute/cromwell/pull/5523#issuecomment-634652078:21,Testability,test,tests,21,"Okay done. Hopefully tests will succeed. I did notice a lot of womtool spec test cases had to be modified, however it seemed as if the extremely vast and comprehensive centaur test set needed only a few tweaks. ; This makes me think that this technically backwards incompatible change will not raise too many problems in practice.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5523#issuecomment-634652078
https://github.com/broadinstitute/cromwell/pull/5523#issuecomment-634652078:76,Testability,test,test,76,"Okay done. Hopefully tests will succeed. I did notice a lot of womtool spec test cases had to be modified, however it seemed as if the extremely vast and comprehensive centaur test set needed only a few tweaks. ; This makes me think that this technically backwards incompatible change will not raise too many problems in practice.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5523#issuecomment-634652078
https://github.com/broadinstitute/cromwell/pull/5523#issuecomment-634652078:176,Testability,test,test,176,"Okay done. Hopefully tests will succeed. I did notice a lot of womtool spec test cases had to be modified, however it seemed as if the extremely vast and comprehensive centaur test set needed only a few tweaks. ; This makes me think that this technically backwards incompatible change will not raise too many problems in practice.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5523#issuecomment-634652078
https://github.com/broadinstitute/cromwell/issues/5524#issuecomment-2113647854:599,Performance,cache,cacheCopy,599,"We also ran for a while and got extra `glob` folders in our paths via the workflow options:. ```json; {; ""final_workflow_outputs_dir"": ""xxx"",; ""use_relative_output_paths"": true; }; ```. For anyone running their instances on a fork, or if someone wants to ask the Cromwell devs to see if this is a breaking change, on our instance I briefly tried out modifying [this line](https://github.com/broadinstitute/cromwell/blob/87/engine/src/main/scala/cromwell/engine/workflow/lifecycle/finalization/CopyWorkflowOutputsActor.scala#L124):. ```scala; lazy val truncateRegex = "".*/call-[^/]*/(shard-[0-9]+/)?(cacheCopy/)?(attempt-[0-9]+/)?(execution/)?"".r; ```. to:. ```scala; lazy val truncateRegex = "".*/call-[^/]*/(shard-[0-9]+/)?(cacheCopy/)?(attempt-[0-9]+/)?(execution/)?(glob-[0-9a-f]+/)?"".r; ```. It seemed to work, removing the glob folder from files copied into xxx. However, I ultimately pursued a different implementation. Using a customized external tool, we now only copy outputs reported as `File` or `Directory` by the `/describe` endpoint, which we are already using for validation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5524#issuecomment-2113647854
https://github.com/broadinstitute/cromwell/issues/5524#issuecomment-2113647854:724,Performance,cache,cacheCopy,724,"We also ran for a while and got extra `glob` folders in our paths via the workflow options:. ```json; {; ""final_workflow_outputs_dir"": ""xxx"",; ""use_relative_output_paths"": true; }; ```. For anyone running their instances on a fork, or if someone wants to ask the Cromwell devs to see if this is a breaking change, on our instance I briefly tried out modifying [this line](https://github.com/broadinstitute/cromwell/blob/87/engine/src/main/scala/cromwell/engine/workflow/lifecycle/finalization/CopyWorkflowOutputsActor.scala#L124):. ```scala; lazy val truncateRegex = "".*/call-[^/]*/(shard-[0-9]+/)?(cacheCopy/)?(attempt-[0-9]+/)?(execution/)?"".r; ```. to:. ```scala; lazy val truncateRegex = "".*/call-[^/]*/(shard-[0-9]+/)?(cacheCopy/)?(attempt-[0-9]+/)?(execution/)?(glob-[0-9a-f]+/)?"".r; ```. It seemed to work, removing the glob folder from files copied into xxx. However, I ultimately pursued a different implementation. Using a customized external tool, we now only copy outputs reported as `File` or `Directory` by the `/describe` endpoint, which we are already using for validation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5524#issuecomment-2113647854
https://github.com/broadinstitute/cromwell/issues/5524#issuecomment-2113647854:1078,Security,validat,validation,1078,"We also ran for a while and got extra `glob` folders in our paths via the workflow options:. ```json; {; ""final_workflow_outputs_dir"": ""xxx"",; ""use_relative_output_paths"": true; }; ```. For anyone running their instances on a fork, or if someone wants to ask the Cromwell devs to see if this is a breaking change, on our instance I briefly tried out modifying [this line](https://github.com/broadinstitute/cromwell/blob/87/engine/src/main/scala/cromwell/engine/workflow/lifecycle/finalization/CopyWorkflowOutputsActor.scala#L124):. ```scala; lazy val truncateRegex = "".*/call-[^/]*/(shard-[0-9]+/)?(cacheCopy/)?(attempt-[0-9]+/)?(execution/)?"".r; ```. to:. ```scala; lazy val truncateRegex = "".*/call-[^/]*/(shard-[0-9]+/)?(cacheCopy/)?(attempt-[0-9]+/)?(execution/)?(glob-[0-9a-f]+/)?"".r; ```. It seemed to work, removing the glob folder from files copied into xxx. However, I ultimately pursued a different implementation. Using a customized external tool, we now only copy outputs reported as `File` or `Directory` by the `/describe` endpoint, which we are already using for validation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5524#issuecomment-2113647854
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488:233,Integrability,message,messages,233,"> Looks good, what were the results of running this to the point that Cromwell did not return a successful response?. It either fails with OOM or becomes totally unresponsive for a long time, while writing different kinds of timeout messages to the log (like ""timeout while trying to fetch new workflows"" or something like that).; Regarding number of rows, I remember that it handled 1.500.000 easily (carbonited within minute or two). I didn't look for precise upper bound, but I think that for 15.000.000 it was failing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488:225,Safety,timeout,timeout,225,"> Looks good, what were the results of running this to the point that Cromwell did not return a successful response?. It either fails with OOM or becomes totally unresponsive for a long time, while writing different kinds of timeout messages to the log (like ""timeout while trying to fetch new workflows"" or something like that).; Regarding number of rows, I remember that it handled 1.500.000 easily (carbonited within minute or two). I didn't look for precise upper bound, but I think that for 15.000.000 it was failing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488:260,Safety,timeout,timeout,260,"> Looks good, what were the results of running this to the point that Cromwell did not return a successful response?. It either fails with OOM or becomes totally unresponsive for a long time, while writing different kinds of timeout messages to the log (like ""timeout while trying to fetch new workflows"" or something like that).; Regarding number of rows, I remember that it handled 1.500.000 easily (carbonited within minute or two). I didn't look for precise upper bound, but I think that for 15.000.000 it was failing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488:249,Testability,log,log,249,"> Looks good, what were the results of running this to the point that Cromwell did not return a successful response?. It either fails with OOM or becomes totally unresponsive for a long time, while writing different kinds of timeout messages to the log (like ""timeout while trying to fetch new workflows"" or something like that).; Regarding number of rows, I remember that it handled 1.500.000 easily (carbonited within minute or two). I didn't look for precise upper bound, but I think that for 15.000.000 it was failing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638326392:128,Availability,resilien,resilient,128,"OK thanks. So it sounds like this test is doing exactly what it was meant to do, but we have some work to do in making Cromwell resilient to this scenario.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638326392
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638326392:34,Testability,test,test,34,"OK thanks. So it sounds like this test is doing exactly what it was meant to do, but we have some work to do in making Cromwell resilient to this scenario.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638326392
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638333679:130,Availability,resilien,resilient,130,"> OK thanks. So it sounds like this test is doing exactly what it was meant to do, but we have some work to do in making Cromwell resilient to this scenario. Actually, that work has already been done and merged into develop. The goal of this ticket was to verify that 1.000.000 rows chosen as default limit is a sane choice.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638333679
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638333679:36,Testability,test,test,36,"> OK thanks. So it sounds like this test is doing exactly what it was meant to do, but we have some work to do in making Cromwell resilient to this scenario. Actually, that work has already been done and merged into develop. The goal of this ticket was to verify that 1.000.000 rows chosen as default limit is a sane choice.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638333679
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639095508:307,Deployability,install,installed,307,"> Looks correct to me. Which environment did you use to run the tests?. I provisioned the dedicated VM of the same shape as used on prod: https://console.cloud.google.com/compute/instancesDetail/zones/us-central1-a/instances/greg-test-oom?project=broad-dsde-cromwell-dev; As a database I used a local MySql installed on the same VM. >>Actually, that work has already been done and merged into develop.; >; >Did you try running this test case rebased onto those changes to confirm that? If not, should we consider making it a future task?. I did not. I'm a bit reluctant to create a new ticket for this, since the whole task is becoming too fine-grained. I can check this within current task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639095508
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639095508:64,Testability,test,tests,64,"> Looks correct to me. Which environment did you use to run the tests?. I provisioned the dedicated VM of the same shape as used on prod: https://console.cloud.google.com/compute/instancesDetail/zones/us-central1-a/instances/greg-test-oom?project=broad-dsde-cromwell-dev; As a database I used a local MySql installed on the same VM. >>Actually, that work has already been done and merged into develop.; >; >Did you try running this test case rebased onto those changes to confirm that? If not, should we consider making it a future task?. I did not. I'm a bit reluctant to create a new ticket for this, since the whole task is becoming too fine-grained. I can check this within current task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639095508
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639095508:230,Testability,test,test-oom,230,"> Looks correct to me. Which environment did you use to run the tests?. I provisioned the dedicated VM of the same shape as used on prod: https://console.cloud.google.com/compute/instancesDetail/zones/us-central1-a/instances/greg-test-oom?project=broad-dsde-cromwell-dev; As a database I used a local MySql installed on the same VM. >>Actually, that work has already been done and merged into develop.; >; >Did you try running this test case rebased onto those changes to confirm that? If not, should we consider making it a future task?. I did not. I'm a bit reluctant to create a new ticket for this, since the whole task is becoming too fine-grained. I can check this within current task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639095508
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639095508:432,Testability,test,test,432,"> Looks correct to me. Which environment did you use to run the tests?. I provisioned the dedicated VM of the same shape as used on prod: https://console.cloud.google.com/compute/instancesDetail/zones/us-central1-a/instances/greg-test-oom?project=broad-dsde-cromwell-dev; As a database I used a local MySql installed on the same VM. >>Actually, that work has already been done and merged into develop.; >; >Did you try running this test case rebased onto those changes to confirm that? If not, should we consider making it a future task?. I did not. I'm a bit reluctant to create a new ticket for this, since the whole task is becoming too fine-grained. I can check this within current task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639095508
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:325,Availability,ERROR,ERROR,325,"@cjllanwarne, checked works as expected:; ```; 2020-06-04 21:43:38,063 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:345,Availability,failure,failure,345,"@cjllanwarne, checked works as expected:; ```; 2020-06-04 21:43:38,063 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:551,Availability,failure,failure,551,"@cjllanwarne, checked works as expected:; ```; 2020-06-04 21:43:38,063 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:945,Availability,failure,failure,945,"@cjllanwarne, checked works as expected:; ```; 2020-06-04 21:43:38,063 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:4317,Availability,ERROR,ERROR,4317,".java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. ```; mysql> select * from WORKFLOW_METADATA_SUMMARY_ENTRY;; +------------------------------------+--------------------------------------+---------------+-----------------+----------------------------+----------------------------+----------------------------+--------------------------------+------------------------------+-------------------------+; | WORKFLOW_METADATA_SUMMARY_ENTRY_ID | WORKFLOW_EXECUTION_UUID | WORKFLOW_NAME | WORKFLOW_STATUS | START_TIMESTAMP | END_TIMESTAMP | SUBMISSION_TIMESTAMP | PARENT_WORKFLOW_EXECUTION_UUID | ROOT_WORKFLOW_EXECUTION_UUID | METADATA_ARCHIVE_STATUS |; +------------------------------------+--------------------------------------+---------------+-----------------+----------------------------+----------------------------+----------------------------+--------------------------------+------------------------------+-------------------------+; | 2 | 796f3949-47e6-497e-9458-59ab53a063c6 | wf_hello | Succeeded | 2020-06-04 21:40:10.924000 | 2020-06-04 21:43:38.055000 | 2020-06-04 21:40:10.726000 | NULL | NULL | TooLargeToArchive |; +------------------------------------+--------------------------------------+---------------+-----------------+----------------------------+----------------------------+----------------------------+--------------------------------+------------------------------+-------------------------+; ```. But there was a small bug:; ```; 2020-06-04 21:43:43,512 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Programmer Error! The CarboniteWorkerActor cannot convert this into a completion metric: CarboniteWorkflowComplete(796f3949-47e6-497e-9458-59ab53a063c6,TooLargeToArchive); ```; I created a ticket for it: https://broadworkbench.atlassian.net/browse/BA-6471",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:4336,Availability,Error,Error,4336,".java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. ```; mysql> select * from WORKFLOW_METADATA_SUMMARY_ENTRY;; +------------------------------------+--------------------------------------+---------------+-----------------+----------------------------+----------------------------+----------------------------+--------------------------------+------------------------------+-------------------------+; | WORKFLOW_METADATA_SUMMARY_ENTRY_ID | WORKFLOW_EXECUTION_UUID | WORKFLOW_NAME | WORKFLOW_STATUS | START_TIMESTAMP | END_TIMESTAMP | SUBMISSION_TIMESTAMP | PARENT_WORKFLOW_EXECUTION_UUID | ROOT_WORKFLOW_EXECUTION_UUID | METADATA_ARCHIVE_STATUS |; +------------------------------------+--------------------------------------+---------------+-----------------+----------------------------+----------------------------+----------------------------+--------------------------------+------------------------------+-------------------------+; | 2 | 796f3949-47e6-497e-9458-59ab53a063c6 | wf_hello | Succeeded | 2020-06-04 21:40:10.924000 | 2020-06-04 21:43:38.055000 | 2020-06-04 21:40:10.726000 | NULL | NULL | TooLargeToArchive |; +------------------------------------+--------------------------------------+---------------+-----------------+----------------------------+----------------------------+----------------------------+--------------------------------+------------------------------+-------------------------+; ```. But there was a small bug:; ```; 2020-06-04 21:43:43,512 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Programmer Error! The CarboniteWorkerActor cannot convert this into a completion metric: CarboniteWorkflowComplete(796f3949-47e6-497e-9458-59ab53a063c6,TooLargeToArchive); ```; I created a ticket for it: https://broadworkbench.atlassian.net/browse/BA-6471",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:672,Modifiability,Config,Configured,672,"@cjllanwarne, checked works as expected:; ```; 2020-06-04 21:43:38,063 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:1066,Modifiability,Config,Configured,1066,"akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:695); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:536,Safety,avoid,avoid,536,"@cjllanwarne, checked works as expected:; ```; 2020-06-04 21:43:38,063 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:692,Safety,safe,safety,692,"@cjllanwarne, checked works as expected:; ```; 2020-06-04 21:43:38,063 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:930,Safety,avoid,avoid,930,"@cjllanwarne, checked works as expected:; ```; 2020-06-04 21:43:38,063 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:1086,Safety,safe,safety,1086,"akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-796f3949-47e6-497e-9458-59ab53a063c6 is in a terminal state: WorkflowSucceededState; 2020-06-04 21:43:43,504 cromwell-system-akka.actor.default-dispatcher-56 ERROR - Carboniting failure: cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:695); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.ac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:1610,Testability,Log,LoggingFSM,1610,"e to die on attempt to read it in memory. Configured metadata safety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:695); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.aroundReceive(MetadataBuilderActor.scala:245); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:1689,Testability,Log,LoggingFSM,1689,"fety limit is 1000000.. Marking as TooLargeToArchive; cromwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:695); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.aroundReceive(MetadataBuilderActor.scala:245); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.fo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073
https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073:1744,Testability,Log,LoggingFSM,1744,"romwell.services.MetadataTooLargeNumberOfRowsException: Metadata for workflow 796f3949-47e6-497e-9458-59ab53a063c6 exists indatabase, but cannot be served. This is done in order to avoid Cromwell failure: metadata is too large - 283000000 rows, and may cause Cromwell instance to die on attempt to read it in memory. Configured metadata safety limit is 1000000.; 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:283); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor$$anonfun$2.applyOrElse(MetadataBuilderActor.scala:267); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.akka$actor$LoggingFSM$$super$processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.processEvent(MetadataBuilderActor.scala:245); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:695); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.services.metadata.impl.builder.MetadataBuilderActor.aroundReceive(MetadataBuilderActor.scala:245); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-639142073
https://github.com/broadinstitute/cromwell/pull/5527#issuecomment-637798466:36,Safety,safe,safety,36,"Not sure, we can err on the side of safety and do just the group for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5527#issuecomment-637798466
https://github.com/broadinstitute/cromwell/pull/5527#issuecomment-637817161:12,Deployability,hotfix,hotfix,12,"This is the hotfix edition, I didn't title it appropriately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5527#issuecomment-637817161
https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637917807:59,Modifiability,config,config,59,"Hey @natechols, not part of broad team, but what does your config looks like for the SGE cluster. The `memory` variable gets turned into `memory_mb` or `memory_gb` in your runtime attributes which you can use to prepare the SGE job:. https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637917807
https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637917807:111,Modifiability,variab,variable,111,"Hey @natechols, not part of broad team, but what does your config looks like for the SGE cluster. The `memory` variable gets turned into `memory_mb` or `memory_gb` in your runtime attributes which you can use to prepare the SGE job:. https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637917807
https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284:260,Availability,error,error,260,"@illusional The key part was this:; ```; runtime-attributes = """"""; Int cpu = 1; String? memory; """"""; ```; plus passing `memory` to the submit command - which in my case is a wrapper script that can interpret strings like ""4GB"". After another hour of trial-and-error I did finally get a working config using `memory_gb` in the config file instead of `memory`. But I have to say, a colleague and I read the documentation on memory repeatedly and we're still confused about what it's *trying* to say and how that relates to what actually happens. (In particular, it is not clear that you can't pass `memory` directly to non-SFS backends.) The fact that both `memory` and `memory_gb` can be task runtime parameters, yet one will presumably be overridden anyway, seems unwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284
https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284:174,Integrability,wrap,wrapper,174,"@illusional The key part was this:; ```; runtime-attributes = """"""; Int cpu = 1; String? memory; """"""; ```; plus passing `memory` to the submit command - which in my case is a wrapper script that can interpret strings like ""4GB"". After another hour of trial-and-error I did finally get a working config using `memory_gb` in the config file instead of `memory`. But I have to say, a colleague and I read the documentation on memory repeatedly and we're still confused about what it's *trying* to say and how that relates to what actually happens. (In particular, it is not clear that you can't pass `memory` directly to non-SFS backends.) The fact that both `memory` and `memory_gb` can be task runtime parameters, yet one will presumably be overridden anyway, seems unwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284
https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284:294,Modifiability,config,config,294,"@illusional The key part was this:; ```; runtime-attributes = """"""; Int cpu = 1; String? memory; """"""; ```; plus passing `memory` to the submit command - which in my case is a wrapper script that can interpret strings like ""4GB"". After another hour of trial-and-error I did finally get a working config using `memory_gb` in the config file instead of `memory`. But I have to say, a colleague and I read the documentation on memory repeatedly and we're still confused about what it's *trying* to say and how that relates to what actually happens. (In particular, it is not clear that you can't pass `memory` directly to non-SFS backends.) The fact that both `memory` and `memory_gb` can be task runtime parameters, yet one will presumably be overridden anyway, seems unwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284
https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284:326,Modifiability,config,config,326,"@illusional The key part was this:; ```; runtime-attributes = """"""; Int cpu = 1; String? memory; """"""; ```; plus passing `memory` to the submit command - which in my case is a wrapper script that can interpret strings like ""4GB"". After another hour of trial-and-error I did finally get a working config using `memory_gb` in the config file instead of `memory`. But I have to say, a colleague and I read the documentation on memory repeatedly and we're still confused about what it's *trying* to say and how that relates to what actually happens. (In particular, it is not clear that you can't pass `memory` directly to non-SFS backends.) The fact that both `memory` and `memory_gb` can be task runtime parameters, yet one will presumably be overridden anyway, seems unwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284
https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284:570,Usability,clear,clear,570,"@illusional The key part was this:; ```; runtime-attributes = """"""; Int cpu = 1; String? memory; """"""; ```; plus passing `memory` to the submit command - which in my case is a wrapper script that can interpret strings like ""4GB"". After another hour of trial-and-error I did finally get a working config using `memory_gb` in the config file instead of `memory`. But I have to say, a colleague and I read the documentation on memory repeatedly and we're still confused about what it's *trying* to say and how that relates to what actually happens. (In particular, it is not clear that you can't pass `memory` directly to non-SFS backends.) The fact that both `memory` and `memory_gb` can be task runtime parameters, yet one will presumably be overridden anyway, seems unwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:141,Deployability,configurat,configuration,141,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:219,Deployability,configurat,configuration,219,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:679,Deployability,configurat,configuration,679,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:854,Deployability,configurat,configuration,854,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:912,Deployability,configurat,configuration,912,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:1234,Deployability,configurat,configuration,1234,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:141,Modifiability,config,configuration,141,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:219,Modifiability,config,configuration,219,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:474,Modifiability,config,config,474,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:481,Modifiability,Config,ConfigBackendLifecycleActorFactory,481,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:520,Modifiability,config,config,520,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:679,Modifiability,config,configuration,679,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:854,Modifiability,config,configuration,854,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:912,Modifiability,config,configuration,912,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:1234,Modifiability,config,configuration,1234,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:48,Performance,cache,cached-copy,48,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:240,Performance,cache,cached-inputs,240,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:556,Performance,cache,cachedcopy,556,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:627,Performance,cache,cached-copy,627,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:706,Performance,cache,cached-copy,706,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:1032,Performance,cache,cached-copy,1032,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:1124,Performance,cache,cached-copy,1124,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399:1177,Performance,cache,cached-copy,1177,"@illusional I tried to reproduce the issue, but cached-copy localization didn't work for me with both cromwell-50 and cromwell-51 when using configuration file you provided.; On the other hand, when using the following configuration file, `cached-inputs/` was successfully populated for me by both 50 and 51 cromwells:; ```; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/gsterin/cachedcopy/exec_dir"",; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; }; }; }; }; ```. It looks like configuration key enabling cached-copy localization should be `filesystems.local.localization` instead of `filesystems.local.duplication-strategy` and also looks like joining configuration keys with `.` symbol doesn't work when your configuration is in pure JSON format (but it works with HOCON format).; So `""filesystems.local.duplication-strategy"": [""cached-copy""]` should be replaced by ; ```; ""filesystems"": {; ""local"": {; ""localization"": [""cached-copy""]; }; }; ```. But I'm still not sure why cached-copy localization worked in Cromwell 50 with your configuration. Would you be able to double-check please?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642196399
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642392908:365,Modifiability,Config,Configuring,365,"You're my saviour @grsterin. . Tbh, this feels like one of those ""how did this ever work"" moments. I think I was using a weird custom build of cromwell-49 + some changes, I tested with a fresh copy and you're right it wouldn't work with cromwell-50, but having the wrong key I don't know what I did. I _think_ I got confused because I was primarily looking at the [Configuring#local-filesystem-options](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options) page on the docs. I've created a PR with a change which I think help clarifies it: https://github.com/broadinstitute/cromwell/pull/5542. Again, thanks so much, I'm so glad to close this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642392908
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642392908:445,Modifiability,Config,Configuring,445,"You're my saviour @grsterin. . Tbh, this feels like one of those ""how did this ever work"" moments. I think I was using a weird custom build of cromwell-49 + some changes, I tested with a fresh copy and you're right it wouldn't work with cromwell-50, but having the wrong key I don't know what I did. I _think_ I got confused because I was primarily looking at the [Configuring#local-filesystem-options](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options) page on the docs. I've created a PR with a change which I think help clarifies it: https://github.com/broadinstitute/cromwell/pull/5542. Again, thanks so much, I'm so glad to close this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642392908
https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642392908:173,Testability,test,tested,173,"You're my saviour @grsterin. . Tbh, this feels like one of those ""how did this ever work"" moments. I think I was using a weird custom build of cromwell-49 + some changes, I tested with a fresh copy and you're right it wouldn't work with cromwell-50, but having the wrong key I don't know what I did. I _think_ I got confused because I was primarily looking at the [Configuring#local-filesystem-options](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options) page on the docs. I've created a PR with a change which I think help clarifies it: https://github.com/broadinstitute/cromwell/pull/5542. Again, thanks so much, I'm so glad to close this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533#issuecomment-642392908
https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-641341856:155,Testability,test,test,155,"Thank you for looking into this! I am using what I think is currently the latest version, that is, 51 (i.e. running it with `java -jar cromwell-51.jar run test.wdl -i test.json` on my laptop). I am new to Dockers/Cromwell/WDL. I have just started a week ago so I have no idea whether this is a new behavior but it should be easy to test this with the WDL presented above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-641341856
https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-641341856:167,Testability,test,test,167,"Thank you for looking into this! I am using what I think is currently the latest version, that is, 51 (i.e. running it with `java -jar cromwell-51.jar run test.wdl -i test.json` on my laptop). I am new to Dockers/Cromwell/WDL. I have just started a week ago so I have no idea whether this is a new behavior but it should be easy to test this with the WDL presented above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-641341856
https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-641341856:332,Testability,test,test,332,"Thank you for looking into this! I am using what I think is currently the latest version, that is, 51 (i.e. running it with `java -jar cromwell-51.jar run test.wdl -i test.json` on my laptop). I am new to Dockers/Cromwell/WDL. I have just started a week ago so I have no idea whether this is a new behavior but it should be easy to test this with the WDL presented above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-641341856
https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592:348,Availability,echo,echo,348,"I think the following workflow (test.wdl) shows some even more insidious issues with localization:; ```; version 1.0. workflow main {; call main {; input:; X = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""],; Y = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""]; }. output {; Array[String] out = main.out; }; }. task main {; input {; Array[File]? X; Array[File]? Y; }. command <<<; echo X=~{if defined(X) then write_lines(select_first([X])) else X}; echo ~{if defined(Y) then ""Y="" + write_lines(select_first([Y])) else Y}; >>>. output {; Array[String] out = read_lines(stdout()); }. runtime {; docker: ""ubuntu:20.04""; }; }; ```. The following run:; ```; $ touch /tmp/{1,2,3}; $ cd /home/freeseek/cromwell; $ java -jar cromwell-51.jar run test.wdl; ```. Generates the following main.out:; ```; [""X=/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp"",; ""Y=/home/freeseek/cromwell/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp""]; ```. I can guess that in one case write_lines() is run before localization and in one case it is run after localization, generating two at first extremely puzzling different outputs. Notice that the [WDL specification](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-input-localization) requires that `Files are localized into the execution directory prior to the task execution commencing.` which does not seem to be the case for Cromwell. This seems to me like a serious bug. Where is it specified when Cromwell performs localization of files?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592
https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592:416,Availability,echo,echo,416,"I think the following workflow (test.wdl) shows some even more insidious issues with localization:; ```; version 1.0. workflow main {; call main {; input:; X = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""],; Y = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""]; }. output {; Array[String] out = main.out; }; }. task main {; input {; Array[File]? X; Array[File]? Y; }. command <<<; echo X=~{if defined(X) then write_lines(select_first([X])) else X}; echo ~{if defined(Y) then ""Y="" + write_lines(select_first([Y])) else Y}; >>>. output {; Array[String] out = read_lines(stdout()); }. runtime {; docker: ""ubuntu:20.04""; }; }; ```. The following run:; ```; $ touch /tmp/{1,2,3}; $ cd /home/freeseek/cromwell; $ java -jar cromwell-51.jar run test.wdl; ```. Generates the following main.out:; ```; [""X=/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp"",; ""Y=/home/freeseek/cromwell/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp""]; ```. I can guess that in one case write_lines() is run before localization and in one case it is run after localization, generating two at first extremely puzzling different outputs. Notice that the [WDL specification](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-input-localization) requires that `Files are localized into the execution directory prior to the task execution commencing.` which does not seem to be the case for Cromwell. This seems to me like a serious bug. Where is it specified when Cromwell performs localization of files?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592
https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592:1592,Performance,perform,performs,1592,"I think the following workflow (test.wdl) shows some even more insidious issues with localization:; ```; version 1.0. workflow main {; call main {; input:; X = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""],; Y = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""]; }. output {; Array[String] out = main.out; }; }. task main {; input {; Array[File]? X; Array[File]? Y; }. command <<<; echo X=~{if defined(X) then write_lines(select_first([X])) else X}; echo ~{if defined(Y) then ""Y="" + write_lines(select_first([Y])) else Y}; >>>. output {; Array[String] out = read_lines(stdout()); }. runtime {; docker: ""ubuntu:20.04""; }; }; ```. The following run:; ```; $ touch /tmp/{1,2,3}; $ cd /home/freeseek/cromwell; $ java -jar cromwell-51.jar run test.wdl; ```. Generates the following main.out:; ```; [""X=/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp"",; ""Y=/home/freeseek/cromwell/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp""]; ```. I can guess that in one case write_lines() is run before localization and in one case it is run after localization, generating two at first extremely puzzling different outputs. Notice that the [WDL specification](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-input-localization) requires that `Files are localized into the execution directory prior to the task execution commencing.` which does not seem to be the case for Cromwell. This seems to me like a serious bug. Where is it specified when Cromwell performs localization of files?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592
https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592:32,Testability,test,test,32,"I think the following workflow (test.wdl) shows some even more insidious issues with localization:; ```; version 1.0. workflow main {; call main {; input:; X = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""],; Y = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""]; }. output {; Array[String] out = main.out; }; }. task main {; input {; Array[File]? X; Array[File]? Y; }. command <<<; echo X=~{if defined(X) then write_lines(select_first([X])) else X}; echo ~{if defined(Y) then ""Y="" + write_lines(select_first([Y])) else Y}; >>>. output {; Array[String] out = read_lines(stdout()); }. runtime {; docker: ""ubuntu:20.04""; }; }; ```. The following run:; ```; $ touch /tmp/{1,2,3}; $ cd /home/freeseek/cromwell; $ java -jar cromwell-51.jar run test.wdl; ```. Generates the following main.out:; ```; [""X=/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp"",; ""Y=/home/freeseek/cromwell/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp""]; ```. I can guess that in one case write_lines() is run before localization and in one case it is run after localization, generating two at first extremely puzzling different outputs. Notice that the [WDL specification](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-input-localization) requires that `Files are localized into the execution directory prior to the task execution commencing.` which does not seem to be the case for Cromwell. This seems to me like a serious bug. Where is it specified when Cromwell performs localization of files?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592
https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592:704,Testability,test,test,704,"I think the following workflow (test.wdl) shows some even more insidious issues with localization:; ```; version 1.0. workflow main {; call main {; input:; X = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""],; Y = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""]; }. output {; Array[String] out = main.out; }; }. task main {; input {; Array[File]? X; Array[File]? Y; }. command <<<; echo X=~{if defined(X) then write_lines(select_first([X])) else X}; echo ~{if defined(Y) then ""Y="" + write_lines(select_first([Y])) else Y}; >>>. output {; Array[String] out = read_lines(stdout()); }. runtime {; docker: ""ubuntu:20.04""; }; }; ```. The following run:; ```; $ touch /tmp/{1,2,3}; $ cd /home/freeseek/cromwell; $ java -jar cromwell-51.jar run test.wdl; ```. Generates the following main.out:; ```; [""X=/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp"",; ""Y=/home/freeseek/cromwell/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp""]; ```. I can guess that in one case write_lines() is run before localization and in one case it is run after localization, generating two at first extremely puzzling different outputs. Notice that the [WDL specification](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-input-localization) requires that `Files are localized into the execution directory prior to the task execution commencing.` which does not seem to be the case for Cromwell. This seems to me like a serious bug. Where is it specified when Cromwell performs localization of files?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592
https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-850005555:4,Deployability,update,updates,4,Any updates on this? I have run into the latter example here.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-850005555
https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-642654094:96,Modifiability,config,config,96,cc @rhpvorderman @gbggrant who might be interested in the singularity aspects of this change to config backend.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-642654094
https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996:93,Deployability,pipeline,pipelines,93,"@cjllanwarne Thanks for notifying! We always use the `docker` attribute in BioWDL, so BioWDL pipelines can run without any extra configuration. On our cluster we have configured this so that we run the docker images using singularity. . I think it is a good thing that custom runtime attributes can be cached now. We recently added a `time_minutes` attribute to our pipelines in order to work better with SLURM. I hope this code ties in nicely when we switch to `hints` in WDL 2.0.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996
https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996:129,Deployability,configurat,configuration,129,"@cjllanwarne Thanks for notifying! We always use the `docker` attribute in BioWDL, so BioWDL pipelines can run without any extra configuration. On our cluster we have configured this so that we run the docker images using singularity. . I think it is a good thing that custom runtime attributes can be cached now. We recently added a `time_minutes` attribute to our pipelines in order to work better with SLURM. I hope this code ties in nicely when we switch to `hints` in WDL 2.0.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996
https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996:366,Deployability,pipeline,pipelines,366,"@cjllanwarne Thanks for notifying! We always use the `docker` attribute in BioWDL, so BioWDL pipelines can run without any extra configuration. On our cluster we have configured this so that we run the docker images using singularity. . I think it is a good thing that custom runtime attributes can be cached now. We recently added a `time_minutes` attribute to our pipelines in order to work better with SLURM. I hope this code ties in nicely when we switch to `hints` in WDL 2.0.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996
https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996:129,Modifiability,config,configuration,129,"@cjllanwarne Thanks for notifying! We always use the `docker` attribute in BioWDL, so BioWDL pipelines can run without any extra configuration. On our cluster we have configured this so that we run the docker images using singularity. . I think it is a good thing that custom runtime attributes can be cached now. We recently added a `time_minutes` attribute to our pipelines in order to work better with SLURM. I hope this code ties in nicely when we switch to `hints` in WDL 2.0.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996
https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996:167,Modifiability,config,configured,167,"@cjllanwarne Thanks for notifying! We always use the `docker` attribute in BioWDL, so BioWDL pipelines can run without any extra configuration. On our cluster we have configured this so that we run the docker images using singularity. . I think it is a good thing that custom runtime attributes can be cached now. We recently added a `time_minutes` attribute to our pipelines in order to work better with SLURM. I hope this code ties in nicely when we switch to `hints` in WDL 2.0.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996
https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996:302,Performance,cache,cached,302,"@cjllanwarne Thanks for notifying! We always use the `docker` attribute in BioWDL, so BioWDL pipelines can run without any extra configuration. On our cluster we have configured this so that we run the docker images using singularity. . I think it is a good thing that custom runtime attributes can be cached now. We recently added a `time_minutes` attribute to our pipelines in order to work better with SLURM. I hope this code ties in nicely when we switch to `hints` in WDL 2.0.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5543#issuecomment-643949996
https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-652146451:98,Deployability,configurat,configuration,98,"Thanks @mcovarr, I've moved this back to draft state, as it's something I'll need to put behind a configuration option, so requires more work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-652146451
https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-652146451:98,Modifiability,config,configuration,98,"Thanks @mcovarr, I've moved this back to draft state, as it's something I'll need to put behind a configuration option, so requires more work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-652146451
https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:190,Availability,down,down,190,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330
https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:251,Deployability,configurat,configuration,251,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330
https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:251,Modifiability,config,configuration,251,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330
https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:511,Modifiability,config,config,511,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330
https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:383,Performance,cache,cache,383,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330
https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:118,Security,hash,hash,118,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330
https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:316,Security,hash,hash,316,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330
https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:374,Security,hash,hash,374,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330
https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330:935,Security,hash,hash-lookup,935,"@illusional Thanks for this. I have been looking into (but not having time for) an easy option that would disable the hash lookup altogether. Cromwell connecting to quay.io while quay.io is down causes crashes we do not want in production. There is a configuration option for this. So it was easy. Unfortunately the hash lookup is coupled with the call-caching mechanic. No hash, no cache. Which is something to be aware of. I was wondering if the easiest way wouldn't be to have the lookup be a command in the config. Just like `docker_kill` there could be a `docker_lookup_hash`. That way you can override the default with a custom command that returns a string (https://stackoverflow.com/a/39376254). . For example:; ```; $ docker inspect --format='{{index .RepoDigests 0}}' mysql:5.6; mysql@sha256:19a164794d3cef15c9ac44754604fa079adb448f82d40e4b8be8381148c785fa; ```; This does NOT need the internet. Similarly, this would enable hash-lookup for singularity users as well without internet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660994330
https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660996406:248,Performance,cache,cache,248,"@rhpvorderman ^^ would just need to keep in mind they do the lookup to get the docker size. After BCC2020 I want to revisit this PR, and could allow you to turn off the digest but keep call caching on?. The only catch i imagine is that you'd get a cache miss if you ran with / without that flag (as a digest vs floating tag would miss). I don't know for sure other technical challenges apart from that. --. My Scala is poor, so can only do minimal code level changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-660996406
https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-661652343:614,Performance,cache,cache,614,"> would just need to keep in mind they do the lookup to get the docker size. @illusional well I suppose a `docker_size` argument can just as easily be implemented. After pulling the image can be queried for size. > After BCC2020 I want to revisit this PR, and could allow you to turn off the digest but keep call caching on?. Nope. Not yet anyway. And the code is intrically linked, so it is not going to be a one-liner fix. So this is why I have postponed working on this. This is an interesting thing to revisit at a later date. We use singularity containers on a SLURM backend, using the `singularity-permanent-cache` program to pull the images. For us that really works well, and our login node has contact with the internet, so this change is not really urgent. But for stability it is always nice if an internet connection is not required anymore after all the images are there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-661652343
https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-661652343:688,Testability,log,login,688,"> would just need to keep in mind they do the lookup to get the docker size. @illusional well I suppose a `docker_size` argument can just as easily be implemented. After pulling the image can be queried for size. > After BCC2020 I want to revisit this PR, and could allow you to turn off the digest but keep call caching on?. Nope. Not yet anyway. And the code is intrically linked, so it is not going to be a one-liner fix. So this is why I have postponed working on this. This is an interesting thing to revisit at a later date. We use singularity containers on a SLURM backend, using the `singularity-permanent-cache` program to pull the images. For us that really works well, and our login node has contact with the internet, so this change is not really urgent. But for stability it is always nice if an internet connection is not required anymore after all the images are there.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5545#issuecomment-661652343
https://github.com/broadinstitute/cromwell/pull/5552#issuecomment-648445377:60,Testability,test,tests,60,@aednichols that's correct. There already exists end-to-end tests to check this functionality.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5552#issuecomment-648445377
https://github.com/broadinstitute/cromwell/pull/5562#issuecomment-655116850:15,Deployability,update,updated,15,"Thanks for the updated PR, we will take a look (should we close https://github.com/broadinstitute/cromwell/pull/5523?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5562#issuecomment-655116850
https://github.com/broadinstitute/cromwell/pull/5564#issuecomment-656253905:217,Availability,redundant,redundant,217,@xuf12 thank you for your contribution and for your interest in Cromwell. We merged our changeset in PR https://github.com/broadinstitute/cromwell/pull/5567 so I'm going to go ahead and close this one since it is now redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5564#issuecomment-656253905
https://github.com/broadinstitute/cromwell/pull/5564#issuecomment-656253905:217,Safety,redund,redundant,217,@xuf12 thank you for your contribution and for your interest in Cromwell. We merged our changeset in PR https://github.com/broadinstitute/cromwell/pull/5567 so I'm going to go ahead and close this one since it is now redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5564#issuecomment-656253905
https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971:646,Availability,Failure,Failure,646,"Looks like there is a test that's guarding against my change. I believe this test is incorrect, but I'll leave that confirmation to you (maybe @cjllanwarne as it looks like you wrote the original test):. [`ValueEvaluatorSpec.scala#L538-L542`](https://github.com/broadinstitute/cromwell/blob/50f28da6a665526c1bdb1d5528400ee9deeaa5d4/wdl/model/draft2/src/test/scala/wdl/expression/ValueEvaluatorSpec.scala#L538-L542); ```scala; ""Optional values"" should ""fail to perform addition with the + operator if the argument is None"" in {; val hello = WomString(""hello ""); val noneWorld = WomOptionalValue.none(WomStringType); hello.add(noneWorld) should be(Failure(OptionalNotSuppliedException(""+""))); }; ```. Relevant text from original PR:. The [WDL Spec: _Interpolating and concatenating optional strings_](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#interpolating-and-concatenating-optional-strings). > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971
https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971:460,Performance,perform,perform,460,"Looks like there is a test that's guarding against my change. I believe this test is incorrect, but I'll leave that confirmation to you (maybe @cjllanwarne as it looks like you wrote the original test):. [`ValueEvaluatorSpec.scala#L538-L542`](https://github.com/broadinstitute/cromwell/blob/50f28da6a665526c1bdb1d5528400ee9deeaa5d4/wdl/model/draft2/src/test/scala/wdl/expression/ValueEvaluatorSpec.scala#L538-L542); ```scala; ""Optional values"" should ""fail to perform addition with the + operator if the argument is None"" in {; val hello = WomString(""hello ""); val noneWorld = WomOptionalValue.none(WomStringType); hello.add(noneWorld) should be(Failure(OptionalNotSuppliedException(""+""))); }; ```. Relevant text from original PR:. The [WDL Spec: _Interpolating and concatenating optional strings_](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#interpolating-and-concatenating-optional-strings). > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971
https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971:22,Testability,test,test,22,"Looks like there is a test that's guarding against my change. I believe this test is incorrect, but I'll leave that confirmation to you (maybe @cjllanwarne as it looks like you wrote the original test):. [`ValueEvaluatorSpec.scala#L538-L542`](https://github.com/broadinstitute/cromwell/blob/50f28da6a665526c1bdb1d5528400ee9deeaa5d4/wdl/model/draft2/src/test/scala/wdl/expression/ValueEvaluatorSpec.scala#L538-L542); ```scala; ""Optional values"" should ""fail to perform addition with the + operator if the argument is None"" in {; val hello = WomString(""hello ""); val noneWorld = WomOptionalValue.none(WomStringType); hello.add(noneWorld) should be(Failure(OptionalNotSuppliedException(""+""))); }; ```. Relevant text from original PR:. The [WDL Spec: _Interpolating and concatenating optional strings_](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#interpolating-and-concatenating-optional-strings). > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971
https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971:77,Testability,test,test,77,"Looks like there is a test that's guarding against my change. I believe this test is incorrect, but I'll leave that confirmation to you (maybe @cjllanwarne as it looks like you wrote the original test):. [`ValueEvaluatorSpec.scala#L538-L542`](https://github.com/broadinstitute/cromwell/blob/50f28da6a665526c1bdb1d5528400ee9deeaa5d4/wdl/model/draft2/src/test/scala/wdl/expression/ValueEvaluatorSpec.scala#L538-L542); ```scala; ""Optional values"" should ""fail to perform addition with the + operator if the argument is None"" in {; val hello = WomString(""hello ""); val noneWorld = WomOptionalValue.none(WomStringType); hello.add(noneWorld) should be(Failure(OptionalNotSuppliedException(""+""))); }; ```. Relevant text from original PR:. The [WDL Spec: _Interpolating and concatenating optional strings_](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#interpolating-and-concatenating-optional-strings). > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971
https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971:196,Testability,test,test,196,"Looks like there is a test that's guarding against my change. I believe this test is incorrect, but I'll leave that confirmation to you (maybe @cjllanwarne as it looks like you wrote the original test):. [`ValueEvaluatorSpec.scala#L538-L542`](https://github.com/broadinstitute/cromwell/blob/50f28da6a665526c1bdb1d5528400ee9deeaa5d4/wdl/model/draft2/src/test/scala/wdl/expression/ValueEvaluatorSpec.scala#L538-L542); ```scala; ""Optional values"" should ""fail to perform addition with the + operator if the argument is None"" in {; val hello = WomString(""hello ""); val noneWorld = WomOptionalValue.none(WomStringType); hello.add(noneWorld) should be(Failure(OptionalNotSuppliedException(""+""))); }; ```. Relevant text from original PR:. The [WDL Spec: _Interpolating and concatenating optional strings_](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#interpolating-and-concatenating-optional-strings). > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971
https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971:353,Testability,test,test,353,"Looks like there is a test that's guarding against my change. I believe this test is incorrect, but I'll leave that confirmation to you (maybe @cjllanwarne as it looks like you wrote the original test):. [`ValueEvaluatorSpec.scala#L538-L542`](https://github.com/broadinstitute/cromwell/blob/50f28da6a665526c1bdb1d5528400ee9deeaa5d4/wdl/model/draft2/src/test/scala/wdl/expression/ValueEvaluatorSpec.scala#L538-L542); ```scala; ""Optional values"" should ""fail to perform addition with the + operator if the argument is None"" in {; val hello = WomString(""hello ""); val noneWorld = WomOptionalValue.none(WomStringType); hello.add(noneWorld) should be(Failure(OptionalNotSuppliedException(""+""))); }; ```. Relevant text from original PR:. The [WDL Spec: _Interpolating and concatenating optional strings_](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#interpolating-and-concatenating-optional-strings). > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971
https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-662051248:337,Testability,log,logic,337,"@illusional it looks like you're changing the global behavior of `+` with respect to optional strings, whereas the spec is asking specifically within interpolations. That discrepancy between ""ok in interpolations"" and ""not ok outside interpolations"" is usually handled by catching the `OptionalNotSuppliedException` in the interpolation logic and replacing the entire output with an empty string. I don't know for sure why that's not working in this case but that would be my first suggestion on where to look",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-662051248
https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-662194759:83,Testability,test,testing,83,"Hey @cjllanwarne, I think that's maybe what this PR should be. I'll have a look at testing that today.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-662194759
https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-662279163:120,Testability,test,test,120,"Hey @cjllanwarne, thanks for the suggestion. I've changed it to return an Optional string value and it seems to pass my test. I've force pushed this branch to the new set of changes to re-run the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-662279163
https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-662279163:196,Testability,test,tests,196,"Hey @cjllanwarne, thanks for the suggestion. I've changed it to return an Optional string value and it seems to pass my test. I've force pushed this branch to the new set of changes to re-run the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-662279163
https://github.com/broadinstitute/cromwell/pull/5584#issuecomment-664506839:8,Deployability,integrat,integration,8,for the integration test suggestion https://broadworkbench.atlassian.net/browse/BA-6526,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5584#issuecomment-664506839
https://github.com/broadinstitute/cromwell/pull/5584#issuecomment-664506839:8,Integrability,integrat,integration,8,for the integration test suggestion https://broadworkbench.atlassian.net/browse/BA-6526,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5584#issuecomment-664506839
https://github.com/broadinstitute/cromwell/pull/5584#issuecomment-664506839:20,Testability,test,test,20,for the integration test suggestion https://broadworkbench.atlassian.net/browse/BA-6526,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5584#issuecomment-664506839
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:42,Deployability,configurat,configuration,42,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:851,Deployability,update,update,851,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:42,Modifiability,config,configuration,42,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:894,Modifiability,config,config,894,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:65,Security,access,access,65,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:269,Security,access,access,269,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:330,Security,access,access,330,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:550,Security,audit,audit,550,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:366,Usability,usab,usable,366,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167:668,Usability,usab,usable,668,"> Another option would be to set your VPC configuration to allow access to GCR from inside it. That might be useful in any case since the workflows you run might want to import tools based in external GCR locations.; > ; > I suspect you might need to allow your VPC to access other google services as well anyway - to allow it to access PAPI for example?. Thats not usable option for us. That would allow unaudited containers in our system and we cannot allow that. ; We have own container registry inside vpc and add required containers there after audit. vpc service control (https://cloud.google.com/vpc-service-controls) allows us select witch google services are usable and which are not. Basically this hardcoded container is only problem with our excisting environment. And because of that, we need to build own custom version of Cromwell (and update it), instead of just changing it in config. And it seems that we aren't only ones with same problem (based by comment in jira ticket).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5586#issuecomment-662704167
https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-663231595:72,Integrability,depend,dependencies,72,">The Guava in Cromwell appears to have it though, could add that to the dependencies. I think it'd be better/simpler to use Java 9 for this app rather than include another lib in the jar",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-663231595
https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-663231595:109,Usability,simpl,simpler,109,">The Guava in Cromwell appears to have it though, could add that to the dependencies. I think it'd be better/simpler to use Java 9 for this app rather than include another lib in the jar",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-663231595
https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517:860,Availability,redundant,redundant,860,"@cjllanwarne ; Are you saying that Cromwell is going to determine if the input file is reference file by checking each input file against the manifest file? This is not how I imagined it. I thought manifest file with checksums is only needed to verify file's up-to-dateness. I imagined it work this way: when user creates a WDL and specifies input files for the workflow, they would look like `gs://gcp-public-data--broad-references/some/path/reference_file.txt`. Cromwell will see this path and think ""ok, this file is a reference file, since it's located in this special bucket, so I will mount a references disk to `/mnt/refdisk` and check for this file in the `/mnt/refdisk/some/path/reference_file.txt` location, but before going on and doing that I'll verify that checksum of that file in GCS matches the one in manifest file"".; I mean bucket name seems redundant in this case, since it's the same for all reference files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517
https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517:860,Safety,redund,redundant,860,"@cjllanwarne ; Are you saying that Cromwell is going to determine if the input file is reference file by checking each input file against the manifest file? This is not how I imagined it. I thought manifest file with checksums is only needed to verify file's up-to-dateness. I imagined it work this way: when user creates a WDL and specifies input files for the workflow, they would look like `gs://gcp-public-data--broad-references/some/path/reference_file.txt`. Cromwell will see this path and think ""ok, this file is a reference file, since it's located in this special bucket, so I will mount a references disk to `/mnt/refdisk` and check for this file in the `/mnt/refdisk/some/path/reference_file.txt` location, but before going on and doing that I'll verify that checksum of that file in GCS matches the one in manifest file"".; I mean bucket name seems redundant in this case, since it's the same for all reference files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517
https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517:217,Security,checksum,checksums,217,"@cjllanwarne ; Are you saying that Cromwell is going to determine if the input file is reference file by checking each input file against the manifest file? This is not how I imagined it. I thought manifest file with checksums is only needed to verify file's up-to-dateness. I imagined it work this way: when user creates a WDL and specifies input files for the workflow, they would look like `gs://gcp-public-data--broad-references/some/path/reference_file.txt`. Cromwell will see this path and think ""ok, this file is a reference file, since it's located in this special bucket, so I will mount a references disk to `/mnt/refdisk` and check for this file in the `/mnt/refdisk/some/path/reference_file.txt` location, but before going on and doing that I'll verify that checksum of that file in GCS matches the one in manifest file"".; I mean bucket name seems redundant in this case, since it's the same for all reference files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517
https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517:770,Security,checksum,checksum,770,"@cjllanwarne ; Are you saying that Cromwell is going to determine if the input file is reference file by checking each input file against the manifest file? This is not how I imagined it. I thought manifest file with checksums is only needed to verify file's up-to-dateness. I imagined it work this way: when user creates a WDL and specifies input files for the workflow, they would look like `gs://gcp-public-data--broad-references/some/path/reference_file.txt`. Cromwell will see this path and think ""ok, this file is a reference file, since it's located in this special bucket, so I will mount a references disk to `/mnt/refdisk` and check for this file in the `/mnt/refdisk/some/path/reference_file.txt` location, but before going on and doing that I'll verify that checksum of that file in GCS matches the one in manifest file"".; I mean bucket name seems redundant in this case, since it's the same for all reference files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664613517
https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664618643:233,Availability,avail,available,233,"@grsterin yes - I [would expect](https://docs.google.com/document/d/1rvLeQYHJATz17VLGJ7xtJjYA0yqrDW_HQ7ja9WCV2-c/edit) we would do a one-time reading of the manifest at start-up time and use that listing to decide whether a file was available on a reference disk or not. We don't know that there will only be one bucket, and in the future people might want to bring their own reference disks containing files from their own buckets. Instead of making the bucket a magic value that's hard-coded into Cromwell I think it's better to have it as part of the reference file path in the manifest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664618643
https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801:183,Deployability,configurat,configuration,183,"@cjllanwarne sorry, I probably missed that part when I was reviewing the Google doc.; I'm not sure I agree with that approach. Reference bucket names can be configurable via Cromwell configuration file.; I'm not sure there will be more than one official bucket with references in future, but if there will be, we may create different disk images and manifests for different buckets (and each bucket contains it's own manifest file) rather than mixing them all together. This approach would also give us enough flexibility for ""user's own reference buckets/disks use-case"". I envisioned a configuration like this:; ```; backend {; PAPIv2beta {; reference-data-conf {; reference-buckets: [; { broad_official_bucket_name: broad_official_disk_image_locator },; { broad_official_bucket_another_one_name: broad_official_disk_image_another_one_locator },; { users_private_bucket_name: users_private_disk_image_locator }; ]; }; }; }; ```. In this example Cromwell then may check the input file GCS path against the configured list of reference buckets and figure out which image to mount based on that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801
https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801:588,Deployability,configurat,configuration,588,"@cjllanwarne sorry, I probably missed that part when I was reviewing the Google doc.; I'm not sure I agree with that approach. Reference bucket names can be configurable via Cromwell configuration file.; I'm not sure there will be more than one official bucket with references in future, but if there will be, we may create different disk images and manifests for different buckets (and each bucket contains it's own manifest file) rather than mixing them all together. This approach would also give us enough flexibility for ""user's own reference buckets/disks use-case"". I envisioned a configuration like this:; ```; backend {; PAPIv2beta {; reference-data-conf {; reference-buckets: [; { broad_official_bucket_name: broad_official_disk_image_locator },; { broad_official_bucket_another_one_name: broad_official_disk_image_another_one_locator },; { users_private_bucket_name: users_private_disk_image_locator }; ]; }; }; }; ```. In this example Cromwell then may check the input file GCS path against the configured list of reference buckets and figure out which image to mount based on that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801
https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801:157,Modifiability,config,configurable,157,"@cjllanwarne sorry, I probably missed that part when I was reviewing the Google doc.; I'm not sure I agree with that approach. Reference bucket names can be configurable via Cromwell configuration file.; I'm not sure there will be more than one official bucket with references in future, but if there will be, we may create different disk images and manifests for different buckets (and each bucket contains it's own manifest file) rather than mixing them all together. This approach would also give us enough flexibility for ""user's own reference buckets/disks use-case"". I envisioned a configuration like this:; ```; backend {; PAPIv2beta {; reference-data-conf {; reference-buckets: [; { broad_official_bucket_name: broad_official_disk_image_locator },; { broad_official_bucket_another_one_name: broad_official_disk_image_another_one_locator },; { users_private_bucket_name: users_private_disk_image_locator }; ]; }; }; }; ```. In this example Cromwell then may check the input file GCS path against the configured list of reference buckets and figure out which image to mount based on that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801
https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801:183,Modifiability,config,configuration,183,"@cjllanwarne sorry, I probably missed that part when I was reviewing the Google doc.; I'm not sure I agree with that approach. Reference bucket names can be configurable via Cromwell configuration file.; I'm not sure there will be more than one official bucket with references in future, but if there will be, we may create different disk images and manifests for different buckets (and each bucket contains it's own manifest file) rather than mixing them all together. This approach would also give us enough flexibility for ""user's own reference buckets/disks use-case"". I envisioned a configuration like this:; ```; backend {; PAPIv2beta {; reference-data-conf {; reference-buckets: [; { broad_official_bucket_name: broad_official_disk_image_locator },; { broad_official_bucket_another_one_name: broad_official_disk_image_another_one_locator },; { users_private_bucket_name: users_private_disk_image_locator }; ]; }; }; }; ```. In this example Cromwell then may check the input file GCS path against the configured list of reference buckets and figure out which image to mount based on that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801
https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801:588,Modifiability,config,configuration,588,"@cjllanwarne sorry, I probably missed that part when I was reviewing the Google doc.; I'm not sure I agree with that approach. Reference bucket names can be configurable via Cromwell configuration file.; I'm not sure there will be more than one official bucket with references in future, but if there will be, we may create different disk images and manifests for different buckets (and each bucket contains it's own manifest file) rather than mixing them all together. This approach would also give us enough flexibility for ""user's own reference buckets/disks use-case"". I envisioned a configuration like this:; ```; backend {; PAPIv2beta {; reference-data-conf {; reference-buckets: [; { broad_official_bucket_name: broad_official_disk_image_locator },; { broad_official_bucket_another_one_name: broad_official_disk_image_another_one_locator },; { users_private_bucket_name: users_private_disk_image_locator }; ]; }; }; }; ```. In this example Cromwell then may check the input file GCS path against the configured list of reference buckets and figure out which image to mount based on that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801
https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801:1007,Modifiability,config,configured,1007,"@cjllanwarne sorry, I probably missed that part when I was reviewing the Google doc.; I'm not sure I agree with that approach. Reference bucket names can be configurable via Cromwell configuration file.; I'm not sure there will be more than one official bucket with references in future, but if there will be, we may create different disk images and manifests for different buckets (and each bucket contains it's own manifest file) rather than mixing them all together. This approach would also give us enough flexibility for ""user's own reference buckets/disks use-case"". I envisioned a configuration like this:; ```; backend {; PAPIv2beta {; reference-data-conf {; reference-buckets: [; { broad_official_bucket_name: broad_official_disk_image_locator },; { broad_official_bucket_another_one_name: broad_official_disk_image_another_one_locator },; { users_private_bucket_name: users_private_disk_image_locator }; ]; }; }; }; ```. In this example Cromwell then may check the input file GCS path against the configured list of reference buckets and figure out which image to mount based on that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-664629801
https://github.com/broadinstitute/cromwell/pull/5588#issuecomment-663632428:68,Testability,test,tests,68,There're several comments from me regarding usage of while loops in tests (the same ones as in another PR). Otherwise LGTM.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5588#issuecomment-663632428
https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078:65,Availability,Ping,Pinging,65,"Looks ok as a patch-- but that's me not being an expert at FSMs. Pinging @salonishah11 to see if I can get her expert opinion? . When things calm down, I'll also want to know if we want to patch now and add tests (even unit tests?) later, or we want to wait and patch -and- test at the same time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078
https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078:146,Availability,down,down,146,"Looks ok as a patch-- but that's me not being an expert at FSMs. Pinging @salonishah11 to see if I can get her expert opinion? . When things calm down, I'll also want to know if we want to patch now and add tests (even unit tests?) later, or we want to wait and patch -and- test at the same time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078
https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078:14,Deployability,patch,patch,14,"Looks ok as a patch-- but that's me not being an expert at FSMs. Pinging @salonishah11 to see if I can get her expert opinion? . When things calm down, I'll also want to know if we want to patch now and add tests (even unit tests?) later, or we want to wait and patch -and- test at the same time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078
https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078:189,Deployability,patch,patch,189,"Looks ok as a patch-- but that's me not being an expert at FSMs. Pinging @salonishah11 to see if I can get her expert opinion? . When things calm down, I'll also want to know if we want to patch now and add tests (even unit tests?) later, or we want to wait and patch -and- test at the same time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078
https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078:262,Deployability,patch,patch,262,"Looks ok as a patch-- but that's me not being an expert at FSMs. Pinging @salonishah11 to see if I can get her expert opinion? . When things calm down, I'll also want to know if we want to patch now and add tests (even unit tests?) later, or we want to wait and patch -and- test at the same time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078
https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078:207,Testability,test,tests,207,"Looks ok as a patch-- but that's me not being an expert at FSMs. Pinging @salonishah11 to see if I can get her expert opinion? . When things calm down, I'll also want to know if we want to patch now and add tests (even unit tests?) later, or we want to wait and patch -and- test at the same time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078
https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078:224,Testability,test,tests,224,"Looks ok as a patch-- but that's me not being an expert at FSMs. Pinging @salonishah11 to see if I can get her expert opinion? . When things calm down, I'll also want to know if we want to patch now and add tests (even unit tests?) later, or we want to wait and patch -and- test at the same time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078
https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078:274,Testability,test,test,274,"Looks ok as a patch-- but that's me not being an expert at FSMs. Pinging @salonishah11 to see if I can get her expert opinion? . When things calm down, I'll also want to know if we want to patch now and add tests (even unit tests?) later, or we want to wait and patch -and- test at the same time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-694897196:177,Deployability,pipeline,pipelines,177,"I am having the same problem. [I think the issue may be here.](https://github.com/broadinstitute/cromwell/blob/b4ec53e0f038c3e27a7a3a8b483066c962cc164d/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/PipelinesApiAsyncBackendJobExecutionActor.scala#L135). This seems to be where we check which outputs are type File or Directory, I think it's perhaps missing File-typed outputs within structs?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-694897196
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-694897196:235,Deployability,pipeline,pipelines,235,"I am having the same problem. [I think the issue may be here.](https://github.com/broadinstitute/cromwell/blob/b4ec53e0f038c3e27a7a3a8b483066c962cc164d/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/PipelinesApiAsyncBackendJobExecutionActor.scala#L135). This seems to be where we check which outputs are type File or Directory, I think it's perhaps missing File-typed outputs within structs?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-694897196
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-694897196:254,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,254,"I am having the same problem. [I think the issue may be here.](https://github.com/broadinstitute/cromwell/blob/b4ec53e0f038c3e27a7a3a8b483066c962cc164d/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/PipelinesApiAsyncBackendJobExecutionActor.scala#L135). This seems to be where we check which outputs are type File or Directory, I think it's perhaps missing File-typed outputs within structs?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-694897196
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386:758,Availability,echo,echo,758,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386:991,Availability,error,error,991,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386:1292,Safety,Abort,Aborting,1292,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386:1334,Safety,Abort,Aborting,1334,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386:1382,Safety,Abort,Aborted,1382,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386:101,Testability,Test,Test,101,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386:144,Testability,Test,Test,144,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386:216,Testability,test,teste,216,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386:1022,Usability,simpl,simple,1022,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386:1209,Usability,simpl,simply,1209,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1017704386
https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1442197318:13,Security,access,access,13,"I don't have access to broad jira so I'm wondering if there is any progress on this bug? @rsasch . We get into the same trouble here. We run our WDL on AWS with batch backend. To share a little more info in addition to what people already see, I saw in the `DELOCALIZING OUTPUTS` section ""reconfigured-script.sh"" I noticed it failed to delocalize files in `Array[File]` in our struct just like what others see. It seems those files are skipped and not ""scanned"" just like @hkeward pointed out above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592#issuecomment-1442197318
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:132,Availability,error,error,132,"My admin has added the policy serviceusage.services.use to my Service Account, whatever that means (I have no idea). Now I get this error:; ```; [2020-07-27 19:13:48,68] [error] PipelinesApiAsyncBackendJobExecutionActor [bf8fa2c2wf_hello.hello:NA:1]: Error attempting to Execute; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Futur",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:171,Availability,error,error,171,"My admin has added the policy serviceusage.services.use to my Service Account, whatever that means (I have no idea). Now I get this error:; ```; [2020-07-27 19:13:48,68] [error] PipelinesApiAsyncBackendJobExecutionActor [bf8fa2c2wf_hello.hello:NA:1]: Error attempting to Execute; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Futur",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:251,Availability,Error,Error,251,"My admin has added the policy serviceusage.services.use to my Service Account, whatever that means (I have no idea). Now I get this error:; ```; [2020-07-27 19:13:48,68] [error] PipelinesApiAsyncBackendJobExecutionActor [bf8fa2c2wf_hello.hello:NA:1]: Error attempting to Execute; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Futur",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:3160,Availability,error,error,3160,"s.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; And instead of terminating immediately, I keep getting the same error multiple times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:178,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,178,"My admin has added the policy serviceusage.services.use to my Service Account, whatever that means (I have no idea). Now I get this error:; ```; [2020-07-27 19:13:48,68] [error] PipelinesApiAsyncBackendJobExecutionActor [bf8fa2c2wf_hello.hello:NA:1]: Error attempting to Execute; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Futur",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1295,Deployability,pipeline,pipelines,1295, Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.Batchin,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1404,Deployability,pipeline,pipelines,1404,pes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1433,Deployability,Pipeline,PipelinesApiRunCreationClient,1433,h.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1475,Deployability,Pipeline,PipelinesApiRunCreationClient,1475,efreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withB,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1544,Deployability,pipeline,pipelines,1544,ogle.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExe,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1573,Deployability,Pipeline,PipelinesApiRunCreationClient,1573,esh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecut,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1616,Deployability,Pipeline,PipelinesApiRunCreationClient,1616, com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskI,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1685,Deployability,pipeline,pipelines,1685,2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.For,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1702,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1702,oogle.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaFor,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1756,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1756,equestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scal,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1837,Deployability,pipeline,pipelines,1837,.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1854,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1854,ialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1921,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1921,.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:312,Modifiability,config,configured,312,"My admin has added the policy serviceusage.services.use to my Service Account, whatever that means (I have no idea). Now I get this error:; ```; [2020-07-27 19:13:48,68] [error] PipelinesApiAsyncBackendJobExecutionActor [bf8fa2c2wf_hello.hello:NA:1]: Error attempting to Execute; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Futur",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1985,Performance,concurren,concurrent,1985,actory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinP,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:2051,Performance,concurren,concurrent,2051,tractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.ru,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:2129,Performance,concurren,concurrent,2129,"); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; And instead of terminating immediately,",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:2454,Performance,concurren,concurrent,2454,"s.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; And instead of terminating immediately, I keep getting the same error multiple times.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665234059:77,Integrability,message,message,77,"Hi @freeseek,. (replying here because this is the currently open issue). The message; ```; 400 Bucket is requester pays bucket but no user project provided.; ```; should be addressed by specifying your project in the config as described [here](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665234059
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665234059:217,Modifiability,config,config,217,"Hi @freeseek,. (replying here because this is the currently open issue). The message; ```; 400 Bucket is requester pays bucket but no user project provided.; ```; should be addressed by specifying your project in the config as described [here](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665234059
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665240872:18,Availability,error,error,18,As for; ```; PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; it sounds like you need to access Google Cloud Console and enable this permission for your project (Cromwell cannot perform this step for you automatically),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665240872
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665240872:214,Performance,perform,perform,214,As for; ```; PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; it sounds like you need to access Google Cloud Console and enable this permission for your project (Cromwell cannot perform this step for you automatically),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665240872
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665240872:125,Security,access,access,125,As for; ```; PAPI error code 7. Required 'compute.zones.list' permission for 'projects/xxx'; ```; it sounds like you need to access Google Cloud Console and enable this permission for your project (Cromwell cannot perform this step for you automatically),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665240872
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:620,Availability,ERROR,ERROR,620,"The problem is that when I started to use Cromwell I was under the impression that I would get shielded from having to learn all the complexities of a given backend and Cromwell would take care of it. I have zero familiarity with permissions in Google Cloud. The admin that set up my account also had no idea what `compute.zones.list` mean. I don't see this in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; //",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:766,Availability,ERROR,ERROR,766,"The problem is that when I started to use Cromwell I was under the impression that I would get shielded from having to learn all the complexities of a given backend and Cromwell would take care of it. I have zero familiarity with permissions in Google Cloud. The admin that set up my account also had no idea what `compute.zones.list` mean. I don't see this in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; //",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:2828,Availability,error,error,2828,"nfig {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:2883,Availability,failure,failure,2883,"nfig {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:3262,Availability,error,error,3262," Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:3522,Availability,error,error,3522,"his allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4798,Availability,error,error,4798," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:5165,Availability,ERROR,ERROR,5165," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:5311,Availability,ERROR,ERROR,5311," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1223,Deployability,configurat,configuration,1223,"ons in Google Cloud. The admin that set up my account also had no idea what `compute.zones.list` mean. I don't see this in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A refere",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1383,Deployability,configurat,configuration,1383,"tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, n",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:2328,Deployability,Pipeline,Pipelines,2328,"wo, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Wo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:2676,Deployability,Pipeline,Pipelines,2676,"nfig {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:3968,Deployability,Pipeline,Pipelines,3968,"ia engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4227,Deployability,Pipeline,Pipelines,4227,"```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud al",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4564,Deployability,configurat,configuration,4564," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1223,Modifiability,config,configuration,1223,"ons in Google Cloud. The admin that set up my account also had no idea what `compute.zones.list` mean. I don't see this in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A refere",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1383,Modifiability,config,configuration,1383,"tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, n",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1803,Modifiability,config,config,1803,"ding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4564,Modifiability,config,configuration,4564," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:2136,Security,access,access,2136,"orage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4851,Security,Access,AccessDeniedException,4851," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:4941,Security,access,access,4941," failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/storage.objects.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.list is not supported for this resource.; ```; No luck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:2802,Testability,log,logic,2802,"nfig {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:3573,Testability,log,logs,3573,"service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:3730,Testability,log,log,3730,"service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:3752,Testability,log,log,3752,"or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:119,Usability,learn,learn,119,"The problem is that when I started to use Cromwell I was under the impression that I would get shielded from having to learn all the complexities of a given backend and Cromwell would take care of it. I have zero familiarity with permissions in Google Cloud. The admin that set up my account also had no idea what `compute.zones.list` mean. I don't see this in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; //",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:956,Usability,simpl,simply,956,"The problem is that when I started to use Cromwell I was under the impression that I would get shielded from having to learn all the complexities of a given backend and Cromwell would take care of it. I have zero familiarity with permissions in Google Cloud. The admin that set up my account also had no idea what `compute.zones.list` mean. I don't see this in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; //",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424:153,Availability,error,error,153,"> Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so?. Granted it's not in the error message itself, but the [page I linked](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) states. > Pipelines API version 1 does not support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will not work. Pipelines API v1 is deprecated by Google and documentation for it is not maintained; new projects should always use v2. ---. As for the `gcloud` issue I've never done this particular operation personally, but I suspect you may have luck looking at the GCP docs or Stack Overflow. You could opt for [Terra](https://app.terra.bio/) which is basically a fully managed version of Cromwell (it configures Cromwell and all of this project stuff for you). Hope this helps.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424:18,Deployability,Pipeline,Pipelines,18,"> Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so?. Granted it's not in the error message itself, but the [page I linked](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) states. > Pipelines API version 1 does not support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will not work. Pipelines API v1 is deprecated by Google and documentation for it is not maintained; new projects should always use v2. ---. As for the `gcloud` issue I've never done this particular operation personally, but I suspect you may have luck looking at the GCP docs or Stack Overflow. You could opt for [Terra](https://app.terra.bio/) which is basically a fully managed version of Cromwell (it configures Cromwell and all of this project stuff for you). Hope this helps.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424:299,Deployability,Pipeline,Pipelines,299,"> Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so?. Granted it's not in the error message itself, but the [page I linked](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) states. > Pipelines API version 1 does not support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will not work. Pipelines API v1 is deprecated by Google and documentation for it is not maintained; new projects should always use v2. ---. As for the `gcloud` issue I've never done this particular operation personally, but I suspect you may have luck looking at the GCP docs or Stack Overflow. You could opt for [Terra](https://app.terra.bio/) which is basically a fully managed version of Cromwell (it configures Cromwell and all of this project stuff for you). Hope this helps.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424:450,Deployability,Pipeline,Pipelines,450,"> Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so?. Granted it's not in the error message itself, but the [page I linked](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) states. > Pipelines API version 1 does not support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will not work. Pipelines API v1 is deprecated by Google and documentation for it is not maintained; new projects should always use v2. ---. As for the `gcloud` issue I've never done this particular operation personally, but I suspect you may have luck looking at the GCP docs or Stack Overflow. You could opt for [Terra](https://app.terra.bio/) which is basically a fully managed version of Cromwell (it configures Cromwell and all of this project stuff for you). Hope this helps.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424:516,Deployability,Pipeline,Pipelines,516,"> Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so?. Granted it's not in the error message itself, but the [page I linked](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) states. > Pipelines API version 1 does not support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will not work. Pipelines API v1 is deprecated by Google and documentation for it is not maintained; new projects should always use v2. ---. As for the `gcloud` issue I've never done this particular operation personally, but I suspect you may have luck looking at the GCP docs or Stack Overflow. You could opt for [Terra](https://app.terra.bio/) which is basically a fully managed version of Cromwell (it configures Cromwell and all of this project stuff for you). Hope this helps.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424:159,Integrability,message,message,159,"> Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so?. Granted it's not in the error message itself, but the [page I linked](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) states. > Pipelines API version 1 does not support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will not work. Pipelines API v1 is deprecated by Google and documentation for it is not maintained; new projects should always use v2. ---. As for the `gcloud` issue I've never done this particular operation personally, but I suspect you may have luck looking at the GCP docs or Stack Overflow. You could opt for [Terra](https://app.terra.bio/) which is basically a fully managed version of Cromwell (it configures Cromwell and all of this project stuff for you). Hope this helps.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424:905,Modifiability,config,configures,905,"> Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so?. Granted it's not in the error message itself, but the [page I linked](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) states. > Pipelines API version 1 does not support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will not work. Pipelines API v1 is deprecated by Google and documentation for it is not maintained; new projects should always use v2. ---. As for the `gcloud` issue I've never done this particular operation personally, but I suspect you may have luck looking at the GCP docs or Stack Overflow. You could opt for [Terra](https://app.terra.bio/) which is basically a fully managed version of Cromwell (it configures Cromwell and all of this project stuff for you). Hope this helps.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424:411,Security,access,access,411,"> Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so?. Granted it's not in the error message itself, but the [page I linked](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) states. > Pipelines API version 1 does not support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will not work. Pipelines API v1 is deprecated by Google and documentation for it is not maintained; new projects should always use v2. ---. As for the `gcloud` issue I've never done this particular operation personally, but I suspect you may have luck looking at the GCP docs or Stack Overflow. You could opt for [Terra](https://app.terra.bio/) which is basically a fully managed version of Cromwell (it configures Cromwell and all of this project stuff for you). Hope this helps.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:1503,Availability,error,error,1503,"engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_M",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:1921,Availability,Error,Error,1921,"cloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. O",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:130,Deployability,configurat,configuration,130,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:510,Deployability,configurat,configuration,510,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:575,Deployability,configurat,configuration,575,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:753,Deployability,configurat,configuration,753,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:822,Deployability,configurat,configuration,822,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:130,Modifiability,config,configuration,130,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:510,Modifiability,config,configuration,510,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:575,Modifiability,config,configuration,575,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:753,Modifiability,config,configuration,753,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:822,Modifiability,config,configuration,822,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:1028,Modifiability,config,config,1028,"ibing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:2157,Modifiability,config,config,2157,"rovided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:2619,Security,Access,AccessDeniedException,2619,"t least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. Maybe once you have a polished WDL it is great for users with less technical expertise. But I want to use Cromwell to develop and test new WDLs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:2709,Security,access,access,2709,"t least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. Maybe once you have a polished WDL it is great for users with less technical expertise. But I want to use Cromwell to develop and test new WDLs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:2923,Security,access,access,2923,"t least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. Maybe once you have a polished WDL it is great for users with less technical expertise. But I want to use Cromwell to develop and test new WDLs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:2988,Security,access,access,2988,"t least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. Maybe once you have a polished WDL it is great for users with less technical expertise. But I want to use Cromwell to develop and test new WDLs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:975,Testability,log,logs,975,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:1112,Testability,log,logs,1112,"ibing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:1213,Testability,log,log,1213,"[page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:1384,Testability,log,logs,1384,"I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LO",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:3271,Testability,test,testing,3271,"t least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. Maybe once you have a polished WDL it is great for users with less technical expertise. But I want to use Cromwell to develop and test new WDLs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:3433,Testability,test,test,3433,"t least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. Maybe once you have a polished WDL it is great for users with less technical expertise. But I want to use Cromwell to develop and test new WDLs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:181,Usability,simpl,simple,181,"However the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/), right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1 nor links to the useful [page](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) you mentioned. I have now switched to the [PAPIv2.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) configuration file which does not contain the important piece of configuration code:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; project = ""<google-billing-project-id>""; }; }; }; ```; This was in the google.conf PAPIv1 configuration file. I guess somehow it did not make it in the PAPIv2 configuration file and users reading the tutorial have the guess that on their own. Now the Requester Pays issue is gone as I get lines like this in the logs instead:; ```; 2020/07/28 21:30:48 rm -f $HOME/.config/gcloud/gce && gsutil -h ""Content-Type: text/plain; charset=UTF-8"" cp /google/logs/output gs://xxx/Mutect2/74c8be5e-f988-49b0-a51d-c87f2ac7cb60/call-TumorCramToBam/TumorCramToBam.log failed; BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; 2020/07/28 21:30:48 Retrying with user project; Copying file:///google/logs/output [Content-Type=text/plain; charset=UTF-8]...; ```; At least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885:2905,Usability,intuit,intuitive,2905,"t least that's fully clarified. However I still get the error:; ```; 2020/07/28 21:30:43 Localizing input gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram -> /cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram; Error attempting to localize file with command: 'mkdir -p '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/' && rm -f /root/.config/gcloud/gce && gsutil -o 'GSUtil:parallel_thread_count=1' -o 'GSUtil:sliced_object_download_max_components=1' cp 'gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram' '/cromwell_root/fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/''; AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I am starting to guess that this is a settings issue with the bucket, not with my service account. My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... As for Terra, I have used it quite a bit for the last week but, and I am not alone in saying this, Terra is not a good environment for development of new WDLs. For example, just to upload a new WDL for testing it takes so many steps. Maybe once you have a polished WDL it is great for users with less technical expertise. But I want to use Cromwell to develop and test new WDLs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665298885
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:52,Deployability,configurat,configuration,52,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:52,Modifiability,config,configuration,52,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:927,Modifiability,config,configured,927,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:494,Security,access,access,494,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:559,Security,access,access,559,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:889,Security,access,access,889,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:1312,Security,access,access,1312,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:103,Usability,simpl,simple,103,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550:476,Usability,intuit,intuitive,476,"> the tutorial, right in the section describing the configuration file for PAPIv1, neither states this simple fact about Requester Pays not working with PAPIv1. We should probably remove the PAPIv1 tutorial entirely, it has carried the deprecation warning for over a year now. It lives [here](https://github.com/broadinstitute/cromwell/blob/develop/docs/tutorials/PipelinesApi101.md) and we will gladly merge improvement PRs. > My best guess is that, albeit extremely counter-intuitive, I have access to this bucket with my personal account but I do not have access to this bucket with my service account. Oh my, this is so complicated ... That does seem like a probable explanation, though I don't know the particulars of how you set up your SA. Cloud architecture is a large beast and Cromwell targets a very specific cross section of it (running workflows). A particular account having access to input data would need to be configured as a prerequisite. Since I see you are at Broad, perhaps BITS can help with it. > `storage.objects.list` issue. I recommend trying to recreate the scenario locally with `gsutil cp` and the desired service account & file. Your turnaround time will be much faster than running the workflow. It is certainly possible that Cromwell has a bug that causes GCS to incorrectly deny access, but we generally would like to see the same file/account combination working correctly outside of Cromwell before we will accept it as a bug report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665310550
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782:232,Deployability,Update,Updated,232,"This is a way to reproduce with `gsutil cp`, as @aednichols suggested (in my case Cromwell runs with service account `30148356615-compute@developer.gserviceaccount.com`):; ```; $ gcloud config set account giulio@broadinstitute.org; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; * giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/. Copying gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai...; / [1 files][143.2 KiB/143.2 KiB]; Operation completed over 1 objects/143.2 KiB.; $ gcloud config set account 30148356615-compute@developer.gserviceaccount.com; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; * 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/; AccessDeniedException: 403 30148356615-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```. So in this case the more appropriate questions would be:; 1) How do I get to have my service account `30148356615-compute@developer.gserviceaccount.com` have the same permissions as my personal account `giulio@broadinstitute.org`?; 2) How do I get Cromwell to run with my personal account `giulio@broadinstit",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782:1059,Deployability,Update,Updated,1059,"cp`, as @aednichols suggested (in my case Cromwell runs with service account `30148356615-compute@developer.gserviceaccount.com`):; ```; $ gcloud config set account giulio@broadinstitute.org; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; * giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/. Copying gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai...; / [1 files][143.2 KiB/143.2 KiB]; Operation completed over 1 objects/143.2 KiB.; $ gcloud config set account 30148356615-compute@developer.gserviceaccount.com; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; * 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/; AccessDeniedException: 403 30148356615-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```. So in this case the more appropriate questions would be:; 1) How do I get to have my service account `30148356615-compute@developer.gserviceaccount.com` have the same permissions as my personal account `giulio@broadinstitute.org`?; 2) How do I get Cromwell to run with my personal account `giulio@broadinstitute.org` instead of my service account?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782:186,Modifiability,config,config,186,"This is a way to reproduce with `gsutil cp`, as @aednichols suggested (in my case Cromwell runs with service account `30148356615-compute@developer.gserviceaccount.com`):; ```; $ gcloud config set account giulio@broadinstitute.org; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; * giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/. Copying gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai...; / [1 files][143.2 KiB/143.2 KiB]; Operation completed over 1 objects/143.2 KiB.; $ gcloud config set account 30148356615-compute@developer.gserviceaccount.com; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; * 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/; AccessDeniedException: 403 30148356615-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```. So in this case the more appropriate questions would be:; 1) How do I get to have my service account `30148356615-compute@developer.gserviceaccount.com` have the same permissions as my personal account `giulio@broadinstitute.org`?; 2) How do I get Cromwell to run with my personal account `giulio@broadinstit",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782:474,Modifiability,config,config,474,"This is a way to reproduce with `gsutil cp`, as @aednichols suggested (in my case Cromwell runs with service account `30148356615-compute@developer.gserviceaccount.com`):; ```; $ gcloud config set account giulio@broadinstitute.org; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; * giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/. Copying gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai...; / [1 files][143.2 KiB/143.2 KiB]; Operation completed over 1 objects/143.2 KiB.; $ gcloud config set account 30148356615-compute@developer.gserviceaccount.com; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; * 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/; AccessDeniedException: 403 30148356615-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```. So in this case the more appropriate questions would be:; 1) How do I get to have my service account `30148356615-compute@developer.gserviceaccount.com` have the same permissions as my personal account `giulio@broadinstitute.org`?; 2) How do I get Cromwell to run with my personal account `giulio@broadinstit",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782:989,Modifiability,config,config,989,"a way to reproduce with `gsutil cp`, as @aednichols suggested (in my case Cromwell runs with service account `30148356615-compute@developer.gserviceaccount.com`):; ```; $ gcloud config set account giulio@broadinstitute.org; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; * giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/. Copying gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai...; / [1 files][143.2 KiB/143.2 KiB]; Operation completed over 1 objects/143.2 KiB.; $ gcloud config set account 30148356615-compute@developer.gserviceaccount.com; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; * 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/; AccessDeniedException: 403 30148356615-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```. So in this case the more appropriate questions would be:; 1) How do I get to have my service account `30148356615-compute@developer.gserviceaccount.com` have the same permissions as my personal account `giulio@broadinstitute.org`?; 2) How do I get Cromwell to run with my personal account `giulio@broadinstitute.org",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782:1301,Modifiability,config,config,1301,"cp`, as @aednichols suggested (in my case Cromwell runs with service account `30148356615-compute@developer.gserviceaccount.com`):; ```; $ gcloud config set account giulio@broadinstitute.org; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; * giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/. Copying gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai...; / [1 files][143.2 KiB/143.2 KiB]; Operation completed over 1 objects/143.2 KiB.; $ gcloud config set account 30148356615-compute@developer.gserviceaccount.com; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; * 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/; AccessDeniedException: 403 30148356615-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```. So in this case the more appropriate questions would be:; 1) How do I get to have my service account `30148356615-compute@developer.gserviceaccount.com` have the same permissions as my personal account `giulio@broadinstitute.org`?; 2) How do I get Cromwell to run with my personal account `giulio@broadinstitute.org` instead of my service account?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782:1532,Security,Access,AccessDeniedException,1532,"cp`, as @aednichols suggested (in my case Cromwell runs with service account `30148356615-compute@developer.gserviceaccount.com`):; ```; $ gcloud config set account giulio@broadinstitute.org; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; * giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/. Copying gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai...; / [1 files][143.2 KiB/143.2 KiB]; Operation completed over 1 objects/143.2 KiB.; $ gcloud config set account 30148356615-compute@developer.gserviceaccount.com; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; * 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/; AccessDeniedException: 403 30148356615-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```. So in this case the more appropriate questions would be:; 1) How do I get to have my service account `30148356615-compute@developer.gserviceaccount.com` have the same permissions as my personal account `giulio@broadinstitute.org`?; 2) How do I get Cromwell to run with my personal account `giulio@broadinstitute.org` instead of my service account?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782:1644,Security,access,access,1644,"cp`, as @aednichols suggested (in my case Cromwell runs with service account `30148356615-compute@developer.gserviceaccount.com`):; ```; $ gcloud config set account giulio@broadinstitute.org; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; * giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/. Copying gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai...; / [1 files][143.2 KiB/143.2 KiB]; Operation completed over 1 objects/143.2 KiB.; $ gcloud config set account 30148356615-compute@developer.gserviceaccount.com; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; * 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/; AccessDeniedException: 403 30148356615-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```. So in this case the more appropriate questions would be:; 1) How do I get to have my service account `30148356615-compute@developer.gserviceaccount.com` have the same permissions as my personal account `giulio@broadinstitute.org`?; 2) How do I get Cromwell to run with my personal account `giulio@broadinstitute.org` instead of my service account?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:638,Availability,error,errors,638,"Since this post was about the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) being quite broken, here a few points:. 1) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; #Create a new service account called ""MyServiceAccount"", and from the output of the command, take the email address that was generated; EMAIL=$(gcloud beta iam service-accounts create MyServiceAccount --description ""to run cromwell"" --display-name ""cromwell service account"" --format json | jq '.email' | sed -e 's/\""//g'); ```; does not work. It errors out with:; ```; ERROR: (gcloud.beta.iam.service-accounts.create) argument NAME: Bad value [MyServiceAccount]: Service account name must be between 6 and 30 characters (inclusive), must begin with a lowercase letter, and consist of lowercase alphanumeric characters that can be separated by hyphens.; ```; I believe `MyServiceAccount` needs to change to `my-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:661,Availability,ERROR,ERROR,661,"Since this post was about the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) being quite broken, here a few points:. 1) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; #Create a new service account called ""MyServiceAccount"", and from the output of the command, take the email address that was generated; EMAIL=$(gcloud beta iam service-accounts create MyServiceAccount --description ""to run cromwell"" --display-name ""cromwell service account"" --format json | jq '.email' | sed -e 's/\""//g'); ```; does not work. It errors out with:; ```; ERROR: (gcloud.beta.iam.service-accounts.create) argument NAME: Bad value [MyServiceAccount]: Service account name must be between 6 and 30 characters (inclusive), must begin with a lowercase letter, and consist of lowercase alphanumeric characters that can be separated by hyphens.; ```; I believe `MyServiceAccount` needs to change to `my-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:1662,Availability,error,errors,1662,"service-accounts.create) argument NAME: Bad value [MyServiceAccount]: Service account name must be between 6 and 30 characters (inclusive), must begin with a lowercase letter, and consist of lowercase alphanumeric characters that can be separated by hyphens.; ```; I believe `MyServiceAccount` needs to change to `my-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:1685,Availability,ERROR,ERROR,1685,"service-accounts.create) argument NAME: Bad value [MyServiceAccount]: Service account name must be between 6 and 30 characters (inclusive), must begin with a lowercase letter, and consist of lowercase alphanumeric characters that can be separated by hyphens.; ```; I believe `MyServiceAccount` needs to change to `my-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:1831,Availability,ERROR,ERROR,1831,"ith a lowercase letter, and consist of lowercase alphanumeric characters that can be separated by hyphens.; ```; I believe `MyServiceAccount` needs to change to `my-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:1395,Deployability,pipeline,pipelinesRunner,1395,"that was generated; EMAIL=$(gcloud beta iam service-accounts create MyServiceAccount --description ""to run cromwell"" --display-name ""cromwell service account"" --format json | jq '.email' | sed -e 's/\""//g'); ```; does not work. It errors out with:; ```; ERROR: (gcloud.beta.iam.service-accounts.create) argument NAME: Bad value [MyServiceAccount]: Service account name must be between 6 and 30 characters (inclusive), must begin with a lowercase letter, and consist of lowercase alphanumeric characters that can be separated by hyphens.; ```; I believe `MyServiceAccount` needs to change to `my-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutori",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2084,Deployability,pipeline,pipelinesRunner,2084,"ackends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cro",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2225,Deployability,Pipeline,Pipelines,2225,"ble/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2567,Deployability,configurat,configuration,2567,"g to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleCon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2663,Deployability,configurat,configuration,2663,"g to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleCon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2791,Deployability,configurat,configuration,2791,"int-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleConfigurationSpec.scala) it should be defined as:; ```; {; name = ""user-account""; scheme = ""user_account""; user = ""me""; secrets-file = ""/very/secret/file.txt""; data-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2567,Modifiability,config,configuration,2567,"g to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleCon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2663,Modifiability,config,configuration,2663,"g to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleCon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2791,Modifiability,config,configuration,2791,"int-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleConfigurationSpec.scala) it should be defined as:; ```; {; name = ""user-account""; scheme = ""user_account""; user = ""me""; secrets-file = ""/very/secret/file.txt""; data-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2914,Modifiability,config,configured,2914,"m-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleConfigurationSpec.scala) it should be defined as:; ```; {; name = ""user-account""; scheme = ""user_account""; user = ""me""; secrets-file = ""/very/secret/file.txt""; data-store-dir = ""/where/the/data/at""; }; ```; But I was not able to get it to work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2594,Security,authoriz,authorization,2594,"g to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleCon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2935,Security,authoriz,authorization,2935,"m-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleConfigurationSpec.scala) it should be defined as:; ```; {; name = ""user-account""; scheme = ""user_account""; user = ""me""; secrets-file = ""/very/secret/file.txt""; data-store-dir = ""/where/the/data/at""; }; ```; But I was not able to get it to work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:3574,Testability,test,test,3574,"m-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleConfigurationSpec.scala) it should be defined as:; ```; {; name = ""user-account""; scheme = ""user_account""; user = ""me""; secrets-file = ""/very/secret/file.txt""; data-store-dir = ""/where/the/data/at""; }; ```; But I was not able to get it to work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:1986,Usability,clear,clearly,1986,"y-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it ve",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2450,Usability,guid,guides,2450,"bjects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further ex",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-680282969:647,Deployability,pipeline,pipelinesRunner,647,"As explained in issue [#4304](https://github.com/broadinstitute/cromwell/issues/4304) now, it seems like the following three roles are required to run Cromwell with the PAPIv2 backend:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (`lifesciences.workflowsRunner`); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (`iam.serviceAccountUser`); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Admin (`storage.objectAdmin`). Rather than the roles `storage.objectCreator` `storage.objectViewer` `genomics.pipelinesRunner` `genomics.admin` `iam.serviceAccountUser` `storage.objects.create` as explained in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-680282969
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-680282969:264,Security,access,access-control,264,"As explained in issue [#4304](https://github.com/broadinstitute/cromwell/issues/4304) now, it seems like the following three roles are required to run Cromwell with the PAPIv2 backend:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (`lifesciences.workflowsRunner`); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (`iam.serviceAccountUser`); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Admin (`storage.objectAdmin`). Rather than the roles `storage.objectCreator` `storage.objectViewer` `genomics.pipelinesRunner` `genomics.admin` `iam.serviceAccountUser` `storage.objects.create` as explained in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-680282969
https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-680282969:511,Security,access,access-control,511,"As explained in issue [#4304](https://github.com/broadinstitute/cromwell/issues/4304) now, it seems like the following three roles are required to run Cromwell with the PAPIv2 backend:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (`lifesciences.workflowsRunner`); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (`iam.serviceAccountUser`); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Admin (`storage.objectAdmin`). Rather than the roles `storage.objectCreator` `storage.objectViewer` `genomics.pipelinesRunner` `genomics.admin` `iam.serviceAccountUser` `storage.objects.create` as explained in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-680282969
https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665232325:0,Availability,Ping,Pinging,0,"Pinging @kshakir for an optional review since (1) you apparently ""recently edited these files"" and (2) faced the pain of this in UL so might have opinions on whether this counts as a fix",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665232325
https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665245650:59,Integrability,message,message,59,What is the current behavior doing exactly? I've seen this message before and thought it was just an alarmist wording of a nbd condition but now I'm thinking I might have had that wrong 😬,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665245650
https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398:90,Availability,recover,recover,90,"As far as I can tell, when this happens today, the WEA crashes and hopes that the WA will recover it, but that doesn't actually seem to happen. What seems to happen is the WA sends a message back to the (now defunct) WEA asking it to please abort whatever it was working on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398
https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398:183,Integrability,message,message,183,"As far as I can tell, when this happens today, the WEA crashes and hopes that the WA will recover it, but that doesn't actually seem to happen. What seems to happen is the WA sends a message back to the (now defunct) WEA asking it to please abort whatever it was working on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398
https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398:90,Safety,recover,recover,90,"As far as I can tell, when this happens today, the WEA crashes and hopes that the WA will recover it, but that doesn't actually seem to happen. What seems to happen is the WA sends a message back to the (now defunct) WEA asking it to please abort whatever it was working on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398
https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398:241,Safety,abort,abort,241,"As far as I can tell, when this happens today, the WEA crashes and hopes that the WA will recover it, but that doesn't actually seem to happen. What seems to happen is the WA sends a message back to the (now defunct) WEA asking it to please abort whatever it was working on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665248398
https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665755219:296,Integrability,message,message,296,"Per Khalid's comments above we may want a new ticket to address the inconsistency in the WA / WEA supervision strategy wrt parent / child interactions: when the WEA died the the WA's supervision strategy said `Stop` (do not restart the crashed WEA), but then the WA sent the defunct WEA an abort message. The WA then parked itself in `WorkflowAbortingState` and waited for a `WorkflowExecutionAbortedResponse` that would never be sent from a WEA that no longer existed. . While the changes in this PR prevent the WEA from crashing in this specific (and alarmingly ordinary) circumstance (hooray!), I agree there are still general structural issues in the WA / WEA relationship we should address separately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665755219
https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665755219:290,Safety,abort,abort,290,"Per Khalid's comments above we may want a new ticket to address the inconsistency in the WA / WEA supervision strategy wrt parent / child interactions: when the WEA died the the WA's supervision strategy said `Stop` (do not restart the crashed WEA), but then the WA sent the defunct WEA an abort message. The WA then parked itself in `WorkflowAbortingState` and waited for a `WorkflowExecutionAbortedResponse` that would never be sent from a WEA that no longer existed. . While the changes in this PR prevent the WEA from crashing in this specific (and alarmingly ordinary) circumstance (hooray!), I agree there are still general structural issues in the WA / WEA relationship we should address separately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665755219
https://github.com/broadinstitute/cromwell/pull/5597#issuecomment-665786875:31,Testability,log,logic,31,"> Quick question: We have some logic in other test cases that if only documentation has changed, the sbt and centaur tests are skipped. Will that also apply to this test suite? Although... since this test only took ~6 minutes instead of ~2 hours for some centaur suites, it probably isn't super important!. @cjllanwarne currently it doesn't apply to this suite. It's possible to do so, but I think given that this test is quite short it's not worth overcomplicating our main `test.sh` with additional conditions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5597#issuecomment-665786875
https://github.com/broadinstitute/cromwell/pull/5597#issuecomment-665786875:46,Testability,test,test,46,"> Quick question: We have some logic in other test cases that if only documentation has changed, the sbt and centaur tests are skipped. Will that also apply to this test suite? Although... since this test only took ~6 minutes instead of ~2 hours for some centaur suites, it probably isn't super important!. @cjllanwarne currently it doesn't apply to this suite. It's possible to do so, but I think given that this test is quite short it's not worth overcomplicating our main `test.sh` with additional conditions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5597#issuecomment-665786875
https://github.com/broadinstitute/cromwell/pull/5597#issuecomment-665786875:117,Testability,test,tests,117,"> Quick question: We have some logic in other test cases that if only documentation has changed, the sbt and centaur tests are skipped. Will that also apply to this test suite? Although... since this test only took ~6 minutes instead of ~2 hours for some centaur suites, it probably isn't super important!. @cjllanwarne currently it doesn't apply to this suite. It's possible to do so, but I think given that this test is quite short it's not worth overcomplicating our main `test.sh` with additional conditions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5597#issuecomment-665786875
https://github.com/broadinstitute/cromwell/pull/5597#issuecomment-665786875:165,Testability,test,test,165,"> Quick question: We have some logic in other test cases that if only documentation has changed, the sbt and centaur tests are skipped. Will that also apply to this test suite? Although... since this test only took ~6 minutes instead of ~2 hours for some centaur suites, it probably isn't super important!. @cjllanwarne currently it doesn't apply to this suite. It's possible to do so, but I think given that this test is quite short it's not worth overcomplicating our main `test.sh` with additional conditions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5597#issuecomment-665786875
https://github.com/broadinstitute/cromwell/pull/5597#issuecomment-665786875:200,Testability,test,test,200,"> Quick question: We have some logic in other test cases that if only documentation has changed, the sbt and centaur tests are skipped. Will that also apply to this test suite? Although... since this test only took ~6 minutes instead of ~2 hours for some centaur suites, it probably isn't super important!. @cjllanwarne currently it doesn't apply to this suite. It's possible to do so, but I think given that this test is quite short it's not worth overcomplicating our main `test.sh` with additional conditions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5597#issuecomment-665786875
https://github.com/broadinstitute/cromwell/pull/5597#issuecomment-665786875:414,Testability,test,test,414,"> Quick question: We have some logic in other test cases that if only documentation has changed, the sbt and centaur tests are skipped. Will that also apply to this test suite? Although... since this test only took ~6 minutes instead of ~2 hours for some centaur suites, it probably isn't super important!. @cjllanwarne currently it doesn't apply to this suite. It's possible to do so, but I think given that this test is quite short it's not worth overcomplicating our main `test.sh` with additional conditions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5597#issuecomment-665786875
https://github.com/broadinstitute/cromwell/pull/5597#issuecomment-665786875:476,Testability,test,test,476,"> Quick question: We have some logic in other test cases that if only documentation has changed, the sbt and centaur tests are skipped. Will that also apply to this test suite? Although... since this test only took ~6 minutes instead of ~2 hours for some centaur suites, it probably isn't super important!. @cjllanwarne currently it doesn't apply to this suite. It's possible to do so, but I think given that this test is quite short it's not worth overcomplicating our main `test.sh` with additional conditions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5597#issuecomment-665786875
https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667193987:44,Availability,echo,echo,44,"Can you try with parens?; ```; command <<<; echo ~{if (x == 1) then 1 else 0}; >>>; ```; The parser does seem to be out of spec, but maybe we can give it a nudge in the right direction this way.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667193987
https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667242825:166,Availability,echo,echo,166,"That was actually my first thought. But with this:; ```; version 1.0. workflow main {; call main {; input:; x = 1; }; }. task main {; input {; Int x; }. command <<<; echo ~{if (x == 1) then 1 else 0}; >>>; }; ```; when I parse it:; ```; $ java -jar womtool-52.jar validate main.wdl ; ERROR: Unexpected symbol (line 16, col 16) when parsing '_gen23'. Expected rparen, got """". echo ~{if (x == 1) then 1 else 0}; ^. $e = :lparen $_gen23 :rparen -> TupleLiteral( values=$1 ); ```; I was under the impression that Cromwell automatically adds parentheses but I am not really sure how it actually works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667242825
https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667242825:284,Availability,ERROR,ERROR,284,"That was actually my first thought. But with this:; ```; version 1.0. workflow main {; call main {; input:; x = 1; }; }. task main {; input {; Int x; }. command <<<; echo ~{if (x == 1) then 1 else 0}; >>>; }; ```; when I parse it:; ```; $ java -jar womtool-52.jar validate main.wdl ; ERROR: Unexpected symbol (line 16, col 16) when parsing '_gen23'. Expected rparen, got """". echo ~{if (x == 1) then 1 else 0}; ^. $e = :lparen $_gen23 :rparen -> TupleLiteral( values=$1 ); ```; I was under the impression that Cromwell automatically adds parentheses but I am not really sure how it actually works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667242825
https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667242825:375,Availability,echo,echo,375,"That was actually my first thought. But with this:; ```; version 1.0. workflow main {; call main {; input:; x = 1; }; }. task main {; input {; Int x; }. command <<<; echo ~{if (x == 1) then 1 else 0}; >>>; }; ```; when I parse it:; ```; $ java -jar womtool-52.jar validate main.wdl ; ERROR: Unexpected symbol (line 16, col 16) when parsing '_gen23'. Expected rparen, got """". echo ~{if (x == 1) then 1 else 0}; ^. $e = :lparen $_gen23 :rparen -> TupleLiteral( values=$1 ); ```; I was under the impression that Cromwell automatically adds parentheses but I am not really sure how it actually works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667242825
https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667242825:264,Security,validat,validate,264,"That was actually my first thought. But with this:; ```; version 1.0. workflow main {; call main {; input:; x = 1; }; }. task main {; input {; Int x; }. command <<<; echo ~{if (x == 1) then 1 else 0}; >>>; }; ```; when I parse it:; ```; $ java -jar womtool-52.jar validate main.wdl ; ERROR: Unexpected symbol (line 16, col 16) when parsing '_gen23'. Expected rparen, got """". echo ~{if (x == 1) then 1 else 0}; ^. $e = :lparen $_gen23 :rparen -> TupleLiteral( values=$1 ); ```; I was under the impression that Cromwell automatically adds parentheses but I am not really sure how it actually works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667242825
https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981:89,Availability,echo,echo,89,"Also the following WDL:; ```; version 1.0. workflow main {; }. task main {; command <<<; echo ~{if 0 < 0.0 then ""yes"" else ""no""}; >>>; }; ```; gives similarly inexplicable error messages (with Cromwell 85):; ```; $ java -jar womtool-85.jar validate main.wdl ; ERROR: Unexpected symbol (line 8, col 21) when parsing 'e'. Expected identifier, got ""0"". echo ~{if 0 < 0.0 then ""yes"" else ""no""}; ^. $e = $e <=> :dot :identifier -> MemberAccess( value=$0, member=$2 ); ```; It almost seems like Cromwell does not like the `0.0` representation of `0` within the `command <<< ... >>>` section of a task",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981
https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981:172,Availability,error,error,172,"Also the following WDL:; ```; version 1.0. workflow main {; }. task main {; command <<<; echo ~{if 0 < 0.0 then ""yes"" else ""no""}; >>>; }; ```; gives similarly inexplicable error messages (with Cromwell 85):; ```; $ java -jar womtool-85.jar validate main.wdl ; ERROR: Unexpected symbol (line 8, col 21) when parsing 'e'. Expected identifier, got ""0"". echo ~{if 0 < 0.0 then ""yes"" else ""no""}; ^. $e = $e <=> :dot :identifier -> MemberAccess( value=$0, member=$2 ); ```; It almost seems like Cromwell does not like the `0.0` representation of `0` within the `command <<< ... >>>` section of a task",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981
https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981:260,Availability,ERROR,ERROR,260,"Also the following WDL:; ```; version 1.0. workflow main {; }. task main {; command <<<; echo ~{if 0 < 0.0 then ""yes"" else ""no""}; >>>; }; ```; gives similarly inexplicable error messages (with Cromwell 85):; ```; $ java -jar womtool-85.jar validate main.wdl ; ERROR: Unexpected symbol (line 8, col 21) when parsing 'e'. Expected identifier, got ""0"". echo ~{if 0 < 0.0 then ""yes"" else ""no""}; ^. $e = $e <=> :dot :identifier -> MemberAccess( value=$0, member=$2 ); ```; It almost seems like Cromwell does not like the `0.0` representation of `0` within the `command <<< ... >>>` section of a task",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981
https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981:350,Availability,echo,echo,350,"Also the following WDL:; ```; version 1.0. workflow main {; }. task main {; command <<<; echo ~{if 0 < 0.0 then ""yes"" else ""no""}; >>>; }; ```; gives similarly inexplicable error messages (with Cromwell 85):; ```; $ java -jar womtool-85.jar validate main.wdl ; ERROR: Unexpected symbol (line 8, col 21) when parsing 'e'. Expected identifier, got ""0"". echo ~{if 0 < 0.0 then ""yes"" else ""no""}; ^. $e = $e <=> :dot :identifier -> MemberAccess( value=$0, member=$2 ); ```; It almost seems like Cromwell does not like the `0.0` representation of `0` within the `command <<< ... >>>` section of a task",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981
https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981:178,Integrability,message,messages,178,"Also the following WDL:; ```; version 1.0. workflow main {; }. task main {; command <<<; echo ~{if 0 < 0.0 then ""yes"" else ""no""}; >>>; }; ```; gives similarly inexplicable error messages (with Cromwell 85):; ```; $ java -jar womtool-85.jar validate main.wdl ; ERROR: Unexpected symbol (line 8, col 21) when parsing 'e'. Expected identifier, got ""0"". echo ~{if 0 < 0.0 then ""yes"" else ""no""}; ^. $e = $e <=> :dot :identifier -> MemberAccess( value=$0, member=$2 ); ```; It almost seems like Cromwell does not like the `0.0` representation of `0` within the `command <<< ... >>>` section of a task",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981
https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981:240,Security,validat,validate,240,"Also the following WDL:; ```; version 1.0. workflow main {; }. task main {; command <<<; echo ~{if 0 < 0.0 then ""yes"" else ""no""}; >>>; }; ```; gives similarly inexplicable error messages (with Cromwell 85):; ```; $ java -jar womtool-85.jar validate main.wdl ; ERROR: Unexpected symbol (line 8, col 21) when parsing 'e'. Expected identifier, got ""0"". echo ~{if 0 < 0.0 then ""yes"" else ""no""}; ^. $e = $e <=> :dot :identifier -> MemberAccess( value=$0, member=$2 ); ```; It almost seems like Cromwell does not like the `0.0` representation of `0` within the `command <<< ... >>>` section of a task",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-1599327981
https://github.com/broadinstitute/cromwell/pull/5620#issuecomment-670986809:11,Deployability,update,update,11,equivalent update applied,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5620#issuecomment-670986809
https://github.com/broadinstitute/cromwell/pull/5621#issuecomment-670986762:11,Deployability,update,update,11,Equivalent update applied.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5621#issuecomment-670986762
https://github.com/broadinstitute/cromwell/pull/5623#issuecomment-670986902:11,Deployability,update,update,11,equivalent update applied,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5623#issuecomment-670986902
https://github.com/broadinstitute/cromwell/pull/5624#issuecomment-670986965:11,Deployability,update,update,11,equivalent update applied,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5624#issuecomment-670986965
https://github.com/broadinstitute/cromwell/pull/5625#issuecomment-670986987:11,Deployability,update,update,11,equivalent update applied,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5625#issuecomment-670986987
https://github.com/broadinstitute/cromwell/pull/5626#issuecomment-670987031:11,Deployability,update,update,11,equivalent update applied,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5626#issuecomment-670987031
https://github.com/broadinstitute/cromwell/pull/5627#issuecomment-670987106:11,Deployability,update,update,11,equivalent update applied,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5627#issuecomment-670987106
https://github.com/broadinstitute/cromwell/pull/5628#issuecomment-670987162:11,Deployability,update,update,11,equivalent update applied,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5628#issuecomment-670987162
https://github.com/broadinstitute/cromwell/pull/5629#issuecomment-670987199:11,Deployability,update,update,11,equivalent update applied,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5629#issuecomment-670987199
https://github.com/broadinstitute/cromwell/pull/5632#issuecomment-672904524:77,Deployability,update,update,77,@scala-steward this string matching has found an incorrect string comment to update.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5632#issuecomment-672904524
https://github.com/broadinstitute/cromwell/issues/5693#issuecomment-672004178:311,Integrability,depend,dependencies,311,"Developer notes:. When first launching single workflow mode, Cromwell calls `cromwell.CommandLineArguments#validateSubmission` and generates a `cromwell.CommandLineArguments.ValidSubmission` if the submission looks good. `ValidSubmission` has the appearance of supporting directories because it has the member `dependencies`. In [both](https://github.com/broadinstitute/cromwell/blob/9249537fd094c6979b0c64e99fcc90d48c861487/server/src/main/scala/cromwell/CromwellEntryPoint.scala#L229) [places](https://github.com/broadinstitute/cromwell/blob/9249537fd094c6979b0c64e99fcc90d48c861487/server/src/main/scala/cromwell/CromwellEntryPoint.scala#L249) where `validateSubmission` is actually called, however, we copy the value into another variable that is named & treated as the path to a zip file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5693#issuecomment-672004178
https://github.com/broadinstitute/cromwell/issues/5693#issuecomment-672004178:734,Modifiability,variab,variable,734,"Developer notes:. When first launching single workflow mode, Cromwell calls `cromwell.CommandLineArguments#validateSubmission` and generates a `cromwell.CommandLineArguments.ValidSubmission` if the submission looks good. `ValidSubmission` has the appearance of supporting directories because it has the member `dependencies`. In [both](https://github.com/broadinstitute/cromwell/blob/9249537fd094c6979b0c64e99fcc90d48c861487/server/src/main/scala/cromwell/CromwellEntryPoint.scala#L229) [places](https://github.com/broadinstitute/cromwell/blob/9249537fd094c6979b0c64e99fcc90d48c861487/server/src/main/scala/cromwell/CromwellEntryPoint.scala#L249) where `validateSubmission` is actually called, however, we copy the value into another variable that is named & treated as the path to a zip file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5693#issuecomment-672004178
https://github.com/broadinstitute/cromwell/issues/5693#issuecomment-672004178:107,Security,validat,validateSubmission,107,"Developer notes:. When first launching single workflow mode, Cromwell calls `cromwell.CommandLineArguments#validateSubmission` and generates a `cromwell.CommandLineArguments.ValidSubmission` if the submission looks good. `ValidSubmission` has the appearance of supporting directories because it has the member `dependencies`. In [both](https://github.com/broadinstitute/cromwell/blob/9249537fd094c6979b0c64e99fcc90d48c861487/server/src/main/scala/cromwell/CromwellEntryPoint.scala#L229) [places](https://github.com/broadinstitute/cromwell/blob/9249537fd094c6979b0c64e99fcc90d48c861487/server/src/main/scala/cromwell/CromwellEntryPoint.scala#L249) where `validateSubmission` is actually called, however, we copy the value into another variable that is named & treated as the path to a zip file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5693#issuecomment-672004178
https://github.com/broadinstitute/cromwell/issues/5693#issuecomment-672004178:654,Security,validat,validateSubmission,654,"Developer notes:. When first launching single workflow mode, Cromwell calls `cromwell.CommandLineArguments#validateSubmission` and generates a `cromwell.CommandLineArguments.ValidSubmission` if the submission looks good. `ValidSubmission` has the appearance of supporting directories because it has the member `dependencies`. In [both](https://github.com/broadinstitute/cromwell/blob/9249537fd094c6979b0c64e99fcc90d48c861487/server/src/main/scala/cromwell/CromwellEntryPoint.scala#L229) [places](https://github.com/broadinstitute/cromwell/blob/9249537fd094c6979b0c64e99fcc90d48c861487/server/src/main/scala/cromwell/CromwellEntryPoint.scala#L249) where `validateSubmission` is actually called, however, we copy the value into another variable that is named & treated as the path to a zip file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5693#issuecomment-672004178
https://github.com/broadinstitute/cromwell/issues/5694#issuecomment-691239382:587,Availability,error,error,587,"Sorry, but this seems very related and I can't see why this is the problem:. ```; task trim_PE {; input {; File f1; File f2; Int? cpu=1; # Int? disk_space_gb=ceil(2*size(f1, ""GB"")+2*size(f2, ""GB"")) + 20; Int? machine_mem_gb; Int? preemptible_attempts; String? adap; String b1 = basename(basename(basename(f1, "".fastq""), "".fq""), "".gz""); String b2 = basename(basename(basename(f2, "".fastq""), "".fq""), "".gz""); String p1 = ""W7_"" + b1 + "".fq.gz""; String p2 = ""W7_"" + b2 + "".fq.gz""; String u1 = ""W7_"" + b1 + ""_unpaired.fq.gz""; String u2 = ""W7_"" + b2 + ""_unpaired.fq.gz""; }; ```. This yields an error:; ![image](https://user-images.githubusercontent.com/16489606/92958456-931bba00-f46a-11ea-97cb-ccc709929ad8.png). Is this a similar problem?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5694#issuecomment-691239382
https://github.com/broadinstitute/cromwell/pull/5695#issuecomment-668787968:12,Deployability,release,release,12,Waiting for release 53. Will not hotfix.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5695#issuecomment-668787968
https://github.com/broadinstitute/cromwell/pull/5695#issuecomment-668787968:33,Deployability,hotfix,hotfix,33,Waiting for release 53. Will not hotfix.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5695#issuecomment-668787968
https://github.com/broadinstitute/cromwell/pull/5701#issuecomment-673225025:0,Availability,Ping,Pinging,0,Pinging for re-review since there were a few non-trivial fixups required,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5701#issuecomment-673225025
https://github.com/broadinstitute/cromwell/pull/5711#issuecomment-671451870:147,Integrability,depend,dependency,147,"(👍 assuming ""force-pushed the mlc_scala_steward_5640_5649 branch from aeaa1f3 to 7f28d1a yesterday"" represents rebasing onto the previously merged dependency bumps)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5711#issuecomment-671451870
https://github.com/broadinstitute/cromwell/pull/5711#issuecomment-671535576:0,Availability,Ping,Ping,0,Ping @grsterin for review,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5711#issuecomment-671535576
https://github.com/broadinstitute/cromwell/issues/5714#issuecomment-669212811:87,Deployability,Pipeline,Pipelines,87,"This is not a Cromwell problem, but rather a known issue affecting all users of Google Pipelines API. Fortunately our contacts at Google report it will be fixed today.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714#issuecomment-669212811
https://github.com/broadinstitute/cromwell/pull/5715#issuecomment-671535839:0,Availability,Ping,Ping,0,Ping @grsterin for review,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5715#issuecomment-671535839
https://github.com/broadinstitute/cromwell/pull/5716#issuecomment-669271678:130,Integrability,depend,depends,130,This PR includes all of the changes in https://github.com/broadinstitute/cromwell/pull/5710 . Can you create this PR such that it depends on that PR/branch and only includes the new changes so that it is easier to review just these changes?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5716#issuecomment-669271678
https://github.com/broadinstitute/cromwell/pull/5716#issuecomment-669274524:32,Testability,test,tests,32,"I will create a new PR once the tests finish running, since that takes a while.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5716#issuecomment-669274524
https://github.com/broadinstitute/cromwell/pull/5719#issuecomment-671453063:0,Availability,ping,pinging,0,"pinging @aednichols for re-review now that this is ""Look at Me"" able",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5719#issuecomment-671453063
https://github.com/broadinstitute/cromwell/pull/5724#issuecomment-671513896:101,Usability,simpl,simple,101,But you could probably also make a list of all the files in a gs directory and their crc32c's with a simple gsutil loop. I thought we wanted to make sure we get the files which are actually on the disk image?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5724#issuecomment-671513896
https://github.com/broadinstitute/cromwell/pull/5724#issuecomment-671534634:517,Availability,down,download,517,"> But you could probably also make a list of all the files in a gs directory and their crc32c's with a simple gsutil loop. I thought we wanted to make sure we get the files which are actually on the disk image?. The problem with this value is that it should be >= than size of the disk from which you created the image, not just the sum of file sizes. So the whole image/manifest creation algorithm is like this:. 1. You use gsutil to see bucket size; 2. You create empty disk of the size >= than bucket size; 3. You download files to that disk and create image from it.; 4. You run manifest creator app against that disk feeding it with the disk size.; 5. You delete the disk.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5724#issuecomment-671534634
https://github.com/broadinstitute/cromwell/pull/5724#issuecomment-671534634:103,Usability,simpl,simple,103,"> But you could probably also make a list of all the files in a gs directory and their crc32c's with a simple gsutil loop. I thought we wanted to make sure we get the files which are actually on the disk image?. The problem with this value is that it should be >= than size of the disk from which you created the image, not just the sum of file sizes. So the whole image/manifest creation algorithm is like this:. 1. You use gsutil to see bucket size; 2. You create empty disk of the size >= than bucket size; 3. You download files to that disk and create image from it.; 4. You run manifest creator app against that disk feeding it with the disk size.; 5. You delete the disk.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5724#issuecomment-671534634
https://github.com/broadinstitute/cromwell/pull/5727#issuecomment-670565924:121,Deployability,update,updates,121,@scala-steward - there may be two small bugs happening in this PR:. 1. I didn't expect scala-steward to continue posting updates to this dependency given that there's a `scala-steward:off` line in the file.; 2. This PR is not actually updating the version of sbt - it's just whitespace tidying?. Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5727#issuecomment-670565924
https://github.com/broadinstitute/cromwell/pull/5727#issuecomment-670565924:137,Integrability,depend,dependency,137,@scala-steward - there may be two small bugs happening in this PR:. 1. I didn't expect scala-steward to continue posting updates to this dependency given that there's a `scala-steward:off` line in the file.; 2. This PR is not actually updating the version of sbt - it's just whitespace tidying?. Thank you!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5727#issuecomment-670565924
https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671414437:21,Integrability,message,message,21,"I have a feeling the message is coming from an underlying Unix command like; ```; $ md5 ~; md5: /Users/anichols: Is a directory; ```; That said, the Cromwell product does seem to make a promise that it can hash & call-cache directories, and I am having trouble reconciling those two premises.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671414437
https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671414437:218,Performance,cache,cache,218,"I have a feeling the message is coming from an underlying Unix command like; ```; $ md5 ~; md5: /Users/anichols: Is a directory; ```; That said, the Cromwell product does seem to make a promise that it can hash & call-cache directories, and I am having trouble reconciling those two premises.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671414437
https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671414437:206,Security,hash,hash,206,"I have a feeling the message is coming from an underlying Unix command like; ```; $ md5 ~; md5: /Users/anichols: Is a directory; ```; That said, the Cromwell product does seem to make a promise that it can hash & call-cache directories, and I am having trouble reconciling those two premises.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671414437
https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671439046:98,Deployability,release,releases,98,"@dformoso I checked in with the team and this issue is scheduled to be fixed by the time Cromwell releases support for WDL 2.0. At that time, `version development` will be promoted to an officially supported version; before then, `development` should be used with caution & probably not in production.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671439046
https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671439046:55,Energy Efficiency,schedul,scheduled,55,"@dformoso I checked in with the team and this issue is scheduled to be fixed by the time Cromwell releases support for WDL 2.0. At that time, `version development` will be promoted to an officially supported version; before then, `development` should be used with caution & probably not in production.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671439046
https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-686181004:53,Availability,error,error,53,"Just adding confirmation that I'm receiving the same error for localizing WDL directories (in the development spec). Notably, if you use the cached-copy strategy, it will result in copying your directories twice - as I understand, Cromwell won't be able to hard-link the original directory so copies it to `cached directory) and hence copies it again. Not a big deal that would get solved by fixes mentioned earlier, but in case anyone finds themselves where I am.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-686181004
https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-686181004:141,Performance,cache,cached-copy,141,"Just adding confirmation that I'm receiving the same error for localizing WDL directories (in the development spec). Notably, if you use the cached-copy strategy, it will result in copying your directories twice - as I understand, Cromwell won't be able to hard-link the original directory so copies it to `cached directory) and hence copies it again. Not a big deal that would get solved by fixes mentioned earlier, but in case anyone finds themselves where I am.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-686181004
https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-686181004:307,Performance,cache,cached,307,"Just adding confirmation that I'm receiving the same error for localizing WDL directories (in the development spec). Notably, if you use the cached-copy strategy, it will result in copying your directories twice - as I understand, Cromwell won't be able to hard-link the original directory so copies it to `cached directory) and hence copies it again. Not a big deal that would get solved by fixes mentioned earlier, but in case anyone finds themselves where I am.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-686181004
https://github.com/broadinstitute/cromwell/pull/5746#issuecomment-678286270:165,Testability,Test,Test,165,"> > Hmm all the beta builds are failing fauxcalization...; > ; > Hmm, it worked on my machine (c) 🙂; > Probably misconfiguration - will have to check once I'm back. Test was running on the wrong backend. Now passing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5746#issuecomment-678286270
https://github.com/broadinstitute/cromwell/pull/5746#issuecomment-678291321:58,Testability,test,tests,58,> Might be tricky to stage but it'd be really nice to see tests for:; > ; > * A file whose crc32c has changed since we created the manifest; > * A file which isn't on the image even though the manifest lists it. These tests may be not required if we use versioned images. Added this topic to today's techtalk agenda.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5746#issuecomment-678291321
https://github.com/broadinstitute/cromwell/pull/5746#issuecomment-678291321:218,Testability,test,tests,218,> Might be tricky to stage but it'd be really nice to see tests for:; > ; > * A file whose crc32c has changed since we created the manifest; > * A file which isn't on the image even though the manifest lists it. These tests may be not required if we use versioned images. Added this topic to today's techtalk agenda.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5746#issuecomment-678291321
https://github.com/broadinstitute/cromwell/pull/5746#issuecomment-678360814:60,Testability,test,tests,60,"> > Might be tricky to stage but it'd be really nice to see tests for:; > > ; > > * A file whose crc32c has changed since we created the manifest; > > * A file which isn't on the image even though the manifest lists it; > ; > These tests may be not required if we use versioned images. Added this topic to today's techtalk agenda. Per techtalk discussion, disk image versioning is not the answer to this problem, but this ticket is: https://broadworkbench.atlassian.net/browse/BA-6577",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5746#issuecomment-678360814
https://github.com/broadinstitute/cromwell/pull/5746#issuecomment-678360814:232,Testability,test,tests,232,"> > Might be tricky to stage but it'd be really nice to see tests for:; > > ; > > * A file whose crc32c has changed since we created the manifest; > > * A file which isn't on the image even though the manifest lists it; > ; > These tests may be not required if we use versioned images. Added this topic to today's techtalk agenda. Per techtalk discussion, disk image versioning is not the answer to this problem, but this ticket is: https://broadworkbench.atlassian.net/browse/BA-6577",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5746#issuecomment-678360814
https://github.com/broadinstitute/cromwell/pull/5752#issuecomment-729792679:0,Availability,Redundant,Redundant,0,Redundant. we're already on 4.10.4,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5752#issuecomment-729792679
https://github.com/broadinstitute/cromwell/pull/5752#issuecomment-729792679:0,Safety,Redund,Redundant,0,Redundant. we're already on 4.10.4,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5752#issuecomment-729792679
https://github.com/broadinstitute/cromwell/issues/5768#issuecomment-676817660:45,Availability,error,error,45,"Hey @OgnjenMilicevic, looks like there is an error in that page. I believe it's supposed to be `${docker_script}` instead of `${script}`. (Edit, I've opened a PR to address this). For context, here is my config I use for Slurm + Singularity: https://gist.github.com/illusional/b70f870fa0e2f8e7a0ba0a9e71d568f5",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5768#issuecomment-676817660
https://github.com/broadinstitute/cromwell/issues/5768#issuecomment-676817660:204,Modifiability,config,config,204,"Hey @OgnjenMilicevic, looks like there is an error in that page. I believe it's supposed to be `${docker_script}` instead of `${script}`. (Edit, I've opened a PR to address this). For context, here is my config I use for Slurm + Singularity: https://gist.github.com/illusional/b70f870fa0e2f8e7a0ba0a9e71d568f5",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5768#issuecomment-676817660
https://github.com/broadinstitute/cromwell/pull/5780#issuecomment-676576576:25,Testability,test,tests,25,"Thank you for adding the tests. Sorry, I didn't have time to work on this. I really appreciate it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5780#issuecomment-676576576
https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677038991:72,Modifiability,config,configs,72,"Yeah I thought it was weird too, I don't have it in any of my container configs. - `${script} is your full path, eg: `/Users/michael/path/to/execution/PIPELINE_NAME/<guid>/execution,; - `{docker_script}` is rebased from execution as `/cromwell-execution/PIPELINE_NAME/<guid>/execution`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677038991
https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677038991:166,Usability,guid,guid,166,"Yeah I thought it was weird too, I don't have it in any of my container configs. - `${script} is your full path, eg: `/Users/michael/path/to/execution/PIPELINE_NAME/<guid>/execution,; - `{docker_script}` is rebased from execution as `/cromwell-execution/PIPELINE_NAME/<guid>/execution`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677038991
https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677038991:269,Usability,guid,guid,269,"Yeah I thought it was weird too, I don't have it in any of my container configs. - `${script} is your full path, eg: `/Users/michael/path/to/execution/PIPELINE_NAME/<guid>/execution,; - `{docker_script}` is rebased from execution as `/cromwell-execution/PIPELINE_NAME/<guid>/execution`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677038991
https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677943101:104,Modifiability,rewrite,rewrite,104,"Thanks @mcovarr and @cjllanwarne. I've retargeted at develop, but to do that I had to checkout locally, rewrite my git history and force push. For context, I created the edits by clicking the ""Edit in GitHub"" button at the top right of the docs pages, but this is auto targeting at master (which makes sense I guess, but potentially a confusing barrier for people who aren't so familiar with Git). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5784#issuecomment-677943101
https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-677685903:42,Testability,test,test,42,"The names in your output don't match your test input. The output I get is; ```; ""basenametest.outbasename"": ""inputs.json"",; ""basenametest.out"": ""inputs.json"",; ""basenametest.outname"": ""inputs.json""; ```. What would you expect to see instead of these values? If you change the `inp` value in `inputs.json` to `""/a/file/path/inputs.json""` then I get:; ```; ""basenametest.outname"": ""/a/file/path/inputs.json"",; ```; (assuming that a file at this path actually exists). This behavior matches what I'd expect to see.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-677685903
https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-677957329:887,Availability,echo,echo,887,"Thanks @pshapiro4broad, sorry for detached example filename. I think I see more now what's happening now, but I'm less convinced it's the right behaviour. I believe the outputs section is using whatever is provided in the input, where I would expect it to be using the localised path. You're correct, if you use the full path in the inputs json (eg: `/path/to/inputs.json`), you get that full path in the outname. I'll illustrate it a little bit differently, note that I've annotated a container in the basenametest task of the following workflow:. **outputsnametest.wdl**; ```wdl ; version development . workflow basenameconnectiontest {; call createFile {}; call basenametest {; input:; inp=createFile.out; }. output {; String outname = basenametest.outname; String outbasename = basenametest.outbasename; String out = basenametest.out; }; }. task createFile {; input {}. command <<<; echo ""Hello, Broad!""; >>>. output {; File out = stdout(); }; }. task basenametest {; input {; File inp; }. command <<<; echo '~{inp}'; echo '~{basename(inp)}'; >>>; ; output {; String outname = inp; String outbasename = basename(inp); String out = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }; ```. This time running with no inputs:. ```bash; java -jar cromwell-52 run outputsnametest.wdl; ```. ```json; {; ""outputs"": {; ""basenameconnectiontest.outbasename"": ""stdout"",; ""basenameconnectiontest.out"": ""/cromwell-executions/basenameconnectiontest/51e0cb4d-f706-4540-9303-ed9c70a8764e/call-basenametest/inputs/-43085659/stdout\nstdout"",; ""basenameconnectiontest.outname"": ""/Users/franklinmichael/Desktop/tmp/wdltests/cromwell-executions/basenameconnectiontest/51e0cb4d-f706-4540-9303-ed9c70a8764e/call-createFile/execution/stdout""; },; ""id"": ""51e0cb4d-f706-4540-9303-ed9c70a8764e""; }; ```. I would expect that the outname be the localised pathname (in the container), in this case that would be `/cromwell-executions/..../`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-677957329
https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-677957329:1007,Availability,echo,echo,1007,"Thanks @pshapiro4broad, sorry for detached example filename. I think I see more now what's happening now, but I'm less convinced it's the right behaviour. I believe the outputs section is using whatever is provided in the input, where I would expect it to be using the localised path. You're correct, if you use the full path in the inputs json (eg: `/path/to/inputs.json`), you get that full path in the outname. I'll illustrate it a little bit differently, note that I've annotated a container in the basenametest task of the following workflow:. **outputsnametest.wdl**; ```wdl ; version development . workflow basenameconnectiontest {; call createFile {}; call basenametest {; input:; inp=createFile.out; }. output {; String outname = basenametest.outname; String outbasename = basenametest.outbasename; String out = basenametest.out; }; }. task createFile {; input {}. command <<<; echo ""Hello, Broad!""; >>>. output {; File out = stdout(); }; }. task basenametest {; input {; File inp; }. command <<<; echo '~{inp}'; echo '~{basename(inp)}'; >>>; ; output {; String outname = inp; String outbasename = basename(inp); String out = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }; ```. This time running with no inputs:. ```bash; java -jar cromwell-52 run outputsnametest.wdl; ```. ```json; {; ""outputs"": {; ""basenameconnectiontest.outbasename"": ""stdout"",; ""basenameconnectiontest.out"": ""/cromwell-executions/basenameconnectiontest/51e0cb4d-f706-4540-9303-ed9c70a8764e/call-basenametest/inputs/-43085659/stdout\nstdout"",; ""basenameconnectiontest.outname"": ""/Users/franklinmichael/Desktop/tmp/wdltests/cromwell-executions/basenameconnectiontest/51e0cb4d-f706-4540-9303-ed9c70a8764e/call-createFile/execution/stdout""; },; ""id"": ""51e0cb4d-f706-4540-9303-ed9c70a8764e""; }; ```. I would expect that the outname be the localised pathname (in the container), in this case that would be `/cromwell-executions/..../`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-677957329
https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-677957329:1022,Availability,echo,echo,1022,"Thanks @pshapiro4broad, sorry for detached example filename. I think I see more now what's happening now, but I'm less convinced it's the right behaviour. I believe the outputs section is using whatever is provided in the input, where I would expect it to be using the localised path. You're correct, if you use the full path in the inputs json (eg: `/path/to/inputs.json`), you get that full path in the outname. I'll illustrate it a little bit differently, note that I've annotated a container in the basenametest task of the following workflow:. **outputsnametest.wdl**; ```wdl ; version development . workflow basenameconnectiontest {; call createFile {}; call basenametest {; input:; inp=createFile.out; }. output {; String outname = basenametest.outname; String outbasename = basenametest.outbasename; String out = basenametest.out; }; }. task createFile {; input {}. command <<<; echo ""Hello, Broad!""; >>>. output {; File out = stdout(); }; }. task basenametest {; input {; File inp; }. command <<<; echo '~{inp}'; echo '~{basename(inp)}'; >>>; ; output {; String outname = inp; String outbasename = basename(inp); String out = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }; ```. This time running with no inputs:. ```bash; java -jar cromwell-52 run outputsnametest.wdl; ```. ```json; {; ""outputs"": {; ""basenameconnectiontest.outbasename"": ""stdout"",; ""basenameconnectiontest.out"": ""/cromwell-executions/basenameconnectiontest/51e0cb4d-f706-4540-9303-ed9c70a8764e/call-basenametest/inputs/-43085659/stdout\nstdout"",; ""basenameconnectiontest.outname"": ""/Users/franklinmichael/Desktop/tmp/wdltests/cromwell-executions/basenameconnectiontest/51e0cb4d-f706-4540-9303-ed9c70a8764e/call-createFile/execution/stdout""; },; ""id"": ""51e0cb4d-f706-4540-9303-ed9c70a8764e""; }; ```. I would expect that the outname be the localised pathname (in the container), in this case that would be `/cromwell-executions/..../`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-677957329
https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-678343746:70,Deployability,update,update,70,"Thanks, this is much better. Given this behavior, it would be good to update the issue summary to match, with something like: `Cromwell File to String conversion uses input name, not localized path name`. Regarding this behavior, the relevant part of the spec that I see says:. > When a WDL author uses a File input in their Command Section, the fully qualified, localized path to the file is substituted into the command string. As I read this, the point is that `File` has special behavior inside a command block. Outside of a command block, it has no special behavior, and therefore when converted to `String` it has the path that was provided as an input. So to me it seems that cromwell's behavior is consistent with the spec.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-678343746
https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-678917243:29,Deployability,update,updated,29,"Thanks @pshapiro4broad, I've updated the issue title. I don't know how much I agree with the behaviour, but to me it sounds strange. It's logged on OpenWDL, so I'll close this for now and reopen if required.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-678917243
https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-678917243:138,Testability,log,logged,138,"Thanks @pshapiro4broad, I've updated the issue title. I don't know how much I agree with the behaviour, but to me it sounds strange. It's logged on OpenWDL, so I'll close this for now and reopen if required.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-678917243
https://github.com/broadinstitute/cromwell/pull/5790#issuecomment-679261370:41,Availability,failure,failure,41,"I actually was not able to reproduce the failure with newer GATK versions, but the test does confirm that we now choose a larger boot disk size than before the changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5790#issuecomment-679261370
https://github.com/broadinstitute/cromwell/pull/5790#issuecomment-679261370:83,Testability,test,test,83,"I actually was not able to reproduce the failure with newer GATK versions, but the test does confirm that we now choose a larger boot disk size than before the changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5790#issuecomment-679261370
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679354011:241,Modifiability,config,config,241,"Answered on slack but for posterity:. According to the original PR, the `sra` filesystem should be referenced in the root `filesystems` stanza, not the one inside the PAPI filesystem. You might also need to add it inside the PAPI filesystem config too to enable it, but most of the config mentioned here (`class`, `docker-image`, `ngc`) looks like it belongs at the root level.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679354011
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679354011:282,Modifiability,config,config,282,"Answered on slack but for posterity:. According to the original PR, the `sra` filesystem should be referenced in the root `filesystems` stanza, not the one inside the PAPI filesystem. You might also need to add it inside the PAPI filesystem config too to enable it, but most of the config mentioned here (`class`, `docker-image`, `ngc`) looks like it belongs at the root level.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679354011
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:80,Availability,error,error,80,"@cjllanwarne Thanks so much for your response. I am unfortunately getting a new error. Which is as follows:; ```; [2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath; java.lang.UnsupportedOperationException: 'nioPath' not implemented for SraPath; 	at cromwell.filesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundRec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:140,Availability,error,error,140,"@cjllanwarne Thanks so much for your response. I am unfortunately getting a new error. Which is as follows:; ```; [2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath; java.lang.UnsupportedOperationException: 'nioPath' not implemented for SraPath; 	at cromwell.filesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundRec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:3418,Availability,down,downsampling-stride,3418,"Task.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I tried reading the code but I don't know scala so did not get very far:; https://github.com/broadinstitute/cromwell/blob/f1cce2cd2723b849c4d8f285510f30913ec188a0/filesystems/sra/src/main/scala/cromwell/filesystems/sra/SraPathBuilder.scala. The input I feed in the following:; ```; ""Mutect2.tumor_reads"": ""sra://SRR9247315/NWD751606.b38.irc.v1.cram"",; ```. The whole json input looks like this:; ```; {; ""Mutect2.gatk_docker"": ""broadinstitute/gatk:4.1.4.1"",. ""Mutect2.intervals"": ""gs://gatk-best-practices/somatic-b37/whole_exome_agilent_1.1_refseq_plus_3_boosters.Homo_sapiens_assembly19.baits.interval_list"",; ""Mutect2.scatter_count"": 50,; ""Mutect2.m2_extra_args"": ""--downsampling-stride 20 --max-reads-per-alignment-start 6 --max-suspicious-reads-per-alignment-start 6"",; ""Mutect2.filter_funcotations"": ""True"",; ""Mutect2.funco_reference_version"": ""hg19"",; ""Mutect2.funco_data_sources_tar_gz"": ""gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124s.tar.gz"",; ""Mutect2.funco_transcript_selection_list"": ""gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt"",. ""Mutect2.ref_fasta"": ""gs://gatk-best-practices/somatic-b37/Homo_sapiens_assembly19.fasta"",; ""Mutect2.ref_dict"": ""gs://gatk-best-practices/somatic-b37/Homo_sapiens_assembly19.dict"",; ""Mutect2.ref_fai"": ""gs://gatk-best-practices/somatic-b37/Homo_sapiens_assembly19.fasta.fai"",; ""Mutect2.tumor_reads"": ""sra://SRR9247315/NWD751606.b38.irc.v1.cram"",; ""Mutect2.tumor_reads_index"": ""sra://SRR9247315/NWD751606NWD751606.b38.irc.v1.cram.crai"",. ""Mutect2.pon"": ""gs://gatk-best-practices/somatic-b37/Mutect2-exome-panel.vcf"",; ""Mutect2.pon_idx"": ""gs://gatk-best-practices/somatic-b37/Mutect2-exome-panel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1668,Performance,cache,cache,1668,ashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dis,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1735,Performance,cache,cache,1735,SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:1,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1810,Performance,cache,cache,1810,h(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I tried reading the code but I don't know scala so did not get ve,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:591,Security,hash,hashCode,591,"@cjllanwarne Thanks so much for your response. I am unfortunately getting a new error. Which is as follows:; ```; [2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath; java.lang.UnsupportedOperationException: 'nioPath' not implemented for SraPath; 	at cromwell.filesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundRec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:670,Security,hash,hashCode,670,"@cjllanwarne Thanks so much for your response. I am unfortunately getting a new error. Which is as follows:; ```; [2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath; java.lang.UnsupportedOperationException: 'nioPath' not implemented for SraPath; 	at cromwell.filesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundRec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:746,Security,hash,hashCode,746,"@cjllanwarne Thanks so much for your response. I am unfortunately getting a new error. Which is as follows:; ```; [2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath; java.lang.UnsupportedOperationException: 'nioPath' not implemented for SraPath; 	at cromwell.filesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundRec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:849,Security,hash,hashing,849,"@cjllanwarne Thanks so much for your response. I am unfortunately getting a new error. Which is as follows:; ```; [2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath; java.lang.UnsupportedOperationException: 'nioPath' not implemented for SraPath; 	at cromwell.filesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundRec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:919,Security,hash,hashing,919,"@cjllanwarne Thanks so much for your response. I am unfortunately getting a new error. Which is as follows:; ```; [2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath; java.lang.UnsupportedOperationException: 'nioPath' not implemented for SraPath; 	at cromwell.filesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundRec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1102,Security,hash,hashCode,1102,"2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath; java.lang.UnsupportedOperationException: 'nioPath' not implemented for SraPath; 	at cromwell.filesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1207,Security,hash,hashing,1207,ationException: 'nioPath' not implemented for SraPath; 	at cromwell.filesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1277,Security,hash,hashing,1277,lesystems.sra.SraPath.nioPath(SraPathBuilder.scala:31); 	at cromwell.core.path.Path.nioPathPrivate(PathBuilder.scala:113); 	at cromwell.core.path.Path.nioPathPrivate$(PathBuilder.scala:113); 	at cromwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.disp,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1463,Security,hash,hashCode,1463,romwell.filesystems.sra.SraPath.nioPathPrivate(SraPathBuilder.scala:26); 	at cromwell.core.path.PathObjectMethods.hashCode(PathObjectMethods.scala:18); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(F,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1618,Security,hash,hash,1618,); 	at cromwell.core.path.PathObjectMethods.hashCode$(PathObjectMethods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.run,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:1685,Security,hash,hash,1685,hods.scala:18); 	at cromwell.filesystems.sra.SraPath.hashCode(SraPathBuilder.scala:26); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.DefaultIoCommand$DefaultIoSizeCommand.hashCode(DefaultIoCommand.scala:14); 	at scala.runtime.Statics.anyHash(Statics.java:122); 	at scala.util.hashing.MurmurHash3.productHash(MurmurHash3.scala:68); 	at scala.util.hashing.MurmurHash3$.productHash(MurmurHash3.scala:215); 	at scala.runtime.ScalaRunTime$._hashCode(ScalaRunTime.scala:149); 	at cromwell.core.io.IoPromiseProxyActor$IoCommandWithPromise.hashCode(IoPromiseProxyActor.scala:11); 	at com.google.common.base.Equivalence$Equals.doHash(Equivalence.java:348); 	at com.google.common.base.Equivalence.hash(Equivalence.java:112); 	at com.google.common.cache.LocalCache.hash(LocalCache.java:1696); 	at com.google.common.cache.LocalCache.getIfPresent(LocalCache.java:3956); 	at com.google.common.cache.LocalCache$LocalManualCache.getIfPresent(LocalCache.java:4865); 	at cromwell.engine.io.IoActorProxy$$anonfun$receive$1.applyOrElse(IoActorProxy.scala:25); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.io.IoActorProxy.aroundReceive(IoActorProxy.scala:16); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoi,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:4923,Testability,test,test-data,4923,"a. The input I feed in the following:; ```; ""Mutect2.tumor_reads"": ""sra://SRR9247315/NWD751606.b38.irc.v1.cram"",; ```. The whole json input looks like this:; ```; {; ""Mutect2.gatk_docker"": ""broadinstitute/gatk:4.1.4.1"",. ""Mutect2.intervals"": ""gs://gatk-best-practices/somatic-b37/whole_exome_agilent_1.1_refseq_plus_3_boosters.Homo_sapiens_assembly19.baits.interval_list"",; ""Mutect2.scatter_count"": 50,; ""Mutect2.m2_extra_args"": ""--downsampling-stride 20 --max-reads-per-alignment-start 6 --max-suspicious-reads-per-alignment-start 6"",; ""Mutect2.filter_funcotations"": ""True"",; ""Mutect2.funco_reference_version"": ""hg19"",; ""Mutect2.funco_data_sources_tar_gz"": ""gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124s.tar.gz"",; ""Mutect2.funco_transcript_selection_list"": ""gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt"",. ""Mutect2.ref_fasta"": ""gs://gatk-best-practices/somatic-b37/Homo_sapiens_assembly19.fasta"",; ""Mutect2.ref_dict"": ""gs://gatk-best-practices/somatic-b37/Homo_sapiens_assembly19.dict"",; ""Mutect2.ref_fai"": ""gs://gatk-best-practices/somatic-b37/Homo_sapiens_assembly19.fasta.fai"",; ""Mutect2.tumor_reads"": ""sra://SRR9247315/NWD751606.b38.irc.v1.cram"",; ""Mutect2.tumor_reads_index"": ""sra://SRR9247315/NWD751606NWD751606.b38.irc.v1.cram.crai"",. ""Mutect2.pon"": ""gs://gatk-best-practices/somatic-b37/Mutect2-exome-panel.vcf"",; ""Mutect2.pon_idx"": ""gs://gatk-best-practices/somatic-b37/Mutect2-exome-panel.vcf.idx"",; ""Mutect2.gnomad"": ""gs://gatk-best-practices/somatic-b37/af-only-gnomad.raw.sites.vcf"",; ""Mutect2.gnomad_idx"": ""gs://gatk-best-practices/somatic-b37/af-only-gnomad.raw.sites.vcf.idx"",; ""Mutect2.variants_for_contamination"": ""gs://gatk-best-practices/somatic-b37/small_exac_common_3.vcf"",; ""Mutect2.variants_for_contamination_idx"": ""gs://gatk-best-practices/somatic-b37/small_exac_common_3.vcf.idx"",; ""Mutect2.realignment_index_bundle"": ""gs://gatk-test-data/mutect2/Homo_sapiens_assembly38.index_bundle""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679400684:162,Testability,test,test,162,The path seems like it should be fine according to:; https://github.com/broadinstitute/cromwell/blob/f1cce2cd2723b849c4d8f285510f30913ec188a0/filesystems/sra/src/test/scala/cromwell/filesystems/sra/SraPathBuilderSpec.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679400684
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852:82,Availability,error,error,82,"Alright so if I have a sra stanza in the `engine` portion of the config I get the error from above. . If I remove it and only keep the sra stanza in the top level filesystems part of the config and an sra stanza in the backend filesystems portion of the config I then get the following error:. ```; [2020-08-24 17:31:17,07] [info] WorkflowManagerActor Workflow fbc40d55-a668-4fd8-982c-e53333ad04f5 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'tumor_only_reads_size' (reason 1 of 1): Evaluating ceil(size(tumor_reads, ""GB"")) failed: java.lang.IllegalArgumentException: Could not build the path ""sra://SRR2841273/SRR2841273"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: Google Cloud Storage, HTTP, LinuxFileSystem. Failures: ; Google Cloud Storage: Cloud Storage URIs must have 'gs' scheme: sra://SRR2841273/SRR2841273 (IllegalArgumentException); HTTP: sra://SRR2841273/SRR2841273 does not have an http or https scheme (IllegalArgumentException); LinuxFileSystem: Cannot build a local path from sra://SRR2841273/SRR2841273 (RuntimeException); Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:538); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$.loop$1(Eval.scala:338); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852:286,Availability,error,error,286,"Alright so if I have a sra stanza in the `engine` portion of the config I get the error from above. . If I remove it and only keep the sra stanza in the top level filesystems part of the config and an sra stanza in the backend filesystems portion of the config I then get the following error:. ```; [2020-08-24 17:31:17,07] [info] WorkflowManagerActor Workflow fbc40d55-a668-4fd8-982c-e53333ad04f5 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'tumor_only_reads_size' (reason 1 of 1): Evaluating ceil(size(tumor_reads, ""GB"")) failed: java.lang.IllegalArgumentException: Could not build the path ""sra://SRR2841273/SRR2841273"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: Google Cloud Storage, HTTP, LinuxFileSystem. Failures: ; Google Cloud Storage: Cloud Storage URIs must have 'gs' scheme: sra://SRR2841273/SRR2841273 (IllegalArgumentException); HTTP: sra://SRR2841273/SRR2841273 does not have an http or https scheme (IllegalArgumentException); LinuxFileSystem: Cannot build a local path from sra://SRR2841273/SRR2841273 (RuntimeException); Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:538); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$.loop$1(Eval.scala:338); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852:812,Availability,Failure,Failures,812,"stanza in the `engine` portion of the config I get the error from above. . If I remove it and only keep the sra stanza in the top level filesystems part of the config and an sra stanza in the backend filesystems portion of the config I then get the following error:. ```; [2020-08-24 17:31:17,07] [info] WorkflowManagerActor Workflow fbc40d55-a668-4fd8-982c-e53333ad04f5 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'tumor_only_reads_size' (reason 1 of 1): Evaluating ceil(size(tumor_reads, ""GB"")) failed: java.lang.IllegalArgumentException: Could not build the path ""sra://SRR2841273/SRR2841273"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: Google Cloud Storage, HTTP, LinuxFileSystem. Failures: ; Google Cloud Storage: Cloud Storage URIs must have 'gs' scheme: sra://SRR2841273/SRR2841273 (IllegalArgumentException); HTTP: sra://SRR2841273/SRR2841273 does not have an http or https scheme (IllegalArgumentException); LinuxFileSystem: Cannot build a local path from sra://SRR2841273/SRR2841273 (RuntimeException); Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:538); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$.loop$1(Eval.scala:338); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852:65,Modifiability,config,config,65,"Alright so if I have a sra stanza in the `engine` portion of the config I get the error from above. . If I remove it and only keep the sra stanza in the top level filesystems part of the config and an sra stanza in the backend filesystems portion of the config I then get the following error:. ```; [2020-08-24 17:31:17,07] [info] WorkflowManagerActor Workflow fbc40d55-a668-4fd8-982c-e53333ad04f5 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'tumor_only_reads_size' (reason 1 of 1): Evaluating ceil(size(tumor_reads, ""GB"")) failed: java.lang.IllegalArgumentException: Could not build the path ""sra://SRR2841273/SRR2841273"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: Google Cloud Storage, HTTP, LinuxFileSystem. Failures: ; Google Cloud Storage: Cloud Storage URIs must have 'gs' scheme: sra://SRR2841273/SRR2841273 (IllegalArgumentException); HTTP: sra://SRR2841273/SRR2841273 does not have an http or https scheme (IllegalArgumentException); LinuxFileSystem: Cannot build a local path from sra://SRR2841273/SRR2841273 (RuntimeException); Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:538); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$.loop$1(Eval.scala:338); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852:187,Modifiability,config,config,187,"Alright so if I have a sra stanza in the `engine` portion of the config I get the error from above. . If I remove it and only keep the sra stanza in the top level filesystems part of the config and an sra stanza in the backend filesystems portion of the config I then get the following error:. ```; [2020-08-24 17:31:17,07] [info] WorkflowManagerActor Workflow fbc40d55-a668-4fd8-982c-e53333ad04f5 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'tumor_only_reads_size' (reason 1 of 1): Evaluating ceil(size(tumor_reads, ""GB"")) failed: java.lang.IllegalArgumentException: Could not build the path ""sra://SRR2841273/SRR2841273"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: Google Cloud Storage, HTTP, LinuxFileSystem. Failures: ; Google Cloud Storage: Cloud Storage URIs must have 'gs' scheme: sra://SRR2841273/SRR2841273 (IllegalArgumentException); HTTP: sra://SRR2841273/SRR2841273 does not have an http or https scheme (IllegalArgumentException); LinuxFileSystem: Cannot build a local path from sra://SRR2841273/SRR2841273 (RuntimeException); Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:538); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$.loop$1(Eval.scala:338); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852:254,Modifiability,config,config,254,"Alright so if I have a sra stanza in the `engine` portion of the config I get the error from above. . If I remove it and only keep the sra stanza in the top level filesystems part of the config and an sra stanza in the backend filesystems portion of the config I then get the following error:. ```; [2020-08-24 17:31:17,07] [info] WorkflowManagerActor Workflow fbc40d55-a668-4fd8-982c-e53333ad04f5 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'tumor_only_reads_size' (reason 1 of 1): Evaluating ceil(size(tumor_reads, ""GB"")) failed: java.lang.IllegalArgumentException: Could not build the path ""sra://SRR2841273/SRR2841273"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: Google Cloud Storage, HTTP, LinuxFileSystem. Failures: ; Google Cloud Storage: Cloud Storage URIs must have 'gs' scheme: sra://SRR2841273/SRR2841273 (IllegalArgumentException); HTTP: sra://SRR2841273/SRR2841273 does not have an http or https scheme (IllegalArgumentException); LinuxFileSystem: Cannot build a local path from sra://SRR2841273/SRR2841273 (RuntimeException); Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:538); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$.loop$1(Eval.scala:338); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852:1205,Modifiability,config,configure,1205,"stanza in the `engine` portion of the config I get the error from above. . If I remove it and only keep the sra stanza in the top level filesystems part of the config and an sra stanza in the backend filesystems portion of the config I then get the following error:. ```; [2020-08-24 17:31:17,07] [info] WorkflowManagerActor Workflow fbc40d55-a668-4fd8-982c-e53333ad04f5 failed (during ExecutingWorkflowState): java.lang.RuntimeException: Failed to evaluate 'tumor_only_reads_size' (reason 1 of 1): Evaluating ceil(size(tumor_reads, ""GB"")) failed: java.lang.IllegalArgumentException: Could not build the path ""sra://SRR2841273/SRR2841273"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: Google Cloud Storage, HTTP, LinuxFileSystem. Failures: ; Google Cloud Storage: Cloud Storage URIs must have 'gs' scheme: sra://SRR2841273/SRR2841273 (IllegalArgumentException); HTTP: sra://SRR2841273/SRR2841273 does not have an http or https scheme (IllegalArgumentException); LinuxFileSystem: Cannot build a local path from sra://SRR2841273/SRR2841273 (RuntimeException); Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:29); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$startRunnableNodes$7(WorkflowExecutionActor.scala:538); 	at cats.instances.ListInstances$$anon$1.$anonfun$traverse$2(list.scala:74); 	at cats.instances.ListInstances$$anon$1.loop$2(list.scala:64); 	at cats.instances.ListInstances$$anon$1.$anonfun$foldRight$1(list.scala:64); 	at cats.Eval$.loop$1(Eval.scala:338); 	at cats.Eval$.cats$Eval$$evaluate(Eval.scala:368); 	at cats.Eval$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852:2902,Testability,Log,LoggingFSM,2902,ances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$2.traverse(Traverse.scala:19); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableNodes(WorkflowExecutionActor.scala:532); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:191); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:189); 	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:172); 	at akka.actor.FSM.processEvent(FSM.scala:710); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:52); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:52); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:695); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:52); 	at akka.actor.Timers.aroundReceive(Timers.scala:51); 	at akka.actor.Timers.aroundReceive$(Timers.scala:40); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:52); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852:2982,Testability,Log,LoggingFSM,2982,stances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$2.traverse(Traverse.scala:19); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableNodes(WorkflowExecutionActor.scala:532); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:191); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:189); 	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:172); 	at akka.actor.FSM.processEvent(FSM.scala:710); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:52); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:52); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:695); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:52); 	at akka.actor.Timers.aroundReceive(Timers.scala:51); 	at akka.actor.Timers.aroundReceive$(Timers.scala:40); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:52); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852:3037,Testability,Log,LoggingFSM,3037,rse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$2.traverse(Traverse.scala:19); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableNodes(WorkflowExecutionActor.scala:532); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:191); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:189); 	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:172); 	at akka.actor.FSM.processEvent(FSM.scala:710); 	at akka.actor.FSM.processEvent$(FSM.scala:704); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:52); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:847); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:829); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:52); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:695); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:52); 	at akka.actor.Timers.aroundReceive(Timers.scala:51); 	at akka.actor.Timers.aroundReceive$(Timers.scala:40); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:52); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.d,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679437852
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:226,Availability,error,error,226,"Okay, I've made more progress. But more issues are popping up. @cjllanwarne . You cannot ask for the filesize of an sra file to configure the disk space you'd like at runtime thats what causes the ; `[2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath` issue mentioned above. When I remove that line from my wdl things get better, but I'm running into two new separate issues. 1. Cromwell tries to chmod the mounted sra directory which is not allowed.; code:; https://github.com/broadinstitute/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:696,Availability,error,error,696,"Okay, I've made more progress. But more issues are popping up. @cjllanwarne . You cannot ask for the filesize of an sra file to configure the disk space you'd like at runtime thats what causes the ; `[2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath` issue mentioned above. When I remove that line from my wdl things get better, but I'm running into two new separate issues. 1. Cromwell tries to chmod the mounted sra directory which is not allowed.; code:; https://github.com/broadinstitute/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:964,Availability,error,error,964,"Okay, I've made more progress. But more issues are popping up. @cjllanwarne . You cannot ask for the filesize of an sra file to configure the disk space you'd like at runtime thats what causes the ; `[2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath` issue mentioned above. When I remove that line from my wdl things get better, but I'm running into two new separate issues. 1. Cromwell tries to chmod the mounted sra directory which is not allowed.; code:; https://github.com/broadinstitute/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2672,Availability,recover,recoverWith,2672,on.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:3923,Availability,error,error,3923,"xecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2020-08-25 10:40:46,27] [info] WorkflowManagerActor WorkflowActor-282f5595-171e-4296-a7fa-9bd9f7a2f33b is in a terminal state: WorkflowFailedState; ```; However, this error occurs only about 80 percent of the time when I'm trying to run a job. 2. Cromwell doesn't localize sra files to the right level; You can see below ; `2020/08/25 17:32:56 Localizing input sra://SRR2806786/SRR2806786 -> /cromwell_root/SRR2806786/SRR2806786`; but the workflow calls the file in my script; `mv: cannot stat ‘/cromwell_root/sra-SRR2806786/SRR2806786/SRR2806786’: No such file or directory`. Full log:; ```; 2020/08/25 17:32:38 Starting container setup.; 2020/08/25 17:32:43 Done container setup.; 2020/08/25 17:32:44 Starting localization.; 2020/08/25 17:32:51 Localization script execution started...; 2020/08/25 17:32:51 Localizing input gs://chip_dbgap/Mutect2/b749ec2f-c549-4d26-8fec-2fe271a37b75/call-renameBamIndex/script -> /cromwell_root/script; 2020/08/25 17:32:52 Localization script execution complete.; 2020/08/25 17:32:56 Localizing input sra://SRR2806786/SRR2806786 -> /cromwell_root/SRR2806786/SRR2806786; 2020/08/25 17:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:593,Deployability,pipeline,pipelines,593,"Okay, I've made more progress. But more issues are popping up. @cjllanwarne . You cannot ask for the filesize of an sra file to configure the disk space you'd like at runtime thats what causes the ; `[2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath` issue mentioned above. When I remove that line from my wdl things get better, but I'm running into two new separate issues. 1. Cromwell tries to chmod the mounted sra directory which is not allowed.; code:; https://github.com/broadinstitute/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:651,Deployability,pipeline,pipelines,651,"Okay, I've made more progress. But more issues are popping up. @cjllanwarne . You cannot ask for the filesize of an sra file to configure the disk space you'd like at runtime thats what causes the ; `[2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath` issue mentioned above. When I remove that line from my wdl things get better, but I'm running into two new separate issues. 1. Cromwell tries to chmod the mounted sra directory which is not allowed.; code:; https://github.com/broadinstitute/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1505,Deployability,pipeline,pipelines,1505,"te/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1522,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1522,"286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionAc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1583,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1583,"pelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(S",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1664,Deployability,pipeline,pipelines,1664,"/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1681,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1681,"; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1747,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1747,"Actor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transfor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1829,Deployability,pipeline,pipelines,1829,"rkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Prom",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1846,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1846,": Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1922,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1922,"ped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecuto",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2046,Deployability,pipeline,pipelines,2046,"-c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.j",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2063,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2063,"od -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFun",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2128,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2128," was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockCont",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2210,Deployability,pipeline,pipelines,2210,"n/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2227,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2227," && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingEx",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2292,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2292,ermissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:128,Modifiability,config,configure,128,"Okay, I've made more progress. But more issues are popping up. @cjllanwarne . You cannot ask for the filesize of an sra file to configure the disk space you'd like at runtime thats what causes the ; `[2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath` issue mentioned above. When I remove that line from my wdl things get better, but I'm running into two new separate issues. 1. Cromwell tries to chmod the mounted sra directory which is not allowed.; code:; https://github.com/broadinstitute/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2645,Performance,concurren,concurrent,2645,ackend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2715,Performance,concurren,concurrent,2715,or.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.ru,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2793,Performance,concurren,concurrent,2793,"); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2020-08-25 10:40:46,27] [info] WorkflowMana",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:3118,Performance,concurren,concurrent,3118,"lure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2020-08-25 10:40:46,27] [info] WorkflowManagerActor WorkflowActor-282f5595-171e-4296-a7fa-9bd9f7a2f33b is in a terminal state: WorkflowFailedState; ```; However, this error occurs only about 80 percent of the time when I'm trying to run a job. 2. Cromwell doesn't localize sra files to the right level; You can see below ; `2020/08/25 17:32:56 Localizing input sra://S",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2672,Safety,recover,recoverWith,2672,on.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:4338,Testability,log,log,4338,"utorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2020-08-25 10:40:46,27] [info] WorkflowManagerActor WorkflowActor-282f5595-171e-4296-a7fa-9bd9f7a2f33b is in a terminal state: WorkflowFailedState; ```; However, this error occurs only about 80 percent of the time when I'm trying to run a job. 2. Cromwell doesn't localize sra files to the right level; You can see below ; `2020/08/25 17:32:56 Localizing input sra://SRR2806786/SRR2806786 -> /cromwell_root/SRR2806786/SRR2806786`; but the workflow calls the file in my script; `mv: cannot stat ‘/cromwell_root/sra-SRR2806786/SRR2806786/SRR2806786’: No such file or directory`. Full log:; ```; 2020/08/25 17:32:38 Starting container setup.; 2020/08/25 17:32:43 Done container setup.; 2020/08/25 17:32:44 Starting localization.; 2020/08/25 17:32:51 Localization script execution started...; 2020/08/25 17:32:51 Localizing input gs://chip_dbgap/Mutect2/b749ec2f-c549-4d26-8fec-2fe271a37b75/call-renameBamIndex/script -> /cromwell_root/script; 2020/08/25 17:32:52 Localization script execution complete.; 2020/08/25 17:32:56 Localizing input sra://SRR2806786/SRR2806786 -> /cromwell_root/SRR2806786/SRR2806786; 2020/08/25 17:32:58 Done localization.; 2020/08/25 17:33:00 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= us.gcr.io/broad-gotc-prod/genomes-in-the-cloud@sha256:4fca8ca945c17fd86e31eeef1c02983e091d4f2cb437199e74b164d177d5b2d1 /bin/bash /cromwell_root/script; mv: cannot stat ‘/cromwell_root/sra-SRR2806786/SRR2806786/SRR2806786’: No such file or directory; 2020/08/25 17:33:02 Starting delocalization.; 2020/08/25 17:33:03 Delocalization script execution started...; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929
https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:1087,Availability,down,downloads,1087,"flow:; ```. workflow Mutect2 {; input {; ; File tumor_reads; Int small_task_cpu = 2; Int small_task_mem = 4; Int small_task_disk = 100; Int boot_disk_size = 12; Int learn_read_orientation_mem = 8000; Int filter_alignment_artifacts_mem = 9000. # Use as a last resort to increase the disk given to every task in case of ill behaving data; Int? emergency_extra_disk. # These are multipliers to multipler inputs by to make sure we have enough disk to accommodate for possible output sizes; # Large is for Bams/WGS vcfs; # Small is for metrics/other vcfs; Float large_input_to_output_multiplier = 2.25; Float small_input_to_output_multiplier = 2.0; Float cram_to_bam_multiplier = 6.0; }. Int preemptible_or_default = 2; Int max_retries_or_default = 2. # Disk sizes used for dynamic sizing; Int ref_size = 10; Int tumor_only_reads_size = 10; Int tumor_reads_size = tumor_only_reads_size + 1; Int gnomad_vcf_size = 1; Int normal_reads_size = 1. # If no tar is provided, the task downloads one from broads ftp server; Int funco_tar_size = 100; Int gatk_override_size = 0. # This is added to every task as padding, should increase if systematically you need more disk for every call; Int disk_pad = 10 + gatk_override_size . # logic about output file names -- these are the names *without* .vcf extensions; String output_basename = ""SRR2619134"" #hacky way to strip either .bam or .cram; String output_fullname = ""SRR2619134""; . Int tumor_cram_to_bam_disk = 10; Int normal_cram_to_bam_disk = 10. ; # assume alignment file without suffix is bam; # rename and index bam files without .bam suffix. call renameBamIndex {; input:; name = output_basename,; bam = tumor_reads,; disk_size = tumor_cram_to_bam_disk,. }; . output {; File filtered_vcf = renameBamIndex.output_bam; }; }. task renameBamIndex {; input {; String name; File bam; Int disk_size; Int? mem; String? sra; File? ngc; }; ; Int machine_mem = if defined(mem) then mem * 1000 else 6000; ; command {; echo ~{bam}; cp ~{bam} ~{name}.bam; cp /cromwell_ro",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161
https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:2064,Availability,echo,echo,2064,"ads_size + 1; Int gnomad_vcf_size = 1; Int normal_reads_size = 1. # If no tar is provided, the task downloads one from broads ftp server; Int funco_tar_size = 100; Int gatk_override_size = 0. # This is added to every task as padding, should increase if systematically you need more disk for every call; Int disk_pad = 10 + gatk_override_size . # logic about output file names -- these are the names *without* .vcf extensions; String output_basename = ""SRR2619134"" #hacky way to strip either .bam or .cram; String output_fullname = ""SRR2619134""; . Int tumor_cram_to_bam_disk = 10; Int normal_cram_to_bam_disk = 10. ; # assume alignment file without suffix is bam; # rename and index bam files without .bam suffix. call renameBamIndex {; input:; name = output_basename,; bam = tumor_reads,; disk_size = tumor_cram_to_bam_disk,. }; . output {; File filtered_vcf = renameBamIndex.output_bam; }; }. task renameBamIndex {; input {; String name; File bam; Int disk_size; Int? mem; String? sra; File? ngc; }; ; Int machine_mem = if defined(mem) then mem * 1000 else 6000; ; command {; echo ~{bam}; cp ~{bam} ~{name}.bam; cp /cromwell_root/~{name}/~{name} ~{name}.bam. samtools index -b ~{name}.bam; cp ~{name}.bam.bai ~{name}.bai; }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.3-1513176735""; memory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161
https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3060,Deployability,pipeline,pipelines,3060,"cho ~{bam}; cp ~{bam} ~{name}.bam; cp /cromwell_root/~{name}/~{name} ~{name}.bam. samtools index -b ~{name}.bam; cp ~{name}.bam.bai ~{name}.bai; }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.3-1513176735""; memory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161
https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3079,Deployability,Pipeline,PipelinesApiLifecycleActorFactory,3079,"mory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161
https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3823,Deployability,Pipeline,Pipelines,3823," output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 12800000; json = 12800000; tsv = 12800000; map = 12800000; object = 12800000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161
https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3115,Modifiability,config,config,3115,"mory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161
https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3125,Performance,concurren,concurrent-job-limit,3125,"mory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161
https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3159,Performance,concurren,concurrent-workflows,3159,"mory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161
https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3631,Security,access,access,3631," output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 12800000; json = 12800000; tsv = 12800000; map = 12800000; object = 12800000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161
https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:1333,Testability,log,logic,1333," as a last resort to increase the disk given to every task in case of ill behaving data; Int? emergency_extra_disk. # These are multipliers to multipler inputs by to make sure we have enough disk to accommodate for possible output sizes; # Large is for Bams/WGS vcfs; # Small is for metrics/other vcfs; Float large_input_to_output_multiplier = 2.25; Float small_input_to_output_multiplier = 2.0; Float cram_to_bam_multiplier = 6.0; }. Int preemptible_or_default = 2; Int max_retries_or_default = 2. # Disk sizes used for dynamic sizing; Int ref_size = 10; Int tumor_only_reads_size = 10; Int tumor_reads_size = tumor_only_reads_size + 1; Int gnomad_vcf_size = 1; Int normal_reads_size = 1. # If no tar is provided, the task downloads one from broads ftp server; Int funco_tar_size = 100; Int gatk_override_size = 0. # This is added to every task as padding, should increase if systematically you need more disk for every call; Int disk_pad = 10 + gatk_override_size . # logic about output file names -- these are the names *without* .vcf extensions; String output_basename = ""SRR2619134"" #hacky way to strip either .bam or .cram; String output_fullname = ""SRR2619134""; . Int tumor_cram_to_bam_disk = 10; Int normal_cram_to_bam_disk = 10. ; # assume alignment file without suffix is bam; # rename and index bam files without .bam suffix. call renameBamIndex {; input:; name = output_basename,; bam = tumor_reads,; disk_size = tumor_cram_to_bam_disk,. }; . output {; File filtered_vcf = renameBamIndex.output_bam; }; }. task renameBamIndex {; input {; String name; File bam; Int disk_size; Int? mem; String? sra; File? ngc; }; ; Int machine_mem = if defined(mem) then mem * 1000 else 6000; ; command {; echo ~{bam}; cp ~{bam} ~{name}.bam; cp /cromwell_root/~{name}/~{name} ~{name}.bam. samtools index -b ~{name}.bam; cp ~{name}.bam.bai ~{name}.bai; }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.3-1513176735""; memory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + """,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161
https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3447,Testability,test,test,3447,"mory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161
https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-756439506:304,Usability,simpl,simply,304,"Hi @nvanaja, thanks for your PR. Since you've got a few PRs open, we've added you as a collaborator so you can make branches directly in our repo. This enables our CI to run automatically for you (we are careful with it because it runs workflows in the cloud with real money). To take advantage of this, simply push a new branch to the broadinstitute/cromwell repo and create a new PR, for this one as well as https://github.com/broadinstitute/cromwell/pull/6058. Thank you again for your contributions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-756439506
https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-759700421:313,Usability,simpl,simply,313,"> Hi @nvanaja, thanks for your PR. Since you've got a few PRs open, we've added you as a collaborator so you can make branches directly in our repo. This enables our CI to run automatically for you (we are careful with it because it runs workflows in the cloud with real money).; > ; > To take advantage of this, simply push a new branch to the broadinstitute/cromwell repo and create a new PR, for this one as >well as #6058. Hi @aednichols, #6058 includes these changes also. So I don't need this PR anymore. #6058 will do.; I'll close this. ; Do I still need to create a new branch for #6058? ; Thanks,; Vanaja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-759700421
https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-759823836:100,Availability,redundant,redundant,100,"The changes for this fix are just a subset of the ones in open request #6058, hence this request is redundant now,",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-759823836
https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-759823836:100,Safety,redund,redundant,100,"The changes for this fix are just a subset of the ones in open request #6058, hence this request is redundant now,",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-759823836
https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-684392711:528,Availability,error,error,528,"On my OS; ```; java -version; openjdk version ""11.0.8"" 2020-07-14; OpenJDK Runtime Environment (build 11.0.8+10-post-Debian-1deb10u1); OpenJDK 64-Bit Server VM (build 11.0.8+10-post-Debian-1deb10u1, mixed mode, sharing); ```; Using conda-forge compiled java (default for conda package of cromwell); ```; java -version; openjdk version ""1.8.0_192""; OpenJDK Runtime Environment (Zulu 8.33.0.1-linux64) (build 1.8.0_192-b01); OpenJDK 64-Bit Server VM (Zulu 8.33.0.1-linux64) (build 25.192-b01, mixed mode); ```. Both give the same error. Maybe it is Oracle vs OpenJDK related errors?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-684392711
https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-684392711:573,Availability,error,errors,573,"On my OS; ```; java -version; openjdk version ""11.0.8"" 2020-07-14; OpenJDK Runtime Environment (build 11.0.8+10-post-Debian-1deb10u1); OpenJDK 64-Bit Server VM (build 11.0.8+10-post-Debian-1deb10u1, mixed mode, sharing); ```; Using conda-forge compiled java (default for conda package of cromwell); ```; java -version; openjdk version ""1.8.0_192""; OpenJDK Runtime Environment (Zulu 8.33.0.1-linux64) (build 1.8.0_192-b01); OpenJDK 64-Bit Server VM (Zulu 8.33.0.1-linux64) (build 25.192-b01, mixed mode); ```. Both give the same error. Maybe it is Oracle vs OpenJDK related errors?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-684392711
https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-685331409:15,Availability,error,error,15,I got the same error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-685331409
https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-686246049:17,Deployability,update,update,17,Fixed in a point update. Apologies for the inconvenience. https://github.com/broadinstitute/cromwell/releases/tag/53.1,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-686246049
https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-686246049:101,Deployability,release,releases,101,Fixed in a point update. Apologies for the inconvenience. https://github.com/broadinstitute/cromwell/releases/tag/53.1,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-686246049
https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-794320092:219,Availability,error,error-keys,219,"If I'm understanding the concern correctly, you're worried about about Cromwell retrying a task based on the return code, even when the problem was not memory related. Cromwell requires a member of `system.memory-retry-error-keys` to be present, so it does not just use the return code. Note that memory retry was marked as an experimental feature and has experienced a breaking change since this issue was filed: https://github.com/broadinstitute/cromwell/releases/tag/56. Since I _think_ your concern is already addressed, I'm going to close the issue. Feel free to open if otherwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-794320092
https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-794320092:457,Deployability,release,releases,457,"If I'm understanding the concern correctly, you're worried about about Cromwell retrying a task based on the return code, even when the problem was not memory related. Cromwell requires a member of `system.memory-retry-error-keys` to be present, so it does not just use the return code. Note that memory retry was marked as an experimental feature and has experienced a breaking change since this issue was filed: https://github.com/broadinstitute/cromwell/releases/tag/56. Since I _think_ your concern is already addressed, I'm going to close the issue. Feel free to open if otherwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-794320092
https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372:1027,Availability,echo,echo,1027,"I would like to reopen this issue. I have been testing the `memory-retry` feature since it should be very useful for my project on GCP. However, Cromwell does not retry any job exited with SIGKILL (`137`) and all jobs killed by an OOM-killer get `137` as an exit-code. So this `memory-retry` feature doesn't work at all. And I found this.; https://github.com/broadinstitute/cromwell/blob/171f12c890373e896b4eab1f9f4ad23660dc80f3/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala#L308. So even though I configure the two `memory-retry` parameters correctly in `backend.conf` and workflow options JSON. it's useless. Cromwell does not retry any job exited with `137`. . I tested with a fake OOM with exit code `1` and `137` and Cromwell retried the task with exit code `1` only.; ```; version 1.0. workflow mem_retry {; call fail_with_fake_oom; call fail_with_true_oom; }. task fail_with_fake_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; >&2 echo ""Killed""; exit 137 # cromwell does not retry the task if it gets 137; #exit 1 # cromwell retries the task if it gets 1 ; fi; >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. task fail_with_true_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero; fi. >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372
https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372:1105,Availability,echo,echo,1105,"I would like to reopen this issue. I have been testing the `memory-retry` feature since it should be very useful for my project on GCP. However, Cromwell does not retry any job exited with SIGKILL (`137`) and all jobs killed by an OOM-killer get `137` as an exit-code. So this `memory-retry` feature doesn't work at all. And I found this.; https://github.com/broadinstitute/cromwell/blob/171f12c890373e896b4eab1f9f4ad23660dc80f3/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala#L308. So even though I configure the two `memory-retry` parameters correctly in `backend.conf` and workflow options JSON. it's useless. Cromwell does not retry any job exited with `137`. . I tested with a fake OOM with exit code `1` and `137` and Cromwell retried the task with exit code `1` only.; ```; version 1.0. workflow mem_retry {; call fail_with_fake_oom; call fail_with_true_oom; }. task fail_with_fake_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; >&2 echo ""Killed""; exit 137 # cromwell does not retry the task if it gets 137; #exit 1 # cromwell retries the task if it gets 1 ; fi; >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. task fail_with_true_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero; fi. >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372
https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372:1134,Availability,echo,echo,1134,"I would like to reopen this issue. I have been testing the `memory-retry` feature since it should be very useful for my project on GCP. However, Cromwell does not retry any job exited with SIGKILL (`137`) and all jobs killed by an OOM-killer get `137` as an exit-code. So this `memory-retry` feature doesn't work at all. And I found this.; https://github.com/broadinstitute/cromwell/blob/171f12c890373e896b4eab1f9f4ad23660dc80f3/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala#L308. So even though I configure the two `memory-retry` parameters correctly in `backend.conf` and workflow options JSON. it's useless. Cromwell does not retry any job exited with `137`. . I tested with a fake OOM with exit code `1` and `137` and Cromwell retried the task with exit code `1` only.; ```; version 1.0. workflow mem_retry {; call fail_with_fake_oom; call fail_with_true_oom; }. task fail_with_fake_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; >&2 echo ""Killed""; exit 137 # cromwell does not retry the task if it gets 137; #exit 1 # cromwell retries the task if it gets 1 ; fi; >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. task fail_with_true_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero; fi. >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372
https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372:1451,Availability,echo,echo,1451,"I would like to reopen this issue. I have been testing the `memory-retry` feature since it should be very useful for my project on GCP. However, Cromwell does not retry any job exited with SIGKILL (`137`) and all jobs killed by an OOM-killer get `137` as an exit-code. So this `memory-retry` feature doesn't work at all. And I found this.; https://github.com/broadinstitute/cromwell/blob/171f12c890373e896b4eab1f9f4ad23660dc80f3/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala#L308. So even though I configure the two `memory-retry` parameters correctly in `backend.conf` and workflow options JSON. it's useless. Cromwell does not retry any job exited with `137`. . I tested with a fake OOM with exit code `1` and `137` and Cromwell retried the task with exit code `1` only.; ```; version 1.0. workflow mem_retry {; call fail_with_fake_oom; call fail_with_true_oom; }. task fail_with_fake_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; >&2 echo ""Killed""; exit 137 # cromwell does not retry the task if it gets 137; #exit 1 # cromwell retries the task if it gets 1 ; fi; >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. task fail_with_true_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero; fi. >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372
https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372:1529,Availability,echo,echo,1529,"I would like to reopen this issue. I have been testing the `memory-retry` feature since it should be very useful for my project on GCP. However, Cromwell does not retry any job exited with SIGKILL (`137`) and all jobs killed by an OOM-killer get `137` as an exit-code. So this `memory-retry` feature doesn't work at all. And I found this.; https://github.com/broadinstitute/cromwell/blob/171f12c890373e896b4eab1f9f4ad23660dc80f3/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala#L308. So even though I configure the two `memory-retry` parameters correctly in `backend.conf` and workflow options JSON. it's useless. Cromwell does not retry any job exited with `137`. . I tested with a fake OOM with exit code `1` and `137` and Cromwell retried the task with exit code `1` only.; ```; version 1.0. workflow mem_retry {; call fail_with_fake_oom; call fail_with_true_oom; }. task fail_with_fake_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; >&2 echo ""Killed""; exit 137 # cromwell does not retry the task if it gets 137; #exit 1 # cromwell retries the task if it gets 1 ; fi; >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. task fail_with_true_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero; fi. >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372
https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372:492,Modifiability,config,config,492,"I would like to reopen this issue. I have been testing the `memory-retry` feature since it should be very useful for my project on GCP. However, Cromwell does not retry any job exited with SIGKILL (`137`) and all jobs killed by an OOM-killer get `137` as an exit-code. So this `memory-retry` feature doesn't work at all. And I found this.; https://github.com/broadinstitute/cromwell/blob/171f12c890373e896b4eab1f9f4ad23660dc80f3/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala#L308. So even though I configure the two `memory-retry` parameters correctly in `backend.conf` and workflow options JSON. it's useless. Cromwell does not retry any job exited with `137`. . I tested with a fake OOM with exit code `1` and `137` and Cromwell retried the task with exit code `1` only.; ```; version 1.0. workflow mem_retry {; call fail_with_fake_oom; call fail_with_true_oom; }. task fail_with_fake_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; >&2 echo ""Killed""; exit 137 # cromwell does not retry the task if it gets 137; #exit 1 # cromwell retries the task if it gets 1 ; fi; >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. task fail_with_true_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero; fi. >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372
https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372:499,Modifiability,Config,ConfigAsyncJobExecutionActor,499,"I would like to reopen this issue. I have been testing the `memory-retry` feature since it should be very useful for my project on GCP. However, Cromwell does not retry any job exited with SIGKILL (`137`) and all jobs killed by an OOM-killer get `137` as an exit-code. So this `memory-retry` feature doesn't work at all. And I found this.; https://github.com/broadinstitute/cromwell/blob/171f12c890373e896b4eab1f9f4ad23660dc80f3/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala#L308. So even though I configure the two `memory-retry` parameters correctly in `backend.conf` and workflow options JSON. it's useless. Cromwell does not retry any job exited with `137`. . I tested with a fake OOM with exit code `1` and `137` and Cromwell retried the task with exit code `1` only.; ```; version 1.0. workflow mem_retry {; call fail_with_fake_oom; call fail_with_true_oom; }. task fail_with_fake_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; >&2 echo ""Killed""; exit 137 # cromwell does not retry the task if it gets 137; #exit 1 # cromwell retries the task if it gets 1 ; fi; >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. task fail_with_true_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero; fi. >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372
https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372:557,Modifiability,config,configure,557,"I would like to reopen this issue. I have been testing the `memory-retry` feature since it should be very useful for my project on GCP. However, Cromwell does not retry any job exited with SIGKILL (`137`) and all jobs killed by an OOM-killer get `137` as an exit-code. So this `memory-retry` feature doesn't work at all. And I found this.; https://github.com/broadinstitute/cromwell/blob/171f12c890373e896b4eab1f9f4ad23660dc80f3/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala#L308. So even though I configure the two `memory-retry` parameters correctly in `backend.conf` and workflow options JSON. it's useless. Cromwell does not retry any job exited with `137`. . I tested with a fake OOM with exit code `1` and `137` and Cromwell retried the task with exit code `1` only.; ```; version 1.0. workflow mem_retry {; call fail_with_fake_oom; call fail_with_true_oom; }. task fail_with_fake_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; >&2 echo ""Killed""; exit 137 # cromwell does not retry the task if it gets 137; #exit 1 # cromwell retries the task if it gets 1 ; fi; >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. task fail_with_true_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero; fi. >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372
https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372:47,Testability,test,testing,47,"I would like to reopen this issue. I have been testing the `memory-retry` feature since it should be very useful for my project on GCP. However, Cromwell does not retry any job exited with SIGKILL (`137`) and all jobs killed by an OOM-killer get `137` as an exit-code. So this `memory-retry` feature doesn't work at all. And I found this.; https://github.com/broadinstitute/cromwell/blob/171f12c890373e896b4eab1f9f4ad23660dc80f3/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala#L308. So even though I configure the two `memory-retry` parameters correctly in `backend.conf` and workflow options JSON. it's useless. Cromwell does not retry any job exited with `137`. . I tested with a fake OOM with exit code `1` and `137` and Cromwell retried the task with exit code `1` only.; ```; version 1.0. workflow mem_retry {; call fail_with_fake_oom; call fail_with_true_oom; }. task fail_with_fake_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; >&2 echo ""Killed""; exit 137 # cromwell does not retry the task if it gets 137; #exit 1 # cromwell retries the task if it gets 1 ; fi; >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. task fail_with_true_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero; fi. >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372
https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372:725,Testability,test,tested,725,"I would like to reopen this issue. I have been testing the `memory-retry` feature since it should be very useful for my project on GCP. However, Cromwell does not retry any job exited with SIGKILL (`137`) and all jobs killed by an OOM-killer get `137` as an exit-code. So this `memory-retry` feature doesn't work at all. And I found this.; https://github.com/broadinstitute/cromwell/blob/171f12c890373e896b4eab1f9f4ad23660dc80f3/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala#L308. So even though I configure the two `memory-retry` parameters correctly in `backend.conf` and workflow options JSON. it's useless. Cromwell does not retry any job exited with `137`. . I tested with a fake OOM with exit code `1` and `137` and Cromwell retried the task with exit code `1` only.; ```; version 1.0. workflow mem_retry {; call fail_with_fake_oom; call fail_with_true_oom; }. task fail_with_fake_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; >&2 echo ""Killed""; exit 137 # cromwell does not retry the task if it gets 137; #exit 1 # cromwell retries the task if it gets 1 ; fi; >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. task fail_with_true_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero; fi. >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372
https://github.com/broadinstitute/cromwell/pull/5821#issuecomment-685044992:242,Availability,Failure,Failure,242,"That's a good point, plugging an offset into [Khalid's playground](https://scastie.scala-lang.org/79qIiIkIRReJLtbyvWQ2cg) makes it explode; ```; val timeStr2 = ""2020-01-15T17:46:25.694148-02:00""; Try(LocalDateTime.parse(timeStr2)); ```; ```; Failure(java.time.format.DateTimeParseException:; Text '2020-01-15T17:46:25.694148-02:00' could not be parsed, unparsed text found at index 26):; scala.util.Try[java.time.LocalDateTime]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5821#issuecomment-685044992
https://github.com/broadinstitute/cromwell/issues/5826#issuecomment-1292936396:31,Modifiability,enhance,enhances,31,"@aednichols is there a PR that enhances the CWL debugging capabilities of Cromwell? Unsure why this was closed as ""completed""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826#issuecomment-1292936396
https://github.com/broadinstitute/cromwell/issues/5826#issuecomment-1293264120:322,Deployability,release,release-with-cwl-support,322,"@alexiswl I agree that all the CWL tickets should have been closed ""as not planned"" given the public statements that CWL in Cromwell is deprecated (and likely to be removed). https://terra.bio/terras-roadmap-to-supporting-more-workflow-languages/. https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#last-release-with-cwl-support. https://github.com/broadinstitute/cromwell/releases/tag/79. ![Screenshot_20221027-113759](https://user-images.githubusercontent.com/1330696/198250098-fe5c53e4-115f-43e4-b334-2898a6e7c687.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826#issuecomment-1293264120
https://github.com/broadinstitute/cromwell/issues/5826#issuecomment-1293264120:391,Deployability,release,releases,391,"@alexiswl I agree that all the CWL tickets should have been closed ""as not planned"" given the public statements that CWL in Cromwell is deprecated (and likely to be removed). https://terra.bio/terras-roadmap-to-supporting-more-workflow-languages/. https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#last-release-with-cwl-support. https://github.com/broadinstitute/cromwell/releases/tag/79. ![Screenshot_20221027-113759](https://user-images.githubusercontent.com/1330696/198250098-fe5c53e4-115f-43e4-b334-2898a6e7c687.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826#issuecomment-1293264120
https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-691768881:89,Deployability,deploy,deployed,89,NOTE: Centaur tests require https://github.com/broadinstitute/martha/pull/186 merged-and-deployed,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-691768881
https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-691768881:14,Testability,test,tests,14,NOTE: Centaur tests require https://github.com/broadinstitute/martha/pull/186 merged-and-deployed,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-691768881
https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-697363597:35,Deployability,release,release,35,Tests will fail until review/merge/release of https://github.com/broadinstitute/martha/pull/190,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-697363597
https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-697363597:0,Testability,Test,Tests,0,Tests will fail until review/merge/release of https://github.com/broadinstitute/martha/pull/190,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-697363597
https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-698985699:166,Availability,failure,failure,166,FYI ready for 👀 @rsasch and @salonishah11. Regarding the failing dbms test: it's not failing due to this PR's changes and is not a blocker for review. I suspect that failure has something to do with lucky number [PostgreSQL 13](https://news.ycombinator.com/item?id=24578166) coming out yesterday. When @cjllanwarne finishes fighting Prod fires he (or perhaps someone else?) might have a small PR that can fix the issue in develop. When that fix merges I'll just restart the test in this PR and it should pick up the develop changes automagically and pass.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-698985699
https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-698985699:70,Testability,test,test,70,FYI ready for 👀 @rsasch and @salonishah11. Regarding the failing dbms test: it's not failing due to this PR's changes and is not a blocker for review. I suspect that failure has something to do with lucky number [PostgreSQL 13](https://news.ycombinator.com/item?id=24578166) coming out yesterday. When @cjllanwarne finishes fighting Prod fires he (or perhaps someone else?) might have a small PR that can fix the issue in develop. When that fix merges I'll just restart the test in this PR and it should pick up the develop changes automagically and pass.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-698985699
https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-698985699:474,Testability,test,test,474,FYI ready for 👀 @rsasch and @salonishah11. Regarding the failing dbms test: it's not failing due to this PR's changes and is not a blocker for review. I suspect that failure has something to do with lucky number [PostgreSQL 13](https://news.ycombinator.com/item?id=24578166) coming out yesterday. When @cjllanwarne finishes fighting Prod fires he (or perhaps someone else?) might have a small PR that can fix the issue in develop. When that fix merges I'll just restart the test in this PR and it should pick up the develop changes automagically and pass.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5850#issuecomment-698985699
https://github.com/broadinstitute/cromwell/issues/5857#issuecomment-693513576:314,Deployability,update,update,314,"I have checked in a new version. Will make a pull request for it soon,. https://github.com/broadinstitute/cromwell/blob/mjs-AWS-config-example-fix/cromwell.example.backends/AWS.conf. On Wed, Sep 16, 2020 at 6:14 AM openbioinfomatics for more people who need; it <notifications@github.com> wrote:. > would you guys update ""cromwell/cromwell.example.backends/AWS.conf""; >; > it seams this file for old version .; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5857>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EK6P5Z7C4RDBY2BIVDSGCFZTANCNFSM4ROS34OQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5857#issuecomment-693513576
https://github.com/broadinstitute/cromwell/issues/5857#issuecomment-693513576:128,Modifiability,config,config-example-fix,128,"I have checked in a new version. Will make a pull request for it soon,. https://github.com/broadinstitute/cromwell/blob/mjs-AWS-config-example-fix/cromwell.example.backends/AWS.conf. On Wed, Sep 16, 2020 at 6:14 AM openbioinfomatics for more people who need; it <notifications@github.com> wrote:. > would you guys update ""cromwell/cromwell.example.backends/AWS.conf""; >; > it seams this file for old version .; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5857>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EK6P5Z7C4RDBY2BIVDSGCFZTANCNFSM4ROS34OQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5857#issuecomment-693513576
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694157851:282,Availability,error,error,282,"Hey @ParkvilleData, just make sure you're formatting your code with three backticks, then starting the code on the next line, like:. ````; ```; <code starts here/>; ```; ````. The single backticks `` ` `` are for in-line entries, eg: `` `code` `` -> `code`. Can you post the actual error you're seeing, or can you confirm none of your tasks are returning a valid value for ""docker"" in the runtime section.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694157851
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:43,Availability,error,error,43,"Thanks @illusional !. Here is the complete error and I can confirm there is no valid value for docker in the run time section. It also completely hangs when I run this and I have to kill the process. The Local provider is placed right after the Slurm provider in the provider block. ```; [2020-09-17 21:41:42,92] [info] MaterializeWorkflowDescriptorActor [866769d0]: Call-to-Backend assignments: hostremoval_subworkflow.interleave_task -> Local, geneprediction_subworkflow.prodigal_task -> Local, qc_subworkflow.flash_task -> Local, assembly_subworkflow.blast_task -> Local, metaGenPipe.merge_task -> Local, geneprediction_subworkflow.diamond_task -> Local, assembly_subworkflow.metaspades_task -> Local, assembly_subworkflow.megahit_task -> Local, hostremoval_subworkflow.hostremoval_task -> Local, geneprediction_subworkflow.collation_task -> Local, assembly_subworkflow.idba_task -> Local, qc_subworkflow.trimmomatic_task -> Local, metaGenPipe.taxonclass_task -> Local, qc_subworkflow.fastqc_task -> Local, metaGenPipe.multiqc_task -> Local; [2020-09-17 21:41:42,97] [error] Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --en",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:1071,Availability,error,error,1071," is no valid value for docker in the run time section. It also completely hangs when I run this and I have to kill the process. The Local provider is placed right after the Slurm provider in the provider block. ```; [2020-09-17 21:41:42,92] [info] MaterializeWorkflowDescriptorActor [866769d0]: Call-to-Backend assignments: hostremoval_subworkflow.interleave_task -> Local, geneprediction_subworkflow.prodigal_task -> Local, qc_subworkflow.flash_task -> Local, assembly_subworkflow.blast_task -> Local, metaGenPipe.merge_task -> Local, geneprediction_subworkflow.diamond_task -> Local, assembly_subworkflow.metaspades_task -> Local, assembly_subworkflow.megahit_task -> Local, hostremoval_subworkflow.hostremoval_task -> Local, geneprediction_subworkflow.collation_task -> Local, assembly_subworkflow.idba_task -> Local, qc_subworkflow.trimmomatic_task -> Local, metaGenPipe.taxonclass_task -> Local, qc_subworkflow.fastqc_task -> Local, metaGenPipe.multiqc_task -> Local; [2020-09-17 21:41:42,97] [error] Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker}",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:1078,Availability,Error,Error,1078," is no valid value for docker in the run time section. It also completely hangs when I run this and I have to kill the process. The Local provider is placed right after the Slurm provider in the provider block. ```; [2020-09-17 21:41:42,92] [info] MaterializeWorkflowDescriptorActor [866769d0]: Call-to-Backend assignments: hostremoval_subworkflow.interleave_task -> Local, geneprediction_subworkflow.prodigal_task -> Local, qc_subworkflow.flash_task -> Local, assembly_subworkflow.blast_task -> Local, metaGenPipe.merge_task -> Local, geneprediction_subworkflow.diamond_task -> Local, assembly_subworkflow.metaspades_task -> Local, assembly_subworkflow.megahit_task -> Local, hostremoval_subworkflow.hostremoval_task -> Local, geneprediction_subworkflow.collation_task -> Local, assembly_subworkflow.idba_task -> Local, qc_subworkflow.trimmomatic_task -> Local, metaGenPipe.taxonclass_task -> Local, qc_subworkflow.fastqc_task -> Local, metaGenPipe.multiqc_task -> Local; [2020-09-17 21:41:42,97] [error] Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker}",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:2447,Availability,Error,Error,2447,"tring job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait cat ${docker_cid}). # remove the container after waiting; docker rm cat ${docker_cid}. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill cat ${docker_cid}; }; }; java.lang.RuntimeException: Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:6562,Availability,ERROR,ERROR,6562,"ckend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:146); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.standard.StandardInitializationActor.aroundReceive(StandardInitializationActor.scala:44); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); at akka.actor.ActorCell.invoke(ActorCell.scala:581); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: wdl.draft2.parser.WdlParser$SyntaxError: ERROR: Variable docker does not reference any declaration in the task (line 50, col 7):. ${docker} ${docker_script}; ^. Task defined here (line 20, col 6):. task submit_docker {; ^. at wdl.draft2.model.WdlNamespace$.$anonfun$apply$55(WdlNamespace.scala:444); at scala.collection.TraversableLike$WithFilter.$anonfun$map$2(TraversableLike.scala:827); at scala.collection.immutable.List.foreach(List.scala:392); at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:826); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$52(WdlNamespace.scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at sc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:1867,Deployability,configurat,configuration,1867,"Local, metaGenPipe.taxonclass_task -> Local, qc_subworkflow.fastqc_task -> Local, metaGenPipe.multiqc_task -> Local; [2020-09-17 21:41:42,97] [error] Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait cat ${docker_cid}). # remove the container after waiting; docker rm cat ${docker_cid}. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill cat ${docker_cid}; }; }; java.lang.RuntimeException: Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3236,Deployability,configurat,configuration,3236,". task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill cat ${docker_cid}; }; }; java.lang.RuntimeException: Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.con",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:1867,Modifiability,config,configuration,1867,"Local, metaGenPipe.taxonclass_task -> Local, qc_subworkflow.fastqc_task -> Local, metaGenPipe.multiqc_task -> Local; [2020-09-17 21:41:42,97] [error] Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait cat ${docker_cid}). # remove the container after waiting; docker rm cat ${docker_cid}. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill cat ${docker_cid}; }; }; java.lang.RuntimeException: Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3236,Modifiability,config,configuration,3236,". task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill cat ${docker_cid}; }; }; java.lang.RuntimeException: Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.con",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3823,Modifiability,config,config,3823,". String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3830,Modifiability,Config,ConfigWdlNamespace,3830,"id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequenc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3856,Modifiability,Config,ConfigWdlNamespace,3856,"ring cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowIniti",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3915,Modifiability,config,config,3915,"job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3922,Modifiability,Config,ConfigInitializationActor,3922," docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializat",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3948,Modifiability,config,configWdlNamespace,3948,"cker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowIni",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:3978,Modifiability,Config,ConfigInitializationActor,3978,"cker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowIni",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4044,Modifiability,config,config,4044,"y = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitia",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4051,Modifiability,Config,ConfigInitializationActor,4051,"String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.in",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4077,Modifiability,config,configWdlNamespace,4077,"GP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationAct",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4096,Modifiability,Config,ConfigInitializationActor,4096,"GP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationAct",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4162,Modifiability,config,config,4162,"xisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationAct",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4169,Modifiability,Config,ConfigInitializationActor,4169,"D file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$rece",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4229,Modifiability,Config,ConfigInitializationActor,4229,"e original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4295,Modifiability,config,config,4295,"n \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4302,Modifiability,Config,ConfigInitializationActor,4302,"docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionTh",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4351,Modifiability,Config,ConfigInitializationActor,4351,cker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4417,Modifiability,config,config,4417,elegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenResp,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4424,Modifiability,Config,ConfigInitializationActor,4424,ker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLife,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4486,Modifiability,Config,ConfigInitializationActor,4486, (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.st,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4552,Modifiability,config,config,4552,ker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitial,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4559,Modifiability,Config,ConfigInitializationActor,4559,ove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitializationActor.sca,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:4610,Modifiability,Config,ConfigInitializationActor,4610,r rm `cat ${docker_cid}`. # return exit code; exit $rc. }; }. task kill_docker {. String job_id; String docker_cid; String job_shell. command {; docker kill `cat ${docker_cid}`; }; }; at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:55); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:39); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:42); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitializationActor.scala:44); at cromwell.backend.BackendWork,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:6569,Modifiability,Variab,Variable,6569,"ckend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:146); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.standard.StandardInitializationActor.aroundReceive(StandardInitializationActor.scala:44); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); at akka.actor.ActorCell.invoke(ActorCell.scala:581); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: wdl.draft2.parser.WdlParser$SyntaxError: ERROR: Variable docker does not reference any declaration in the task (line 50, col 7):. ${docker} ${docker_script}; ^. Task defined here (line 20, col 6):. task submit_docker {; ^. at wdl.draft2.model.WdlNamespace$.$anonfun$apply$55(WdlNamespace.scala:444); at scala.collection.TraversableLike$WithFilter.$anonfun$map$2(TraversableLike.scala:827); at scala.collection.immutable.List.foreach(List.scala:392); at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:826); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$52(WdlNamespace.scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at sc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:8913,Modifiability,config,config,8913,".scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$51(WdlNamespace.scala:441); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.apply(WdlNamespace.scala:440); at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:174); at scala.util.Try$.apply(Try.scala:213); at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:53); ... 27 common frames omitted; [2020-09-17 21:41:46,29] [info] Not triggering log of token queue status. Effective log interval = None; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:8920,Modifiability,Config,ConfigWdlNamespace,8920,".scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$51(WdlNamespace.scala:441); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.apply(WdlNamespace.scala:440); at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:174); at scala.util.Try$.apply(Try.scala:213); at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:53); ... 27 common frames omitted; [2020-09-17 21:41:46,29] [info] Not triggering log of token queue status. Effective log interval = None; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:8946,Modifiability,Config,ConfigWdlNamespace,8946,".scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$51(WdlNamespace.scala:441); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.apply(WdlNamespace.scala:440); at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:174); at scala.util.Try$.apply(Try.scala:213); at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:53); ... 27 common frames omitted; [2020-09-17 21:41:46,29] [info] Not triggering log of token queue status. Effective log interval = None; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:5300,Performance,perform,performActionThenRespond,5300,ctor.declarationValidations(ConfigInitializationActor.scala:41); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:146); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.standard.StandardInitializationActor.aroundReceive(StandardInitializationActor.scala:44); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); at akka.actor.ActorCell.invoke(ActorCell.scala:581); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runT,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:5400,Performance,perform,performActionThenRespond,5400,ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:53); at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:146); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.standard.StandardInitializationActor.aroundReceive(StandardInitializationActor.scala:44); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); at akka.actor.ActorCell.invoke(ActorCell.scala:581); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:5516,Performance,perform,performActionThenRespond,5516,.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:52); at cromwell.backend.standard.StandardInitializationActor.coerceDefaultRuntimeAttributes(StandardInitializationActor.scala:82); at cromwell.backend.BackendWorkflowInitializationActor.initSequence(BackendWorkflowInitializationActor.scala:155); at cromwell.backend.BackendWorkflowInitializationActor.initSequence$(BackendWorkflowInitializationActor.scala:153); at cromwell.backend.standard.StandardInitializationActor.initSequence(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.$anonfun$applyOrElse$1(BackendWorkflowInitializationActor.scala:146); at cromwell.backend.BackendLifecycleActor.performActionThenRespond(BackendLifecycleActor.scala:44); at cromwell.backend.BackendLifecycleActor.performActionThenRespond$(BackendLifecycleActor.scala:40); at cromwell.backend.standard.StandardInitializationActor.performActionThenRespond(StandardInitializationActor.scala:44); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:146); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.standard.StandardInitializationActor.aroundReceive(StandardInitializationActor.scala:44); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); at akka.actor.ActorCell.invoke(ActorCell.scala:581); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: wdl.draft2.parser.WdlP,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:8672,Performance,load,load,8672,".scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$51(WdlNamespace.scala:441); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.apply(WdlNamespace.scala:440); at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:174); at scala.util.Try$.apply(Try.scala:213); at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:53); ... 27 common frames omitted; [2020-09-17 21:41:46,29] [info] Not triggering log of token queue status. Effective log interval = None; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:8779,Performance,load,load,8779,".scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$51(WdlNamespace.scala:441); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.apply(WdlNamespace.scala:440); at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:174); at scala.util.Try$.apply(Try.scala:213); at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:53); ... 27 common frames omitted; [2020-09-17 21:41:46,29] [info] Not triggering log of token queue status. Effective log interval = None; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:8843,Performance,load,loadUsingSource,8843,".scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$51(WdlNamespace.scala:441); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.apply(WdlNamespace.scala:440); at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:174); at scala.util.Try$.apply(Try.scala:213); at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:53); ... 27 common frames omitted; [2020-09-17 21:41:46,29] [info] Not triggering log of token queue status. Effective log interval = None; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:9066,Performance,queue,queue,9066,".scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$51(WdlNamespace.scala:441); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.apply(WdlNamespace.scala:440); at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:174); at scala.util.Try$.apply(Try.scala:213); at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:53); ... 27 common frames omitted; [2020-09-17 21:41:46,29] [info] Not triggering log of token queue status. Effective log interval = None; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:9053,Testability,log,log,9053,".scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$51(WdlNamespace.scala:441); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.apply(WdlNamespace.scala:440); at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:174); at scala.util.Try$.apply(Try.scala:213); at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:53); ... 27 common frames omitted; [2020-09-17 21:41:46,29] [info] Not triggering log of token queue status. Effective log interval = None; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:9090,Testability,log,log,9090,".scala:442); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.$anonfun$apply$51(WdlNamespace.scala:441); at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); at scala.collection.Iterator.foreach(Iterator.scala:941); at scala.collection.Iterator.foreach$(Iterator.scala:941); at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); at scala.collection.IterableLike.foreach(IterableLike.scala:74); at scala.collection.IterableLike.foreach$(IterableLike.scala:73); at scala.collection.AbstractIterable.foreach(Iterable.scala:56); at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); at wdl.draft2.model.WdlNamespace$.apply(WdlNamespace.scala:440); at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:174); at scala.util.Try$.apply(Try.scala:213); at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:169); at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:161); at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:53); ... 27 common frames omitted; [2020-09-17 21:41:46,29] [info] Not triggering log of token queue status. Effective log interval = None; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694534461:33,Modifiability,config,config,33,In the runtime attributes in the config file or each of the tasks?. I added it to the config and it did the same thing. Does having a Local provider use Docker by default?. Thanks for your help!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694534461
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694534461:86,Modifiability,config,config,86,In the runtime attributes in the config file or each of the tasks?. I added it to the config and it did the same thing. Does having a Local provider use Docker by default?. Thanks for your help!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694534461
https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694570098:658,Modifiability,config,config,658,"Sorry @ParkvilleData, I mean the `runtime-attributes` in your `cromwell.conf` (eg: [Cromwell containers tute](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#local-environments) | [my slurm example conf](https://gist.github.com/illusional/b70f870fa0e2f8e7a0ba0a9e71d568f5#file-cromwell-slurm-singularity-conf-L61)), so it would now look something like:. ```hocon; runtime-attributes = """"""; String? docker; String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg""; """"""; ```. Yep you're right, the `Local` template by default uses Docker, and for some reason overriding the `runtime-attributes` in your config breaks the `submit-docker` task (even if it doesn't get used) - though I'm struggling to find references in the docs and I don't agree it should break.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694570098
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-699969603:55,Modifiability,config,config,55,Did you try to implement your own docker-submit in the config file?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-699969603
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701190239:51,Deployability,configurat,configuration,51,"""submit-docker"" (sorry for reversal) is one of the configuration option in Cromwell config file. See eg. here how additional volumes are mounted (last section): https://davetang.org/muse/2019/12/24/execute-gatk-workflows-locally. In the same way, you can run docker command that passes `--privileged=true` option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701190239
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701190239:51,Modifiability,config,configuration,51,"""submit-docker"" (sorry for reversal) is one of the configuration option in Cromwell config file. See eg. here how additional volumes are mounted (last section): https://davetang.org/muse/2019/12/24/execute-gatk-workflows-locally. In the same way, you can run docker command that passes `--privileged=true` option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701190239
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701190239:84,Modifiability,config,config,84,"""submit-docker"" (sorry for reversal) is one of the configuration option in Cromwell config file. See eg. here how additional volumes are mounted (last section): https://davetang.org/muse/2019/12/24/execute-gatk-workflows-locally. In the same way, you can run docker command that passes `--privileged=true` option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701190239
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701201752:325,Modifiability,config,config,325,"thanks for you reply. i mean in `aws backend ` mode, instead of `local mode`. there is no option to set `submit-docker`, i attached the backend part of my aws.conf as follows. ```bash; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://yuce/cromwell-execution""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 16. default-runtime-attributes {; queueArn: ""arn:aws-cn:batch:cn-northwest-1:723230375162:job-queue/first-run-job-queue"",; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701201752
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701201752:619,Performance,concurren,concurrent-job-limit,619,"thanks for you reply. i mean in `aws backend ` mode, instead of `local mode`. there is no option to set `submit-docker`, i attached the backend part of my aws.conf as follows. ```bash; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://yuce/cromwell-execution""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 16. default-runtime-attributes {; queueArn: ""arn:aws-cn:batch:cn-northwest-1:723230375162:job-queue/first-run-job-queue"",; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701201752
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701201752:676,Performance,queue,queueArn,676,"thanks for you reply. i mean in `aws backend ` mode, instead of `local mode`. there is no option to set `submit-docker`, i attached the backend part of my aws.conf as follows. ```bash; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://yuce/cromwell-execution""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 16. default-runtime-attributes {; queueArn: ""arn:aws-cn:batch:cn-northwest-1:723230375162:job-queue/first-run-job-queue"",; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701201752
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701201752:736,Performance,queue,queue,736,"thanks for you reply. i mean in `aws backend ` mode, instead of `local mode`. there is no option to set `submit-docker`, i attached the backend part of my aws.conf as follows. ```bash; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://yuce/cromwell-execution""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 16. default-runtime-attributes {; queueArn: ""arn:aws-cn:batch:cn-northwest-1:723230375162:job-queue/first-run-job-queue"",; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701201752
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701201752:756,Performance,queue,queue,756,"thanks for you reply. i mean in `aws backend ` mode, instead of `local mode`. there is no option to set `submit-docker`, i attached the backend part of my aws.conf as follows. ```bash; backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; // Base bucket for workflow executions; root = ""s3://yuce/cromwell-execution""; // A reference to an auth defined in the `aws` stanza at the top. This auth is used to create; // Jobs and manipulate auth JSONs.; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 16. default-runtime-attributes {; queueArn: ""arn:aws-cn:batch:cn-northwest-1:723230375162:job-queue/first-run-job-queue"",; }. filesystems {; s3 {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }; }; }; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-701201752
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-754946394:103,Modifiability,parameteriz,parameterized,103,We have a similar need. This overlaps a little with #4579. It would be useful if the submit-docker was parameterized similar to how it is for some of the other backends.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-754946394
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082:131,Modifiability,parameteriz,parameterized,131,"Thanks for the feedback. Can you elaborate more on the need to be able to; run a container as privileged?. It could (in theory) be parameterized if required but it seems hazardous to; have this be the default. On Tue, Jan 5, 2021 at 5:42 PM Shane Canon <notifications@github.com> wrote:. > We have a similar need. This overlaps a little with #4579; > <https://github.com/broadinstitute/cromwell/issues/4579>. It would be; > useful if the submit-docker was parameterized similar to how it is for some; > of the other backends.; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-754946394>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ELZQ5HVOSV2JRMIH3LSYOIVRANCNFSM4RQDAKPQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082:456,Modifiability,parameteriz,parameterized,456,"Thanks for the feedback. Can you elaborate more on the need to be able to; run a container as privileged?. It could (in theory) be parameterized if required but it seems hazardous to; have this be the default. On Tue, Jan 5, 2021 at 5:42 PM Shane Canon <notifications@github.com> wrote:. > We have a similar need. This overlaps a little with #4579; > <https://github.com/broadinstitute/cromwell/issues/4579>. It would be; > useful if the submit-docker was parameterized similar to how it is for some; > of the other backends.; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-754946394>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ELZQ5HVOSV2JRMIH3LSYOIVRANCNFSM4RQDAKPQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082:170,Safety,hazard,hazardous,170,"Thanks for the feedback. Can you elaborate more on the need to be able to; run a container as privileged?. It could (in theory) be parameterized if required but it seems hazardous to; have this be the default. On Tue, Jan 5, 2021 at 5:42 PM Shane Canon <notifications@github.com> wrote:. > We have a similar need. This overlaps a little with #4579; > <https://github.com/broadinstitute/cromwell/issues/4579>. It would be; > useful if the submit-docker was parameterized similar to how it is for some; > of the other backends.; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-754946394>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ELZQ5HVOSV2JRMIH3LSYOIVRANCNFSM4RQDAKPQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082
https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082:15,Usability,feedback,feedback,15,"Thanks for the feedback. Can you elaborate more on the need to be able to; run a container as privileged?. It could (in theory) be parameterized if required but it seems hazardous to; have this be the default. On Tue, Jan 5, 2021 at 5:42 PM Shane Canon <notifications@github.com> wrote:. > We have a similar need. This overlaps a little with #4579; > <https://github.com/broadinstitute/cromwell/issues/4579>. It would be; > useful if the submit-docker was parameterized similar to how it is for some; > of the other backends.; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-754946394>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ELZQ5HVOSV2JRMIH3LSYOIVRANCNFSM4RQDAKPQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5863#issuecomment-755294082
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-696150328:66,Availability,avail,available,66,"When this occurs it is because the fetch_and_run.sh script is not available; on the worker nodes of the batch compute environment so when docker mounts; that it mounts as a directory because there is no file. Possible causes that I can think of:; 1. The script is not available in the S3 bucket you used for the genomics; workflow core setup; 2. When you ran the Cromwell install you didn't use the exact same; namespace that you used for the genomics workflow core so the required; scripts are not available. On Sat, Sep 19, 2020 at 9:26 AM openbioinfomatics for more people who need; it <notifications@github.com> wrote:. > version: v53; >; > backend: aws; >; > [image: image]; > <https://user-images.githubusercontent.com/45682016/93668392-8d535380-fabe-11ea-870e-36786c6c3d9d.png>; >; > i think this part code may go wrong. mount file indeed.; >; > [image: image]; > <https://user-images.githubusercontent.com/45682016/93668410-a1975080-fabe-11ea-9571-b7ce8b9080ef.png>; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5872>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EOOFWDEMRNZCUXJ5CDSGSWPNANCNFSM4RTAXSGA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-696150328
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-696150328:268,Availability,avail,available,268,"When this occurs it is because the fetch_and_run.sh script is not available; on the worker nodes of the batch compute environment so when docker mounts; that it mounts as a directory because there is no file. Possible causes that I can think of:; 1. The script is not available in the S3 bucket you used for the genomics; workflow core setup; 2. When you ran the Cromwell install you didn't use the exact same; namespace that you used for the genomics workflow core so the required; scripts are not available. On Sat, Sep 19, 2020 at 9:26 AM openbioinfomatics for more people who need; it <notifications@github.com> wrote:. > version: v53; >; > backend: aws; >; > [image: image]; > <https://user-images.githubusercontent.com/45682016/93668392-8d535380-fabe-11ea-870e-36786c6c3d9d.png>; >; > i think this part code may go wrong. mount file indeed.; >; > [image: image]; > <https://user-images.githubusercontent.com/45682016/93668410-a1975080-fabe-11ea-9571-b7ce8b9080ef.png>; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5872>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EOOFWDEMRNZCUXJ5CDSGSWPNANCNFSM4RTAXSGA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-696150328
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-696150328:499,Availability,avail,available,499,"When this occurs it is because the fetch_and_run.sh script is not available; on the worker nodes of the batch compute environment so when docker mounts; that it mounts as a directory because there is no file. Possible causes that I can think of:; 1. The script is not available in the S3 bucket you used for the genomics; workflow core setup; 2. When you ran the Cromwell install you didn't use the exact same; namespace that you used for the genomics workflow core so the required; scripts are not available. On Sat, Sep 19, 2020 at 9:26 AM openbioinfomatics for more people who need; it <notifications@github.com> wrote:. > version: v53; >; > backend: aws; >; > [image: image]; > <https://user-images.githubusercontent.com/45682016/93668392-8d535380-fabe-11ea-870e-36786c6c3d9d.png>; >; > i think this part code may go wrong. mount file indeed.; >; > [image: image]; > <https://user-images.githubusercontent.com/45682016/93668410-a1975080-fabe-11ea-9571-b7ce8b9080ef.png>; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5872>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EOOFWDEMRNZCUXJ5CDSGSWPNANCNFSM4RTAXSGA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-696150328
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-696150328:372,Deployability,install,install,372,"When this occurs it is because the fetch_and_run.sh script is not available; on the worker nodes of the batch compute environment so when docker mounts; that it mounts as a directory because there is no file. Possible causes that I can think of:; 1. The script is not available in the S3 bucket you used for the genomics; workflow core setup; 2. When you ran the Cromwell install you didn't use the exact same; namespace that you used for the genomics workflow core so the required; scripts are not available. On Sat, Sep 19, 2020 at 9:26 AM openbioinfomatics for more people who need; it <notifications@github.com> wrote:. > version: v53; >; > backend: aws; >; > [image: image]; > <https://user-images.githubusercontent.com/45682016/93668392-8d535380-fabe-11ea-870e-36786c6c3d9d.png>; >; > i think this part code may go wrong. mount file indeed.; >; > [image: image]; > <https://user-images.githubusercontent.com/45682016/93668410-a1975080-fabe-11ea-9571-b7ce8b9080ef.png>; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5872>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EOOFWDEMRNZCUXJ5CDSGSWPNANCNFSM4RTAXSGA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-696150328
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410:171,Availability,avail,available,171,"Hi,. I encountered the same problem randomly. In a scattered task, some attempts passed while some attempts failed. ; As you mentioned, I think the script was not locally available on the worker at the moment of mounting, due to a network problem.; It would be better if Cromwell can re-try on this. . > When this occurs it is because the fetch_and_run.sh script is not available on the worker nodes of the batch compute environment so when docker mounts that it mounts as a directory because there is no file. Possible causes that I can think of: 1. The script is not available in the S3 bucket you used for the genomics workflow core setup 2. When you ran the Cromwell install you didn't use the exact same namespace that you used for the genomics workflow core so the required scripts are not available.; > […](#); > On Sat, Sep 19, 2020 at 9:26 AM openbioinfomatics for more people who need it ***@***.***> wrote: version: v53 backend: aws [image: image] <https://user-images.githubusercontent.com/45682016/93668392-8d535380-fabe-11ea-870e-36786c6c3d9d.png> i think this part code may go wrong. mount file indeed. [image: image] <https://user-images.githubusercontent.com/45682016/93668410-a1975080-fabe-11ea-9571-b7ce8b9080ef.png> — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5872>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EOOFWDEMRNZCUXJ5CDSGSWPNANCNFSM4RTAXSGA> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410:370,Availability,avail,available,370,"Hi,. I encountered the same problem randomly. In a scattered task, some attempts passed while some attempts failed. ; As you mentioned, I think the script was not locally available on the worker at the moment of mounting, due to a network problem.; It would be better if Cromwell can re-try on this. . > When this occurs it is because the fetch_and_run.sh script is not available on the worker nodes of the batch compute environment so when docker mounts that it mounts as a directory because there is no file. Possible causes that I can think of: 1. The script is not available in the S3 bucket you used for the genomics workflow core setup 2. When you ran the Cromwell install you didn't use the exact same namespace that you used for the genomics workflow core so the required scripts are not available.; > […](#); > On Sat, Sep 19, 2020 at 9:26 AM openbioinfomatics for more people who need it ***@***.***> wrote: version: v53 backend: aws [image: image] <https://user-images.githubusercontent.com/45682016/93668392-8d535380-fabe-11ea-870e-36786c6c3d9d.png> i think this part code may go wrong. mount file indeed. [image: image] <https://user-images.githubusercontent.com/45682016/93668410-a1975080-fabe-11ea-9571-b7ce8b9080ef.png> — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5872>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EOOFWDEMRNZCUXJ5CDSGSWPNANCNFSM4RTAXSGA> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410:569,Availability,avail,available,569,"Hi,. I encountered the same problem randomly. In a scattered task, some attempts passed while some attempts failed. ; As you mentioned, I think the script was not locally available on the worker at the moment of mounting, due to a network problem.; It would be better if Cromwell can re-try on this. . > When this occurs it is because the fetch_and_run.sh script is not available on the worker nodes of the batch compute environment so when docker mounts that it mounts as a directory because there is no file. Possible causes that I can think of: 1. The script is not available in the S3 bucket you used for the genomics workflow core setup 2. When you ran the Cromwell install you didn't use the exact same namespace that you used for the genomics workflow core so the required scripts are not available.; > […](#); > On Sat, Sep 19, 2020 at 9:26 AM openbioinfomatics for more people who need it ***@***.***> wrote: version: v53 backend: aws [image: image] <https://user-images.githubusercontent.com/45682016/93668392-8d535380-fabe-11ea-870e-36786c6c3d9d.png> i think this part code may go wrong. mount file indeed. [image: image] <https://user-images.githubusercontent.com/45682016/93668410-a1975080-fabe-11ea-9571-b7ce8b9080ef.png> — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5872>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EOOFWDEMRNZCUXJ5CDSGSWPNANCNFSM4RTAXSGA> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410:796,Availability,avail,available,796,"Hi,. I encountered the same problem randomly. In a scattered task, some attempts passed while some attempts failed. ; As you mentioned, I think the script was not locally available on the worker at the moment of mounting, due to a network problem.; It would be better if Cromwell can re-try on this. . > When this occurs it is because the fetch_and_run.sh script is not available on the worker nodes of the batch compute environment so when docker mounts that it mounts as a directory because there is no file. Possible causes that I can think of: 1. The script is not available in the S3 bucket you used for the genomics workflow core setup 2. When you ran the Cromwell install you didn't use the exact same namespace that you used for the genomics workflow core so the required scripts are not available.; > […](#); > On Sat, Sep 19, 2020 at 9:26 AM openbioinfomatics for more people who need it ***@***.***> wrote: version: v53 backend: aws [image: image] <https://user-images.githubusercontent.com/45682016/93668392-8d535380-fabe-11ea-870e-36786c6c3d9d.png> i think this part code may go wrong. mount file indeed. [image: image] <https://user-images.githubusercontent.com/45682016/93668410-a1975080-fabe-11ea-9571-b7ce8b9080ef.png> — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5872>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EOOFWDEMRNZCUXJ5CDSGSWPNANCNFSM4RTAXSGA> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410:671,Deployability,install,install,671,"Hi,. I encountered the same problem randomly. In a scattered task, some attempts passed while some attempts failed. ; As you mentioned, I think the script was not locally available on the worker at the moment of mounting, due to a network problem.; It would be better if Cromwell can re-try on this. . > When this occurs it is because the fetch_and_run.sh script is not available on the worker nodes of the batch compute environment so when docker mounts that it mounts as a directory because there is no file. Possible causes that I can think of: 1. The script is not available in the S3 bucket you used for the genomics workflow core setup 2. When you ran the Cromwell install you didn't use the exact same namespace that you used for the genomics workflow core so the required scripts are not available.; > […](#); > On Sat, Sep 19, 2020 at 9:26 AM openbioinfomatics for more people who need it ***@***.***> wrote: version: v53 backend: aws [image: image] <https://user-images.githubusercontent.com/45682016/93668392-8d535380-fabe-11ea-870e-36786c6c3d9d.png> i think this part code may go wrong. mount file indeed. [image: image] <https://user-images.githubusercontent.com/45682016/93668410-a1975080-fabe-11ea-9571-b7ce8b9080ef.png> — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5872>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EOOFWDEMRNZCUXJ5CDSGSWPNANCNFSM4RTAXSGA> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:1406,Availability,failure,failures,1406,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:537,Deployability,install,installed,537,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:693,Deployability,install,install,693,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:1344,Energy Efficiency,adapt,adaptive,1344,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:1344,Modifiability,adapt,adaptive,1344,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:173,Security,access,access,173,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:890,Security,validat,validation,890,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:184,Testability,log,logs,184,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341
https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-733360486:52,Deployability,hotfix,hotfix,52,This also happens to me with scattered tasks. Can a hotfix be enabling RetryStrategy?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-733360486
https://github.com/broadinstitute/cromwell/pull/5881#issuecomment-720758951:24,Testability,test,testing,24,Didn't actually work in testing. Closing PR.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5881#issuecomment-720758951
https://github.com/broadinstitute/cromwell/pull/5886#issuecomment-698589438:44,Safety,risk,risky,44,Closing. Preferring #5887 because it's less risky and this one has another known problem lurking just around the corner.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5886#issuecomment-698589438
https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700222843:113,Testability,test,tests,113,> Hopefully all those Codecov claims of uncovered lines are false? 🤞. I think they are true - I didn't write any tests,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700222843
https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700234400:190,Energy Efficiency,efficient,efficient,190,"> I think they are true - I didn't write any tests. I was hoping there were already some... 😬 Perhaps not as part of this ticket since this seems to be concerned with making the lookup more efficient, but it seems there really should be some Centaur tests for the overall feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700234400
https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700234400:45,Testability,test,tests,45,"> I think they are true - I didn't write any tests. I was hoping there were already some... 😬 Perhaps not as part of this ticket since this seems to be concerned with making the lookup more efficient, but it seems there really should be some Centaur tests for the overall feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700234400
https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700234400:250,Testability,test,tests,250,"> I think they are true - I didn't write any tests. I was hoping there were already some... 😬 Perhaps not as part of this ticket since this seems to be concerned with making the lookup more efficient, but it seems there really should be some Centaur tests for the overall feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700234400
https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239033:198,Energy Efficiency,efficient,efficient,198,"> > I think they are true - I didn't write any tests; > ; > I was hoping there were already some... 😬 Perhaps not as part of this ticket since this seems to be concerned with making the lookup more efficient, but it seems there really should be some Centaur tests for the overall feature. There's actually a Centaur tests which verifies that the whole VPC-thing works, so I guess that might be enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239033
https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239033:47,Testability,test,tests,47,"> > I think they are true - I didn't write any tests; > ; > I was hoping there were already some... 😬 Perhaps not as part of this ticket since this seems to be concerned with making the lookup more efficient, but it seems there really should be some Centaur tests for the overall feature. There's actually a Centaur tests which verifies that the whole VPC-thing works, so I guess that might be enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239033
https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239033:258,Testability,test,tests,258,"> > I think they are true - I didn't write any tests; > ; > I was hoping there were already some... 😬 Perhaps not as part of this ticket since this seems to be concerned with making the lookup more efficient, but it seems there really should be some Centaur tests for the overall feature. There's actually a Centaur tests which verifies that the whole VPC-thing works, so I guess that might be enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239033
https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239033:316,Testability,test,tests,316,"> > I think they are true - I didn't write any tests; > ; > I was hoping there were already some... 😬 Perhaps not as part of this ticket since this seems to be concerned with making the lookup more efficient, but it seems there really should be some Centaur tests for the overall feature. There's actually a Centaur tests which verifies that the whole VPC-thing works, so I guess that might be enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239033
https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239566:29,Testability,test,tests,29,"> There's actually a Centaur tests which verifies that the whole VPC-thing works, so I guess that might be enough. Yes that would be fine. I thought we considered Centaur coverage in our Codecov analysis though...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239566
https://github.com/broadinstitute/cromwell/issues/5901#issuecomment-702846067:102,Safety,avoid,avoided,102,"@nickp60 not specifically related to cromwell, but in WDL in general, entrypoints generally should be avoided. Any behaviour you need the task to do should be coded in the task itself. . So if you need `auth.sh` to be run, then in your wdl task you should do:. ```wdl; task a {. command <<<; ./auth.sh; .... other commands here ....; >>>; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5901#issuecomment-702846067
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-703704308:45,Availability,failure,failure,45,"The Travis complaint appears to be a genuine failure, investigating",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-703704308
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:45,Availability,failure,failures,45,"Alpha testing results. **Before:**. Mean = 6 failures, stdev = 10. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/978/ (32 total failed workflows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:121,Performance,Perform,PerformanceTest-against-Alpha,121,"Alpha testing results. **Before:**. Mean = 6 failures, stdev = 10. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/978/ (32 total failed workflows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:251,Performance,Perform,PerformanceTest-against-Alpha,251,"Alpha testing results. **Before:**. Mean = 6 failures, stdev = 10. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/978/ (32 total failed workflows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:376,Performance,Perform,PerformanceTest-against-Alpha,376,"Alpha testing results. **Before:**. Mean = 6 failures, stdev = 10. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/978/ (32 total failed workflows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:505,Performance,Perform,PerformanceTest-against-Alpha,505,"Alpha testing results. **Before:**. Mean = 6 failures, stdev = 10. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/978/ (32 total failed workflows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:636,Performance,Perform,PerformanceTest-against-Alpha,636,"Alpha testing results. **Before:**. Mean = 6 failures, stdev = 10. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/978/ (32 total failed workflows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:761,Performance,Perform,PerformanceTest-against-Alpha,761,"Alpha testing results. **Before:**. Mean = 6 failures, stdev = 10. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/978/ (32 total failed workflows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:886,Performance,Perform,PerformanceTest-against-Alpha,886,"Alpha testing results. **Before:**. Mean = 6 failures, stdev = 10. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/978/ (32 total failed workflows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:1011,Performance,Perform,PerformanceTest-against-Alpha,1011,"s, stdev = 10. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/978/ (32 total failed workflows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/Pe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:1136,Performance,Perform,PerformanceTest-against-Alpha,1136,"flows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/Per",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:1262,Performance,Perform,PerformanceTest-against-Alpha,1262,"l failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/993/ (1 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.or",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:1425,Performance,Perform,PerformanceTest-against-Alpha,1425,"ins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/993/ (1 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/994/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/995/ (1 tot",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:1550,Performance,Perform,PerformanceTest-against-Alpha,1550,"c-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/993/ (1 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/994/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/995/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/996/ (0 tot",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:1675,Performance,Perform,PerformanceTest-against-Alpha,1675,"nstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/993/ (1 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/994/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/995/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/996/ (0 total failed workflows, 50 min)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:1800,Performance,Perform,PerformanceTest-against-Alpha,1800,"nstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/993/ (1 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/994/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/995/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/996/ (0 total failed workflows, 50 min)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:1925,Performance,Perform,PerformanceTest-against-Alpha,1925,"nstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/993/ (1 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/994/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/995/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/996/ (0 total failed workflows, 50 min)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:2050,Performance,Perform,PerformanceTest-against-Alpha,2050,"nstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/993/ (1 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/994/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/995/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/996/ (0 total failed workflows, 50 min)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:2175,Performance,Perform,PerformanceTest-against-Alpha,2175,"nstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/993/ (1 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/994/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/995/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/996/ (0 total failed workflows, 50 min)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:2300,Performance,Perform,PerformanceTest-against-Alpha,2300,"nstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/993/ (1 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/994/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/995/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/996/ (0 total failed workflows, 50 min)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:2425,Performance,Perform,PerformanceTest-against-Alpha,2425,"nstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/993/ (1 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/994/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/995/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/996/ (0 total failed workflows, 50 min)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:2550,Performance,Perform,PerformanceTest-against-Alpha,2550,"nstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/992/ (0 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/993/ (1 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/994/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/995/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/996/ (0 total failed workflows, 50 min)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526:6,Testability,test,testing,6,"Alpha testing results. **Before:**. Mean = 6 failures, stdev = 10. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/978/ (32 total failed workflows, 1 hr 1 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/979/ (1 total failed workflows, 55 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/984/ (2 total failed workflows, 1 hr 5 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/988/ (15 total failed workflows, 1 hr 40 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/989/ (3 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/997/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/998/ (1 total failed workflows, 50 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/999/ (1 total failed workflows, 49 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1000/ (0 total failed workflows, 52 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/1001/ (0 total failed workflows, 51 min). **After:**. Mean = 0.5, stdev = 0.5. https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/985/ (0 total failed workflows, 53 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/986/ (0 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/987/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/990/ (1 total failed workflows, 51 min); https://fc-jenkins.dsp-techops.broadinstitute.org/job/PerformanceTest-against-Alpha/991/ (1 total failed workflows, 50 min); https",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-712950526
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:120,Integrability,message,messages,120,"Suspicious:. [Travis](https://travis-ci.com/github/broadinstitute/cromwell/jobs/403803242) times out at 180m with these messages:; ```; 2020-10-22 15:23:30,919 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:221,Safety,Abort,Abort,221,"Suspicious:. [Travis](https://travis-ci.com/github/broadinstitute/cromwell/jobs/403803242) times out at 180m with these messages:; ```; 2020-10-22 15:23:30,919 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:373,Safety,Abort,Abort,373,"Suspicious:. [Travis](https://travis-ci.com/github/broadinstitute/cromwell/jobs/403803242) times out at 180m with these messages:; ```; 2020-10-22 15:23:30,919 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:525,Safety,Abort,Abort,525,"Suspicious:. [Travis](https://travis-ci.com/github/broadinstitute/cromwell/jobs/403803242) times out at 180m with these messages:; ```; 2020-10-22 15:23:30,919 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:677,Safety,Abort,Abort,677,"Suspicious:. [Travis](https://travis-ci.com/github/broadinstitute/cromwell/jobs/403803242) times out at 180m with these messages:; ```; 2020-10-22 15:23:30,919 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:829,Safety,Abort,Abort,829,"Suspicious:. [Travis](https://travis-ci.com/github/broadinstitute/cromwell/jobs/403803242) times out at 180m with these messages:; ```; 2020-10-22 15:23:30,919 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:982,Safety,Abort,Abort,982,"uspicious:. [Travis](https://travis-ci.com/github/broadinstitute/cromwell/jobs/403803242) times out at 180m with these messages:; ```; 2020-10-22 15:23:30,919 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1134,Safety,Abort,Abort,1134,"30,919 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1286,Safety,Abort,Abort,1286,"35,939 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1438,Safety,Abort,Abort,1438,"40,959 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1589,Safety,Abort,Abort,1589,":45,978 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:23:50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akk",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1741,Safety,Abort,Abort,1741,":50,999 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:23:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:1893,Safety,Abort,Abort,1893,"3:56,019 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2045,Safety,Abort,Abort,2045,"4:01,039 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2197,Safety,Abort,Abort,2197,"4:06,058 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2348,Safety,Abort,Abort,2348,"24:11,079 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2499,Safety,Abort,Abort,2499,":24:16,108 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
