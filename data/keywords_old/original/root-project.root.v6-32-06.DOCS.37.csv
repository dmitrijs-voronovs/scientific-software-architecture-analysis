id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:3269,Performance,load,loading,3269,"trace_LIBFILE}); set(system_libs ${system_libs} ${Backtrace_LIBFILE}); endif(); if( LLVM_ENABLE_TERMINFO ); set(imported_libs ${imported_libs} Terminfo::terminfo); endif(); set(system_libs ${system_libs} ${LLVM_ATOMIC_LIB}); set(system_libs ${system_libs} ${LLVM_PTHREAD_LIB}); if( UNIX AND NOT (BEOS OR HAIKU) ); set(system_libs ${system_libs} m); endif(); if( UNIX AND ${CMAKE_SYSTEM_NAME} MATCHES ""SunOS"" ); set(system_libs ${system_libs} kstat socket); endif(); if( FUCHSIA ); set(system_libs ${system_libs} zircon); endif(); if ( HAIKU ); add_compile_definitions(_BSD_SOURCE); set(system_libs ${system_libs} bsd network); endif(); endif( MSVC OR MINGW ). # Delay load shell32.dll if possible to speed up process startup.; set (delayload_flags); if (MSVC); # When linking with Swift, `swiftc.exe` is used as the linker drive rather; # than invoking `link.exe` directly. In such a case, the flags should be; # marked as `-Xlinker` to pass them directly to the linker. As a temporary; # workaround simply elide the delay loading.; set (delayload_flags $<$<NOT:$<LINK_LANGUAGE:Swift>>:delayimp -delayload:shell32.dll -delayload:ole32.dll>); endif(). # Link Z3 if the user wants to build it.; if(LLVM_WITH_Z3); set(system_libs ${system_libs} ${Z3_LIBRARIES}); endif(). # Override the C runtime allocator on Windows and embed it into LLVM tools & libraries; if(LLVM_INTEGRATED_CRT_ALLOC); if (NOT CMAKE_MSVC_RUNTIME_LIBRARY OR CMAKE_MSVC_RUNTIME_LIBRARY MATCHES ""DLL$""); message(FATAL_ERROR ""LLVM_INTEGRATED_CRT_ALLOC only works with CMAKE_MSVC_RUNTIME_LIBRARY set to MultiThreaded or MultiThreadedDebug.""); endif(). string(REGEX REPLACE ""(/|\\\\)$"" """" LLVM_INTEGRATED_CRT_ALLOC ""${LLVM_INTEGRATED_CRT_ALLOC}""). if(NOT EXISTS ""${LLVM_INTEGRATED_CRT_ALLOC}""); message(FATAL_ERROR ""Cannot find the path to `git clone` for the CRT allocator! (${LLVM_INTEGRATED_CRT_ALLOC}). Currently, rpmalloc, snmalloc and mimalloc are supported.""); endif(). if(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""rpmalloc$""); add_compil",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:5950,Performance,Cache,CachePruning,5950,"EGRATED_CRT_ALLOC}\\ide\\vs2019\\mimalloc.sln""); endif(); set(system_libs ${system_libs} ""${MIMALLOC_LIB}"" ""-INCLUDE:malloc""); endif(); endif(). # FIXME: We are currently guarding AIX headers with _XOPEN_SOURCE=700.; # See llvm/CMakeLists.txt. However, we need _SC_NPROCESSORS_ONLN in; # unistd.h and it is guarded by _ALL_SOURCE, so we remove the _XOPEN_SOURCE; # guard here. We should remove the guards all together once AIX cleans up; # the system headers.; if (UNIX AND ${CMAKE_SYSTEM_NAME} MATCHES ""AIX""); remove_definitions(""-D_XOPEN_SOURCE=700""); endif(). add_subdirectory(BLAKE3). add_llvm_component_library(LLVMSupport; ABIBreak.cpp; AMDGPUMetadata.cpp; APFixedPoint.cpp; APFloat.cpp; APInt.cpp; APSInt.cpp; ARMBuildAttrs.cpp; ARMAttributeParser.cpp; ARMWinEH.cpp; Allocator.cpp; AutoConvert.cpp; Base64.cpp; BalancedPartitioning.cpp; BinaryStreamError.cpp; BinaryStreamReader.cpp; BinaryStreamRef.cpp; BinaryStreamWriter.cpp; BlockFrequency.cpp; BranchProbability.cpp; BuryPointer.cpp; CachePruning.cpp; Caching.cpp; circular_raw_ostream.cpp; Chrono.cpp; COM.cpp; CodeGenCoverage.cpp; CommandLine.cpp; Compression.cpp; CRC.cpp; ConvertUTF.cpp; ConvertEBCDIC.cpp; ConvertUTFWrapper.cpp; CrashRecoveryContext.cpp; CSKYAttributes.cpp; CSKYAttributeParser.cpp; DataExtractor.cpp; Debug.cpp; DebugCounter.cpp; DeltaAlgorithm.cpp; DivisionByConstantInfo.cpp; DAGDeltaAlgorithm.cpp; DJB.cpp; ELFAttributeParser.cpp; ELFAttributes.cpp; Error.cpp; ErrorHandling.cpp; ExtensibleRTTI.cpp; FileCollector.cpp; FileUtilities.cpp; FileOutputBuffer.cpp; FloatingPointMode.cpp; FoldingSet.cpp; FormattedStream.cpp; FormatVariadic.cpp; GlobPattern.cpp; GraphWriter.cpp; Hashing.cpp; InitLLVM.cpp; InstructionCost.cpp; IntEqClasses.cpp; IntervalMap.cpp; JSON.cpp; KnownBits.cpp; LEB128.cpp; LineIterator.cpp; Locale.cpp; LockFileManager.cpp; ManagedStatic.cpp; MathExtras.cpp; MemAlloc.cpp; MemoryBuffer.cpp; MemoryBufferRef.cpp; MD5.cpp; MSP430Attributes.cpp; MSP430AttributeParser.cpp; NativeFormatting.cpp;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:6955,Performance,Optimiz,OptimizedStructLayout,6955,cpp; Caching.cpp; circular_raw_ostream.cpp; Chrono.cpp; COM.cpp; CodeGenCoverage.cpp; CommandLine.cpp; Compression.cpp; CRC.cpp; ConvertUTF.cpp; ConvertEBCDIC.cpp; ConvertUTFWrapper.cpp; CrashRecoveryContext.cpp; CSKYAttributes.cpp; CSKYAttributeParser.cpp; DataExtractor.cpp; Debug.cpp; DebugCounter.cpp; DeltaAlgorithm.cpp; DivisionByConstantInfo.cpp; DAGDeltaAlgorithm.cpp; DJB.cpp; ELFAttributeParser.cpp; ELFAttributes.cpp; Error.cpp; ErrorHandling.cpp; ExtensibleRTTI.cpp; FileCollector.cpp; FileUtilities.cpp; FileOutputBuffer.cpp; FloatingPointMode.cpp; FoldingSet.cpp; FormattedStream.cpp; FormatVariadic.cpp; GlobPattern.cpp; GraphWriter.cpp; Hashing.cpp; InitLLVM.cpp; InstructionCost.cpp; IntEqClasses.cpp; IntervalMap.cpp; JSON.cpp; KnownBits.cpp; LEB128.cpp; LineIterator.cpp; Locale.cpp; LockFileManager.cpp; ManagedStatic.cpp; MathExtras.cpp; MemAlloc.cpp; MemoryBuffer.cpp; MemoryBufferRef.cpp; MD5.cpp; MSP430Attributes.cpp; MSP430AttributeParser.cpp; NativeFormatting.cpp; OptimizedStructLayout.cpp; Optional.cpp; PGOOptions.cpp; Parallel.cpp; PluginLoader.cpp; PrettyStackTrace.cpp; RandomNumberGenerator.cpp; Regex.cpp; RISCVAttributes.cpp; RISCVAttributeParser.cpp; RISCVISAInfo.cpp; ScaledNumber.cpp; ScopedPrinter.cpp; SHA1.cpp; SHA256.cpp; Signposts.cpp; SmallPtrSet.cpp; SmallVector.cpp; SourceMgr.cpp; SpecialCaseList.cpp; Statistic.cpp; StringExtras.cpp; StringMap.cpp; StringSaver.cpp; StringRef.cpp; SuffixTreeNode.cpp; SuffixTree.cpp; SystemUtils.cpp; TarWriter.cpp; ThreadPool.cpp; TimeProfiler.cpp; Timer.cpp; ToolOutputFile.cpp; Twine.cpp; TypeSize.cpp; Unicode.cpp; UnicodeCaseFold.cpp; UnicodeNameToCodepoint.cpp; UnicodeNameToCodepointGenerated.cpp; VersionTuple.cpp; VirtualFileSystem.cpp; WithColor.cpp; YAMLParser.cpp; YAMLTraits.cpp; raw_os_ostream.cpp; raw_ostream.cpp; raw_socket_stream.cpp; regcomp.c; regerror.c; regexec.c; regfree.c; regstrlcpy.c; xxhash.cpp; Z3Solver.cpp. ${ALLOCATOR_FILES}; $<TARGET_OBJECTS:LLVMSupportBlake3>. # System; Atomic.cpp; Dy,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:6616,Security,Hash,Hashing,6616,APFixedPoint.cpp; APFloat.cpp; APInt.cpp; APSInt.cpp; ARMBuildAttrs.cpp; ARMAttributeParser.cpp; ARMWinEH.cpp; Allocator.cpp; AutoConvert.cpp; Base64.cpp; BalancedPartitioning.cpp; BinaryStreamError.cpp; BinaryStreamReader.cpp; BinaryStreamRef.cpp; BinaryStreamWriter.cpp; BlockFrequency.cpp; BranchProbability.cpp; BuryPointer.cpp; CachePruning.cpp; Caching.cpp; circular_raw_ostream.cpp; Chrono.cpp; COM.cpp; CodeGenCoverage.cpp; CommandLine.cpp; Compression.cpp; CRC.cpp; ConvertUTF.cpp; ConvertEBCDIC.cpp; ConvertUTFWrapper.cpp; CrashRecoveryContext.cpp; CSKYAttributes.cpp; CSKYAttributeParser.cpp; DataExtractor.cpp; Debug.cpp; DebugCounter.cpp; DeltaAlgorithm.cpp; DivisionByConstantInfo.cpp; DAGDeltaAlgorithm.cpp; DJB.cpp; ELFAttributeParser.cpp; ELFAttributes.cpp; Error.cpp; ErrorHandling.cpp; ExtensibleRTTI.cpp; FileCollector.cpp; FileUtilities.cpp; FileOutputBuffer.cpp; FloatingPointMode.cpp; FoldingSet.cpp; FormattedStream.cpp; FormatVariadic.cpp; GlobPattern.cpp; GraphWriter.cpp; Hashing.cpp; InitLLVM.cpp; InstructionCost.cpp; IntEqClasses.cpp; IntervalMap.cpp; JSON.cpp; KnownBits.cpp; LEB128.cpp; LineIterator.cpp; Locale.cpp; LockFileManager.cpp; ManagedStatic.cpp; MathExtras.cpp; MemAlloc.cpp; MemoryBuffer.cpp; MemoryBufferRef.cpp; MD5.cpp; MSP430Attributes.cpp; MSP430AttributeParser.cpp; NativeFormatting.cpp; OptimizedStructLayout.cpp; Optional.cpp; PGOOptions.cpp; Parallel.cpp; PluginLoader.cpp; PrettyStackTrace.cpp; RandomNumberGenerator.cpp; Regex.cpp; RISCVAttributes.cpp; RISCVAttributeParser.cpp; RISCVISAInfo.cpp; ScaledNumber.cpp; ScopedPrinter.cpp; SHA1.cpp; SHA256.cpp; Signposts.cpp; SmallPtrSet.cpp; SmallVector.cpp; SourceMgr.cpp; SpecialCaseList.cpp; Statistic.cpp; StringExtras.cpp; StringMap.cpp; StringSaver.cpp; StringRef.cpp; SuffixTreeNode.cpp; SuffixTree.cpp; SystemUtils.cpp; TarWriter.cpp; ThreadPool.cpp; TimeProfiler.cpp; Timer.cpp; ToolOutputFile.cpp; Twine.cpp; TypeSize.cpp; Unicode.cpp; UnicodeCaseFold.cpp; UnicodeNameToCodepoint.cpp; Unico,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:750,Testability,Test,TestGlobalCtorDtor,750,"include(GetLibraryName). # Ensure that libSupport does not carry any static global initializer.; # libSupport can be embedded in use cases where we don't want to load all; # cl::opt unless we want to parse the command line.; # ManagedStatic can be used to enable lazy-initialization of globals.; # We don't use `add_flag_if_supported` as instead of compiling an empty file we; # check if the current platform is able to compile global std::mutex with this; # flag (Linux can, Darwin can't for example).; check_cxx_compiler_flag(""-Werror=global-constructors"" HAS_WERROR_GLOBAL_CTORS); if (HAS_WERROR_GLOBAL_CTORS); SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -Werror=global-constructors""); CHECK_CXX_SOURCE_COMPILES(""; #include <mutex>; static std::mutex TestGlobalCtorDtor;; static std::recursive_mutex TestGlobalCtorDtor2;; int main() { (void)TestGlobalCtorDtor; (void)TestGlobalCtorDtor2; return 0;}; "" LLVM_HAS_NOGLOBAL_CTOR_MUTEX); if (NOT LLVM_HAS_NOGLOBAL_CTOR_MUTEX); string(REPLACE ""-Werror=global-constructors"" """" CMAKE_CXX_FLAGS ${CMAKE_CXX_FLAGS}); endif(); endif(). if(LLVM_ENABLE_ZLIB); list(APPEND imported_libs ZLIB::ZLIB); endif(). if(LLVM_ENABLE_ZSTD); if(TARGET zstd::libzstd_shared AND NOT LLVM_USE_STATIC_ZSTD); set(zstd_target zstd::libzstd_shared); else(); set(zstd_target zstd::libzstd_static); endif(); endif(). if(LLVM_ENABLE_ZSTD); list(APPEND imported_libs ${zstd_target}); endif(). if( MSVC OR MINGW ); # libuuid required for FOLDERID_Profile usage in lib/Support/Windows/Path.inc.; # advapi32 required for CryptAcquireContextW in lib/Support/Windows/Path.inc.; set(system_libs ${system_libs} psapi shell32 ole32 uuid advapi32 ws2_32); elseif( CMAKE_HOST_UNIX ); if( HAVE_LIBRT ); set(system_libs ${system_libs} rt); endif(); if( HAVE_LIBDL ); set(system_libs ${system_libs} ${CMAKE_DL_LIBS}); endif(); if( HAVE_BACKTRACE AND NOT ""${Backtrace_LIBRARIES}"" STREQUAL """" ); # On BSDs, CMake returns a fully qualified path to the backtrace library.; # We need to remove the path and",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:840,Testability,Test,TestGlobalCtorDtor,840,"include(GetLibraryName). # Ensure that libSupport does not carry any static global initializer.; # libSupport can be embedded in use cases where we don't want to load all; # cl::opt unless we want to parse the command line.; # ManagedStatic can be used to enable lazy-initialization of globals.; # We don't use `add_flag_if_supported` as instead of compiling an empty file we; # check if the current platform is able to compile global std::mutex with this; # flag (Linux can, Darwin can't for example).; check_cxx_compiler_flag(""-Werror=global-constructors"" HAS_WERROR_GLOBAL_CTORS); if (HAS_WERROR_GLOBAL_CTORS); SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -Werror=global-constructors""); CHECK_CXX_SOURCE_COMPILES(""; #include <mutex>; static std::mutex TestGlobalCtorDtor;; static std::recursive_mutex TestGlobalCtorDtor2;; int main() { (void)TestGlobalCtorDtor; (void)TestGlobalCtorDtor2; return 0;}; "" LLVM_HAS_NOGLOBAL_CTOR_MUTEX); if (NOT LLVM_HAS_NOGLOBAL_CTOR_MUTEX); string(REPLACE ""-Werror=global-constructors"" """" CMAKE_CXX_FLAGS ${CMAKE_CXX_FLAGS}); endif(); endif(). if(LLVM_ENABLE_ZLIB); list(APPEND imported_libs ZLIB::ZLIB); endif(). if(LLVM_ENABLE_ZSTD); if(TARGET zstd::libzstd_shared AND NOT LLVM_USE_STATIC_ZSTD); set(zstd_target zstd::libzstd_shared); else(); set(zstd_target zstd::libzstd_static); endif(); endif(). if(LLVM_ENABLE_ZSTD); list(APPEND imported_libs ${zstd_target}); endif(). if( MSVC OR MINGW ); # libuuid required for FOLDERID_Profile usage in lib/Support/Windows/Path.inc.; # advapi32 required for CryptAcquireContextW in lib/Support/Windows/Path.inc.; set(system_libs ${system_libs} psapi shell32 ole32 uuid advapi32 ws2_32); elseif( CMAKE_HOST_UNIX ); if( HAVE_LIBRT ); set(system_libs ${system_libs} rt); endif(); if( HAVE_LIBDL ); set(system_libs ${system_libs} ${CMAKE_DL_LIBS}); endif(); if( HAVE_BACKTRACE AND NOT ""${Backtrace_LIBRARIES}"" STREQUAL """" ); # On BSDs, CMake returns a fully qualified path to the backtrace library.; # We need to remove the path and",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:3246,Usability,simpl,simply,3246,"trace_LIBFILE}); set(system_libs ${system_libs} ${Backtrace_LIBFILE}); endif(); if( LLVM_ENABLE_TERMINFO ); set(imported_libs ${imported_libs} Terminfo::terminfo); endif(); set(system_libs ${system_libs} ${LLVM_ATOMIC_LIB}); set(system_libs ${system_libs} ${LLVM_PTHREAD_LIB}); if( UNIX AND NOT (BEOS OR HAIKU) ); set(system_libs ${system_libs} m); endif(); if( UNIX AND ${CMAKE_SYSTEM_NAME} MATCHES ""SunOS"" ); set(system_libs ${system_libs} kstat socket); endif(); if( FUCHSIA ); set(system_libs ${system_libs} zircon); endif(); if ( HAIKU ); add_compile_definitions(_BSD_SOURCE); set(system_libs ${system_libs} bsd network); endif(); endif( MSVC OR MINGW ). # Delay load shell32.dll if possible to speed up process startup.; set (delayload_flags); if (MSVC); # When linking with Swift, `swiftc.exe` is used as the linker drive rather; # than invoking `link.exe` directly. In such a case, the flags should be; # marked as `-Xlinker` to pass them directly to the linker. As a temporary; # workaround simply elide the delay loading.; set (delayload_flags $<$<NOT:$<LINK_LANGUAGE:Swift>>:delayimp -delayload:shell32.dll -delayload:ole32.dll>); endif(). # Link Z3 if the user wants to build it.; if(LLVM_WITH_Z3); set(system_libs ${system_libs} ${Z3_LIBRARIES}); endif(). # Override the C runtime allocator on Windows and embed it into LLVM tools & libraries; if(LLVM_INTEGRATED_CRT_ALLOC); if (NOT CMAKE_MSVC_RUNTIME_LIBRARY OR CMAKE_MSVC_RUNTIME_LIBRARY MATCHES ""DLL$""); message(FATAL_ERROR ""LLVM_INTEGRATED_CRT_ALLOC only works with CMAKE_MSVC_RUNTIME_LIBRARY set to MultiThreaded or MultiThreadedDebug.""); endif(). string(REGEX REPLACE ""(/|\\\\)$"" """" LLVM_INTEGRATED_CRT_ALLOC ""${LLVM_INTEGRATED_CRT_ALLOC}""). if(NOT EXISTS ""${LLVM_INTEGRATED_CRT_ALLOC}""); message(FATAL_ERROR ""Cannot find the path to `git clone` for the CRT allocator! (${LLVM_INTEGRATED_CRT_ALLOC}). Currently, rpmalloc, snmalloc and mimalloc are supported.""); endif(). if(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""rpmalloc$""); add_compil",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/CMakeLists.txt:69,Availability,Error,Error,69,add_llvm_component_library(LLVMTableGen; DetailedRecordsBackend.cpp; Error.cpp; JSONBackend.cpp; Main.cpp; Parser.cpp; Record.cpp; SetTheory.cpp; StringMatcher.cpp; TableGenBackend.cpp; TableGenBackendSkeleton.cpp; TGLexer.cpp; TGParser.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/TableGen. LINK_COMPONENTS; Support; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/TableGen/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt:924,Integrability,message,message,924,"list(APPEND LLVM_COMMON_DEPENDS intrinsics_gen). list(APPEND LLVM_TABLEGEN_FLAGS -I ${LLVM_MAIN_SRC_DIR}/lib/Target). add_llvm_component_library(LLVMTarget; Target.cpp; TargetIntrinsicInfo.cpp; TargetLoweringObjectFile.cpp; TargetMachine.cpp; TargetMachineC.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Target. LINK_COMPONENTS; Analysis; Core; MC; Support; TargetParser; ). # When building shared objects for each target there are some internal APIs; # that are used across shared objects which we can't hide.; if (NOT BUILD_SHARED_LIBS AND NOT APPLE AND; (NOT (WIN32 OR CYGWIN) OR (MINGW AND CMAKE_CXX_COMPILER_ID MATCHES ""Clang"")) AND; NOT (${CMAKE_SYSTEM_NAME} MATCHES ""AIX"") AND; NOT DEFINED CMAKE_CXX_VISIBILITY_PRESET); # Set default visibility to hidden, so we don't export all the Target classes; # in libLLVM.so.; set(CMAKE_CXX_VISIBILITY_PRESET hidden); endif(). foreach(t ${LLVM_TARGETS_TO_BUILD}); message(STATUS ""Targeting ${t}""); add_subdirectory(${t}); endforeach(). # Currently we do not allow libraries from lib to reference targets directly.; # This property is used to enforce that convention. It is important because the; # logic in llvm_map_components_to_libnames is order dependent on the target; # libraries being created.; set_property(GLOBAL PROPERTY LLVM_TARGETS_CONFIGURED On); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt:1208,Integrability,depend,dependent,1208,"list(APPEND LLVM_COMMON_DEPENDS intrinsics_gen). list(APPEND LLVM_TABLEGEN_FLAGS -I ${LLVM_MAIN_SRC_DIR}/lib/Target). add_llvm_component_library(LLVMTarget; Target.cpp; TargetIntrinsicInfo.cpp; TargetLoweringObjectFile.cpp; TargetMachine.cpp; TargetMachineC.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Target. LINK_COMPONENTS; Analysis; Core; MC; Support; TargetParser; ). # When building shared objects for each target there are some internal APIs; # that are used across shared objects which we can't hide.; if (NOT BUILD_SHARED_LIBS AND NOT APPLE AND; (NOT (WIN32 OR CYGWIN) OR (MINGW AND CMAKE_CXX_COMPILER_ID MATCHES ""Clang"")) AND; NOT (${CMAKE_SYSTEM_NAME} MATCHES ""AIX"") AND; NOT DEFINED CMAKE_CXX_VISIBILITY_PRESET); # Set default visibility to hidden, so we don't export all the Target classes; # in libLLVM.so.; set(CMAKE_CXX_VISIBILITY_PRESET hidden); endif(). foreach(t ${LLVM_TARGETS_TO_BUILD}); message(STATUS ""Targeting ${t}""); add_subdirectory(${t}); endforeach(). # Currently we do not allow libraries from lib to reference targets directly.; # This property is used to enforce that convention. It is important because the; # logic in llvm_map_components_to_libnames is order dependent on the target; # libraries being created.; set_property(GLOBAL PROPERTY LLVM_TARGETS_CONFIGURED On); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt:1158,Testability,log,logic,1158,"list(APPEND LLVM_COMMON_DEPENDS intrinsics_gen). list(APPEND LLVM_TABLEGEN_FLAGS -I ${LLVM_MAIN_SRC_DIR}/lib/Target). add_llvm_component_library(LLVMTarget; Target.cpp; TargetIntrinsicInfo.cpp; TargetLoweringObjectFile.cpp; TargetMachine.cpp; TargetMachineC.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Target. LINK_COMPONENTS; Analysis; Core; MC; Support; TargetParser; ). # When building shared objects for each target there are some internal APIs; # that are used across shared objects which we can't hide.; if (NOT BUILD_SHARED_LIBS AND NOT APPLE AND; (NOT (WIN32 OR CYGWIN) OR (MINGW AND CMAKE_CXX_COMPILER_ID MATCHES ""Clang"")) AND; NOT (${CMAKE_SYSTEM_NAME} MATCHES ""AIX"") AND; NOT DEFINED CMAKE_CXX_VISIBILITY_PRESET); # Set default visibility to hidden, so we don't export all the Target classes; # in libLLVM.so.; set(CMAKE_CXX_VISIBILITY_PRESET hidden); endif(). foreach(t ${LLVM_TARGETS_TO_BUILD}); message(STATUS ""Targeting ${t}""); add_subdirectory(${t}); endforeach(). # Currently we do not allow libraries from lib to reference targets directly.; # This property is used to enforce that convention. It is important because the; # logic in llvm_map_components_to_libnames is order dependent on the target; # libraries being created.; set_property(GLOBAL PROPERTY LLVM_TARGETS_CONFIGURED On); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:457,Availability,robust,robust,457,"Target Independent Opportunities:. //===---------------------------------------------------------------------===//. We should recognized various ""overflow detection"" idioms and translate them into; llvm.uadd.with.overflow and similar intrinsics. Here is a multiply idiom:. unsigned int mul(unsigned int a,unsigned int b) {; if ((unsigned long long)a*b>0xffffffff); exit(0);; return a*b;; }. The legalization code for mul-with-overflow needs to be made more robust before; this can be implemented though. //===---------------------------------------------------------------------===//. Get the C front-end to expand hypot(x,y) -> llvm.sqrt(x*x+y*y) when errno and; precision don't matter (ffastmath). Misc/mandel will like this. :) This isn't; safe in general, even on darwin. See the libm implementation of hypot for; examples (which special case when x/y are exactly zero to get signed zeros etc; right). //===---------------------------------------------------------------------===//. On targets with expensive 64-bit multiply, we could LSR this:. for (i = ...; ++i) {; x = 1ULL << i;. into:; long long tmp = 1;; for (i = ...; ++i, tmp+=tmp); x = tmp;. This would be a win on ppc32, but not x86 or ppc64. //===---------------------------------------------------------------------===//. Shrink: (setlt (loadi32 P), 0) -> (setlt (loadi8 Phi), 0). //===---------------------------------------------------------------------===//. Reassociate should turn things like:. int factorial(int X) {; return X*X*X*X*X*X*X*X;; }. into llvm.powi calls, allowing the code generator to produce balanced; multiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:2825,Availability,avail,available,2825,"---------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n) + bar(2*z, 2*n);; }. This is blocked on not handling X*X*X -> powi(X, 3) (see note above). The issue; is that we end up getting t = 2*X s = t*t and don't turn this into 4*X*X,; which is the same number of multiplies and is canonical, because the 2*X has; multiple uses. Here's a simple example:. define i32 @test15(i32 %X1) {; %B = mul i32 %X1, 47 ; X1*47; %C = mul i32 %B, %B; ret i32 %C; }. //===---------------------------------------------------------------------===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ; b3 = a3 + a2 + a1 + a0; ; b2 = a2 + a1 + a0; ; b1 = a1 + a0; ; } . This requires reassociating to forms of expressions that are already available,; something that reassoc doesn't think about yet. //===---------------------------------------------------------------------===//. These two functions should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===---------------------------------------------------------------------===//. It would be nice to revert this patch:; http://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20060213/031986.html. And teach the dag combiner enough to simplify the code expanded before ; legalize. It seems plausible that this knowledge would let it simplify other; stuff too. //===---------------------------------------------------------------------===//. For vector types, DataLayout.cpp::getTypeInfo() returns alignment that is equal; to the type size. It works but can be overly conservative as the alig",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:8869,Availability,down,down,8869,"ign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64 effective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; b2 = (temp >> 12) & 0xf;; if (b2) effective_addr2 += regs[b2];; effective_addr2 &= regs[4];; if ((effective_addr2 & 3) == 0); return 1;; return 0;; }. Note that only the low 2 bits of effective_addr2 are used. On 32-bit systems,; we don't eliminate the computation of the top half of effective_addr2 because; we don't have whole-function selection dags. On x86, this means we use one; extra register for the function when effective_addr2 is declared as U64 than; when it is declared U32. PHI Slicing could be extended to do this. //===-------------------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:12515,Availability,down,down,12515,"functions, like ; this:. ; RUN: llvm-as < %s | opt -argpromotion | llvm-dis | grep x.val. define internal i32 @foo(i32* %x) {; entry:; 	%tmp = load i32* %x		; <i32> [#uses=0]; 	%tmp.foo = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp.foo; }. define i32 @bar(i32* %x) {; entry:; 	%tmp3 = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp3; }. //===---------------------------------------------------------------------===//. We should investigate an instruction sinking pass. Consider this silly; example in pic mode:. #include <assert.h>; void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:28581,Availability,redundant,redundant,28581,"-------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) {; int i;; *res = 0;; for (i = 0; i < n; i++); if (*cond); *res ^= 234; /* (*) */; }. On this function GVN hoists the fully redundant value of *res, but nothing; moves the store out. This gives us this code:. bb:		; preds = %bb2, %entry; 	%.rle = phi i32 [ 0, %entry ], [ %.rle6, %bb2 ]	; 	%i.05 = phi i32 [ 0, %entry ], [ %indvar.next, %bb2 ]; 	%1 = load i32* %cond, align 4; 	%2 = icmp eq i32 %1, 0; 	br i1 %2, label %bb2, label %bb1. bb1:		; preds = %bb; 	%3 = xor i32 %.rle, 234	; 	store i32 %3, i32* %res, align 4; 	br label %bb2. bb2:		; preds = %bb, %bb1; 	%.rle6 = phi i32 [ %3, %bb1 ], [ %.rle, %bb ]	; 	%indvar.next = add i32 %i.05, 1	; 	%exitcond = icmp eq i32 %indvar.next, %n; 	br i1 %exitcond, label %return, label %bb. DSE should sink partially dead stores to get the store out of the loop. Here's another partial dead case:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=12395. //===---------------------------------------------------------------------===//. Scalar PRE hoists the mul in the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31095,Availability,fault,fault,31095," phi i64 [ 0, %bb.nph ], [ %indvar.next, %for.inc ]; %i.01718 = phi i32 [ 0, %bb.nph ], [ %i.01719, %for.inc ]; %tmp4 = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31184,Availability,redundant,redundant,31184," = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-c",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31375,Availability,redundant,redundant,31375,"p10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33230,Availability,redundant,redundant,33230,"..; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = s",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:39356,Availability,redundant,redundant,39356,"0, i32 0; %3073 = call i8* @strcpy(i8* %3072, i8* %3071) nounwind; %strlen = call i32 @strlen(i8* %3072) ; uses = 1; %endptr = getelementptr [100 x i8]* %tempString, i32 0, i32 %strlen; call void @llvm.memcpy.i32(i8* %endptr, ; i8* getelementptr ([5 x i8]* @""\01LC42"", i32 0, i32 0), i32 5, i32 1); %3074 = call i32 @strlen(i8* %endptr) nounwind readonly ; ; This is interesting for a couple reasons. First, in this:. The memcpy+strlen strlen can be replaced with:. %3074 = call i32 @strlen([5 x i8]* @""\01LC42"") nounwind readonly . Because the destination was just copied into the specified memory buffer. This,; in turn, can be constant folded to ""4"". In other code, it contains:. %endptr6978 = bitcast i8* %endptr69 to i32* ; store i32 7107374, i32* %endptr6978, align 1; %3167 = call i32 @strlen(i8* %endptr69) nounwind readonly . Which could also be constant folded. Whatever is producing this should probably; be fixed to leave this as a memcpy from a string. Further, eon also has an interesting partially redundant strlen call:. bb8: ; preds = %_ZN18eonImageCalculatorC1Ev.exit; %682 = getelementptr i8** %argv, i32 6 ; <i8**> [#uses=2]; %683 = load i8** %682, align 4 ; <i8*> [#uses=4]; %684 = load i8* %683, align 1 ; <i8> [#uses=1]; %685 = icmp eq i8 %684, 0 ; <i1> [#uses=1]; br i1 %685, label %bb10, label %bb9. bb9: ; preds = %bb8; %686 = call i32 @strlen(i8* %683) nounwind readonly ; %687 = icmp ugt i32 %686, 254 ; <i1> [#uses=1]; br i1 %687, label %bb10, label %bb11. bb10: ; preds = %bb9, %bb8; %688 = call i32 @strlen(i8* %683) nounwind readonly . This could be eliminated by doing the strlen once in bb8, saving code size and; improving perf on the bb8->9->10 path. //===---------------------------------------------------------------------===//. I see an interesting fully redundant call to strlen left in 186.crafty:InputMove; which looks like:; %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 ; . bb62: ; preds = %bb55, %bb53; %promote.0 = phi i32 [ %169, %bb55",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:40138,Availability,redundant,redundant,40138,"(i8* %endptr69) nounwind readonly . Which could also be constant folded. Whatever is producing this should probably; be fixed to leave this as a memcpy from a string. Further, eon also has an interesting partially redundant strlen call:. bb8: ; preds = %_ZN18eonImageCalculatorC1Ev.exit; %682 = getelementptr i8** %argv, i32 6 ; <i8**> [#uses=2]; %683 = load i8** %682, align 4 ; <i8*> [#uses=4]; %684 = load i8* %683, align 1 ; <i8> [#uses=1]; %685 = icmp eq i8 %684, 0 ; <i1> [#uses=1]; br i1 %685, label %bb10, label %bb9. bb9: ; preds = %bb8; %686 = call i32 @strlen(i8* %683) nounwind readonly ; %687 = icmp ugt i32 %686, 254 ; <i1> [#uses=1]; br i1 %687, label %bb10, label %bb11. bb10: ; preds = %bb9, %bb8; %688 = call i32 @strlen(i8* %683) nounwind readonly . This could be eliminated by doing the strlen once in bb8, saving code size and; improving perf on the bb8->9->10 path. //===---------------------------------------------------------------------===//. I see an interesting fully redundant call to strlen left in 186.crafty:InputMove; which looks like:; %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 ; . bb62: ; preds = %bb55, %bb53; %promote.0 = phi i32 [ %169, %bb55 ], [ 0, %bb53 ] ; %171 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1; %172 = add i32 %171, -1 ; <i32> [#uses=1]; %173 = getelementptr [128 x i8]* %movetext, i32 0, i32 %172 . ... no stores ...; br i1 %or.cond, label %bb65, label %bb72. bb65: ; preds = %bb62; store i8 0, i8* %173, align 1; br label %bb72. bb72: ; preds = %bb65, %bb62; %trank.1 = phi i32 [ %176, %bb65 ], [ -1, %bb62 ] ; %177 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1. Note that on the bb62->bb72 path, that the %177 strlen call is partially; redundant with the %171 call. At worst, we could shove the %177 strlen call; up into the bb65 block moving it out of the bb62->bb72 path. However, note; that bb65 stores to the string, zeroing out the last byte. This means that on; that path the value ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:40891,Availability,redundant,redundant,40891," i32 @strlen(i8* %683) nounwind readonly . This could be eliminated by doing the strlen once in bb8, saving code size and; improving perf on the bb8->9->10 path. //===---------------------------------------------------------------------===//. I see an interesting fully redundant call to strlen left in 186.crafty:InputMove; which looks like:; %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 ; . bb62: ; preds = %bb55, %bb53; %promote.0 = phi i32 [ %169, %bb55 ], [ 0, %bb53 ] ; %171 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1; %172 = add i32 %171, -1 ; <i32> [#uses=1]; %173 = getelementptr [128 x i8]* %movetext, i32 0, i32 %172 . ... no stores ...; br i1 %or.cond, label %bb65, label %bb72. bb65: ; preds = %bb62; store i8 0, i8* %173, align 1; br label %bb72. bb72: ; preds = %bb65, %bb62; %trank.1 = phi i32 [ %176, %bb65 ], [ -1, %bb62 ] ; %177 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1. Note that on the bb62->bb72 path, that the %177 strlen call is partially; redundant with the %171 call. At worst, we could shove the %177 strlen call; up into the bb65 block moving it out of the bb62->bb72 path. However, note; that bb65 stores to the string, zeroing out the last byte. This means that on; that path the value of %177 is actually just %171-1. A sub is cheaper than a; strlen!. This pattern repeats several times, basically doing:. A = strlen(P);; P[A-1] = 0;; B = strlen(P);; where it is ""obvious"" that B = A-1. //===---------------------------------------------------------------------===//. 186.crafty has this interesting pattern with the ""out.4543"" variable:. call void @llvm.memcpy.i32(; i8* getelementptr ([10 x i8]* @out.4543, i32 0, i32 0),; i8* getelementptr ([7 x i8]* @""\01LC28700"", i32 0, i32 0), i32 7, i32 1) ; %101 = call@printf(i8* ... @out.4543, i32 0, i32 0)) nounwind . It is basically doing:. memcpy(globalarray, ""string"");; printf(..., globalarray);; ; Anyway, by knowing that printf just reads the memory and forw",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:45329,Availability,mask,masked,45329,"readnone ssp {; entry:; %0 = and i32 %b, -129 ; <i32> [#uses=1]; %1 = and i32 %a, 128 ; <i32> [#uses=1]; %2 = or i32 %0, %1 ; <i32> [#uses=1]; ret i32 %2; }. This can be generalized for other forms:. b = (b & ~0x80) | (a & 0x40) << 1;. //===---------------------------------------------------------------------===//. These two functions produce different code. They shouldn't:. #include <stdint.h>; ; uint8_t p1(uint8_t b, uint8_t a) {; b = (b & ~0xc0) | (a & 0xc0);; return (b);; }; ; uint8_t p2(uint8_t b, uint8_t a) {; b = (b & ~0x40) | (a & 0x40);; b = (b & ~0x80) | (a & 0x80);; return (b);; }. define zeroext i8 @p1(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %1 = and i8 %a, -64 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; ret i8 %2; }. define zeroext i8 @p2(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %.masked = and i8 %a, 64 ; <i8> [#uses=1]; %1 = and i8 %a, -128 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; %3 = or i8 %2, %.masked ; <i8> [#uses=1]; ret i8 %3; }. //===---------------------------------------------------------------------===//. IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of co",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:45462,Availability,mask,masked,45462,"; <i32> [#uses=1]; %2 = or i32 %0, %1 ; <i32> [#uses=1]; ret i32 %2; }. This can be generalized for other forms:. b = (b & ~0x80) | (a & 0x40) << 1;. //===---------------------------------------------------------------------===//. These two functions produce different code. They shouldn't:. #include <stdint.h>; ; uint8_t p1(uint8_t b, uint8_t a) {; b = (b & ~0xc0) | (a & 0xc0);; return (b);; }; ; uint8_t p2(uint8_t b, uint8_t a) {; b = (b & ~0x40) | (a & 0x40);; b = (b & ~0x80) | (a & 0x80);; return (b);; }. define zeroext i8 @p1(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %1 = and i8 %a, -64 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; ret i8 %2; }. define zeroext i8 @p2(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %.masked = and i8 %a, 64 ; <i8> [#uses=1]; %1 = and i8 %a, -128 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; %3 = or i8 %2, %.masked ; <i8> [#uses=1]; ret i8 %3; }. //===---------------------------------------------------------------------===//. IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:55191,Availability,avail,available,55191,"lowing it to; fold to %62. This is a security win (overflows of malloc will get caught); and also a performance win by exposing more memsets to the optimizer. This occurs several times in viterbi. Note that this would change the semantics of @llvm.objectsize which by its; current definition always folds to a constant. We also should make sure that; we remove checking in code like. char *p = malloc(strlen(s)+1);; __strcpy_chk(p, s, __builtin_object_size(p, 0));. //===---------------------------------------------------------------------===//. clang -O3 currently compiles this code. int g(unsigned int a) {; unsigned int c[100];; c[10] = a;; c[11] = a;; unsigned int b = c[10] + c[11];; if(b > a*2) a = 4;; else a = 8;; return a + 7;; }. into. define i32 @g(i32 a) nounwind readnone {; %add = shl i32 %a, 1; %mul = shl i32 %a, 1; %cmp = icmp ugt i32 %add, %mul; %a.addr.0 = select i1 %cmp, i32 11, i32 15; ret i32 %a.addr.0; }. The icmp should fold to false. This CSE opportunity is only available; after GVN and InstCombine have run. //===---------------------------------------------------------------------===//. memcpyopt should turn this:. define i8* @test10(i32 %x) {; %alloc = call noalias i8* @malloc(i32 %x) nounwind; call void @llvm.memset.p0i8.i32(i8* %alloc, i8 0, i32 %x, i32 1, i1 false); ret i8* %alloc; }. into a call to calloc. We should make sure that we analyze calloc as; aggressively as malloc though. //===---------------------------------------------------------------------===//. clang -O3 doesn't optimize this:. void f1(int* begin, int* end) {; std::fill(begin, end, 0);; }. into a memset. This is PR8942. //===---------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. void f(int N) {; std::vector<int> v(N);. extern void sink(void*); sink(&v);; }. into. define void @_Z1fi(i32 %N) nounwind {; entry:; %v2 = alloca [3 x i32*], align 8; %v2.sub = getelementptr inbounds [3 x i32*]* %v2, i64 0, i6",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:3340,Deployability,patch,patch,3340,"mul i32 %X1, 47 ; X1*47; %C = mul i32 %B, %B; ret i32 %C; }. //===---------------------------------------------------------------------===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ; b3 = a3 + a2 + a1 + a0; ; b2 = a2 + a1 + a0; ; b1 = a1 + a0; ; } . This requires reassociating to forms of expressions that are already available,; something that reassoc doesn't think about yet. //===---------------------------------------------------------------------===//. These two functions should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===---------------------------------------------------------------------===//. It would be nice to revert this patch:; http://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20060213/031986.html. And teach the dag combiner enough to simplify the code expanded before ; legalize. It seems plausible that this knowledge would let it simplify other; stuff too. //===---------------------------------------------------------------------===//. For vector types, DataLayout.cpp::getTypeInfo() returns alignment that is equal; to the type size. It works but can be overly conservative as the alignment of; specific vector types are target dependent. //===---------------------------------------------------------------------===//. We should produce an unaligned load from code like this:. v4sf example(float *P) {; return (v4sf){P[0], P[1], P[2], P[3] };; }. //===---------------------------------------------------------------------===//. Add support for conditional increments, and other related patterns. Instead; of:. 	movl 136(%esp), %eax; 	cmpl $0, %eax; 	je LBB16_2	#cond_next; LBB16_1:	#cond_true; 	inc",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31247,Deployability,update,updated,31247," 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tm",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:50790,Deployability,update,updates,50790,"ssues (PR6212):. int test1(int mainType, int subType) {; if (mainType == 7); subType = 4;; else if (mainType == 9); subType = 6;; else if (mainType == 11); subType = 9;; return subType;; }. int test2(int mainType, int subType) {; if (mainType == 7); subType = 4;; if (mainType == 9); subType = 6;; if (mainType == 11); subType = 9;; return subType;; }. //===---------------------------------------------------------------------===//. The following test case (from PR6576):. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %cond1 = icmp eq i32 %b, 0 ; <i1> [#uses=1]; br i1 %cond1, label %exit, label %bb.nph; bb.nph: ; preds = %entry; %tmp = mul i32 %b, %a ; <i32> [#uses=1]; ret i32 %tmp; exit: ; preds = %entry; ret i32 0; }. could be reduced to:. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = mul i32 %b, %a; ret i32 %tmp; }. //===---------------------------------------------------------------------===//. We should use DSE + llvm.lifetime.end to delete dead vtable pointer updates.; See GCC PR34949. Another interesting case is that something related could be used for variables; that go const after their ctor has finished. In these cases, globalopt (which; can statically run the constructor) could mark the global const (so it gets put; in the readonly section). A testcase would be:. #include <complex>; using namespace std;; const complex<char> should_be_in_rodata (42,-42);; complex<char> should_be_in_data (42,-42);; complex<char> should_be_in_bss;. Where we currently evaluate the ctors but the globals don't become const because; the optimizer doesn't know they ""become const"" after the ctor is done. See; GCC PR4131 for more examples. //===---------------------------------------------------------------------===//. In this code:. long foo(long x) {; return x > 1 ? x : 1;; }. LLVM emits a comparison with 1 instead of 0. 0 would be equivalent; and cheaper on most targets. LLVM prefers comparisons with zero over non-zero in general, but in this; ca",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:59372,Deployability,update,updated,59372,"-------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. void f(char* a, int n) {; __builtin_memset(a, 0, n);; for (int i = 0; i < n; ++i); a[i] = 0;; }. into:. define void @_Z1fPci(i8* nocapture %a, i32 %n) nounwind {; entry:; %conv = sext i32 %n to i64; tail call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %conv, i32 1, i1 false); %cmp8 = icmp sgt i32 %n, 0; br i1 %cmp8, label %for.body.lr.ph, label %for.end. for.body.lr.ph: ; preds = %entry; %tmp10 = add i32 %n, -1; %tmp11 = zext i32 %tmp10 to i64; %tmp12 = add i64 %tmp11, 1; call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %tmp12, i32 1, i1 false); ret void. for.end: ; preds = %entry; ret void; }. This shouldn't need the ((zext (%n - 1)) + 1) game, and it should ideally fold; the two memset's together. The issue with the addition only occurs in 64-bit mode, and appears to be at; least partially caused by Scalar Evolution not keeping its cache updated: it; returns the ""wrong"" result immediately after indvars runs, but figures out the; expected result if it is run from scratch on IR resulting from running indvars. //===---------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. struct S {; unsigned short m1, m2;; unsigned char m3, m4;; };. void f(int N) {; std::vector<S> v(N);; extern void sink(void*); sink(&v);; }. into poor code for zero-initializing 'v' when N is >0. The problem is that; S is only 6 bytes, but each element is 8 byte-aligned. We generate a loop and; 4 stores on each iteration. If the struct were 8 bytes, this gets turned into; a memset. In order to handle this we have to:; A) Teach clang to generate metadata for memsets of structs that have holes in; them.; B) Teach clang to use such a memset for zero init of this struct (since it has; a hole), instead of doing elementwise zeroing. //===---------------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:16789,Energy Efficiency,reduce,reduces,16789,"return y;; }. //===---------------------------------------------------------------------===//. The loop unroller should partially unroll loops (instead of peeling them); when code growth isn't too bad and when an unroll count allows simplification; of some code within the loop. One trivial example is:. #include <stdio.h>; int main() {; int nRet = 17;; int nLoop;; for ( nLoop = 0; nLoop < 1000; nLoop++ ) {; if ( nLoop & 1 ); nRet += 2;; else; nRet -= 1;; }; return nRet;; }. Unrolling by 2 would eliminate the '&1' in both copies, leading to a net; reduction in code size. The resultant code would then also be suitable for; exit value computation. //===---------------------------------------------------------------------===//. We miss a bunch of rotate opportunities on various targets, including ppc, x86,; etc. On X86, we miss a bunch of 'rotate by variable' cases because the rotate; matching code in dag combine doesn't look through truncates aggressively ; enough. Here are some testcases reduces from GCC PR17886:. unsigned long long f5(unsigned long long x, unsigned long long y) {; return (x << 8) | ((y >> 48) & 0xffull);; }; unsigned long long f6(unsigned long long x, unsigned long long y, int z) {; switch(z) {; case 1:; return (x << 8) | ((y >> 48) & 0xffull);; case 2:; return (x << 16) | ((y >> 40) & 0xffffull);; case 3:; return (x << 24) | ((y >> 32) & 0xffffffull);; case 4:; return (x << 32) | ((y >> 24) & 0xffffffffull);; default:; return (x << 40) | ((y >> 16) & 0xffffffffffull);; }; }. //===---------------------------------------------------------------------===//. This (and similar related idioms):. unsigned int foo(unsigned char i) {; return i | (i<<8) | (i<<16) | (i<<24);; } . compiles into:. define i32 @foo(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %shl5 = shl i32 %conv, 16; %shl9 = shl i32 %conv, 24; %or = or i32 %shl9, %conv; %or6 = or i32 %or, %shl5; %or10 = or i32 %or6, %shl; ret i32 %or",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:18510,Energy Efficiency,reduce,reduce,18510,"o:. define i32 @foo(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %shl5 = shl i32 %conv, 16; %shl9 = shl i32 %conv, 24; %or = or i32 %shl9, %conv; %or6 = or i32 %or, %shl5; %or10 = or i32 %or6, %shl; ret i32 %or10; }. it would be better as:. unsigned int bar(unsigned char i) {; unsigned int j=i | (i << 8); ; return j | (j<<16);; }. aka:. define i32 @bar(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %or = or i32 %shl, %conv; %shl5 = shl i32 %or, 16; %or6 = or i32 %shl5, %or; ret i32 %or6; }. or even i*0x01010101, depending on the speed of the multiplier. The best way to; handle this is to canonicalize it to a multiply in IR and have codegen handle; lowering multiplies to shifts on cpus where shifts are faster. //===---------------------------------------------------------------------===//. We do a number of simplifications in simplify libcalls to strength reduce; standard library functions, but we don't currently merge them together. For; example, it is useful to merge memcpy(a,b,strlen(b)) -> strcpy. This can only; be done safely if ""b"" isn't modified between the strlen and memcpy of course. //===---------------------------------------------------------------------===//. We compile this program: (from GCC PR11680); http://gcc.gnu.org/bugzilla/attachment.cgi?id=4487. Into code that runs the same speed in fast/slow modes, but both modes run 2x; slower than when compile with GCC (either 4.0 or 4.2):. $ llvm-g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 1.821u 0.003s 0:01.82 100.0%	0+0k 0+0io 0pf+0w. $ g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 0.821u 0.001s 0:00.82 100.0%	0+0k 0+0io 0pf+0w. It looks like we are making the same inlining decisions, so this may be raw; codegen badness or something else (haven't investigated). //===---------------------------------------------------------------------===//. Divisibility by const",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:29658,Energy Efficiency,reduce,reduce,29658,"us this code:. bb:		; preds = %bb2, %entry; 	%.rle = phi i32 [ 0, %entry ], [ %.rle6, %bb2 ]	; 	%i.05 = phi i32 [ 0, %entry ], [ %indvar.next, %bb2 ]; 	%1 = load i32* %cond, align 4; 	%2 = icmp eq i32 %1, 0; 	br i1 %2, label %bb2, label %bb1. bb1:		; preds = %bb; 	%3 = xor i32 %.rle, 234	; 	store i32 %3, i32* %res, align 4; 	br label %bb2. bb2:		; preds = %bb, %bb1; 	%.rle6 = phi i32 [ %3, %bb1 ], [ %.rle, %bb ]	; 	%indvar.next = add i32 %i.05, 1	; 	%exitcond = icmp eq i32 %indvar.next, %n; 	br i1 %exitcond, label %return, label %bb. DSE should sink partially dead stores to get the store out of the loop. Here's another partial dead case:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=12395. //===---------------------------------------------------------------------===//. Scalar PRE hoists the mul in the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * c + g;; return d + e;; }. It would be better to do the mul once to reduce codesize above the if.; This is GCC PR38204. //===---------------------------------------------------------------------===//; This simple function from 179.art:. int winner, numf2s;; struct { double y; int reset; } *Y;. void find_match() {; int i;; winner = 0;; for (i=0;i<numf2s;i++); if (Y[i].y > Y[winner].y); winner =i;; }. Compiles into (with clang TBAA):. for.body: ; preds = %for.inc, %bb.nph; %indvar = phi i64 [ 0, %bb.nph ], [ %indvar.next, %for.inc ]; %i.01718 = phi i32 [ 0, %bb.nph ], [ %i.01719, %for.inc ]; %tmp4 = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; pre",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:50527,Energy Efficiency,reduce,reduced,50527,"shift should be eliminated. Testcase derived from gcc. //===---------------------------------------------------------------------===//. These compile into different code, one gets recognized as a switch and the; other doesn't due to phase ordering issues (PR6212):. int test1(int mainType, int subType) {; if (mainType == 7); subType = 4;; else if (mainType == 9); subType = 6;; else if (mainType == 11); subType = 9;; return subType;; }. int test2(int mainType, int subType) {; if (mainType == 7); subType = 4;; if (mainType == 9); subType = 6;; if (mainType == 11); subType = 9;; return subType;; }. //===---------------------------------------------------------------------===//. The following test case (from PR6576):. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %cond1 = icmp eq i32 %b, 0 ; <i1> [#uses=1]; br i1 %cond1, label %exit, label %bb.nph; bb.nph: ; preds = %entry; %tmp = mul i32 %b, %a ; <i32> [#uses=1]; ret i32 %tmp; exit: ; preds = %entry; ret i32 0; }. could be reduced to:. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = mul i32 %b, %a; ret i32 %tmp; }. //===---------------------------------------------------------------------===//. We should use DSE + llvm.lifetime.end to delete dead vtable pointer updates.; See GCC PR34949. Another interesting case is that something related could be used for variables; that go const after their ctor has finished. In these cases, globalopt (which; can statically run the constructor) could mark the global const (so it gets put; in the readonly section). A testcase would be:. #include <complex>; using namespace std;; const complex<char> should_be_in_rodata (42,-42);; complex<char> should_be_in_data (42,-42);; complex<char> should_be_in_bss;. Where we currently evaluate the ctors but the globals don't become const because; the optimizer doesn't know they ""become const"" after the ctor is done. See; GCC PR4131 for more examples. //===-------------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:52916,Energy Efficiency,power,power,52916,"id @a(i32 %x) nounwind {; entry:; switch i32 %x, label %if.end [; i32 0, label %if.then; i32 1, label %if.then; i32 2, label %if.then; i32 3, label %if.then; i32 5, label %if.then; ]; if.then:; tail call void @foo() nounwind; ret void; if.end:; ret void; }; declare void @foo(). Generated code on x86-64 (other platforms give similar results):; a:; 	cmpl	$5, %edi; 	ja	LBB2_2; 	cmpl	$4, %edi; 	jne	LBB2_3; .LBB0_2:; 	ret; .LBB0_3:; 	jmp	foo # TAILCALL. If we wanted to be really clever, we could simplify the whole thing to; something like the following, which eliminates a branch:; 	xorl $1, %edi; 	cmpl	$4, %edi; 	ja	.LBB0_2; 	ret; .LBB0_2:; 	jmp	foo # TAILCALL. //===---------------------------------------------------------------------===//. We compile this:. int foo(int a) { return (a & (~15)) / 16; }. Into:. define i32 @foo(i32 %a) nounwind readnone ssp {; entry:; %and = and i32 %a, -16; %div = sdiv i32 %and, 16; ret i32 %div; }. but this code (X & -A)/A is X >> log2(A) when A is a power of 2, so this case; should be instcombined into just ""a >> 4"". We do get this at the codegen level, so something knows about it, but ; instcombine should catch it earlier:. _foo: ## @foo; ## %bb.0: ## %entry; 	movl	%edi, %eax; 	sarl	$4, %eax; 	ret. //===---------------------------------------------------------------------===//. This code (from GCC PR28685):. int test(int a, int b) {; int lt = a < b;; int eq = a == b;; if (lt); return 1;; return eq;; }. Is compiled to:. define i32 @test(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %cmp = icmp slt i32 %a, %b; br i1 %cmp, label %return, label %if.end. if.end: ; preds = %entry; %cmp5 = icmp eq i32 %a, %b; %conv6 = zext i1 %cmp5 to i32; ret i32 %conv6. return: ; preds = %entry; ret i32 1; }. it could be:. define i32 @test__(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = icmp sle i32 %a, %b; %retval = zext i1 %0 to i32; ret i32 %retval; }. //===---------------------------------------------------------------------===//. This code ca",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:3863,Integrability,depend,dependent,3863,"eady available,; something that reassoc doesn't think about yet. //===---------------------------------------------------------------------===//. These two functions should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===---------------------------------------------------------------------===//. It would be nice to revert this patch:; http://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20060213/031986.html. And teach the dag combiner enough to simplify the code expanded before ; legalize. It seems plausible that this knowledge would let it simplify other; stuff too. //===---------------------------------------------------------------------===//. For vector types, DataLayout.cpp::getTypeInfo() returns alignment that is equal; to the type size. It works but can be overly conservative as the alignment of; specific vector types are target dependent. //===---------------------------------------------------------------------===//. We should produce an unaligned load from code like this:. v4sf example(float *P) {; return (v4sf){P[0], P[1], P[2], P[3] };; }. //===---------------------------------------------------------------------===//. Add support for conditional increments, and other related patterns. Instead; of:. 	movl 136(%esp), %eax; 	cmpl $0, %eax; 	je LBB16_2	#cond_next; LBB16_1:	#cond_true; 	incl _foo; LBB16_2:	#cond_next. emit:; 	movl	_foo, %eax; 	cmpl	$1, %edi; 	sbbl	$-1, %eax; 	movl	%eax, _foo. //===---------------------------------------------------------------------===//. Combine: a = sin(x), b = cos(x) into a,b = sincos(x). Expand these to calls of sin/cos and stores:; double sincos(double x, double *sin, double *cos);; float sincosf(float x, float *sin, float *cos);; long double sincosl(long double x, long double *sin, long double *cos);. Doing so could allow SROA o",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:18161,Integrability,depend,depending,18161,"fffull);; case 4:; return (x << 32) | ((y >> 24) & 0xffffffffull);; default:; return (x << 40) | ((y >> 16) & 0xffffffffffull);; }; }. //===---------------------------------------------------------------------===//. This (and similar related idioms):. unsigned int foo(unsigned char i) {; return i | (i<<8) | (i<<16) | (i<<24);; } . compiles into:. define i32 @foo(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %shl5 = shl i32 %conv, 16; %shl9 = shl i32 %conv, 24; %or = or i32 %shl9, %conv; %or6 = or i32 %or, %shl5; %or10 = or i32 %or6, %shl; ret i32 %or10; }. it would be better as:. unsigned int bar(unsigned char i) {; unsigned int j=i | (i << 8); ; return j | (j<<16);; }. aka:. define i32 @bar(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %or = or i32 %shl, %conv; %shl5 = shl i32 %or, 16; %or6 = or i32 %shl5, %or; ret i32 %or6; }. or even i*0x01010101, depending on the speed of the multiplier. The best way to; handle this is to canonicalize it to a multiply in IR and have codegen handle; lowering multiplies to shifts on cpus where shifts are faster. //===---------------------------------------------------------------------===//. We do a number of simplifications in simplify libcalls to strength reduce; standard library functions, but we don't currently merge them together. For; example, it is useful to merge memcpy(a,b,strlen(b)) -> strcpy. This can only; be done safely if ""b"" isn't modified between the strlen and memcpy of course. //===---------------------------------------------------------------------===//. We compile this program: (from GCC PR11680); http://gcc.gnu.org/bugzilla/attachment.cgi?id=4487. Into code that runs the same speed in fast/slow modes, but both modes run 2x; slower than when compile with GCC (either 4.0 or 4.2):. $ llvm-g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 1.821u 0.003s 0:01.82 100.0%	0+0k 0+0io 0pf+",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:32415,Integrability,wrap,wrap,32415,". In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:45627,Integrability,depend,dependent,45627,"-------------------------------------------------------------===//. These two functions produce different code. They shouldn't:. #include <stdint.h>; ; uint8_t p1(uint8_t b, uint8_t a) {; b = (b & ~0xc0) | (a & 0xc0);; return (b);; }; ; uint8_t p2(uint8_t b, uint8_t a) {; b = (b & ~0x40) | (a & 0x40);; b = (b & ~0x80) | (a & 0x80);; return (b);; }. define zeroext i8 @p1(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %1 = and i8 %a, -64 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; ret i8 %2; }. define zeroext i8 @p2(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %.masked = and i8 %a, 64 ; <i8> [#uses=1]; %1 = and i8 %a, -128 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; %3 = or i8 %2, %.masked ; <i8> [#uses=1]; ret i8 %3; }. //===---------------------------------------------------------------------===//. IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------------------------------------------===//. The libcall constant folding stuff should be moved out of SimplifyLibcalls into; libanalysis' constantfolding logic. Thi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:1644,Modifiability,extend,extended,1644,"(ffastmath). Misc/mandel will like this. :) This isn't; safe in general, even on darwin. See the libm implementation of hypot for; examples (which special case when x/y are exactly zero to get signed zeros etc; right). //===---------------------------------------------------------------------===//. On targets with expensive 64-bit multiply, we could LSR this:. for (i = ...; ++i) {; x = 1ULL << i;. into:; long long tmp = 1;; for (i = ...; ++i, tmp+=tmp); x = tmp;. This would be a win on ppc32, but not x86 or ppc64. //===---------------------------------------------------------------------===//. Shrink: (setlt (loadi32 P), 0) -> (setlt (loadi8 Phi), 0). //===---------------------------------------------------------------------===//. Reassociate should turn things like:. int factorial(int X) {; return X*X*X*X*X*X*X*X;; }. into llvm.powi calls, allowing the code generator to produce balanced; multiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n) + bar(2*z, 2*n);; }. This is blocked on not handling X*X*X -> powi(X, 3) (see note above). The issue; is that we end up getting t = 2*X s = t*t and don't turn this into 4*X*X,; which is the same number of multiplies and is canonical, because the 2*X has; multiple uses. Here's a simple example:. define i32 @test15(i32 %X1) {; %B = mul i32 %X1, 47 ; X1*47; %C = mul i32 %B, %B; ret i32 %C; }. //===---------------------------------------------------------------------===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:1717,Modifiability,enhance,enhanced,1717,"(ffastmath). Misc/mandel will like this. :) This isn't; safe in general, even on darwin. See the libm implementation of hypot for; examples (which special case when x/y are exactly zero to get signed zeros etc; right). //===---------------------------------------------------------------------===//. On targets with expensive 64-bit multiply, we could LSR this:. for (i = ...; ++i) {; x = 1ULL << i;. into:; long long tmp = 1;; for (i = ...; ++i, tmp+=tmp); x = tmp;. This would be a win on ppc32, but not x86 or ppc64. //===---------------------------------------------------------------------===//. Shrink: (setlt (loadi32 P), 0) -> (setlt (loadi8 Phi), 0). //===---------------------------------------------------------------------===//. Reassociate should turn things like:. int factorial(int X) {; return X*X*X*X*X*X*X*X;; }. into llvm.powi calls, allowing the code generator to produce balanced; multiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n) + bar(2*z, 2*n);; }. This is blocked on not handling X*X*X -> powi(X, 3) (see note above). The issue; is that we end up getting t = 2*X s = t*t and don't turn this into 4*X*X,; which is the same number of multiplies and is canonical, because the 2*X has; multiple uses. Here's a simple example:. define i32 @test15(i32 %X1) {; %B = mul i32 %X1, 47 ; X1*47; %C = mul i32 %B, %B; ret i32 %C; }. //===---------------------------------------------------------------------===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:8488,Modifiability,variab,variable,8488,"eturn adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64 effective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; b2 = (temp >> 12) & 0xf;; if (b2) effective_addr2 += regs[b2];; effective_addr2 &= regs[4];; if ((effective_addr2 & 3) == 0); return 1;; return 0;; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:9790,Modifiability,extend,extended,9790,"$131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64 effective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; b2 = (temp >> 12) & 0xf;; if (b2) effective_addr2 += regs[b2];; effective_addr2 &= regs[4];; if ((effective_addr2 & 3) == 0); return 1;; return 0;; }. Note that only the low 2 bits of effective_addr2 are used. On 32-bit systems,; we don't eliminate the computation of the top half of effective_addr2 because; we don't have whole-function selection dags. On x86, this means we use one; extra register for the function when effective_addr2 is declared as U64 than; when it is declared U32. PHI Slicing could be extended to do this. //===---------------------------------------------------------------------===//. Tail call elim should be more aggressive, checking to see if the call is; followed by an uncond branch to an exit block. ; This testcase is due to tail-duplication not wanting to copy the return; ; instruction into the terminating blocks because there was other code; ; optimized out of the function after the taildup happened.; ; RUN: llvm-as < %s | opt -tailcallelim | llvm-dis | not grep call. define i32 @t4(i32 %a) {; entry:; 	%tmp.1 = and i32 %a, 1		; <i32> [#uses=1]; 	%tmp.2 = icmp ne i32 %tmp.1, 0		; <i1> [#uses=1]; 	br i1 %tmp.2, label %then.0, label %else.0. then.0:		; preds = %entry; 	%tmp.5 = add i32 %a, -1		; <i32> [#uses=1]; 	%tmp.3 = call i32 @t4( i32 %tmp.5 )		; <i32> [#uses=1]; 	br label %return. else.0:		; preds = %entry; 	%tmp.7 = icmp ne i32 %a, 0		; <i1> [#uses=1]; 	br i1 %tmp.7, label %then.1, label %return. then.1:		; preds = %else.0; 	%tmp.11 = add i32 %a, -2		; <i",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:16646,Modifiability,variab,variable,16646,"et int undef"". Instead, LLVM; produces ""ret int 0"":. int f() {; int x = 4;; int y;; if (x == 3) y = 0;; return y;; }. //===---------------------------------------------------------------------===//. The loop unroller should partially unroll loops (instead of peeling them); when code growth isn't too bad and when an unroll count allows simplification; of some code within the loop. One trivial example is:. #include <stdio.h>; int main() {; int nRet = 17;; int nLoop;; for ( nLoop = 0; nLoop < 1000; nLoop++ ) {; if ( nLoop & 1 ); nRet += 2;; else; nRet -= 1;; }; return nRet;; }. Unrolling by 2 would eliminate the '&1' in both copies, leading to a net; reduction in code size. The resultant code would then also be suitable for; exit value computation. //===---------------------------------------------------------------------===//. We miss a bunch of rotate opportunities on various targets, including ppc, x86,; etc. On X86, we miss a bunch of 'rotate by variable' cases because the rotate; matching code in dag combine doesn't look through truncates aggressively ; enough. Here are some testcases reduces from GCC PR17886:. unsigned long long f5(unsigned long long x, unsigned long long y) {; return (x << 8) | ((y >> 48) & 0xffull);; }; unsigned long long f6(unsigned long long x, unsigned long long y, int z) {; switch(z) {; case 1:; return (x << 8) | ((y >> 48) & 0xffull);; case 2:; return (x << 16) | ((y >> 40) & 0xffffull);; case 3:; return (x << 24) | ((y >> 32) & 0xffffffull);; case 4:; return (x << 32) | ((y >> 24) & 0xffffffffull);; default:; return (x << 40) | ((y >> 16) & 0xffffffffffull);; }; }. //===---------------------------------------------------------------------===//. This (and similar related idioms):. unsigned int foo(unsigned char i) {; return i | (i<<8) | (i<<16) | (i<<24);; } . compiles into:. define i32 @foo(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %shl5 = shl i32 %conv, 16; %shl9 = shl i3",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:41486,Modifiability,variab,variable,41486,"entptr [128 x i8]* %movetext, i32 0, i32 %172 . ... no stores ...; br i1 %or.cond, label %bb65, label %bb72. bb65: ; preds = %bb62; store i8 0, i8* %173, align 1; br label %bb72. bb72: ; preds = %bb65, %bb62; %trank.1 = phi i32 [ %176, %bb65 ], [ -1, %bb62 ] ; %177 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1. Note that on the bb62->bb72 path, that the %177 strlen call is partially; redundant with the %171 call. At worst, we could shove the %177 strlen call; up into the bb65 block moving it out of the bb62->bb72 path. However, note; that bb65 stores to the string, zeroing out the last byte. This means that on; that path the value of %177 is actually just %171-1. A sub is cheaper than a; strlen!. This pattern repeats several times, basically doing:. A = strlen(P);; P[A-1] = 0;; B = strlen(P);; where it is ""obvious"" that B = A-1. //===---------------------------------------------------------------------===//. 186.crafty has this interesting pattern with the ""out.4543"" variable:. call void @llvm.memcpy.i32(; i8* getelementptr ([10 x i8]* @out.4543, i32 0, i32 0),; i8* getelementptr ([7 x i8]* @""\01LC28700"", i32 0, i32 0), i32 7, i32 1) ; %101 = call@printf(i8* ... @out.4543, i32 0, i32 0)) nounwind . It is basically doing:. memcpy(globalarray, ""string"");; printf(..., globalarray);; ; Anyway, by knowing that printf just reads the memory and forward substituting; the string directly into the printf, this eliminates reads from globalarray.; Since this pattern occurs frequently in crafty (due to the ""DisplayTime"" and; other similar functions) there are many stores to ""out"". Once all the printfs; stop using ""out"", all that is left is the memcpy's into it. This should allow; globalopt to remove the ""stored only"" global. //===---------------------------------------------------------------------===//. This code:. define inreg i32 @foo(i8* inreg %p) nounwind {; %tmp0 = load i8* %p; %tmp1 = ashr i8 %tmp0, 5; %tmp2 = sext i8 %tmp1 to i32; ret i32 %tmp2; }. could ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:42514,Modifiability,extend,extending,42514,"m.memcpy.i32(; i8* getelementptr ([10 x i8]* @out.4543, i32 0, i32 0),; i8* getelementptr ([7 x i8]* @""\01LC28700"", i32 0, i32 0), i32 7, i32 1) ; %101 = call@printf(i8* ... @out.4543, i32 0, i32 0)) nounwind . It is basically doing:. memcpy(globalarray, ""string"");; printf(..., globalarray);; ; Anyway, by knowing that printf just reads the memory and forward substituting; the string directly into the printf, this eliminates reads from globalarray.; Since this pattern occurs frequently in crafty (due to the ""DisplayTime"" and; other similar functions) there are many stores to ""out"". Once all the printfs; stop using ""out"", all that is left is the memcpy's into it. This should allow; globalopt to remove the ""stored only"" global. //===---------------------------------------------------------------------===//. This code:. define inreg i32 @foo(i8* inreg %p) nounwind {; %tmp0 = load i8* %p; %tmp1 = ashr i8 %tmp0, 5; %tmp2 = sext i8 %tmp1 to i32; ret i32 %tmp2; }. could be dagcombine'd to a sign-extending load with a shift.; For example, on x86 this currently gets this:. 	movb	(%eax), %al; 	sarb	$5, %al; 	movsbl	%al, %eax. while it could get this:. 	movsbl	(%eax), %eax; 	sarl	$5, %eax. //===---------------------------------------------------------------------===//. GCC PR31029:. int test(int x) { return 1-x == x; } // --> return false; int test2(int x) { return 2-x == x; } // --> return x == 1 ?. Always foldable for odd constants, what is the rule for even?. //===---------------------------------------------------------------------===//. PR 3381: GEP to field of size 0 inside a struct could be turned into GEP; for next field in struct (which is at same address). For example: store of float into { {{}}, float } could be turned into a store to; the float directly. //===---------------------------------------------------------------------===//. The arg promotion pass should make use of nocapture to make its alias analysis; stuff much more precise. //===-------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:46148,Modifiability,extend,extend,46148," ret i8 %2; }. define zeroext i8 @p2(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %.masked = and i8 %a, 64 ; <i8> [#uses=1]; %1 = and i8 %a, -128 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; %3 = or i8 %2, %.masked ; <i8> [#uses=1]; ret i8 %3; }. //===---------------------------------------------------------------------===//. IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------------------------------------------===//. The libcall constant folding stuff should be moved out of SimplifyLibcalls into; libanalysis' constantfolding logic. This would allow IPSCCP to be able to; handle simple things like this:. static int foo(const char *X) { return strlen(X); }; int bar() { return foo(""abcd""); }. //===---------------------------------------------------------------------===//. function-attrs doesn't know much about memcpy/memset. This function should be; marked readnone rather than readonly, since it only twiddles local memory, but; function-attrs doesn't handle memset/memcpy/memmove aggressively:. struct X { int *p; int *q; };; int foo() {; int i = 0, j = 1;; struct X x, y;; int ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:50886,Modifiability,variab,variables,50886,"ainType == 9); subType = 6;; else if (mainType == 11); subType = 9;; return subType;; }. int test2(int mainType, int subType) {; if (mainType == 7); subType = 4;; if (mainType == 9); subType = 6;; if (mainType == 11); subType = 9;; return subType;; }. //===---------------------------------------------------------------------===//. The following test case (from PR6576):. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %cond1 = icmp eq i32 %b, 0 ; <i1> [#uses=1]; br i1 %cond1, label %exit, label %bb.nph; bb.nph: ; preds = %entry; %tmp = mul i32 %b, %a ; <i32> [#uses=1]; ret i32 %tmp; exit: ; preds = %entry; ret i32 0; }. could be reduced to:. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = mul i32 %b, %a; ret i32 %tmp; }. //===---------------------------------------------------------------------===//. We should use DSE + llvm.lifetime.end to delete dead vtable pointer updates.; See GCC PR34949. Another interesting case is that something related could be used for variables; that go const after their ctor has finished. In these cases, globalopt (which; can statically run the constructor) could mark the global const (so it gets put; in the readonly section). A testcase would be:. #include <complex>; using namespace std;; const complex<char> should_be_in_rodata (42,-42);; complex<char> should_be_in_data (42,-42);; complex<char> should_be_in_bss;. Where we currently evaluate the ctors but the globals don't become const because; the optimizer doesn't know they ""become const"" after the ctor is done. See; GCC PR4131 for more examples. //===---------------------------------------------------------------------===//. In this code:. long foo(long x) {; return x > 1 ? x : 1;; }. LLVM emits a comparison with 1 instead of 0. 0 would be equivalent; and cheaper on most targets. LLVM prefers comparisons with zero over non-zero in general, but in this; case it choses instead to keep the max operation obvious. //===-----------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:60963,Modifiability,extend,extended,60963," on each iteration. If the struct were 8 bytes, this gets turned into; a memset. In order to handle this we have to:; A) Teach clang to generate metadata for memsets of structs that have holes in; them.; B) Teach clang to use such a memset for zero init of this struct (since it has; a hole), instead of doing elementwise zeroing. //===---------------------------------------------------------------------===//. clang -O3 currently compiles this code:. extern const int magic;; double f() { return 0.0 * magic; }. into. @magic = external constant i32. define double @_Z1fv() nounwind readnone {; entry:; %tmp = load i32* @magic, align 4, !tbaa !0; %conv = sitofp i32 %tmp to double; %mul = fmul double %conv, 0.000000e+00; ret double %mul; }. We should be able to fold away this fmul to 0.0. More generally, fmul(x,0.0); can be folded to 0.0 if we can prove that the LHS is not -0.0, not a NaN, and; not an INF. The CannotBeNegativeZero predicate in value tracking should be; extended to support general ""fpclassify"" operations that can return ; yes/no/unknown for each of these predicates. In this predicate, we know that uitofp is trivially never NaN or -0.0, and; we know that it isn't +/-Inf if the floating point type has enough exponent bits; to represent the largest integer value as < inf. //===---------------------------------------------------------------------===//. When optimizing a transformation that can change the sign of 0.0 (such as the; 0.0*val -> 0.0 transformation above), it might be provable that the sign of the; expression doesn't matter. For example, by the above rules, we can't transform; fmul(sitofp(x), 0.0) into 0.0, because x might be -1 and the result of the; expression is defined to be -0.0. If we look at the uses of the fmul for example, we might be able to prove that; all uses don't care about the sign of zero. For example, if we have:. fadd(fmul(sitofp(x), 0.0), 2.0). Since we know that x+2.0 doesn't care about the sign of any zeros in X, we can; transform",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:62123,Modifiability,enhance,enhance,62123,"/-Inf if the floating point type has enough exponent bits; to represent the largest integer value as < inf. //===---------------------------------------------------------------------===//. When optimizing a transformation that can change the sign of 0.0 (such as the; 0.0*val -> 0.0 transformation above), it might be provable that the sign of the; expression doesn't matter. For example, by the above rules, we can't transform; fmul(sitofp(x), 0.0) into 0.0, because x might be -1 and the result of the; expression is defined to be -0.0. If we look at the uses of the fmul for example, we might be able to prove that; all uses don't care about the sign of zero. For example, if we have:. fadd(fmul(sitofp(x), 0.0), 2.0). Since we know that x+2.0 doesn't care about the sign of any zeros in X, we can; transform the fmul to 0.0, and then the fadd to 2.0. //===---------------------------------------------------------------------===//. We should enhance memcpy/memcpy/memset to allow a metadata node on them; indicating that some bytes of the transfer are undefined. This is useful for; frontends like clang when lowering struct copies, when some elements of the; struct are undefined. Consider something like this:. struct x {; char a;; int b[4];; };; void foo(struct x*P);; struct x testfunc() {; struct x V1, V2;; foo(&V1);; V2 = V1;. return V2;; }. We currently compile this to:; $ clang t.c -S -o - -O0 -emit-llvm | opt -sroa -S. %struct.x = type { i8, [4 x i32] }. define void @testfunc(%struct.x* sret %agg.result) nounwind ssp {; entry:; %V1 = alloca %struct.x, align 4; call void @foo(%struct.x* %V1); %tmp1 = bitcast %struct.x* %V1 to i8*; %0 = bitcast %struct.x* %V1 to i160*; %srcval1 = load i160* %0, align 4; %tmp2 = bitcast %struct.x* %agg.result to i8*; %1 = bitcast %struct.x* %agg.result to i160*; store i160 %srcval1, i160* %1, align 4; ret void; }. This happens because SRoA sees that the temp alloca has is being memcpy'd into; and out of and it has holes and it has to be conserv",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:64691,Modifiability,variab,variables,64691,".bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this because we don't want to introduce an overlapped live range; of the ivar. However if we can make this more aggressive without causing; performance issues in two ways:. 1. If *either* the LHS or RHS has a single use, we can definitely do the; transformation. In the overlapping liverange case we're trading one register; use for one fewer operation, which is a reasonable trade. Before doing this; we should verify that the llc output actually shrinks for some benchmarks.; 2. If both ops have multiple uses, we can still fold it if the operations are; both sinkable to *after* the icmp (e.g. in a subsequent block) which doesn't; increase register pressure. There are a ton of icmp's we aren't simplifying because of the reg pressure; concern. Care is warranted here though because many of these are induction; variables and other cases that matter a lot to performance, like the above.; Here's a blob of code that you can drop into the bottom of visitICmp to see some; missed cases:. { Value *A, *B, *C, *D;; if (match(Op0, m_Add(m_Value(A), m_Value(B))) && ; match(Op1, m_Add(m_Value(C), m_Value(D))) &&; (A == C || A == D || B == C || B == D)) {; errs() << ""OP0 = "" << *Op0 << "" U="" << Op0->getNumUses() << ""\n"";; errs() << ""OP1 = "" << *Op1 << "" U="" << Op1->getNumUses() << ""\n"";; errs() << ""CMP = "" << I << ""\n\n"";; }; }. //===---------------------------------------------------------------------===//. define i1 @test1(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ult i32 %and, 2; ret i1 %cmp; }. Can be folded to (x & 2) == 0. define i1 @test2(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ugt i32 %and, 1; ret i1 %cmp; }. Can be folded to (x & 2) != 0. SimplifyDemandedBits shrinks the ""and"" constant to 2 but instcombine misses the; icmp transform. //===------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:2622,Performance,optimiz,optimized,2622,"ultiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n) + bar(2*z, 2*n);; }. This is blocked on not handling X*X*X -> powi(X, 3) (see note above). The issue; is that we end up getting t = 2*X s = t*t and don't turn this into 4*X*X,; which is the same number of multiplies and is canonical, because the 2*X has; multiple uses. Here's a simple example:. define i32 @test15(i32 %X1) {; %B = mul i32 %X1, 47 ; X1*47; %C = mul i32 %B, %B; ret i32 %C; }. //===---------------------------------------------------------------------===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ; b3 = a3 + a2 + a1 + a0; ; b2 = a2 + a1 + a0; ; b1 = a1 + a0; ; } . This requires reassociating to forms of expressions that are already available,; something that reassoc doesn't think about yet. //===---------------------------------------------------------------------===//. These two functions should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===---------------------------------------------------------------------===//. It would be nice to revert this patch:; http://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20060213/031986.html. And teach the dag combiner enough to simplify the code expanded before ; legalize. It seems plausible that this knowledge would let it simplify other; stuff too. /",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:3986,Performance,load,load,3986,"ns should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===---------------------------------------------------------------------===//. It would be nice to revert this patch:; http://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20060213/031986.html. And teach the dag combiner enough to simplify the code expanded before ; legalize. It seems plausible that this knowledge would let it simplify other; stuff too. //===---------------------------------------------------------------------===//. For vector types, DataLayout.cpp::getTypeInfo() returns alignment that is equal; to the type size. It works but can be overly conservative as the alignment of; specific vector types are target dependent. //===---------------------------------------------------------------------===//. We should produce an unaligned load from code like this:. v4sf example(float *P) {; return (v4sf){P[0], P[1], P[2], P[3] };; }. //===---------------------------------------------------------------------===//. Add support for conditional increments, and other related patterns. Instead; of:. 	movl 136(%esp), %eax; 	cmpl $0, %eax; 	je LBB16_2	#cond_next; LBB16_1:	#cond_true; 	incl _foo; LBB16_2:	#cond_next. emit:; 	movl	_foo, %eax; 	cmpl	$1, %edi; 	sbbl	$-1, %eax; 	movl	%eax, _foo. //===---------------------------------------------------------------------===//. Combine: a = sin(x), b = cos(x) into a,b = sincos(x). Expand these to calls of sin/cos and stores:; double sincos(double x, double *sin, double *cos);; float sincosf(float x, float *sin, float *cos);; long double sincosl(long double x, long double *sin, long double *cos);. Doing so could allow SROA of the destination pointers. See also:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=17687. This is now easily doable with MRVs. We could even make an intrinsic for",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:7332,Performance,load,loads,7332,"realize that it is finite (if it were infinite, it would be undefined). Not; having this blocks Loop Idiom from matching strlen and friends. . void foo(char *C) {; int x = 0;; while (*C); ++x,++C;; }. //===---------------------------------------------------------------------===//. [LOOP RECOGNITION]. These idioms should be recognized as popcount (see PR1488):. unsigned countbits_slow(unsigned v) {; unsigned c;; for (c = 0; v; v >>= 1); c += v & 1;; return c;; }. unsigned int popcount(unsigned int input) {; unsigned int count = 0;; for (unsigned int i = 0; i < 4 * 8; i++); count += (input >> i) & i;; return count;; }. This should be recognized as CLZ: https://github.com/llvm/llvm-project/issues/64167. unsigned clz_a(unsigned a) {; int i;; for (i=0;i<32;i++); if (a & (1<<(31-i))); return i;; return 32;; }. This sort of thing should be added to the loop idiom pass. //===---------------------------------------------------------------------===//. These should turn into single 16-bit (unaligned?) loads on little/big endian; processors. unsigned short read_16_le(const unsigned char *adr) {; return adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting o",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:8250,Performance,OPTIMIZ,OPTIMIZATION,8250,"----------------------===//. These should turn into single 16-bit (unaligned?) loads on little/big endian; processors. unsigned short read_16_le(const unsigned char *adr) {; return adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64 effective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) eff",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:8325,Performance,optimiz,optimization,8325,"eturn adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64 effective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; b2 = (temp >> 12) & 0xf;; if (b2) effective_addr2 += regs[b2];; effective_addr2 &= regs[4];; if ((effective_addr2 & 3) == 0); return 1;; return 0;; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:10162,Performance,optimiz,optimized,10162,"fective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; b2 = (temp >> 12) & 0xf;; if (b2) effective_addr2 += regs[b2];; effective_addr2 &= regs[4];; if ((effective_addr2 & 3) == 0); return 1;; return 0;; }. Note that only the low 2 bits of effective_addr2 are used. On 32-bit systems,; we don't eliminate the computation of the top half of effective_addr2 because; we don't have whole-function selection dags. On x86, this means we use one; extra register for the function when effective_addr2 is declared as U64 than; when it is declared U32. PHI Slicing could be extended to do this. //===---------------------------------------------------------------------===//. Tail call elim should be more aggressive, checking to see if the call is; followed by an uncond branch to an exit block. ; This testcase is due to tail-duplication not wanting to copy the return; ; instruction into the terminating blocks because there was other code; ; optimized out of the function after the taildup happened.; ; RUN: llvm-as < %s | opt -tailcallelim | llvm-dis | not grep call. define i32 @t4(i32 %a) {; entry:; 	%tmp.1 = and i32 %a, 1		; <i32> [#uses=1]; 	%tmp.2 = icmp ne i32 %tmp.1, 0		; <i1> [#uses=1]; 	br i1 %tmp.2, label %then.0, label %else.0. then.0:		; preds = %entry; 	%tmp.5 = add i32 %a, -1		; <i32> [#uses=1]; 	%tmp.3 = call i32 @t4( i32 %tmp.5 )		; <i32> [#uses=1]; 	br label %return. else.0:		; preds = %entry; 	%tmp.7 = icmp ne i32 %a, 0		; <i1> [#uses=1]; 	br i1 %tmp.7, label %then.1, label %return. then.1:		; preds = %else.0; 	%tmp.11 = add i32 %a, -2		; <i32> [#uses=1]; 	%tmp.9 = call i32 @t4( i32 %tmp.11 )		; <i32> [#uses=1]; 	br label %return. return:		; preds = %then.1, %else.0, %then.0; 	%result.0 = phi i32 [ 0, %else.0 ], [ %tmp.3, %then.0 ],; [ %tmp.9, %then.1 ]; 	ret i32 %result.0; }. //===---------------------------------------------------------------------===//. Tail ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:11669,Performance,load,load,11669,"a, 0		; <i1> [#uses=1]; 	br i1 %tmp.7, label %then.1, label %return. then.1:		; preds = %else.0; 	%tmp.11 = add i32 %a, -2		; <i32> [#uses=1]; 	%tmp.9 = call i32 @t4( i32 %tmp.11 )		; <i32> [#uses=1]; 	br label %return. return:		; preds = %then.1, %else.0, %then.0; 	%result.0 = phi i32 [ 0, %else.0 ], [ %tmp.3, %then.0 ],; [ %tmp.9, %then.1 ]; 	ret i32 %result.0; }. //===---------------------------------------------------------------------===//. Tail recursion elimination should handle:. int pow2m1(int n) {; if (n == 0); return 0;; return 2 * pow2m1 (n - 1) + 1;; }. Also, multiplies can be turned into SHL's, so they should be handled as if; they were associative. ""return foo() << 1"" can be tail recursion eliminated. //===---------------------------------------------------------------------===//. Argument promotion should promote arguments for recursive functions, like ; this:. ; RUN: llvm-as < %s | opt -argpromotion | llvm-dis | grep x.val. define internal i32 @foo(i32* %x) {; entry:; 	%tmp = load i32* %x		; <i32> [#uses=0]; 	%tmp.foo = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp.foo; }. define i32 @bar(i32* %x) {; entry:; 	%tmp3 = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp3; }. //===---------------------------------------------------------------------===//. We should investigate an instruction sinking pass. Consider this silly; example in pic mode:. #include <assert.h>; void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is l",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:12661,Performance,load,loads,12661,"es=0]; 	%tmp.foo = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp.foo; }. define i32 @bar(i32* %x) {; entry:; 	%tmp3 = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp3; }. //===---------------------------------------------------------------------===//. We should investigate an instruction sinking pass. Consider this silly; example in pic mode:. #include <assert.h>; void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:13212,Performance,load,load,13212,"2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing this with integers allows the use of read/modify/write; instructions. //===---------------------------------------------------------------------===//. DAG Combiner should try to combine small loads into larger loads when ; profitable. For example, we compile this C++ example:. struct THotKey { short Key; bool Control; bool Shift; bool Alt; };; extern THotKey m_HotKey;; THotKey GetHotKey () { return m_HotKey; }. into (-m64 -O3 -fno-exceptions -static -fomit-frame-pointer):. __Z9GetHotKeyv: ## @_Z9GetHotKeyv; 	movq	_m_HotKey@GOTPCREL(%rip)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:13234,Performance,load,load,13234,"2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing this with integers allows the use of read/modify/write; instructions. //===---------------------------------------------------------------------===//. DAG Combiner should try to combine small loads into larger loads when ; profitable. For example, we compile this C++ example:. struct THotKey { short Key; bool Control; bool Shift; bool Alt; };; extern THotKey m_HotKey;; THotKey GetHotKey () { return m_HotKey; }. into (-m64 -O3 -fno-exceptions -static -fomit-frame-pointer):. __Z9GetHotKeyv: ## @_Z9GetHotKeyv; 	movq	_m_HotKey@GOTPCREL(%rip)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:13888,Performance,load,loads,13888,"ndle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing this with integers allows the use of read/modify/write; instructions. //===---------------------------------------------------------------------===//. DAG Combiner should try to combine small loads into larger loads when ; profitable. For example, we compile this C++ example:. struct THotKey { short Key; bool Control; bool Shift; bool Alt; };; extern THotKey m_HotKey;; THotKey GetHotKey () { return m_HotKey; }. into (-m64 -O3 -fno-exceptions -static -fomit-frame-pointer):. __Z9GetHotKeyv: ## @_Z9GetHotKeyv; 	movq	_m_HotKey@GOTPCREL(%rip), %rax; 	movzwl	(%rax), %ecx; 	movzbl	2(%rax), %edx; 	shlq	$16, %rdx; 	orq	%rcx, %rdx; 	movzbl	3(%rax), %ecx; 	shlq	$24, %rcx; 	orq	%rdx, %rcx; 	movzbl	4(%rax), %eax; 	shlq	$32, %rax; 	orq	%rcx, %rax; 	ret. //===---------------------------------------------------------------------===//. We should add an FRINT node to the DAG to model targets that have legal; implementations of ceil/floor/rint. //===---------------------------------------------------------------------===//. Consider:. int test() {; long long input[8] = {1,0,1,0,1,0,1,0};; foo(input);; }. Clang compiles this into:. call void @llvm.memset.p0i8.i64(i8* %tmp, i8 0, i64 64, i32 16",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:13906,Performance,load,loads,13906,"ndle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing this with integers allows the use of read/modify/write; instructions. //===---------------------------------------------------------------------===//. DAG Combiner should try to combine small loads into larger loads when ; profitable. For example, we compile this C++ example:. struct THotKey { short Key; bool Control; bool Shift; bool Alt; };; extern THotKey m_HotKey;; THotKey GetHotKey () { return m_HotKey; }. into (-m64 -O3 -fno-exceptions -static -fomit-frame-pointer):. __Z9GetHotKeyv: ## @_Z9GetHotKeyv; 	movq	_m_HotKey@GOTPCREL(%rip), %rax; 	movzwl	(%rax), %ecx; 	movzbl	2(%rax), %edx; 	shlq	$16, %rdx; 	orq	%rcx, %rdx; 	movzbl	3(%rax), %ecx; 	shlq	$24, %rcx; 	orq	%rdx, %rcx; 	movzbl	4(%rax), %eax; 	shlq	$32, %rax; 	orq	%rcx, %rax; 	ret. //===---------------------------------------------------------------------===//. We should add an FRINT node to the DAG to model targets that have legal; implementations of ceil/floor/rint. //===---------------------------------------------------------------------===//. Consider:. int test() {; long long input[8] = {1,0,1,0,1,0,1,0};; foo(input);; }. Clang compiles this into:. call void @llvm.memset.p0i8.i64(i8* %tmp, i8 0, i64 64, i32 16",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:20837,Performance,perform,perform,20837,"6U); true();; }. The same transformation can work with an even modulo with the addition of a; rotate: rotate the result of the multiply to the right by the number of bits; which need to be zero for the condition to be true, and shrink the compare RHS; by the same amount. Unless the target supports rotates, though, that; transformation probably isn't worthwhile. The transformation can also easily be made to work with non-zero equality; comparisons: just transform, for example, ""n % 3 == 1"" to ""(n-1) % 3 == 0"". //===---------------------------------------------------------------------===//. Better mod/ref analysis for scanf would allow us to eliminate the vtable and a; bunch of other stuff from this example (see PR1604): . #include <cstdio>; struct test {; int val;; virtual ~test() {}; };. int main() {; test t;; std::scanf(""%d"", &t.val);; std::printf(""%d\n"", t.val);; }. //===---------------------------------------------------------------------===//. These functions perform the same computation, but produce different assembly. define i8 @select(i8 %x) readnone nounwind {; %A = icmp ult i8 %x, 250; %B = select i1 %A, i8 0, i8 1; ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:21581,Performance,optimiz,optimized,21581,"e <cstdio>; struct test {; int val;; virtual ~test() {}; };. int main() {; test t;; std::scanf(""%d"", &t.val);; std::printf(""%d\n"", t.val);; }. //===---------------------------------------------------------------------===//. These functions perform the same computation, but produce different assembly. define i8 @select(i8 %x) readnone nounwind {; %A = icmp ult i8 %x, 250; %B = select i1 %A, i8 0, i8 1; ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK) && !(end & ~PMD_MASK)); f();; }; The expression should optimize to something like; ""!((start|end)&~PMD_MASK). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned int f(unsigned int i, unsigned int n) {++i; if (i == n) ++i; return; i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:21916,Performance,optimiz,optimize,21916,"form the same computation, but produce different assembly. define i8 @select(i8 %x) readnone nounwind {; %A = icmp ult i8 %x, 250; %B = select i1 %A, i8 0, i8 1; ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK) && !(end & ~PMD_MASK)); f();; }; The expression should optimize to something like; ""!((start|end)&~PMD_MASK). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned int f(unsigned int i, unsigned int n) {++i; if (i == n) ++i; return; i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; i",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:21985,Performance,optimiz,optimized,21985," ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK) && !(end & ~PMD_MASK)); f();; }; The expression should optimize to something like; ""!((start|end)&~PMD_MASK). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned int f(unsigned int i, unsigned int n) {++i; if (i == n) ++i; return; i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:22566,Performance,optimiz,optimize,22566,"th should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK) && !(end & ~PMD_MASK)); f();; }; The expression should optimize to something like; ""!((start|end)&~PMD_MASK). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned int f(unsigned int i, unsigned int n) {++i; if (i == n) ++i; return; i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:22623,Performance,optimiz,optimized,22623,"------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK) && !(end & ~PMD_MASK)); f();; }; The expression should optimize to something like; ""!((start|end)&~PMD_MASK). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned int f(unsigned int i, unsigned int n) {++i; if (i == n) ++i; return; i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------==",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:23248,Performance,optimiz,optimized,23248,"= i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimize",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:23482,Performance,optimiz,optimized,23482,"x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===----",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:23548,Performance,optimiz,optimize,23548,"x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===----",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:23760,Performance,optimiz,optimized,23760,"d; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:24006,Performance,optimiz,optimized,24006,"o a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===--------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:24248,Performance,optimiz,optimized,24248,"clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===--------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:24456,Performance,optimiz,optimized,24456,"ently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===-----------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:24669,Performance,optimiz,optimized,24669,"<< 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===--------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:24926,Performance,optimiz,optimized,24926,"& 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsig",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:25167,Performance,optimiz,optimized,25167,"(c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. un",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:25378,Performance,optimiz,optimized,25378,"b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===----------------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:25594,Performance,optimiz,optimized,25594,"urn (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===-------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:25807,Performance,optimiz,optimized,25807,"c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:26052,Performance,optimiz,optimized,26052,"int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:26527,Performance,optimiz,optimized,26527,"^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimize",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:26756,Performance,optimiz,optimized,26756,"ld combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %de",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:26986,Performance,optimiz,optimized,26986,"d combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:27266,Performance,optimiz,optimized,27266," code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#us",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:27474,Performance,optimiz,optimized,27474,">> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:27535,Performance,optimiz,optimized,27535,"emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) {; int i;; *res = 0;; for (i = 0; i < n; i++); if (*cond); *res ^= 234; /* (*) */; }. On this ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:28808,Performance,load,load,28808," = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) {; int i;; *res = 0;; for (i = 0; i < n; i++); if (*cond); *res ^= 234; /* (*) */; }. On this function GVN hoists the fully redundant value of *res, but nothing; moves the store out. This gives us this code:. bb:		; preds = %bb2, %entry; 	%.rle = phi i32 [ 0, %entry ], [ %.rle6, %bb2 ]	; 	%i.05 = phi i32 [ 0, %entry ], [ %indvar.next, %bb2 ]; 	%1 = load i32* %cond, align 4; 	%2 = icmp eq i32 %1, 0; 	br i1 %2, label %bb2, label %bb1. bb1:		; preds = %bb; 	%3 = xor i32 %.rle, 234	; 	store i32 %3, i32* %res, align 4; 	br label %bb2. bb2:		; preds = %bb, %bb1; 	%.rle6 = phi i32 [ %3, %bb1 ], [ %.rle, %bb ]	; 	%indvar.next = add i32 %i.05, 1	; 	%exitcond = icmp eq i32 %indvar.next, %n; 	br i1 %exitcond, label %return, label %bb. DSE should sink partially dead stores to get the store out of the loop. Here's another partial dead case:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=12395. //===---------------------------------------------------------------------===//. Scalar PRE hoists the mul in the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * c + g;; return d + e;; }. It would be better to do the mul once to reduce codesize above the if.; This is GCC PR38204. //===---------------------------------------------------------------------===//; This simple function from 179.art:. int winner, nu",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:30267,Performance,load,load,30267,"partial dead case:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=12395. //===---------------------------------------------------------------------===//. Scalar PRE hoists the mul in the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * c + g;; return d + e;; }. It would be better to do the mul once to reduce codesize above the if.; This is GCC PR38204. //===---------------------------------------------------------------------===//; This simple function from 179.art:. int winner, numf2s;; struct { double y; int reset; } *Y;. void find_match() {; int i;; winner = 0;; for (i=0;i<numf2s;i++); if (Y[i].y > Y[winner].y); winner =i;; }. Compiles into (with clang TBAA):. for.body: ; preds = %for.inc, %bb.nph; %indvar = phi i64 [ 0, %bb.nph ], [ %indvar.next, %for.inc ]; %i.01718 = phi i32 [ 0, %bb.nph ], [ %i.01719, %for.inc ]; %tmp4 = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:30428,Performance,load,load,30428," the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * c + g;; return d + e;; }. It would be better to do the mul once to reduce codesize above the if.; This is GCC PR38204. //===---------------------------------------------------------------------===//; This simple function from 179.art:. int winner, numf2s;; struct { double y; int reset; } *Y;. void find_match() {; int i;; winner = 0;; for (i=0;i<numf2s;i++); if (Y[i].y > Y[winner].y); winner =i;; }. Compiles into (with clang TBAA):. for.body: ; preds = %for.inc, %bb.nph; %indvar = phi i64 [ 0, %bb.nph ], [ %indvar.next, %for.inc ]; %i.01718 = phi i32 [ 0, %bb.nph ], [ %i.01719, %for.inc ]; %tmp4 = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31194,Performance,load,load,31194," = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-c",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31795,Performance,load,load,31795,"%indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbol",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:32092,Performance,load,load,32092,"pletely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:32575,Performance,LOAD,LOAD,32575,"movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:32630,Performance,load,loads,32630,"q	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRAN",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33189,Performance,load,load,33189,"on-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33405,Performance,LOAD,LOAD,33405,"-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33431,Performance,load,load,33431,"nk the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; stor",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33479,Performance,load,loadpre,33479,"---------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33832,Performance,LOAD,LOAD,33832," = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:34574,Performance,load,loaded,34574,"checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANAL",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:35284,Performance,load,loads,35284,"itable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OV",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:35396,Performance,load,loads,35396,"low the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS. //===---------------------------------------------------------------------===//. http://gcc.gnu.org",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:35440,Performance,load,load,35440,"low the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS. //===---------------------------------------------------------------------===//. http://gcc.gnu.org",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:36036,Performance,load,loads,36036,"dcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS. //===---------------------------------------------------------------------===//. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19633; We could eliminate the branch condition here, loading from null is undefined:. struct S { int w, x, y, z; };; struct T { int r; struct S s; };; void bar (struct S, int);; void foo (int a, struct T b); {; struct S *c = 0;; if (a); c = &b.s;; bar (*c, a);; }. //===---------------------------------------------------------------------===//. simplifylibcalls should do several optimizations for strspn/strcspn:. strcspn(x, ""a"") -> inlined loop for up to 3 letters (similarly for strspn):. size_t __strcspn_c3 (__const char *__s, int __reject1, int __reject2,; int __reject3",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:36486,Performance,load,loading,36486,"------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS. //===---------------------------------------------------------------------===//. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19633; We could eliminate the branch condition here, loading from null is undefined:. struct S { int w, x, y, z; };; struct T { int r; struct S s; };; void bar (struct S, int);; void foo (int a, struct T b); {; struct S *c = 0;; if (a); c = &b.s;; bar (*c, a);; }. //===---------------------------------------------------------------------===//. simplifylibcalls should do several optimizations for strspn/strcspn:. strcspn(x, ""a"") -> inlined loop for up to 3 letters (similarly for strspn):. size_t __strcspn_c3 (__const char *__s, int __reject1, int __reject2,; int __reject3) {; register size_t __result = 0;; while (__s[__result] != '\0' && __s[__result] != __reject1 &&; __s[__result] != __reject2 && __s[__result] != __reject3); ++__result;; return __result;; }. This should turn into a switch on the character. See PR3253 for some notes on; codegen. 456.hmmer apparently uses strcspn and strspn a lot. 471.omnetpp uses strspn. //===---------------------------------------------------------------------===//. simplifylibcalls should ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:36814,Performance,optimiz,optimizations,36814,"alloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS. //===---------------------------------------------------------------------===//. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19633; We could eliminate the branch condition here, loading from null is undefined:. struct S { int w, x, y, z; };; struct T { int r; struct S s; };; void bar (struct S, int);; void foo (int a, struct T b); {; struct S *c = 0;; if (a); c = &b.s;; bar (*c, a);; }. //===---------------------------------------------------------------------===//. simplifylibcalls should do several optimizations for strspn/strcspn:. strcspn(x, ""a"") -> inlined loop for up to 3 letters (similarly for strspn):. size_t __strcspn_c3 (__const char *__s, int __reject1, int __reject2,; int __reject3) {; register size_t __result = 0;; while (__s[__result] != '\0' && __s[__result] != __reject1 &&; __s[__result] != __reject2 && __s[__result] != __reject3); ++__result;; return __result;; }. This should turn into a switch on the character. See PR3253 for some notes on; codegen. 456.hmmer apparently uses strcspn and strspn a lot. 471.omnetpp uses strspn. //===---------------------------------------------------------------------===//. simplifylibcalls should turn these snprintf idioms into memcpy (GCC PR47917). char buf1[6], buf2[6], buf3[4], buf4[4];; int i;. int foo (void) {; int ret = snprintf (buf1, sizeof buf1, ""abcde"");; ret += snprintf (buf2, sizeof buf2, ""abcdef"") * 16;; ret += snprintf (buf3, sizeof buf3, ""%s"", i++ < 6 ? ""abc"" : ""def"") * 256;; ret += snprintf (buf4, sizeof buf4, ""%s"",",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:39496,Performance,load,load,39496,"memcpy.i32(i8* %endptr, ; i8* getelementptr ([5 x i8]* @""\01LC42"", i32 0, i32 0), i32 5, i32 1); %3074 = call i32 @strlen(i8* %endptr) nounwind readonly ; ; This is interesting for a couple reasons. First, in this:. The memcpy+strlen strlen can be replaced with:. %3074 = call i32 @strlen([5 x i8]* @""\01LC42"") nounwind readonly . Because the destination was just copied into the specified memory buffer. This,; in turn, can be constant folded to ""4"". In other code, it contains:. %endptr6978 = bitcast i8* %endptr69 to i32* ; store i32 7107374, i32* %endptr6978, align 1; %3167 = call i32 @strlen(i8* %endptr69) nounwind readonly . Which could also be constant folded. Whatever is producing this should probably; be fixed to leave this as a memcpy from a string. Further, eon also has an interesting partially redundant strlen call:. bb8: ; preds = %_ZN18eonImageCalculatorC1Ev.exit; %682 = getelementptr i8** %argv, i32 6 ; <i8**> [#uses=2]; %683 = load i8** %682, align 4 ; <i8*> [#uses=4]; %684 = load i8* %683, align 1 ; <i8> [#uses=1]; %685 = icmp eq i8 %684, 0 ; <i1> [#uses=1]; br i1 %685, label %bb10, label %bb9. bb9: ; preds = %bb8; %686 = call i32 @strlen(i8* %683) nounwind readonly ; %687 = icmp ugt i32 %686, 254 ; <i1> [#uses=1]; br i1 %687, label %bb10, label %bb11. bb10: ; preds = %bb9, %bb8; %688 = call i32 @strlen(i8* %683) nounwind readonly . This could be eliminated by doing the strlen once in bb8, saving code size and; improving perf on the bb8->9->10 path. //===---------------------------------------------------------------------===//. I see an interesting fully redundant call to strlen left in 186.crafty:InputMove; which looks like:; %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 ; . bb62: ; preds = %bb55, %bb53; %promote.0 = phi i32 [ %169, %bb55 ], [ 0, %bb53 ] ; %171 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1; %172 = add i32 %171, -1 ; <i32> [#uses=1]; %173 = getelementptr [128 x i8]* %movetext, i32 0, i32 %172 . ... no sto",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:39546,Performance,load,load,39546,"memcpy.i32(i8* %endptr, ; i8* getelementptr ([5 x i8]* @""\01LC42"", i32 0, i32 0), i32 5, i32 1); %3074 = call i32 @strlen(i8* %endptr) nounwind readonly ; ; This is interesting for a couple reasons. First, in this:. The memcpy+strlen strlen can be replaced with:. %3074 = call i32 @strlen([5 x i8]* @""\01LC42"") nounwind readonly . Because the destination was just copied into the specified memory buffer. This,; in turn, can be constant folded to ""4"". In other code, it contains:. %endptr6978 = bitcast i8* %endptr69 to i32* ; store i32 7107374, i32* %endptr6978, align 1; %3167 = call i32 @strlen(i8* %endptr69) nounwind readonly . Which could also be constant folded. Whatever is producing this should probably; be fixed to leave this as a memcpy from a string. Further, eon also has an interesting partially redundant strlen call:. bb8: ; preds = %_ZN18eonImageCalculatorC1Ev.exit; %682 = getelementptr i8** %argv, i32 6 ; <i8**> [#uses=2]; %683 = load i8** %682, align 4 ; <i8*> [#uses=4]; %684 = load i8* %683, align 1 ; <i8> [#uses=1]; %685 = icmp eq i8 %684, 0 ; <i1> [#uses=1]; br i1 %685, label %bb10, label %bb9. bb9: ; preds = %bb8; %686 = call i32 @strlen(i8* %683) nounwind readonly ; %687 = icmp ugt i32 %686, 254 ; <i1> [#uses=1]; br i1 %687, label %bb10, label %bb11. bb10: ; preds = %bb9, %bb8; %688 = call i32 @strlen(i8* %683) nounwind readonly . This could be eliminated by doing the strlen once in bb8, saving code size and; improving perf on the bb8->9->10 path. //===---------------------------------------------------------------------===//. I see an interesting fully redundant call to strlen left in 186.crafty:InputMove; which looks like:; %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 ; . bb62: ; preds = %bb55, %bb53; %promote.0 = phi i32 [ %169, %bb55 ], [ 0, %bb53 ] ; %171 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1; %172 = add i32 %171, -1 ; <i32> [#uses=1]; %173 = getelementptr [128 x i8]* %movetext, i32 0, i32 %172 . ... no sto",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:42395,Performance,load,load,42395,"----------===//. 186.crafty has this interesting pattern with the ""out.4543"" variable:. call void @llvm.memcpy.i32(; i8* getelementptr ([10 x i8]* @out.4543, i32 0, i32 0),; i8* getelementptr ([7 x i8]* @""\01LC28700"", i32 0, i32 0), i32 7, i32 1) ; %101 = call@printf(i8* ... @out.4543, i32 0, i32 0)) nounwind . It is basically doing:. memcpy(globalarray, ""string"");; printf(..., globalarray);; ; Anyway, by knowing that printf just reads the memory and forward substituting; the string directly into the printf, this eliminates reads from globalarray.; Since this pattern occurs frequently in crafty (due to the ""DisplayTime"" and; other similar functions) there are many stores to ""out"". Once all the printfs; stop using ""out"", all that is left is the memcpy's into it. This should allow; globalopt to remove the ""stored only"" global. //===---------------------------------------------------------------------===//. This code:. define inreg i32 @foo(i8* inreg %p) nounwind {; %tmp0 = load i8* %p; %tmp1 = ashr i8 %tmp0, 5; %tmp2 = sext i8 %tmp1 to i32; ret i32 %tmp2; }. could be dagcombine'd to a sign-extending load with a shift.; For example, on x86 this currently gets this:. 	movb	(%eax), %al; 	sarb	$5, %al; 	movsbl	%al, %eax. while it could get this:. 	movsbl	(%eax), %eax; 	sarl	$5, %eax. //===---------------------------------------------------------------------===//. GCC PR31029:. int test(int x) { return 1-x == x; } // --> return false; int test2(int x) { return 2-x == x; } // --> return x == 1 ?. Always foldable for odd constants, what is the rule for even?. //===---------------------------------------------------------------------===//. PR 3381: GEP to field of size 0 inside a struct could be turned into GEP; for next field in struct (which is at same address). For example: store of float into { {{}}, float } could be turned into a store to; the float directly. //===---------------------------------------------------------------------===//. The arg promotion pass should mak",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:42524,Performance,load,load,42524,"m.memcpy.i32(; i8* getelementptr ([10 x i8]* @out.4543, i32 0, i32 0),; i8* getelementptr ([7 x i8]* @""\01LC28700"", i32 0, i32 0), i32 7, i32 1) ; %101 = call@printf(i8* ... @out.4543, i32 0, i32 0)) nounwind . It is basically doing:. memcpy(globalarray, ""string"");; printf(..., globalarray);; ; Anyway, by knowing that printf just reads the memory and forward substituting; the string directly into the printf, this eliminates reads from globalarray.; Since this pattern occurs frequently in crafty (due to the ""DisplayTime"" and; other similar functions) there are many stores to ""out"". Once all the printfs; stop using ""out"", all that is left is the memcpy's into it. This should allow; globalopt to remove the ""stored only"" global. //===---------------------------------------------------------------------===//. This code:. define inreg i32 @foo(i8* inreg %p) nounwind {; %tmp0 = load i8* %p; %tmp1 = ashr i8 %tmp0, 5; %tmp2 = sext i8 %tmp1 to i32; ret i32 %tmp2; }. could be dagcombine'd to a sign-extending load with a shift.; For example, on x86 this currently gets this:. 	movb	(%eax), %al; 	sarb	$5, %al; 	movsbl	%al, %eax. while it could get this:. 	movsbl	(%eax), %eax; 	sarl	$5, %eax. //===---------------------------------------------------------------------===//. GCC PR31029:. int test(int x) { return 1-x == x; } // --> return false; int test2(int x) { return 2-x == x; } // --> return x == 1 ?. Always foldable for odd constants, what is the rule for even?. //===---------------------------------------------------------------------===//. PR 3381: GEP to field of size 0 inside a struct could be turned into GEP; for next field in struct (which is at same address). For example: store of float into { {{}}, float } could be turned into a store to; the float directly. //===---------------------------------------------------------------------===//. The arg promotion pass should make use of nocapture to make its alias analysis; stuff much more precise. //===-------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:43597,Performance,optimiz,optimized,43597,"rb	$5, %al; 	movsbl	%al, %eax. while it could get this:. 	movsbl	(%eax), %eax; 	sarl	$5, %eax. //===---------------------------------------------------------------------===//. GCC PR31029:. int test(int x) { return 1-x == x; } // --> return false; int test2(int x) { return 2-x == x; } // --> return x == 1 ?. Always foldable for odd constants, what is the rule for even?. //===---------------------------------------------------------------------===//. PR 3381: GEP to field of size 0 inside a struct could be turned into GEP; for next field in struct (which is at same address). For example: store of float into { {{}}, float } could be turned into a store to; the float directly. //===---------------------------------------------------------------------===//. The arg promotion pass should make use of nocapture to make its alias analysis; stuff much more precise. //===---------------------------------------------------------------------===//. The following functions should be optimized to use a select instead of a; branch (from gcc PR40072):. char char_int(int m) {if(m>7) return 0; return m;}; int int_char(char m) {if(m>7) return 0; return m;}. //===---------------------------------------------------------------------===//. int func(int a, int b) { if (a & 0x80) b |= 0x80; else b &= ~0x80; return b; }. Generates this:. define i32 @func(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = and i32 %a, 128 ; <i32> [#uses=1]; %1 = icmp eq i32 %0, 0 ; <i1> [#uses=1]; %2 = or i32 %b, 128 ; <i32> [#uses=1]; %3 = and i32 %b, -129 ; <i32> [#uses=1]; %b_addr.0 = select i1 %1, i32 %3, i32 %2 ; <i32> [#uses=1]; ret i32 %b_addr.0; }. However, it's functionally equivalent to:. b = (b & ~0x80) | (a & 0x80);. Which generates this:. define i32 @func(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = and i32 %b, -129 ; <i32> [#uses=1]; %1 = and i32 %a, 128 ; <i32> [#uses=1]; %2 = or i32 %0, %1 ; <i32> [#uses=1]; ret i32 %2; }. This can be generalized for other forms:. b = (b & ~0x80) | ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:47678,Performance,optimiz,optimized,47678," SimplifyLibcalls into; libanalysis' constantfolding logic. This would allow IPSCCP to be able to; handle simple things like this:. static int foo(const char *X) { return strlen(X); }; int bar() { return foo(""abcd""); }. //===---------------------------------------------------------------------===//. function-attrs doesn't know much about memcpy/memset. This function should be; marked readnone rather than readonly, since it only twiddles local memory, but; function-attrs doesn't handle memset/memcpy/memmove aggressively:. struct X { int *p; int *q; };; int foo() {; int i = 0, j = 1;; struct X x, y;; int **p;; y.p = &i;; x.q = &j;; p = __builtin_memcpy (&x, &y, sizeof (int *));; return **p;; }. This can be seen at:; $ clang t.c -S -o - -mkernel -O0 -emit-llvm | opt -function-attrs -S. //===---------------------------------------------------------------------===//. Missed instcombine transformation:; define i1 @a(i32 %x) nounwind readnone {; entry:; %cmp = icmp eq i32 %x, 30; %sub = add i32 %x, -30; %cmp2 = icmp ugt i32 %sub, 9; %or = or i1 %cmp, %cmp2; ret i1 %or; }; This should be optimized to a single compare. Testcase derived from gcc. //===---------------------------------------------------------------------===//. Missed instcombine or reassociate transformation:; int a(int a, int b) { return (a==12)&(b>47)&(b<58); }. The sgt and slt should be combined into a single comparison. Testcase derived; from gcc. //===---------------------------------------------------------------------===//. Missed instcombine transformation:. %382 = srem i32 %tmp14.i, 64 ; [#uses=1]; %383 = zext i32 %382 to i64 ; [#uses=1]; %384 = shl i64 %381, %383 ; [#uses=1]; %385 = icmp slt i32 %tmp14.i, 64 ; [#uses=1]. The srem can be transformed to an and because if %tmp14.i is negative, the; shift is undefined. Testcase derived from 403.gcc. //===---------------------------------------------------------------------===//. This is a range comparison on a divided result (from 403.gcc):. %1337 = sdiv ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:51360,Performance,optimiz,optimizer,51360,"ses=1]; br i1 %cond1, label %exit, label %bb.nph; bb.nph: ; preds = %entry; %tmp = mul i32 %b, %a ; <i32> [#uses=1]; ret i32 %tmp; exit: ; preds = %entry; ret i32 0; }. could be reduced to:. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = mul i32 %b, %a; ret i32 %tmp; }. //===---------------------------------------------------------------------===//. We should use DSE + llvm.lifetime.end to delete dead vtable pointer updates.; See GCC PR34949. Another interesting case is that something related could be used for variables; that go const after their ctor has finished. In these cases, globalopt (which; can statically run the constructor) could mark the global const (so it gets put; in the readonly section). A testcase would be:. #include <complex>; using namespace std;; const complex<char> should_be_in_rodata (42,-42);; complex<char> should_be_in_data (42,-42);; complex<char> should_be_in_bss;. Where we currently evaluate the ctors but the globals don't become const because; the optimizer doesn't know they ""become const"" after the ctor is done. See; GCC PR4131 for more examples. //===---------------------------------------------------------------------===//. In this code:. long foo(long x) {; return x > 1 ? x : 1;; }. LLVM emits a comparison with 1 instead of 0. 0 would be equivalent; and cheaper on most targets. LLVM prefers comparisons with zero over non-zero in general, but in this; case it choses instead to keep the max operation obvious. //===---------------------------------------------------------------------===//. define void @a(i32 %x) nounwind {; entry:; switch i32 %x, label %if.end [; i32 0, label %if.then; i32 1, label %if.then; i32 2, label %if.then; i32 3, label %if.then; i32 5, label %if.then; ]; if.then:; tail call void @foo() nounwind; ret void; if.end:; ret void; }; declare void @foo(). Generated code on x86-64 (other platforms give similar results):; a:; 	cmpl	$5, %edi; 	ja	LBB2_2; 	cmpl	$4, %edi; 	jne	LBB2_3; .LBB0_2:; 	ret; .LBB0",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:54299,Performance,perform,performance,54299,"(int a, int b) {; int lt = a < b;; int eq = a == b;; if (lt); return 1;; return eq;; }. Is compiled to:. define i32 @test(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %cmp = icmp slt i32 %a, %b; br i1 %cmp, label %return, label %if.end. if.end: ; preds = %entry; %cmp5 = icmp eq i32 %a, %b; %conv6 = zext i1 %cmp5 to i32; ret i32 %conv6. return: ; preds = %entry; ret i32 1; }. it could be:. define i32 @test__(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = icmp sle i32 %a, %b; %retval = zext i1 %0 to i32; ret i32 %retval; }. //===---------------------------------------------------------------------===//. This code can be seen in viterbi:. %64 = call noalias i8* @malloc(i64 %62) nounwind; ...; %67 = call i64 @llvm.objectsize.i64(i8* %64, i1 false) nounwind; %68 = call i8* @__memset_chk(i8* %64, i32 0, i64 %62, i64 %67) nounwind. llvm.objectsize.i64 should be taught about malloc/calloc, allowing it to; fold to %62. This is a security win (overflows of malloc will get caught); and also a performance win by exposing more memsets to the optimizer. This occurs several times in viterbi. Note that this would change the semantics of @llvm.objectsize which by its; current definition always folds to a constant. We also should make sure that; we remove checking in code like. char *p = malloc(strlen(s)+1);; __strcpy_chk(p, s, __builtin_object_size(p, 0));. //===---------------------------------------------------------------------===//. clang -O3 currently compiles this code. int g(unsigned int a) {; unsigned int c[100];; c[10] = a;; c[11] = a;; unsigned int b = c[10] + c[11];; if(b > a*2) a = 4;; else a = 8;; return a + 7;; }. into. define i32 @g(i32 a) nounwind readnone {; %add = shl i32 %a, 1; %mul = shl i32 %a, 1; %cmp = icmp ugt i32 %add, %mul; %a.addr.0 = select i1 %cmp, i32 11, i32 15; ret i32 %a.addr.0; }. The icmp should fold to false. This CSE opportunity is only available; after GVN and InstCombine have run. //===------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:54347,Performance,optimiz,optimizer,54347,"(int a, int b) {; int lt = a < b;; int eq = a == b;; if (lt); return 1;; return eq;; }. Is compiled to:. define i32 @test(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %cmp = icmp slt i32 %a, %b; br i1 %cmp, label %return, label %if.end. if.end: ; preds = %entry; %cmp5 = icmp eq i32 %a, %b; %conv6 = zext i1 %cmp5 to i32; ret i32 %conv6. return: ; preds = %entry; ret i32 1; }. it could be:. define i32 @test__(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = icmp sle i32 %a, %b; %retval = zext i1 %0 to i32; ret i32 %retval; }. //===---------------------------------------------------------------------===//. This code can be seen in viterbi:. %64 = call noalias i8* @malloc(i64 %62) nounwind; ...; %67 = call i64 @llvm.objectsize.i64(i8* %64, i1 false) nounwind; %68 = call i8* @__memset_chk(i8* %64, i32 0, i64 %62, i64 %67) nounwind. llvm.objectsize.i64 should be taught about malloc/calloc, allowing it to; fold to %62. This is a security win (overflows of malloc will get caught); and also a performance win by exposing more memsets to the optimizer. This occurs several times in viterbi. Note that this would change the semantics of @llvm.objectsize which by its; current definition always folds to a constant. We also should make sure that; we remove checking in code like. char *p = malloc(strlen(s)+1);; __strcpy_chk(p, s, __builtin_object_size(p, 0));. //===---------------------------------------------------------------------===//. clang -O3 currently compiles this code. int g(unsigned int a) {; unsigned int c[100];; c[10] = a;; c[11] = a;; unsigned int b = c[10] + c[11];; if(b > a*2) a = 4;; else a = 8;; return a + 7;; }. into. define i32 @g(i32 a) nounwind readnone {; %add = shl i32 %a, 1; %mul = shl i32 %a, 1; %cmp = icmp ugt i32 %add, %mul; %a.addr.0 = select i1 %cmp, i32 11, i32 15; ret i32 %a.addr.0; }. The icmp should fold to false. This CSE opportunity is only available; after GVN and InstCombine have run. //===------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:55725,Performance,optimiz,optimize,55725,"----------------===//. clang -O3 currently compiles this code. int g(unsigned int a) {; unsigned int c[100];; c[10] = a;; c[11] = a;; unsigned int b = c[10] + c[11];; if(b > a*2) a = 4;; else a = 8;; return a + 7;; }. into. define i32 @g(i32 a) nounwind readnone {; %add = shl i32 %a, 1; %mul = shl i32 %a, 1; %cmp = icmp ugt i32 %add, %mul; %a.addr.0 = select i1 %cmp, i32 11, i32 15; ret i32 %a.addr.0; }. The icmp should fold to false. This CSE opportunity is only available; after GVN and InstCombine have run. //===---------------------------------------------------------------------===//. memcpyopt should turn this:. define i8* @test10(i32 %x) {; %alloc = call noalias i8* @malloc(i32 %x) nounwind; call void @llvm.memset.p0i8.i32(i8* %alloc, i8 0, i32 %x, i32 1, i1 false); ret i8* %alloc; }. into a call to calloc. We should make sure that we analyze calloc as; aggressively as malloc though. //===---------------------------------------------------------------------===//. clang -O3 doesn't optimize this:. void f1(int* begin, int* end) {; std::fill(begin, end, 0);; }. into a memset. This is PR8942. //===---------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. void f(int N) {; std::vector<int> v(N);. extern void sink(void*); sink(&v);; }. into. define void @_Z1fi(i32 %N) nounwind {; entry:; %v2 = alloca [3 x i32*], align 8; %v2.sub = getelementptr inbounds [3 x i32*]* %v2, i64 0, i64 0; %tmpcast = bitcast [3 x i32*]* %v2 to %""class.std::vector""*; %conv = sext i32 %N to i64; store i32* null, i32** %v2.sub, align 8, !tbaa !0; %tmp3.i.i.i.i.i = getelementptr inbounds [3 x i32*]* %v2, i64 0, i64 1; store i32* null, i32** %tmp3.i.i.i.i.i, align 8, !tbaa !0; %tmp4.i.i.i.i.i = getelementptr inbounds [3 x i32*]* %v2, i64 0, i64 2; store i32* null, i32** %tmp4.i.i.i.i.i, align 8, !tbaa !0; %cmp.i.i.i.i = icmp eq i32 %N, 0; br i1 %cmp.i.i.i.i, label %_ZNSt12_Vector_baseIiSaIiEEC2EmRKS0_.exit.thread.i.i, la",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:59366,Performance,cache,cache,59366,"-------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. void f(char* a, int n) {; __builtin_memset(a, 0, n);; for (int i = 0; i < n; ++i); a[i] = 0;; }. into:. define void @_Z1fPci(i8* nocapture %a, i32 %n) nounwind {; entry:; %conv = sext i32 %n to i64; tail call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %conv, i32 1, i1 false); %cmp8 = icmp sgt i32 %n, 0; br i1 %cmp8, label %for.body.lr.ph, label %for.end. for.body.lr.ph: ; preds = %entry; %tmp10 = add i32 %n, -1; %tmp11 = zext i32 %tmp10 to i64; %tmp12 = add i64 %tmp11, 1; call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %tmp12, i32 1, i1 false); ret void. for.end: ; preds = %entry; ret void; }. This shouldn't need the ((zext (%n - 1)) + 1) game, and it should ideally fold; the two memset's together. The issue with the addition only occurs in 64-bit mode, and appears to be at; least partially caused by Scalar Evolution not keeping its cache updated: it; returns the ""wrong"" result immediately after indvars runs, but figures out the; expected result if it is run from scratch on IR resulting from running indvars. //===---------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. struct S {; unsigned short m1, m2;; unsigned char m3, m4;; };. void f(int N) {; std::vector<S> v(N);; extern void sink(void*); sink(&v);; }. into poor code for zero-initializing 'v' when N is >0. The problem is that; S is only 6 bytes, but each element is 8 byte-aligned. We generate a loop and; 4 stores on each iteration. If the struct were 8 bytes, this gets turned into; a memset. In order to handle this we have to:; A) Teach clang to generate metadata for memsets of structs that have holes in; them.; B) Teach clang to use such a memset for zero init of this struct (since it has; a hole), instead of doing elementwise zeroing. //===---------------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:60598,Performance,load,load,60598,"-===//. clang -O3 -fno-exceptions currently compiles this code:. struct S {; unsigned short m1, m2;; unsigned char m3, m4;; };. void f(int N) {; std::vector<S> v(N);; extern void sink(void*); sink(&v);; }. into poor code for zero-initializing 'v' when N is >0. The problem is that; S is only 6 bytes, but each element is 8 byte-aligned. We generate a loop and; 4 stores on each iteration. If the struct were 8 bytes, this gets turned into; a memset. In order to handle this we have to:; A) Teach clang to generate metadata for memsets of structs that have holes in; them.; B) Teach clang to use such a memset for zero init of this struct (since it has; a hole), instead of doing elementwise zeroing. //===---------------------------------------------------------------------===//. clang -O3 currently compiles this code:. extern const int magic;; double f() { return 0.0 * magic; }. into. @magic = external constant i32. define double @_Z1fv() nounwind readnone {; entry:; %tmp = load i32* @magic, align 4, !tbaa !0; %conv = sitofp i32 %tmp to double; %mul = fmul double %conv, 0.000000e+00; ret double %mul; }. We should be able to fold away this fmul to 0.0. More generally, fmul(x,0.0); can be folded to 0.0 if we can prove that the LHS is not -0.0, not a NaN, and; not an INF. The CannotBeNegativeZero predicate in value tracking should be; extended to support general ""fpclassify"" operations that can return ; yes/no/unknown for each of these predicates. In this predicate, we know that uitofp is trivially never NaN or -0.0, and; we know that it isn't +/-Inf if the floating point type has enough exponent bits; to represent the largest integer value as < inf. //===---------------------------------------------------------------------===//. When optimizing a transformation that can change the sign of 0.0 (such as the; 0.0*val -> 0.0 transformation above), it might be provable that the sign of the; expression doesn't matter. For example, by the above rules, we can't transform; fmul(sitofp(",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:61371,Performance,optimiz,optimizing,61371,". clang -O3 currently compiles this code:. extern const int magic;; double f() { return 0.0 * magic; }. into. @magic = external constant i32. define double @_Z1fv() nounwind readnone {; entry:; %tmp = load i32* @magic, align 4, !tbaa !0; %conv = sitofp i32 %tmp to double; %mul = fmul double %conv, 0.000000e+00; ret double %mul; }. We should be able to fold away this fmul to 0.0. More generally, fmul(x,0.0); can be folded to 0.0 if we can prove that the LHS is not -0.0, not a NaN, and; not an INF. The CannotBeNegativeZero predicate in value tracking should be; extended to support general ""fpclassify"" operations that can return ; yes/no/unknown for each of these predicates. In this predicate, we know that uitofp is trivially never NaN or -0.0, and; we know that it isn't +/-Inf if the floating point type has enough exponent bits; to represent the largest integer value as < inf. //===---------------------------------------------------------------------===//. When optimizing a transformation that can change the sign of 0.0 (such as the; 0.0*val -> 0.0 transformation above), it might be provable that the sign of the; expression doesn't matter. For example, by the above rules, we can't transform; fmul(sitofp(x), 0.0) into 0.0, because x might be -1 and the result of the; expression is defined to be -0.0. If we look at the uses of the fmul for example, we might be able to prove that; all uses don't care about the sign of zero. For example, if we have:. fadd(fmul(sitofp(x), 0.0), 2.0). Since we know that x+2.0 doesn't care about the sign of any zeros in X, we can; transform the fmul to 0.0, and then the fadd to 2.0. //===---------------------------------------------------------------------===//. We should enhance memcpy/memcpy/memset to allow a metadata node on them; indicating that some bytes of the transfer are undefined. This is useful for; frontends like clang when lowering struct copies, when some elements of the; struct are undefined. Consider something like this:. str",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:62876,Performance,load,load,62876," 0.0), 2.0). Since we know that x+2.0 doesn't care about the sign of any zeros in X, we can; transform the fmul to 0.0, and then the fadd to 2.0. //===---------------------------------------------------------------------===//. We should enhance memcpy/memcpy/memset to allow a metadata node on them; indicating that some bytes of the transfer are undefined. This is useful for; frontends like clang when lowering struct copies, when some elements of the; struct are undefined. Consider something like this:. struct x {; char a;; int b[4];; };; void foo(struct x*P);; struct x testfunc() {; struct x V1, V2;; foo(&V1);; V2 = V1;. return V2;; }. We currently compile this to:; $ clang t.c -S -o - -O0 -emit-llvm | opt -sroa -S. %struct.x = type { i8, [4 x i32] }. define void @testfunc(%struct.x* sret %agg.result) nounwind ssp {; entry:; %V1 = alloca %struct.x, align 4; call void @foo(%struct.x* %V1); %tmp1 = bitcast %struct.x* %V1 to i8*; %0 = bitcast %struct.x* %V1 to i160*; %srcval1 = load i160* %0, align 4; %tmp2 = bitcast %struct.x* %agg.result to i8*; %1 = bitcast %struct.x* %agg.result to i160*; store i160 %srcval1, i160* %1, align 4; ret void; }. This happens because SRoA sees that the temp alloca has is being memcpy'd into; and out of and it has holes and it has to be conservative. If we knew about the; holes, then this could be much much better. Having information about these holes would also improve memcpy (etc) lowering at; llc time when it gets inlined, because we can use smaller transfers. This also; avoids partial register stalls in some important cases. //===---------------------------------------------------------------------===//. We don't fold (icmp (add) (add)) unless the two adds only have a single use.; There are a lot of cases that we're refusing to fold in (e.g.) 256.bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this becaus",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:64015,Performance,perform,performance,64015,"160 %srcval1, i160* %1, align 4; ret void; }. This happens because SRoA sees that the temp alloca has is being memcpy'd into; and out of and it has holes and it has to be conservative. If we knew about the; holes, then this could be much much better. Having information about these holes would also improve memcpy (etc) lowering at; llc time when it gets inlined, because we can use smaller transfers. This also; avoids partial register stalls in some important cases. //===---------------------------------------------------------------------===//. We don't fold (icmp (add) (add)) unless the two adds only have a single use.; There are a lot of cases that we're refusing to fold in (e.g.) 256.bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this because we don't want to introduce an overlapped live range; of the ivar. However if we can make this more aggressive without causing; performance issues in two ways:. 1. If *either* the LHS or RHS has a single use, we can definitely do the; transformation. In the overlapping liverange case we're trading one register; use for one fewer operation, which is a reasonable trade. Before doing this; we should verify that the llc output actually shrinks for some benchmarks.; 2. If both ops have multiple uses, we can still fold it if the operations are; both sinkable to *after* the icmp (e.g. in a subsequent block) which doesn't; increase register pressure. There are a ton of icmp's we aren't simplifying because of the reg pressure; concern. Care is warranted here though because many of these are induction; variables and other cases that matter a lot to performance, like the above.; Here's a blob of code that you can drop into the bottom of visitICmp to see some; missed cases:. { Value *A, *B, *C, *D;; if (match(Op0, m_Add(m_Value(A), m_Value(B))) && ; match(Op1, m_Add(m_Value(C), m_Value(D))) &&; (A == C || A ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:64738,Performance,perform,performance,64738,".bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this because we don't want to introduce an overlapped live range; of the ivar. However if we can make this more aggressive without causing; performance issues in two ways:. 1. If *either* the LHS or RHS has a single use, we can definitely do the; transformation. In the overlapping liverange case we're trading one register; use for one fewer operation, which is a reasonable trade. Before doing this; we should verify that the llc output actually shrinks for some benchmarks.; 2. If both ops have multiple uses, we can still fold it if the operations are; both sinkable to *after* the icmp (e.g. in a subsequent block) which doesn't; increase register pressure. There are a ton of icmp's we aren't simplifying because of the reg pressure; concern. Care is warranted here though because many of these are induction; variables and other cases that matter a lot to performance, like the above.; Here's a blob of code that you can drop into the bottom of visitICmp to see some; missed cases:. { Value *A, *B, *C, *D;; if (match(Op0, m_Add(m_Value(A), m_Value(B))) && ; match(Op1, m_Add(m_Value(C), m_Value(D))) &&; (A == C || A == D || B == C || B == D)) {; errs() << ""OP0 = "" << *Op0 << "" U="" << Op0->getNumUses() << ""\n"";; errs() << ""OP1 = "" << *Op1 << "" U="" << Op1->getNumUses() << ""\n"";; errs() << ""CMP = "" << I << ""\n\n"";; }; }. //===---------------------------------------------------------------------===//. define i1 @test1(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ult i32 %and, 2; ret i1 %cmp; }. Can be folded to (x & 2) == 0. define i1 @test2(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ugt i32 %and, 1; ret i1 %cmp; }. Can be folded to (x & 2) != 0. SimplifyDemandedBits shrinks the ""and"" constant to 2 but instcombine misses the; icmp transform. //===------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:66189,Performance,load,load,66189,"\n"";; errs() << ""CMP = "" << I << ""\n\n"";; }; }. //===---------------------------------------------------------------------===//. define i1 @test1(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ult i32 %and, 2; ret i1 %cmp; }. Can be folded to (x & 2) == 0. define i1 @test2(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ugt i32 %and, 1; ret i1 %cmp; }. Can be folded to (x & 2) != 0. SimplifyDemandedBits shrinks the ""and"" constant to 2 but instcombine misses the; icmp transform. //===---------------------------------------------------------------------===//. This code:. typedef struct {; int f1:1;; int f2:1;; int f3:1;; int f4:29;; } t1;. typedef struct {; int f1:1;; int f2:1;; int f3:30;; } t2;. t1 s1;; t2 s2;. void func1(void); {; s1.f1 = s2.f1;; s1.f2 = s2.f2;; }. Compiles into this IR (on x86-64 at least):. %struct.t1 = type { i8, [3 x i8] }; @s2 = global %struct.t1 zeroinitializer, align 4; @s1 = global %struct.t1 zeroinitializer, align 4; define void @func1() nounwind ssp noredzone {; entry:; %0 = load i32* bitcast (%struct.t1* @s2 to i32*), align 4; %bf.val.sext5 = and i32 %0, 1; %1 = load i32* bitcast (%struct.t1* @s1 to i32*), align 4; %2 = and i32 %1, -4; %3 = or i32 %2, %bf.val.sext5; %bf.val.sext26 = and i32 %0, 2; %4 = or i32 %3, %bf.val.sext26; store i32 %4, i32* bitcast (%struct.t1* @s1 to i32*), align 4; ret void; }. The two or/and's should be merged into one each. //===---------------------------------------------------------------------===//. Machine level code hoisting can be useful in some cases. For example, PR9408; is about:. typedef union {; void (*f1)(int);; void (*f2)(long);; } funcs;. void foo(funcs f, int which) {; int a = 5;; if (which) {; f.f1(a);; } else {; f.f2(a);; }; }. which we compile to:. foo: # @foo; # %bb.0: # %entry; pushq %rbp; movq %rsp, %rbp; testl %esi, %esi; movq %rdi, %rax; je .LBB0_2; # %bb.1: # %if.then; movl $5, %edi; callq *%rax; popq %rbp; ret; .LBB0_2: # %if.else; movl $5, %edi; callq *%rax; popq %rbp; r",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:66279,Performance,load,load,66279,"--===//. define i1 @test1(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ult i32 %and, 2; ret i1 %cmp; }. Can be folded to (x & 2) == 0. define i1 @test2(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ugt i32 %and, 1; ret i1 %cmp; }. Can be folded to (x & 2) != 0. SimplifyDemandedBits shrinks the ""and"" constant to 2 but instcombine misses the; icmp transform. //===---------------------------------------------------------------------===//. This code:. typedef struct {; int f1:1;; int f2:1;; int f3:1;; int f4:29;; } t1;. typedef struct {; int f1:1;; int f2:1;; int f3:30;; } t2;. t1 s1;; t2 s2;. void func1(void); {; s1.f1 = s2.f1;; s1.f2 = s2.f2;; }. Compiles into this IR (on x86-64 at least):. %struct.t1 = type { i8, [3 x i8] }; @s2 = global %struct.t1 zeroinitializer, align 4; @s1 = global %struct.t1 zeroinitializer, align 4; define void @func1() nounwind ssp noredzone {; entry:; %0 = load i32* bitcast (%struct.t1* @s2 to i32*), align 4; %bf.val.sext5 = and i32 %0, 1; %1 = load i32* bitcast (%struct.t1* @s1 to i32*), align 4; %2 = and i32 %1, -4; %3 = or i32 %2, %bf.val.sext5; %bf.val.sext26 = and i32 %0, 2; %4 = or i32 %3, %bf.val.sext26; store i32 %4, i32* bitcast (%struct.t1* @s1 to i32*), align 4; ret void; }. The two or/and's should be merged into one each. //===---------------------------------------------------------------------===//. Machine level code hoisting can be useful in some cases. For example, PR9408; is about:. typedef union {; void (*f1)(int);; void (*f2)(long);; } funcs;. void foo(funcs f, int which) {; int a = 5;; if (which) {; f.f1(a);; } else {; f.f2(a);; }; }. which we compile to:. foo: # @foo; # %bb.0: # %entry; pushq %rbp; movq %rsp, %rbp; testl %esi, %esi; movq %rdi, %rax; je .LBB0_2; # %bb.1: # %if.then; movl $5, %edi; callq *%rax; popq %rbp; ret; .LBB0_2: # %if.else; movl $5, %edi; callq *%rax; popq %rbp; ret. Note that bb1 and bb2 are the same. This doesn't happen at the IR level; because one call is passing an i32 and the o",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:155,Safety,detect,detection,155,"Target Independent Opportunities:. //===---------------------------------------------------------------------===//. We should recognized various ""overflow detection"" idioms and translate them into; llvm.uadd.with.overflow and similar intrinsics. Here is a multiply idiom:. unsigned int mul(unsigned int a,unsigned int b) {; if ((unsigned long long)a*b>0xffffffff); exit(0);; return a*b;; }. The legalization code for mul-with-overflow needs to be made more robust before; this can be implemented though. //===---------------------------------------------------------------------===//. Get the C front-end to expand hypot(x,y) -> llvm.sqrt(x*x+y*y) when errno and; precision don't matter (ffastmath). Misc/mandel will like this. :) This isn't; safe in general, even on darwin. See the libm implementation of hypot for; examples (which special case when x/y are exactly zero to get signed zeros etc; right). //===---------------------------------------------------------------------===//. On targets with expensive 64-bit multiply, we could LSR this:. for (i = ...; ++i) {; x = 1ULL << i;. into:; long long tmp = 1;; for (i = ...; ++i, tmp+=tmp); x = tmp;. This would be a win on ppc32, but not x86 or ppc64. //===---------------------------------------------------------------------===//. Shrink: (setlt (loadi32 P), 0) -> (setlt (loadi8 Phi), 0). //===---------------------------------------------------------------------===//. Reassociate should turn things like:. int factorial(int X) {; return X*X*X*X*X*X*X*X;; }. into llvm.powi calls, allowing the code generator to produce balanced; multiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:743,Safety,safe,safe,743,"Target Independent Opportunities:. //===---------------------------------------------------------------------===//. We should recognized various ""overflow detection"" idioms and translate them into; llvm.uadd.with.overflow and similar intrinsics. Here is a multiply idiom:. unsigned int mul(unsigned int a,unsigned int b) {; if ((unsigned long long)a*b>0xffffffff); exit(0);; return a*b;; }. The legalization code for mul-with-overflow needs to be made more robust before; this can be implemented though. //===---------------------------------------------------------------------===//. Get the C front-end to expand hypot(x,y) -> llvm.sqrt(x*x+y*y) when errno and; precision don't matter (ffastmath). Misc/mandel will like this. :) This isn't; safe in general, even on darwin. See the libm implementation of hypot for; examples (which special case when x/y are exactly zero to get signed zeros etc; right). //===---------------------------------------------------------------------===//. On targets with expensive 64-bit multiply, we could LSR this:. for (i = ...; ++i) {; x = 1ULL << i;. into:; long long tmp = 1;; for (i = ...; ++i, tmp+=tmp); x = tmp;. This would be a win on ppc32, but not x86 or ppc64. //===---------------------------------------------------------------------===//. Shrink: (setlt (loadi32 P), 0) -> (setlt (loadi8 Phi), 0). //===---------------------------------------------------------------------===//. Reassociate should turn things like:. int factorial(int X) {; return X*X*X*X*X*X*X*X;; }. into llvm.powi calls, allowing the code generator to produce balanced; multiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:7804,Safety,avoid,avoids,7804,"ned int count = 0;; for (unsigned int i = 0; i < 4 * 8; i++); count += (input >> i) & i;; return count;; }. This should be recognized as CLZ: https://github.com/llvm/llvm-project/issues/64167. unsigned clz_a(unsigned a) {; int i;; for (i=0;i<32;i++); if (a & (1<<(31-i))); return i;; return 32;; }. This sort of thing should be added to the loop idiom pass. //===---------------------------------------------------------------------===//. These should turn into single 16-bit (unaligned?) loads on little/big endian; processors. unsigned short read_16_le(const unsigned char *adr) {; return adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:18682,Safety,safe,safely,18682," or i32 %shl9, %conv; %or6 = or i32 %or, %shl5; %or10 = or i32 %or6, %shl; ret i32 %or10; }. it would be better as:. unsigned int bar(unsigned char i) {; unsigned int j=i | (i << 8); ; return j | (j<<16);; }. aka:. define i32 @bar(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %or = or i32 %shl, %conv; %shl5 = shl i32 %or, 16; %or6 = or i32 %shl5, %or; ret i32 %or6; }. or even i*0x01010101, depending on the speed of the multiplier. The best way to; handle this is to canonicalize it to a multiply in IR and have codegen handle; lowering multiplies to shifts on cpus where shifts are faster. //===---------------------------------------------------------------------===//. We do a number of simplifications in simplify libcalls to strength reduce; standard library functions, but we don't currently merge them together. For; example, it is useful to merge memcpy(a,b,strlen(b)) -> strcpy. This can only; be done safely if ""b"" isn't modified between the strlen and memcpy of course. //===---------------------------------------------------------------------===//. We compile this program: (from GCC PR11680); http://gcc.gnu.org/bugzilla/attachment.cgi?id=4487. Into code that runs the same speed in fast/slow modes, but both modes run 2x; slower than when compile with GCC (either 4.0 or 4.2):. $ llvm-g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 1.821u 0.003s 0:01.82 100.0%	0+0k 0+0io 0pf+0w. $ g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 0.821u 0.001s 0:00.82 100.0%	0+0k 0+0io 0pf+0w. It looks like we are making the same inlining decisions, so this may be raw; codegen badness or something else (haven't investigated). //===---------------------------------------------------------------------===//. Divisibility by constant can be simplified (according to GCC PR12849) from; being a mulhi to being a mul lo (cheaper). Testcase:. void bar(unsigned n) {; if (n % 3 == 0); true();; }. This is equivalent to the f",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:28581,Safety,redund,redundant,28581,"-------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) {; int i;; *res = 0;; for (i = 0; i < n; i++); if (*cond); *res ^= 234; /* (*) */; }. On this function GVN hoists the fully redundant value of *res, but nothing; moves the store out. This gives us this code:. bb:		; preds = %bb2, %entry; 	%.rle = phi i32 [ 0, %entry ], [ %.rle6, %bb2 ]	; 	%i.05 = phi i32 [ 0, %entry ], [ %indvar.next, %bb2 ]; 	%1 = load i32* %cond, align 4; 	%2 = icmp eq i32 %1, 0; 	br i1 %2, label %bb2, label %bb1. bb1:		; preds = %bb; 	%3 = xor i32 %.rle, 234	; 	store i32 %3, i32* %res, align 4; 	br label %bb2. bb2:		; preds = %bb, %bb1; 	%.rle6 = phi i32 [ %3, %bb1 ], [ %.rle, %bb ]	; 	%indvar.next = add i32 %i.05, 1	; 	%exitcond = icmp eq i32 %indvar.next, %n; 	br i1 %exitcond, label %return, label %bb. DSE should sink partially dead stores to get the store out of the loop. Here's another partial dead case:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=12395. //===---------------------------------------------------------------------===//. Scalar PRE hoists the mul in the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31184,Safety,redund,redundant,31184," = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-c",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31375,Safety,redund,redundant,31375,"p10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33230,Safety,redund,redundant,33230,"..; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = s",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:39356,Safety,redund,redundant,39356,"0, i32 0; %3073 = call i8* @strcpy(i8* %3072, i8* %3071) nounwind; %strlen = call i32 @strlen(i8* %3072) ; uses = 1; %endptr = getelementptr [100 x i8]* %tempString, i32 0, i32 %strlen; call void @llvm.memcpy.i32(i8* %endptr, ; i8* getelementptr ([5 x i8]* @""\01LC42"", i32 0, i32 0), i32 5, i32 1); %3074 = call i32 @strlen(i8* %endptr) nounwind readonly ; ; This is interesting for a couple reasons. First, in this:. The memcpy+strlen strlen can be replaced with:. %3074 = call i32 @strlen([5 x i8]* @""\01LC42"") nounwind readonly . Because the destination was just copied into the specified memory buffer. This,; in turn, can be constant folded to ""4"". In other code, it contains:. %endptr6978 = bitcast i8* %endptr69 to i32* ; store i32 7107374, i32* %endptr6978, align 1; %3167 = call i32 @strlen(i8* %endptr69) nounwind readonly . Which could also be constant folded. Whatever is producing this should probably; be fixed to leave this as a memcpy from a string. Further, eon also has an interesting partially redundant strlen call:. bb8: ; preds = %_ZN18eonImageCalculatorC1Ev.exit; %682 = getelementptr i8** %argv, i32 6 ; <i8**> [#uses=2]; %683 = load i8** %682, align 4 ; <i8*> [#uses=4]; %684 = load i8* %683, align 1 ; <i8> [#uses=1]; %685 = icmp eq i8 %684, 0 ; <i1> [#uses=1]; br i1 %685, label %bb10, label %bb9. bb9: ; preds = %bb8; %686 = call i32 @strlen(i8* %683) nounwind readonly ; %687 = icmp ugt i32 %686, 254 ; <i1> [#uses=1]; br i1 %687, label %bb10, label %bb11. bb10: ; preds = %bb9, %bb8; %688 = call i32 @strlen(i8* %683) nounwind readonly . This could be eliminated by doing the strlen once in bb8, saving code size and; improving perf on the bb8->9->10 path. //===---------------------------------------------------------------------===//. I see an interesting fully redundant call to strlen left in 186.crafty:InputMove; which looks like:; %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 ; . bb62: ; preds = %bb55, %bb53; %promote.0 = phi i32 [ %169, %bb55",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:40138,Safety,redund,redundant,40138,"(i8* %endptr69) nounwind readonly . Which could also be constant folded. Whatever is producing this should probably; be fixed to leave this as a memcpy from a string. Further, eon also has an interesting partially redundant strlen call:. bb8: ; preds = %_ZN18eonImageCalculatorC1Ev.exit; %682 = getelementptr i8** %argv, i32 6 ; <i8**> [#uses=2]; %683 = load i8** %682, align 4 ; <i8*> [#uses=4]; %684 = load i8* %683, align 1 ; <i8> [#uses=1]; %685 = icmp eq i8 %684, 0 ; <i1> [#uses=1]; br i1 %685, label %bb10, label %bb9. bb9: ; preds = %bb8; %686 = call i32 @strlen(i8* %683) nounwind readonly ; %687 = icmp ugt i32 %686, 254 ; <i1> [#uses=1]; br i1 %687, label %bb10, label %bb11. bb10: ; preds = %bb9, %bb8; %688 = call i32 @strlen(i8* %683) nounwind readonly . This could be eliminated by doing the strlen once in bb8, saving code size and; improving perf on the bb8->9->10 path. //===---------------------------------------------------------------------===//. I see an interesting fully redundant call to strlen left in 186.crafty:InputMove; which looks like:; %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 ; . bb62: ; preds = %bb55, %bb53; %promote.0 = phi i32 [ %169, %bb55 ], [ 0, %bb53 ] ; %171 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1; %172 = add i32 %171, -1 ; <i32> [#uses=1]; %173 = getelementptr [128 x i8]* %movetext, i32 0, i32 %172 . ... no stores ...; br i1 %or.cond, label %bb65, label %bb72. bb65: ; preds = %bb62; store i8 0, i8* %173, align 1; br label %bb72. bb72: ; preds = %bb65, %bb62; %trank.1 = phi i32 [ %176, %bb65 ], [ -1, %bb62 ] ; %177 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1. Note that on the bb62->bb72 path, that the %177 strlen call is partially; redundant with the %171 call. At worst, we could shove the %177 strlen call; up into the bb65 block moving it out of the bb62->bb72 path. However, note; that bb65 stores to the string, zeroing out the last byte. This means that on; that path the value ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:40891,Safety,redund,redundant,40891," i32 @strlen(i8* %683) nounwind readonly . This could be eliminated by doing the strlen once in bb8, saving code size and; improving perf on the bb8->9->10 path. //===---------------------------------------------------------------------===//. I see an interesting fully redundant call to strlen left in 186.crafty:InputMove; which looks like:; %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 ; . bb62: ; preds = %bb55, %bb53; %promote.0 = phi i32 [ %169, %bb55 ], [ 0, %bb53 ] ; %171 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1; %172 = add i32 %171, -1 ; <i32> [#uses=1]; %173 = getelementptr [128 x i8]* %movetext, i32 0, i32 %172 . ... no stores ...; br i1 %or.cond, label %bb65, label %bb72. bb65: ; preds = %bb62; store i8 0, i8* %173, align 1; br label %bb72. bb72: ; preds = %bb65, %bb62; %trank.1 = phi i32 [ %176, %bb65 ], [ -1, %bb62 ] ; %177 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1. Note that on the bb62->bb72 path, that the %177 strlen call is partially; redundant with the %171 call. At worst, we could shove the %177 strlen call; up into the bb65 block moving it out of the bb62->bb72 path. However, note; that bb65 stores to the string, zeroing out the last byte. This means that on; that path the value of %177 is actually just %171-1. A sub is cheaper than a; strlen!. This pattern repeats several times, basically doing:. A = strlen(P);; P[A-1] = 0;; B = strlen(P);; where it is ""obvious"" that B = A-1. //===---------------------------------------------------------------------===//. 186.crafty has this interesting pattern with the ""out.4543"" variable:. call void @llvm.memcpy.i32(; i8* getelementptr ([10 x i8]* @out.4543, i32 0, i32 0),; i8* getelementptr ([7 x i8]* @""\01LC28700"", i32 0, i32 0), i32 7, i32 1) ; %101 = call@printf(i8* ... @out.4543, i32 0, i32 0)) nounwind . It is basically doing:. memcpy(globalarray, ""string"");; printf(..., globalarray);; ; Anyway, by knowing that printf just reads the memory and forw",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:63413,Safety,avoid,avoids,63413," foo(struct x*P);; struct x testfunc() {; struct x V1, V2;; foo(&V1);; V2 = V1;. return V2;; }. We currently compile this to:; $ clang t.c -S -o - -O0 -emit-llvm | opt -sroa -S. %struct.x = type { i8, [4 x i32] }. define void @testfunc(%struct.x* sret %agg.result) nounwind ssp {; entry:; %V1 = alloca %struct.x, align 4; call void @foo(%struct.x* %V1); %tmp1 = bitcast %struct.x* %V1 to i8*; %0 = bitcast %struct.x* %V1 to i160*; %srcval1 = load i160* %0, align 4; %tmp2 = bitcast %struct.x* %agg.result to i8*; %1 = bitcast %struct.x* %agg.result to i160*; store i160 %srcval1, i160* %1, align 4; ret void; }. This happens because SRoA sees that the temp alloca has is being memcpy'd into; and out of and it has holes and it has to be conservative. If we knew about the; holes, then this could be much much better. Having information about these holes would also improve memcpy (etc) lowering at; llc time when it gets inlined, because we can use smaller transfers. This also; avoids partial register stalls in some important cases. //===---------------------------------------------------------------------===//. We don't fold (icmp (add) (add)) unless the two adds only have a single use.; There are a lot of cases that we're refusing to fold in (e.g.) 256.bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this because we don't want to introduce an overlapped live range; of the ivar. However if we can make this more aggressive without causing; performance issues in two ways:. 1. If *either* the LHS or RHS has a single use, we can definitely do the; transformation. In the overlapping liverange case we're trading one register; use for one fewer operation, which is a reasonable trade. Before doing this; we should verify that the llc output actually shrinks for some benchmarks.; 2. If both ops have multiple uses, we can still fold it if the operations are; bot",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:8106,Security,hash,hash,8106,")); return i;; return 32;; }. This sort of thing should be added to the loop idiom pass. //===---------------------------------------------------------------------===//. These should turn into single 16-bit (unaligned?) loads on little/big endian; processors. unsigned short read_16_le(const unsigned char *adr) {; return adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:13042,Security,hash,hash,13042,"der this silly; example in pic mode:. #include <assert.h>; void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing this with integers allows the use of read/modify/write; instructions. //===---------------------------------------------------------------------===//. DAG Combiner should try to combine small loads into larger loads when ; profitable. For example, we compile this C++ example:. struct THotKey { short Key; bool Control; bool Shift; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:13084,Security,hash,hash,13084,"void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing this with integers allows the use of read/modify/write; instructions. //===---------------------------------------------------------------------===//. DAG Combiner should try to combine small loads into larger loads when ; profitable. For example, we compile this C++ example:. struct THotKey { short Key; bool Control; bool Shift; bool Alt; };; extern THotKey m_HotKey;; THotKey GetHotKey (",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:54236,Security,secur,security,54236,"(int a, int b) {; int lt = a < b;; int eq = a == b;; if (lt); return 1;; return eq;; }. Is compiled to:. define i32 @test(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %cmp = icmp slt i32 %a, %b; br i1 %cmp, label %return, label %if.end. if.end: ; preds = %entry; %cmp5 = icmp eq i32 %a, %b; %conv6 = zext i1 %cmp5 to i32; ret i32 %conv6. return: ; preds = %entry; ret i32 1; }. it could be:. define i32 @test__(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = icmp sle i32 %a, %b; %retval = zext i1 %0 to i32; ret i32 %retval; }. //===---------------------------------------------------------------------===//. This code can be seen in viterbi:. %64 = call noalias i8* @malloc(i64 %62) nounwind; ...; %67 = call i64 @llvm.objectsize.i64(i8* %64, i1 false) nounwind; %68 = call i8* @__memset_chk(i8* %64, i32 0, i64 %62, i64 %67) nounwind. llvm.objectsize.i64 should be taught about malloc/calloc, allowing it to; fold to %62. This is a security win (overflows of malloc will get caught); and also a performance win by exposing more memsets to the optimizer. This occurs several times in viterbi. Note that this would change the semantics of @llvm.objectsize which by its; current definition always folds to a constant. We also should make sure that; we remove checking in code like. char *p = malloc(strlen(s)+1);; __strcpy_chk(p, s, __builtin_object_size(p, 0));. //===---------------------------------------------------------------------===//. clang -O3 currently compiles this code. int g(unsigned int a) {; unsigned int c[100];; c[10] = a;; c[11] = a;; unsigned int b = c[10] + c[11];; if(b > a*2) a = 4;; else a = 8;; return a + 7;; }. into. define i32 @g(i32 a) nounwind readnone {; %add = shl i32 %a, 1; %mul = shl i32 %a, 1; %cmp = icmp ugt i32 %add, %mul; %a.addr.0 = select i1 %cmp, i32 11, i32 15; ret i32 %a.addr.0; }. The icmp should fold to false. This CSE opportunity is only available; after GVN and InstCombine have run. //===------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:1860,Testability,test,testcase,1860,"o to get signed zeros etc; right). //===---------------------------------------------------------------------===//. On targets with expensive 64-bit multiply, we could LSR this:. for (i = ...; ++i) {; x = 1ULL << i;. into:; long long tmp = 1;; for (i = ...; ++i, tmp+=tmp); x = tmp;. This would be a win on ppc32, but not x86 or ppc64. //===---------------------------------------------------------------------===//. Shrink: (setlt (loadi32 P), 0) -> (setlt (loadi8 Phi), 0). //===---------------------------------------------------------------------===//. Reassociate should turn things like:. int factorial(int X) {; return X*X*X*X*X*X*X*X;; }. into llvm.powi calls, allowing the code generator to produce balanced; multiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n) + bar(2*z, 2*n);; }. This is blocked on not handling X*X*X -> powi(X, 3) (see note above). The issue; is that we end up getting t = 2*X s = t*t and don't turn this into 4*X*X,; which is the same number of multiplies and is canonical, because the 2*X has; multiple uses. Here's a simple example:. define i32 @test15(i32 %X1) {; %B = mul i32 %X1, 47 ; X1*47; %C = mul i32 %B, %B; ret i32 %C; }. //===---------------------------------------------------------------------===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ; b3 = a3 + a2 + a1 + a0; ; b2 = a2 + a1 + a0; ; b1 = a1 + a0; ; } . This requires reassociating to forms of expressions that are already available,; something that reassoc doesn't thi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:8082,Testability,Benchmark,Benchmarks,8082,")); return i;; return 32;; }. This sort of thing should be added to the loop idiom pass. //===---------------------------------------------------------------------===//. These should turn into single 16-bit (unaligned?) loads on little/big endian; processors. unsigned short read_16_le(const unsigned char *adr) {; return adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:8121,Testability,test,tests,8121,")); return i;; return 32;; }. This sort of thing should be added to the loop idiom pass. //===---------------------------------------------------------------------===//. These should turn into single 16-bit (unaligned?) loads on little/big endian; processors. unsigned short read_16_le(const unsigned char *adr) {; return adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:8278,Testability,Benchmark,Benchmarks,8278,". These should turn into single 16-bit (unaligned?) loads on little/big endian; processors. unsigned short read_16_le(const unsigned char *adr) {; return adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64 effective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:9078,Testability,test,test,9078,"-------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64 effective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; b2 = (temp >> 12) & 0xf;; if (b2) effective_addr2 += regs[b2];; effective_addr2 &= regs[4];; if ((effective_addr2 & 3) == 0); return 1;; return 0;; }. Note that only the low 2 bits of effective_addr2 are used. On 32-bit systems,; we don't eliminate the computation of the top half of effective_addr2 because; we don't have whole-function selection dags. On x86, this means we use one; extra register for the function when effective_addr2 is declared as U64 than; when it is declared U32. PHI Slicing could be extended to do this. //===---------------------------------------------------------------------===//. Tail call elim should be more aggressive, checking to see if the call is; followed by an uncond branch to an exit block. ; This testcase is due to tail-duplication not wanting to copy the return; ; instruction into the terminating blocks because there was other code; ; optimized out of the function after the taildup happened.; ; R",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:10020,Testability,test,testcase,10020,"fective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; b2 = (temp >> 12) & 0xf;; if (b2) effective_addr2 += regs[b2];; effective_addr2 &= regs[4];; if ((effective_addr2 & 3) == 0); return 1;; return 0;; }. Note that only the low 2 bits of effective_addr2 are used. On 32-bit systems,; we don't eliminate the computation of the top half of effective_addr2 because; we don't have whole-function selection dags. On x86, this means we use one; extra register for the function when effective_addr2 is declared as U64 than; when it is declared U32. PHI Slicing could be extended to do this. //===---------------------------------------------------------------------===//. Tail call elim should be more aggressive, checking to see if the call is; followed by an uncond branch to an exit block. ; This testcase is due to tail-duplication not wanting to copy the return; ; instruction into the terminating blocks because there was other code; ; optimized out of the function after the taildup happened.; ; RUN: llvm-as < %s | opt -tailcallelim | llvm-dis | not grep call. define i32 @t4(i32 %a) {; entry:; 	%tmp.1 = and i32 %a, 1		; <i32> [#uses=1]; 	%tmp.2 = icmp ne i32 %tmp.1, 0		; <i1> [#uses=1]; 	br i1 %tmp.2, label %then.0, label %else.0. then.0:		; preds = %entry; 	%tmp.5 = add i32 %a, -1		; <i32> [#uses=1]; 	%tmp.3 = call i32 @t4( i32 %tmp.5 )		; <i32> [#uses=1]; 	br label %return. else.0:		; preds = %entry; 	%tmp.7 = icmp ne i32 %a, 0		; <i1> [#uses=1]; 	br i1 %tmp.7, label %then.1, label %return. then.1:		; preds = %else.0; 	%tmp.11 = add i32 %a, -2		; <i32> [#uses=1]; 	%tmp.9 = call i32 @t4( i32 %tmp.11 )		; <i32> [#uses=1]; 	br label %return. return:		; preds = %then.1, %else.0, %then.0; 	%result.0 = phi i32 [ 0, %else.0 ], [ %tmp.3, %then.0 ],; [ %tmp.9, %then.1 ]; 	ret i32 %result.0; }. //===---------------------------------------------------------------------===//. Tail ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:12075,Testability,assert,assert,12075,"-------------------------------===//. Tail recursion elimination should handle:. int pow2m1(int n) {; if (n == 0); return 0;; return 2 * pow2m1 (n - 1) + 1;; }. Also, multiplies can be turned into SHL's, so they should be handled as if; they were associative. ""return foo() << 1"" can be tail recursion eliminated. //===---------------------------------------------------------------------===//. Argument promotion should promote arguments for recursive functions, like ; this:. ; RUN: llvm-as < %s | opt -argpromotion | llvm-dis | grep x.val. define internal i32 @foo(i32* %x) {; entry:; 	%tmp = load i32* %x		; <i32> [#uses=0]; 	%tmp.foo = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp.foo; }. define i32 @bar(i32* %x) {; entry:; 	%tmp3 = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp3; }. //===---------------------------------------------------------------------===//. We should investigate an instruction sinking pass. Consider this silly; example in pic mode:. #include <assert.h>; void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburt",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:12105,Testability,assert,assert,12105,"---===//. Tail recursion elimination should handle:. int pow2m1(int n) {; if (n == 0); return 0;; return 2 * pow2m1 (n - 1) + 1;; }. Also, multiplies can be turned into SHL's, so they should be handled as if; they were associative. ""return foo() << 1"" can be tail recursion eliminated. //===---------------------------------------------------------------------===//. Argument promotion should promote arguments for recursive functions, like ; this:. ; RUN: llvm-as < %s | opt -argpromotion | llvm-dis | grep x.val. define internal i32 @foo(i32* %x) {; entry:; 	%tmp = load i32* %x		; <i32> [#uses=0]; 	%tmp.foo = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp.foo; }. define i32 @bar(i32* %x) {; entry:; 	%tmp3 = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp3; }. //===---------------------------------------------------------------------===//. We should investigate an instruction sinking pass. Consider this silly; example in pic mode:. #include <assert.h>; void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:12545,Testability,assert,assertion,12545,"functions, like ; this:. ; RUN: llvm-as < %s | opt -argpromotion | llvm-dis | grep x.val. define internal i32 @foo(i32* %x) {; entry:; 	%tmp = load i32* %x		; <i32> [#uses=0]; 	%tmp.foo = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp.foo; }. define i32 @bar(i32* %x) {; entry:; 	%tmp3 = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp3; }. //===---------------------------------------------------------------------===//. We should investigate an instruction sinking pass. Consider this silly; example in pic mode:. #include <assert.h>; void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:14732,Testability,test,test,14732,"ns. //===---------------------------------------------------------------------===//. DAG Combiner should try to combine small loads into larger loads when ; profitable. For example, we compile this C++ example:. struct THotKey { short Key; bool Control; bool Shift; bool Alt; };; extern THotKey m_HotKey;; THotKey GetHotKey () { return m_HotKey; }. into (-m64 -O3 -fno-exceptions -static -fomit-frame-pointer):. __Z9GetHotKeyv: ## @_Z9GetHotKeyv; 	movq	_m_HotKey@GOTPCREL(%rip), %rax; 	movzwl	(%rax), %ecx; 	movzbl	2(%rax), %edx; 	shlq	$16, %rdx; 	orq	%rcx, %rdx; 	movzbl	3(%rax), %ecx; 	shlq	$24, %rcx; 	orq	%rdx, %rcx; 	movzbl	4(%rax), %eax; 	shlq	$32, %rax; 	orq	%rcx, %rax; 	ret. //===---------------------------------------------------------------------===//. We should add an FRINT node to the DAG to model targets that have legal; implementations of ceil/floor/rint. //===---------------------------------------------------------------------===//. Consider:. int test() {; long long input[8] = {1,0,1,0,1,0,1,0};; foo(input);; }. Clang compiles this into:. call void @llvm.memset.p0i8.i64(i8* %tmp, i8 0, i64 64, i32 16, i1 false); %0 = getelementptr [8 x i64]* %input, i64 0, i64 0; store i64 1, i64* %0, align 16; %1 = getelementptr [8 x i64]* %input, i64 0, i64 2; store i64 1, i64* %1, align 16; %2 = getelementptr [8 x i64]* %input, i64 0, i64 4; store i64 1, i64* %2, align 16; %3 = getelementptr [8 x i64]* %input, i64 0, i64 6; store i64 1, i64* %3, align 16. Which gets codegen'd into:. 	pxor	%xmm0, %xmm0; 	movaps	%xmm0, -16(%rbp); 	movaps	%xmm0, -32(%rbp); 	movaps	%xmm0, -48(%rbp); 	movaps	%xmm0, -64(%rbp); 	movq	$1, -64(%rbp); 	movq	$1, -48(%rbp); 	movq	$1, -32(%rbp); 	movq	$1, -16(%rbp). It would be better to have 4 movq's of 0 instead of the movaps's. //===---------------------------------------------------------------------===//. http://llvm.org/PR717:. The following code should compile into ""ret int undef"". Instead, LLVM; produces ""ret int 0"":. int f() {; int x = 4;; in",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:16779,Testability,test,testcases,16779,"return y;; }. //===---------------------------------------------------------------------===//. The loop unroller should partially unroll loops (instead of peeling them); when code growth isn't too bad and when an unroll count allows simplification; of some code within the loop. One trivial example is:. #include <stdio.h>; int main() {; int nRet = 17;; int nLoop;; for ( nLoop = 0; nLoop < 1000; nLoop++ ) {; if ( nLoop & 1 ); nRet += 2;; else; nRet -= 1;; }; return nRet;; }. Unrolling by 2 would eliminate the '&1' in both copies, leading to a net; reduction in code size. The resultant code would then also be suitable for; exit value computation. //===---------------------------------------------------------------------===//. We miss a bunch of rotate opportunities on various targets, including ppc, x86,; etc. On X86, we miss a bunch of 'rotate by variable' cases because the rotate; matching code in dag combine doesn't look through truncates aggressively ; enough. Here are some testcases reduces from GCC PR17886:. unsigned long long f5(unsigned long long x, unsigned long long y) {; return (x << 8) | ((y >> 48) & 0xffull);; }; unsigned long long f6(unsigned long long x, unsigned long long y, int z) {; switch(z) {; case 1:; return (x << 8) | ((y >> 48) & 0xffull);; case 2:; return (x << 16) | ((y >> 40) & 0xffffull);; case 3:; return (x << 24) | ((y >> 32) & 0xffffffull);; case 4:; return (x << 32) | ((y >> 24) & 0xffffffffull);; default:; return (x << 40) | ((y >> 16) & 0xffffffffffull);; }; }. //===---------------------------------------------------------------------===//. This (and similar related idioms):. unsigned int foo(unsigned char i) {; return i | (i<<8) | (i<<16) | (i<<24);; } . compiles into:. define i32 @foo(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %shl5 = shl i32 %conv, 16; %shl9 = shl i32 %conv, 24; %or = or i32 %shl9, %conv; %or6 = or i32 %or, %shl5; %or10 = or i32 %or6, %shl; ret i32 %or",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:19614,Testability,Test,Testcase,19614,"o merge memcpy(a,b,strlen(b)) -> strcpy. This can only; be done safely if ""b"" isn't modified between the strlen and memcpy of course. //===---------------------------------------------------------------------===//. We compile this program: (from GCC PR11680); http://gcc.gnu.org/bugzilla/attachment.cgi?id=4487. Into code that runs the same speed in fast/slow modes, but both modes run 2x; slower than when compile with GCC (either 4.0 or 4.2):. $ llvm-g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 1.821u 0.003s 0:01.82 100.0%	0+0k 0+0io 0pf+0w. $ g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 0.821u 0.001s 0:00.82 100.0%	0+0k 0+0io 0pf+0w. It looks like we are making the same inlining decisions, so this may be raw; codegen badness or something else (haven't investigated). //===---------------------------------------------------------------------===//. Divisibility by constant can be simplified (according to GCC PR12849) from; being a mulhi to being a mul lo (cheaper). Testcase:. void bar(unsigned n) {; if (n % 3 == 0); true();; }. This is equivalent to the following, where 2863311531 is the multiplicative; inverse of 3, and 1431655766 is ((2^32)-1)/3+1:; void bar(unsigned n) {; if (n * 2863311531U < 1431655766U); true();; }. The same transformation can work with an even modulo with the addition of a; rotate: rotate the result of the multiply to the right by the number of bits; which need to be zero for the condition to be true, and shrink the compare RHS; by the same amount. Unless the target supports rotates, though, that; transformation probably isn't worthwhile. The transformation can also easily be made to work with non-zero equality; comparisons: just transform, for example, ""n % 3 == 1"" to ""(n-1) % 3 == 0"". //===---------------------------------------------------------------------===//. Better mod/ref analysis for scanf would allow us to eliminate the vtable and a; bunch of other stuff from this example (see PR1604): . #include <cstdio>; struct tes",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:20616,Testability,test,test,20616,". void bar(unsigned n) {; if (n % 3 == 0); true();; }. This is equivalent to the following, where 2863311531 is the multiplicative; inverse of 3, and 1431655766 is ((2^32)-1)/3+1:; void bar(unsigned n) {; if (n * 2863311531U < 1431655766U); true();; }. The same transformation can work with an even modulo with the addition of a; rotate: rotate the result of the multiply to the right by the number of bits; which need to be zero for the condition to be true, and shrink the compare RHS; by the same amount. Unless the target supports rotates, though, that; transformation probably isn't worthwhile. The transformation can also easily be made to work with non-zero equality; comparisons: just transform, for example, ""n % 3 == 1"" to ""(n-1) % 3 == 0"". //===---------------------------------------------------------------------===//. Better mod/ref analysis for scanf would allow us to eliminate the vtable and a; bunch of other stuff from this example (see PR1604): . #include <cstdio>; struct test {; int val;; virtual ~test() {}; };. int main() {; test t;; std::scanf(""%d"", &t.val);; std::printf(""%d\n"", t.val);; }. //===---------------------------------------------------------------------===//. These functions perform the same computation, but produce different assembly. define i8 @select(i8 %x) readnone nounwind {; %A = icmp ult i8 %x, 250; %B = select i1 %A, i8 0, i8 1; ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:20643,Testability,test,test,20643,". void bar(unsigned n) {; if (n % 3 == 0); true();; }. This is equivalent to the following, where 2863311531 is the multiplicative; inverse of 3, and 1431655766 is ((2^32)-1)/3+1:; void bar(unsigned n) {; if (n * 2863311531U < 1431655766U); true();; }. The same transformation can work with an even modulo with the addition of a; rotate: rotate the result of the multiply to the right by the number of bits; which need to be zero for the condition to be true, and shrink the compare RHS; by the same amount. Unless the target supports rotates, though, that; transformation probably isn't worthwhile. The transformation can also easily be made to work with non-zero equality; comparisons: just transform, for example, ""n % 3 == 1"" to ""(n-1) % 3 == 0"". //===---------------------------------------------------------------------===//. Better mod/ref analysis for scanf would allow us to eliminate the vtable and a; bunch of other stuff from this example (see PR1604): . #include <cstdio>; struct test {; int val;; virtual ~test() {}; };. int main() {; test t;; std::scanf(""%d"", &t.val);; std::printf(""%d\n"", t.val);; }. //===---------------------------------------------------------------------===//. These functions perform the same computation, but produce different assembly. define i8 @select(i8 %x) readnone nounwind {; %A = icmp ult i8 %x, 250; %B = select i1 %A, i8 0, i8 1; ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:20672,Testability,test,test,20672,"his is equivalent to the following, where 2863311531 is the multiplicative; inverse of 3, and 1431655766 is ((2^32)-1)/3+1:; void bar(unsigned n) {; if (n * 2863311531U < 1431655766U); true();; }. The same transformation can work with an even modulo with the addition of a; rotate: rotate the result of the multiply to the right by the number of bits; which need to be zero for the condition to be true, and shrink the compare RHS; by the same amount. Unless the target supports rotates, though, that; transformation probably isn't worthwhile. The transformation can also easily be made to work with non-zero equality; comparisons: just transform, for example, ""n % 3 == 1"" to ""(n-1) % 3 == 0"". //===---------------------------------------------------------------------===//. Better mod/ref analysis for scanf would allow us to eliminate the vtable and a; bunch of other stuff from this example (see PR1604): . #include <cstdio>; struct test {; int val;; virtual ~test() {}; };. int main() {; test t;; std::scanf(""%d"", &t.val);; std::printf(""%d\n"", t.val);; }. //===---------------------------------------------------------------------===//. These functions perform the same computation, but produce different assembly. define i8 @select(i8 %x) readnone nounwind {; %A = icmp ult i8 %x, 250; %B = select i1 %A, i8 0, i8 1; ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===--------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:29497,Testability,test,test,29497,"ction GVN hoists the fully redundant value of *res, but nothing; moves the store out. This gives us this code:. bb:		; preds = %bb2, %entry; 	%.rle = phi i32 [ 0, %entry ], [ %.rle6, %bb2 ]	; 	%i.05 = phi i32 [ 0, %entry ], [ %indvar.next, %bb2 ]; 	%1 = load i32* %cond, align 4; 	%2 = icmp eq i32 %1, 0; 	br i1 %2, label %bb2, label %bb1. bb1:		; preds = %bb; 	%3 = xor i32 %.rle, 234	; 	store i32 %3, i32* %res, align 4; 	br label %bb2. bb2:		; preds = %bb, %bb1; 	%.rle6 = phi i32 [ %3, %bb1 ], [ %.rle, %bb ]	; 	%indvar.next = add i32 %i.05, 1	; 	%exitcond = icmp eq i32 %indvar.next, %n; 	br i1 %exitcond, label %return, label %bb. DSE should sink partially dead stores to get the store out of the loop. Here's another partial dead case:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=12395. //===---------------------------------------------------------------------===//. Scalar PRE hoists the mul in the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * c + g;; return d + e;; }. It would be better to do the mul once to reduce codesize above the if.; This is GCC PR38204. //===---------------------------------------------------------------------===//; This simple function from 179.art:. int winner, numf2s;; struct { double y; int reset; } *Y;. void find_match() {; int i;; winner = 0;; for (i=0;i<numf2s;i++); if (Y[i].y > Y[winner].y); winner =i;; }. Compiles into (with clang TBAA):. for.body: ; preds = %for.inc, %bb.nph; %indvar = phi i64 [ 0, %bb.nph ], [ %indvar.next, %for.inc ]; %i.01718 = phi i32 [ 0, %bb.nph ], [ %i.01719, %for.inc ]; %tmp4 = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33440,Testability,test,testcases,33440,"nk the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; stor",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33453,Testability,test,testsuite,33453,"nk the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; stor",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33500,Testability,test,testsuite,33500,"---------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:34304,Testability,benchmark,benchmark,34304,"----------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:34714,Testability,test,testcases,34714,"nal increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:34727,Testability,test,testsuite,34727,"nal increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:34777,Testability,test,testsuite,34777,"-------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; re",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:34905,Testability,test,testsuite,34905,"e from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:34954,Testability,test,testsuite,34954,"p, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:35861,Testability,Benchmark,Benchmarks,35861,"------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS. //===---------------------------------------------------------------------===//. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19633; We could eliminate the branch condition here, loading from null is undefined:. struct S { int w, x, y, z; };; struct T { int r; struct S s; };; void bar (struct S, int);; void foo (int a, struct T b); {; struct S *c = 0;; if (a); c = &b.s;; bar (*c, a);; }. //===---------------------------------------------------------------------===//. simplifylibcalls should do several optimizations for strspn/strcspn:. s",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:42807,Testability,test,test,42807,"d forward substituting; the string directly into the printf, this eliminates reads from globalarray.; Since this pattern occurs frequently in crafty (due to the ""DisplayTime"" and; other similar functions) there are many stores to ""out"". Once all the printfs; stop using ""out"", all that is left is the memcpy's into it. This should allow; globalopt to remove the ""stored only"" global. //===---------------------------------------------------------------------===//. This code:. define inreg i32 @foo(i8* inreg %p) nounwind {; %tmp0 = load i8* %p; %tmp1 = ashr i8 %tmp0, 5; %tmp2 = sext i8 %tmp1 to i32; ret i32 %tmp2; }. could be dagcombine'd to a sign-extending load with a shift.; For example, on x86 this currently gets this:. 	movb	(%eax), %al; 	sarb	$5, %al; 	movsbl	%al, %eax. while it could get this:. 	movsbl	(%eax), %eax; 	sarl	$5, %eax. //===---------------------------------------------------------------------===//. GCC PR31029:. int test(int x) { return 1-x == x; } // --> return false; int test2(int x) { return 2-x == x; } // --> return x == 1 ?. Always foldable for odd constants, what is the rule for even?. //===---------------------------------------------------------------------===//. PR 3381: GEP to field of size 0 inside a struct could be turned into GEP; for next field in struct (which is at same address). For example: store of float into { {{}}, float } could be turned into a store to; the float directly. //===---------------------------------------------------------------------===//. The arg promotion pass should make use of nocapture to make its alias analysis; stuff much more precise. //===---------------------------------------------------------------------===//. The following functions should be optimized to use a select instead of a; branch (from gcc PR40072):. char char_int(int m) {if(m>7) return 0; return m;}; int int_char(char m) {if(m>7) return 0; return m;}. //===---------------------------------------------------------------------===//. int func(int",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:45858,Testability,test,test,45858,") | (a & 0x40);; b = (b & ~0x80) | (a & 0x80);; return (b);; }. define zeroext i8 @p1(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %1 = and i8 %a, -64 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; ret i8 %2; }. define zeroext i8 @p2(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %.masked = and i8 %a, 64 ; <i8> [#uses=1]; %1 = and i8 %a, -128 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; %3 = or i8 %2, %.masked ; <i8> [#uses=1]; ret i8 %3; }. //===---------------------------------------------------------------------===//. IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------------------------------------------===//. The libcall constant folding stuff should be moved out of SimplifyLibcalls into; libanalysis' constantfolding logic. This would allow IPSCCP to be able to; handle simple things like this:. static int foo(const char *X) { return strlen(X); }; int bar() { return foo(""abcd""); }. //===---------------------------------------------------------------------===//. function-attrs doesn't know much about memcpy/mem",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:46073,Testability,test,test,46073,"; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %1 = and i8 %a, -64 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; ret i8 %2; }. define zeroext i8 @p2(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %.masked = and i8 %a, 64 ; <i8> [#uses=1]; %1 = and i8 %a, -128 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; %3 = or i8 %2, %.masked ; <i8> [#uses=1]; ret i8 %3; }. //===---------------------------------------------------------------------===//. IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------------------------------------------===//. The libcall constant folding stuff should be moved out of SimplifyLibcalls into; libanalysis' constantfolding logic. This would allow IPSCCP to be able to; handle simple things like this:. static int foo(const char *X) { return strlen(X); }; int bar() { return foo(""abcd""); }. //===---------------------------------------------------------------------===//. function-attrs doesn't know much about memcpy/memset. This function should be; marked readnone rather than readonly, since it only twiddles local memory, but; function-attrs doesn't handle",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:46634,Testability,log,logic,46634," IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------------------------------------------===//. The libcall constant folding stuff should be moved out of SimplifyLibcalls into; libanalysis' constantfolding logic. This would allow IPSCCP to be able to; handle simple things like this:. static int foo(const char *X) { return strlen(X); }; int bar() { return foo(""abcd""); }. //===---------------------------------------------------------------------===//. function-attrs doesn't know much about memcpy/memset. This function should be; marked readnone rather than readonly, since it only twiddles local memory, but; function-attrs doesn't handle memset/memcpy/memmove aggressively:. struct X { int *p; int *q; };; int foo() {; int i = 0, j = 1;; struct X x, y;; int **p;; y.p = &i;; x.q = &j;; p = __builtin_memcpy (&x, &y, sizeof (int *));; return **p;; }. This can be seen at:; $ clang t.c -S -o - -mkernel -O0 -emit-llvm | opt -function-attrs -S. //===---------------------------------------------------------------------===//. Missed instcombine transformation:; define i1 @a(i32 %x) nounwind readnone {; entry:; %cmp = icmp eq i32 %x, 30; %sub = add i3",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:47709,Testability,Test,Testcase,47709,"nt foo(const char *X) { return strlen(X); }; int bar() { return foo(""abcd""); }. //===---------------------------------------------------------------------===//. function-attrs doesn't know much about memcpy/memset. This function should be; marked readnone rather than readonly, since it only twiddles local memory, but; function-attrs doesn't handle memset/memcpy/memmove aggressively:. struct X { int *p; int *q; };; int foo() {; int i = 0, j = 1;; struct X x, y;; int **p;; y.p = &i;; x.q = &j;; p = __builtin_memcpy (&x, &y, sizeof (int *));; return **p;; }. This can be seen at:; $ clang t.c -S -o - -mkernel -O0 -emit-llvm | opt -function-attrs -S. //===---------------------------------------------------------------------===//. Missed instcombine transformation:; define i1 @a(i32 %x) nounwind readnone {; entry:; %cmp = icmp eq i32 %x, 30; %sub = add i32 %x, -30; %cmp2 = icmp ugt i32 %sub, 9; %or = or i1 %cmp, %cmp2; ret i1 %or; }; This should be optimized to a single compare. Testcase derived from gcc. //===---------------------------------------------------------------------===//. Missed instcombine or reassociate transformation:; int a(int a, int b) { return (a==12)&(b>47)&(b<58); }. The sgt and slt should be combined into a single comparison. Testcase derived; from gcc. //===---------------------------------------------------------------------===//. Missed instcombine transformation:. %382 = srem i32 %tmp14.i, 64 ; [#uses=1]; %383 = zext i32 %382 to i64 ; [#uses=1]; %384 = shl i64 %381, %383 ; [#uses=1]; %385 = icmp slt i32 %tmp14.i, 64 ; [#uses=1]. The srem can be transformed to an and because if %tmp14.i is negative, the; shift is undefined. Testcase derived from 403.gcc. //===---------------------------------------------------------------------===//. This is a range comparison on a divided result (from 403.gcc):. %1337 = sdiv i32 %1336, 8 ; [#uses=1]; %.off.i208 = add i32 %1336, 7 ; [#uses=1]; %1338 = icmp ult i32 %.off.i208, 15 ; [#uses=1]; ; We already catch thi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:47984,Testability,Test,Testcase,47984,", since it only twiddles local memory, but; function-attrs doesn't handle memset/memcpy/memmove aggressively:. struct X { int *p; int *q; };; int foo() {; int i = 0, j = 1;; struct X x, y;; int **p;; y.p = &i;; x.q = &j;; p = __builtin_memcpy (&x, &y, sizeof (int *));; return **p;; }. This can be seen at:; $ clang t.c -S -o - -mkernel -O0 -emit-llvm | opt -function-attrs -S. //===---------------------------------------------------------------------===//. Missed instcombine transformation:; define i1 @a(i32 %x) nounwind readnone {; entry:; %cmp = icmp eq i32 %x, 30; %sub = add i32 %x, -30; %cmp2 = icmp ugt i32 %sub, 9; %or = or i1 %cmp, %cmp2; ret i1 %or; }; This should be optimized to a single compare. Testcase derived from gcc. //===---------------------------------------------------------------------===//. Missed instcombine or reassociate transformation:; int a(int a, int b) { return (a==12)&(b>47)&(b<58); }. The sgt and slt should be combined into a single comparison. Testcase derived; from gcc. //===---------------------------------------------------------------------===//. Missed instcombine transformation:. %382 = srem i32 %tmp14.i, 64 ; [#uses=1]; %383 = zext i32 %382 to i64 ; [#uses=1]; %384 = shl i64 %381, %383 ; [#uses=1]; %385 = icmp slt i32 %tmp14.i, 64 ; [#uses=1]. The srem can be transformed to an and because if %tmp14.i is negative, the; shift is undefined. Testcase derived from 403.gcc. //===---------------------------------------------------------------------===//. This is a range comparison on a divided result (from 403.gcc):. %1337 = sdiv i32 %1336, 8 ; [#uses=1]; %.off.i208 = add i32 %1336, 7 ; [#uses=1]; %1338 = icmp ult i32 %.off.i208, 15 ; [#uses=1]; ; We already catch this (removing the sdiv) if there isn't an add, we should; handle the 'add' as well. This is a common idiom with it's builtin_alloca code.; C testcase:. int a(int x) { return (unsigned)(x/16+7) < 15; }. Another similar case involves truncations on 64-bit targets:. %361 = sdiv i",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:48393,Testability,Test,Testcase,48393,"--------------------------------------------===//. Missed instcombine transformation:; define i1 @a(i32 %x) nounwind readnone {; entry:; %cmp = icmp eq i32 %x, 30; %sub = add i32 %x, -30; %cmp2 = icmp ugt i32 %sub, 9; %or = or i1 %cmp, %cmp2; ret i1 %or; }; This should be optimized to a single compare. Testcase derived from gcc. //===---------------------------------------------------------------------===//. Missed instcombine or reassociate transformation:; int a(int a, int b) { return (a==12)&(b>47)&(b<58); }. The sgt and slt should be combined into a single comparison. Testcase derived; from gcc. //===---------------------------------------------------------------------===//. Missed instcombine transformation:. %382 = srem i32 %tmp14.i, 64 ; [#uses=1]; %383 = zext i32 %382 to i64 ; [#uses=1]; %384 = shl i64 %381, %383 ; [#uses=1]; %385 = icmp slt i32 %tmp14.i, 64 ; [#uses=1]. The srem can be transformed to an and because if %tmp14.i is negative, the; shift is undefined. Testcase derived from 403.gcc. //===---------------------------------------------------------------------===//. This is a range comparison on a divided result (from 403.gcc):. %1337 = sdiv i32 %1336, 8 ; [#uses=1]; %.off.i208 = add i32 %1336, 7 ; [#uses=1]; %1338 = icmp ult i32 %.off.i208, 15 ; [#uses=1]; ; We already catch this (removing the sdiv) if there isn't an add, we should; handle the 'add' as well. This is a common idiom with it's builtin_alloca code.; C testcase:. int a(int x) { return (unsigned)(x/16+7) < 15; }. Another similar case involves truncations on 64-bit targets:. %361 = sdiv i64 %.046, 8 ; [#uses=1]; %362 = trunc i64 %361 to i32 ; [#uses=2]; ...; %367 = icmp eq i32 %362, 0 ; [#uses=1]. //===---------------------------------------------------------------------===//. Missed instcombine/dagcombine transformation:; define void @lshift_lt(i8 zeroext %a) nounwind {; entry:; %conv = zext i8 %a to i32; %shl = shl i32 %conv, 3; %cmp = icmp ult i32 %shl, 33; br i1 %cmp, label %if.then, l",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:48861,Testability,test,testcase,48861,"n:; int a(int a, int b) { return (a==12)&(b>47)&(b<58); }. The sgt and slt should be combined into a single comparison. Testcase derived; from gcc. //===---------------------------------------------------------------------===//. Missed instcombine transformation:. %382 = srem i32 %tmp14.i, 64 ; [#uses=1]; %383 = zext i32 %382 to i64 ; [#uses=1]; %384 = shl i64 %381, %383 ; [#uses=1]; %385 = icmp slt i32 %tmp14.i, 64 ; [#uses=1]. The srem can be transformed to an and because if %tmp14.i is negative, the; shift is undefined. Testcase derived from 403.gcc. //===---------------------------------------------------------------------===//. This is a range comparison on a divided result (from 403.gcc):. %1337 = sdiv i32 %1336, 8 ; [#uses=1]; %.off.i208 = add i32 %1336, 7 ; [#uses=1]; %1338 = icmp ult i32 %.off.i208, 15 ; [#uses=1]; ; We already catch this (removing the sdiv) if there isn't an add, we should; handle the 'add' as well. This is a common idiom with it's builtin_alloca code.; C testcase:. int a(int x) { return (unsigned)(x/16+7) < 15; }. Another similar case involves truncations on 64-bit targets:. %361 = sdiv i64 %.046, 8 ; [#uses=1]; %362 = trunc i64 %361 to i32 ; [#uses=2]; ...; %367 = icmp eq i32 %362, 0 ; [#uses=1]. //===---------------------------------------------------------------------===//. Missed instcombine/dagcombine transformation:; define void @lshift_lt(i8 zeroext %a) nounwind {; entry:; %conv = zext i8 %a to i32; %shl = shl i32 %conv, 3; %cmp = icmp ult i32 %shl, 33; br i1 %cmp, label %if.then, label %if.end. if.then:; tail call void @bar() nounwind; ret void. if.end:; ret void; }; declare void @bar() nounwind. The shift should be eliminated. Testcase derived from gcc. //===---------------------------------------------------------------------===//. These compile into different code, one gets recognized as a switch and the; other doesn't due to phase ordering issues (PR6212):. int test1(int mainType, int subType) {; if (mainType == 7); subType = ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:49556,Testability,Test,Testcase,49556," %1337 = sdiv i32 %1336, 8 ; [#uses=1]; %.off.i208 = add i32 %1336, 7 ; [#uses=1]; %1338 = icmp ult i32 %.off.i208, 15 ; [#uses=1]; ; We already catch this (removing the sdiv) if there isn't an add, we should; handle the 'add' as well. This is a common idiom with it's builtin_alloca code.; C testcase:. int a(int x) { return (unsigned)(x/16+7) < 15; }. Another similar case involves truncations on 64-bit targets:. %361 = sdiv i64 %.046, 8 ; [#uses=1]; %362 = trunc i64 %361 to i32 ; [#uses=2]; ...; %367 = icmp eq i32 %362, 0 ; [#uses=1]. //===---------------------------------------------------------------------===//. Missed instcombine/dagcombine transformation:; define void @lshift_lt(i8 zeroext %a) nounwind {; entry:; %conv = zext i8 %a to i32; %shl = shl i32 %conv, 3; %cmp = icmp ult i32 %shl, 33; br i1 %cmp, label %if.then, label %if.end. if.then:; tail call void @bar() nounwind; ret void. if.end:; ret void; }; declare void @bar() nounwind. The shift should be eliminated. Testcase derived from gcc. //===---------------------------------------------------------------------===//. These compile into different code, one gets recognized as a switch and the; other doesn't due to phase ordering issues (PR6212):. int test1(int mainType, int subType) {; if (mainType == 7); subType = 4;; else if (mainType == 9); subType = 6;; else if (mainType == 11); subType = 9;; return subType;; }. int test2(int mainType, int subType) {; if (mainType == 7); subType = 4;; if (mainType == 9); subType = 6;; if (mainType == 11); subType = 9;; return subType;; }. //===---------------------------------------------------------------------===//. The following test case (from PR6576):. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %cond1 = icmp eq i32 %b, 0 ; <i1> [#uses=1]; br i1 %cond1, label %exit, label %bb.nph; bb.nph: ; preds = %entry; %tmp = mul i32 %b, %a ; <i32> [#uses=1]; ret i32 %tmp; exit: ; preds = %entry; ret i32 0; }. could be reduced to:. define i32 @mul(i32 %a, i32 %",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:50225,Testability,test,test,50225,"tion:; define void @lshift_lt(i8 zeroext %a) nounwind {; entry:; %conv = zext i8 %a to i32; %shl = shl i32 %conv, 3; %cmp = icmp ult i32 %shl, 33; br i1 %cmp, label %if.then, label %if.end. if.then:; tail call void @bar() nounwind; ret void. if.end:; ret void; }; declare void @bar() nounwind. The shift should be eliminated. Testcase derived from gcc. //===---------------------------------------------------------------------===//. These compile into different code, one gets recognized as a switch and the; other doesn't due to phase ordering issues (PR6212):. int test1(int mainType, int subType) {; if (mainType == 7); subType = 4;; else if (mainType == 9); subType = 6;; else if (mainType == 11); subType = 9;; return subType;; }. int test2(int mainType, int subType) {; if (mainType == 7); subType = 4;; if (mainType == 9); subType = 6;; if (mainType == 11); subType = 9;; return subType;; }. //===---------------------------------------------------------------------===//. The following test case (from PR6576):. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %cond1 = icmp eq i32 %b, 0 ; <i1> [#uses=1]; br i1 %cond1, label %exit, label %bb.nph; bb.nph: ; preds = %entry; %tmp = mul i32 %b, %a ; <i32> [#uses=1]; ret i32 %tmp; exit: ; preds = %entry; ret i32 0; }. could be reduced to:. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = mul i32 %b, %a; ret i32 %tmp; }. //===---------------------------------------------------------------------===//. We should use DSE + llvm.lifetime.end to delete dead vtable pointer updates.; See GCC PR34949. Another interesting case is that something related could be used for variables; that go const after their ctor has finished. In these cases, globalopt (which; can statically run the constructor) could mark the global const (so it gets put; in the readonly section). A testcase would be:. #include <complex>; using namespace std;; const complex<char> should_be_in_rodata (42,-42);; complex<char> should_be_in_data ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:51085,Testability,test,testcase,51085,"); subType = 9;; return subType;; }. //===---------------------------------------------------------------------===//. The following test case (from PR6576):. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %cond1 = icmp eq i32 %b, 0 ; <i1> [#uses=1]; br i1 %cond1, label %exit, label %bb.nph; bb.nph: ; preds = %entry; %tmp = mul i32 %b, %a ; <i32> [#uses=1]; ret i32 %tmp; exit: ; preds = %entry; ret i32 0; }. could be reduced to:. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = mul i32 %b, %a; ret i32 %tmp; }. //===---------------------------------------------------------------------===//. We should use DSE + llvm.lifetime.end to delete dead vtable pointer updates.; See GCC PR34949. Another interesting case is that something related could be used for variables; that go const after their ctor has finished. In these cases, globalopt (which; can statically run the constructor) could mark the global const (so it gets put; in the readonly section). A testcase would be:. #include <complex>; using namespace std;; const complex<char> should_be_in_rodata (42,-42);; complex<char> should_be_in_data (42,-42);; complex<char> should_be_in_bss;. Where we currently evaluate the ctors but the globals don't become const because; the optimizer doesn't know they ""become const"" after the ctor is done. See; GCC PR4131 for more examples. //===---------------------------------------------------------------------===//. In this code:. long foo(long x) {; return x > 1 ? x : 1;; }. LLVM emits a comparison with 1 instead of 0. 0 would be equivalent; and cheaper on most targets. LLVM prefers comparisons with zero over non-zero in general, but in this; case it choses instead to keep the max operation obvious. //===---------------------------------------------------------------------===//. define void @a(i32 %x) nounwind {; entry:; switch i32 %x, label %if.end [; i32 0, label %if.then; i32 1, label %if.then; i32 2, label %if.then; i32 3, label %if.then; i32 5, label",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:53287,Testability,test,test,53287,"LBB0_2:; 	ret; .LBB0_3:; 	jmp	foo # TAILCALL. If we wanted to be really clever, we could simplify the whole thing to; something like the following, which eliminates a branch:; 	xorl $1, %edi; 	cmpl	$4, %edi; 	ja	.LBB0_2; 	ret; .LBB0_2:; 	jmp	foo # TAILCALL. //===---------------------------------------------------------------------===//. We compile this:. int foo(int a) { return (a & (~15)) / 16; }. Into:. define i32 @foo(i32 %a) nounwind readnone ssp {; entry:; %and = and i32 %a, -16; %div = sdiv i32 %and, 16; ret i32 %div; }. but this code (X & -A)/A is X >> log2(A) when A is a power of 2, so this case; should be instcombined into just ""a >> 4"". We do get this at the codegen level, so something knows about it, but ; instcombine should catch it earlier:. _foo: ## @foo; ## %bb.0: ## %entry; 	movl	%edi, %eax; 	sarl	$4, %eax; 	ret. //===---------------------------------------------------------------------===//. This code (from GCC PR28685):. int test(int a, int b) {; int lt = a < b;; int eq = a == b;; if (lt); return 1;; return eq;; }. Is compiled to:. define i32 @test(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %cmp = icmp slt i32 %a, %b; br i1 %cmp, label %return, label %if.end. if.end: ; preds = %entry; %cmp5 = icmp eq i32 %a, %b; %conv6 = zext i1 %cmp5 to i32; ret i32 %conv6. return: ; preds = %entry; ret i32 1; }. it could be:. define i32 @test__(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = icmp sle i32 %a, %b; %retval = zext i1 %0 to i32; ret i32 %retval; }. //===---------------------------------------------------------------------===//. This code can be seen in viterbi:. %64 = call noalias i8* @malloc(i64 %62) nounwind; ...; %67 = call i64 @llvm.objectsize.i64(i8* %64, i1 false) nounwind; %68 = call i8* @__memset_chk(i8* %64, i32 0, i64 %62, i64 %67) nounwind. llvm.objectsize.i64 should be taught about malloc/calloc, allowing it to; fold to %62. This is a security win (overflows of malloc will get caught); and also a performance win by exposing mor",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:53408,Testability,test,test,53408,"e the following, which eliminates a branch:; 	xorl $1, %edi; 	cmpl	$4, %edi; 	ja	.LBB0_2; 	ret; .LBB0_2:; 	jmp	foo # TAILCALL. //===---------------------------------------------------------------------===//. We compile this:. int foo(int a) { return (a & (~15)) / 16; }. Into:. define i32 @foo(i32 %a) nounwind readnone ssp {; entry:; %and = and i32 %a, -16; %div = sdiv i32 %and, 16; ret i32 %div; }. but this code (X & -A)/A is X >> log2(A) when A is a power of 2, so this case; should be instcombined into just ""a >> 4"". We do get this at the codegen level, so something knows about it, but ; instcombine should catch it earlier:. _foo: ## @foo; ## %bb.0: ## %entry; 	movl	%edi, %eax; 	sarl	$4, %eax; 	ret. //===---------------------------------------------------------------------===//. This code (from GCC PR28685):. int test(int a, int b) {; int lt = a < b;; int eq = a == b;; if (lt); return 1;; return eq;; }. Is compiled to:. define i32 @test(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %cmp = icmp slt i32 %a, %b; br i1 %cmp, label %return, label %if.end. if.end: ; preds = %entry; %cmp5 = icmp eq i32 %a, %b; %conv6 = zext i1 %cmp5 to i32; ret i32 %conv6. return: ; preds = %entry; ret i32 1; }. it could be:. define i32 @test__(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = icmp sle i32 %a, %b; %retval = zext i1 %0 to i32; ret i32 %retval; }. //===---------------------------------------------------------------------===//. This code can be seen in viterbi:. %64 = call noalias i8* @malloc(i64 %62) nounwind; ...; %67 = call i64 @llvm.objectsize.i64(i8* %64, i1 false) nounwind; %68 = call i8* @__memset_chk(i8* %64, i32 0, i64 %62, i64 %67) nounwind. llvm.objectsize.i64 should be taught about malloc/calloc, allowing it to; fold to %62. This is a security win (overflows of malloc will get caught); and also a performance win by exposing more memsets to the optimizer. This occurs several times in viterbi. Note that this would change the semantics of @llvm.objectsize whi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:62462,Testability,test,testfunc,62462,"-> 0.0 transformation above), it might be provable that the sign of the; expression doesn't matter. For example, by the above rules, we can't transform; fmul(sitofp(x), 0.0) into 0.0, because x might be -1 and the result of the; expression is defined to be -0.0. If we look at the uses of the fmul for example, we might be able to prove that; all uses don't care about the sign of zero. For example, if we have:. fadd(fmul(sitofp(x), 0.0), 2.0). Since we know that x+2.0 doesn't care about the sign of any zeros in X, we can; transform the fmul to 0.0, and then the fadd to 2.0. //===---------------------------------------------------------------------===//. We should enhance memcpy/memcpy/memset to allow a metadata node on them; indicating that some bytes of the transfer are undefined. This is useful for; frontends like clang when lowering struct copies, when some elements of the; struct are undefined. Consider something like this:. struct x {; char a;; int b[4];; };; void foo(struct x*P);; struct x testfunc() {; struct x V1, V2;; foo(&V1);; V2 = V1;. return V2;; }. We currently compile this to:; $ clang t.c -S -o - -O0 -emit-llvm | opt -sroa -S. %struct.x = type { i8, [4 x i32] }. define void @testfunc(%struct.x* sret %agg.result) nounwind ssp {; entry:; %V1 = alloca %struct.x, align 4; call void @foo(%struct.x* %V1); %tmp1 = bitcast %struct.x* %V1 to i8*; %0 = bitcast %struct.x* %V1 to i160*; %srcval1 = load i160* %0, align 4; %tmp2 = bitcast %struct.x* %agg.result to i8*; %1 = bitcast %struct.x* %agg.result to i160*; store i160 %srcval1, i160* %1, align 4; ret void; }. This happens because SRoA sees that the temp alloca has is being memcpy'd into; and out of and it has holes and it has to be conservative. If we knew about the; holes, then this could be much much better. Having information about these holes would also improve memcpy (etc) lowering at; llc time when it gets inlined, because we can use smaller transfers. This also; avoids partial register stalls in some im",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:62661,Testability,test,testfunc,62661," the result of the; expression is defined to be -0.0. If we look at the uses of the fmul for example, we might be able to prove that; all uses don't care about the sign of zero. For example, if we have:. fadd(fmul(sitofp(x), 0.0), 2.0). Since we know that x+2.0 doesn't care about the sign of any zeros in X, we can; transform the fmul to 0.0, and then the fadd to 2.0. //===---------------------------------------------------------------------===//. We should enhance memcpy/memcpy/memset to allow a metadata node on them; indicating that some bytes of the transfer are undefined. This is useful for; frontends like clang when lowering struct copies, when some elements of the; struct are undefined. Consider something like this:. struct x {; char a;; int b[4];; };; void foo(struct x*P);; struct x testfunc() {; struct x V1, V2;; foo(&V1);; V2 = V1;. return V2;; }. We currently compile this to:; $ clang t.c -S -o - -O0 -emit-llvm | opt -sroa -S. %struct.x = type { i8, [4 x i32] }. define void @testfunc(%struct.x* sret %agg.result) nounwind ssp {; entry:; %V1 = alloca %struct.x, align 4; call void @foo(%struct.x* %V1); %tmp1 = bitcast %struct.x* %V1 to i8*; %0 = bitcast %struct.x* %V1 to i160*; %srcval1 = load i160* %0, align 4; %tmp2 = bitcast %struct.x* %agg.result to i8*; %1 = bitcast %struct.x* %agg.result to i160*; store i160 %srcval1, i160* %1, align 4; ret void; }. This happens because SRoA sees that the temp alloca has is being memcpy'd into; and out of and it has holes and it has to be conservative. If we knew about the; holes, then this could be much much better. Having information about these holes would also improve memcpy (etc) lowering at; llc time when it gets inlined, because we can use smaller transfers. This also; avoids partial register stalls in some important cases. //===---------------------------------------------------------------------===//. We don't fold (icmp (add) (add)) unless the two adds only have a single use.; There are a lot of cases that we're",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:64340,Testability,benchmark,benchmarks,64340,"ve memcpy (etc) lowering at; llc time when it gets inlined, because we can use smaller transfers. This also; avoids partial register stalls in some important cases. //===---------------------------------------------------------------------===//. We don't fold (icmp (add) (add)) unless the two adds only have a single use.; There are a lot of cases that we're refusing to fold in (e.g.) 256.bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this because we don't want to introduce an overlapped live range; of the ivar. However if we can make this more aggressive without causing; performance issues in two ways:. 1. If *either* the LHS or RHS has a single use, we can definitely do the; transformation. In the overlapping liverange case we're trading one register; use for one fewer operation, which is a reasonable trade. Before doing this; we should verify that the llc output actually shrinks for some benchmarks.; 2. If both ops have multiple uses, we can still fold it if the operations are; both sinkable to *after* the icmp (e.g. in a subsequent block) which doesn't; increase register pressure. There are a ton of icmp's we aren't simplifying because of the reg pressure; concern. Care is warranted here though because many of these are induction; variables and other cases that matter a lot to performance, like the above.; Here's a blob of code that you can drop into the bottom of visitICmp to see some; missed cases:. { Value *A, *B, *C, *D;; if (match(Op0, m_Add(m_Value(A), m_Value(B))) && ; match(Op1, m_Add(m_Value(C), m_Value(D))) &&; (A == C || A == D || B == C || B == D)) {; errs() << ""OP0 = "" << *Op0 << "" U="" << Op0->getNumUses() << ""\n"";; errs() << ""OP1 = "" << *Op1 << "" U="" << Op1->getNumUses() << ""\n"";; errs() << ""CMP = "" << I << ""\n\n"";; }; }. //===---------------------------------------------------------------------===//. define i1 @test1(",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:66985,Testability,test,testl,66985," least):. %struct.t1 = type { i8, [3 x i8] }; @s2 = global %struct.t1 zeroinitializer, align 4; @s1 = global %struct.t1 zeroinitializer, align 4; define void @func1() nounwind ssp noredzone {; entry:; %0 = load i32* bitcast (%struct.t1* @s2 to i32*), align 4; %bf.val.sext5 = and i32 %0, 1; %1 = load i32* bitcast (%struct.t1* @s1 to i32*), align 4; %2 = and i32 %1, -4; %3 = or i32 %2, %bf.val.sext5; %bf.val.sext26 = and i32 %0, 2; %4 = or i32 %3, %bf.val.sext26; store i32 %4, i32* bitcast (%struct.t1* @s1 to i32*), align 4; ret void; }. The two or/and's should be merged into one each. //===---------------------------------------------------------------------===//. Machine level code hoisting can be useful in some cases. For example, PR9408; is about:. typedef union {; void (*f1)(int);; void (*f2)(long);; } funcs;. void foo(funcs f, int which) {; int a = 5;; if (which) {; f.f1(a);; } else {; f.f2(a);; }; }. which we compile to:. foo: # @foo; # %bb.0: # %entry; pushq %rbp; movq %rsp, %rbp; testl %esi, %esi; movq %rdi, %rax; je .LBB0_2; # %bb.1: # %if.then; movl $5, %edi; callq *%rax; popq %rbp; ret; .LBB0_2: # %if.else; movl $5, %edi; callq *%rax; popq %rbp; ret. Note that bb1 and bb2 are the same. This doesn't happen at the IR level; because one call is passing an i32 and the other is passing an i64. //===---------------------------------------------------------------------===//. I see this sort of pattern in 176.gcc in a few places (e.g. the start of; store_bit_field). The rem should be replaced with a multiply and subtract:. %3 = sdiv i32 %A, %B; %4 = srem i32 %A, %B. Similarly for udiv/urem. Note that this shouldn't be done on X86 or ARM,; which can do this in a single operation (instruction or libcall). It is; probably best to do this in the code generator. //===---------------------------------------------------------------------===//. unsigned foo(unsigned x, unsigned y) { return (x & y) == 0 || x == 0; }; should fold to (x & y) == 0. //===-----------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:2281,Usability,simpl,simple,2281,"//. Shrink: (setlt (loadi32 P), 0) -> (setlt (loadi8 Phi), 0). //===---------------------------------------------------------------------===//. Reassociate should turn things like:. int factorial(int X) {; return X*X*X*X*X*X*X*X;; }. into llvm.powi calls, allowing the code generator to produce balanced; multiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n) + bar(2*z, 2*n);; }. This is blocked on not handling X*X*X -> powi(X, 3) (see note above). The issue; is that we end up getting t = 2*X s = t*t and don't turn this into 4*X*X,; which is the same number of multiplies and is canonical, because the 2*X has; multiple uses. Here's a simple example:. define i32 @test15(i32 %X1) {; %B = mul i32 %X1, 47 ; X1*47; %C = mul i32 %B, %B; ret i32 %C; }. //===---------------------------------------------------------------------===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ; b3 = a3 + a2 + a1 + a0; ; b2 = a2 + a1 + a0; ; b1 = a1 + a0; ; } . This requires reassociating to forms of expressions that are already available,; something that reassoc doesn't think about yet. //===---------------------------------------------------------------------===//. These two functions should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===----------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:3464,Usability,simpl,simplify,3464,"--===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ; b3 = a3 + a2 + a1 + a0; ; b2 = a2 + a1 + a0; ; b1 = a1 + a0; ; } . This requires reassociating to forms of expressions that are already available,; something that reassoc doesn't think about yet. //===---------------------------------------------------------------------===//. These two functions should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===---------------------------------------------------------------------===//. It would be nice to revert this patch:; http://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20060213/031986.html. And teach the dag combiner enough to simplify the code expanded before ; legalize. It seems plausible that this knowledge would let it simplify other; stuff too. //===---------------------------------------------------------------------===//. For vector types, DataLayout.cpp::getTypeInfo() returns alignment that is equal; to the type size. It works but can be overly conservative as the alignment of; specific vector types are target dependent. //===---------------------------------------------------------------------===//. We should produce an unaligned load from code like this:. v4sf example(float *P) {; return (v4sf){P[0], P[1], P[2], P[3] };; }. //===---------------------------------------------------------------------===//. Add support for conditional increments, and other related patterns. Instead; of:. 	movl 136(%esp), %eax; 	cmpl $0, %eax; 	je LBB16_2	#cond_next; LBB16_1:	#cond_true; 	incl _foo; LBB16_2:	#cond_next. emit:; 	movl	_foo, %eax; 	cmpl	$1, %edi; 	sbbl	$-1, %eax; 	movl	%eax, _foo. //===------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:3562,Usability,simpl,simplify,3562,", a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ; b3 = a3 + a2 + a1 + a0; ; b2 = a2 + a1 + a0; ; b1 = a1 + a0; ; } . This requires reassociating to forms of expressions that are already available,; something that reassoc doesn't think about yet. //===---------------------------------------------------------------------===//. These two functions should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===---------------------------------------------------------------------===//. It would be nice to revert this patch:; http://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20060213/031986.html. And teach the dag combiner enough to simplify the code expanded before ; legalize. It seems plausible that this knowledge would let it simplify other; stuff too. //===---------------------------------------------------------------------===//. For vector types, DataLayout.cpp::getTypeInfo() returns alignment that is equal; to the type size. It works but can be overly conservative as the alignment of; specific vector types are target dependent. //===---------------------------------------------------------------------===//. We should produce an unaligned load from code like this:. v4sf example(float *P) {; return (v4sf){P[0], P[1], P[2], P[3] };; }. //===---------------------------------------------------------------------===//. Add support for conditional increments, and other related patterns. Instead; of:. 	movl 136(%esp), %eax; 	cmpl $0, %eax; 	je LBB16_2	#cond_next; LBB16_1:	#cond_true; 	incl _foo; LBB16_2:	#cond_next. emit:; 	movl	_foo, %eax; 	cmpl	$1, %edi; 	sbbl	$-1, %eax; 	movl	%eax, _foo. //===---------------------------------------------------------------------===//. Combine: a = sin(x), b = cos(",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:16022,Usability,simpl,simplification,16022,"align 16; %1 = getelementptr [8 x i64]* %input, i64 0, i64 2; store i64 1, i64* %1, align 16; %2 = getelementptr [8 x i64]* %input, i64 0, i64 4; store i64 1, i64* %2, align 16; %3 = getelementptr [8 x i64]* %input, i64 0, i64 6; store i64 1, i64* %3, align 16. Which gets codegen'd into:. 	pxor	%xmm0, %xmm0; 	movaps	%xmm0, -16(%rbp); 	movaps	%xmm0, -32(%rbp); 	movaps	%xmm0, -48(%rbp); 	movaps	%xmm0, -64(%rbp); 	movq	$1, -64(%rbp); 	movq	$1, -48(%rbp); 	movq	$1, -32(%rbp); 	movq	$1, -16(%rbp). It would be better to have 4 movq's of 0 instead of the movaps's. //===---------------------------------------------------------------------===//. http://llvm.org/PR717:. The following code should compile into ""ret int undef"". Instead, LLVM; produces ""ret int 0"":. int f() {; int x = 4;; int y;; if (x == 3) y = 0;; return y;; }. //===---------------------------------------------------------------------===//. The loop unroller should partially unroll loops (instead of peeling them); when code growth isn't too bad and when an unroll count allows simplification; of some code within the loop. One trivial example is:. #include <stdio.h>; int main() {; int nRet = 17;; int nLoop;; for ( nLoop = 0; nLoop < 1000; nLoop++ ) {; if ( nLoop & 1 ); nRet += 2;; else; nRet -= 1;; }; return nRet;; }. Unrolling by 2 would eliminate the '&1' in both copies, leading to a net; reduction in code size. The resultant code would then also be suitable for; exit value computation. //===---------------------------------------------------------------------===//. We miss a bunch of rotate opportunities on various targets, including ppc, x86,; etc. On X86, we miss a bunch of 'rotate by variable' cases because the rotate; matching code in dag combine doesn't look through truncates aggressively ; enough. Here are some testcases reduces from GCC PR17886:. unsigned long long f5(unsigned long long x, unsigned long long y) {; return (x << 8) | ((y >> 48) & 0xffull);; }; unsigned long long f6(unsigned long long x, u",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:18461,Usability,simpl,simplifications,18461,"o:. define i32 @foo(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %shl5 = shl i32 %conv, 16; %shl9 = shl i32 %conv, 24; %or = or i32 %shl9, %conv; %or6 = or i32 %or, %shl5; %or10 = or i32 %or6, %shl; ret i32 %or10; }. it would be better as:. unsigned int bar(unsigned char i) {; unsigned int j=i | (i << 8); ; return j | (j<<16);; }. aka:. define i32 @bar(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %or = or i32 %shl, %conv; %shl5 = shl i32 %or, 16; %or6 = or i32 %shl5, %or; ret i32 %or6; }. or even i*0x01010101, depending on the speed of the multiplier. The best way to; handle this is to canonicalize it to a multiply in IR and have codegen handle; lowering multiplies to shifts on cpus where shifts are faster. //===---------------------------------------------------------------------===//. We do a number of simplifications in simplify libcalls to strength reduce; standard library functions, but we don't currently merge them together. For; example, it is useful to merge memcpy(a,b,strlen(b)) -> strcpy. This can only; be done safely if ""b"" isn't modified between the strlen and memcpy of course. //===---------------------------------------------------------------------===//. We compile this program: (from GCC PR11680); http://gcc.gnu.org/bugzilla/attachment.cgi?id=4487. Into code that runs the same speed in fast/slow modes, but both modes run 2x; slower than when compile with GCC (either 4.0 or 4.2):. $ llvm-g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 1.821u 0.003s 0:01.82 100.0%	0+0k 0+0io 0pf+0w. $ g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 0.821u 0.001s 0:00.82 100.0%	0+0k 0+0io 0pf+0w. It looks like we are making the same inlining decisions, so this may be raw; codegen badness or something else (haven't investigated). //===---------------------------------------------------------------------===//. Divisibility by const",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:18480,Usability,simpl,simplify,18480,"o:. define i32 @foo(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %shl5 = shl i32 %conv, 16; %shl9 = shl i32 %conv, 24; %or = or i32 %shl9, %conv; %or6 = or i32 %or, %shl5; %or10 = or i32 %or6, %shl; ret i32 %or10; }. it would be better as:. unsigned int bar(unsigned char i) {; unsigned int j=i | (i << 8); ; return j | (j<<16);; }. aka:. define i32 @bar(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %or = or i32 %shl, %conv; %shl5 = shl i32 %or, 16; %or6 = or i32 %shl5, %or; ret i32 %or6; }. or even i*0x01010101, depending on the speed of the multiplier. The best way to; handle this is to canonicalize it to a multiply in IR and have codegen handle; lowering multiplies to shifts on cpus where shifts are faster. //===---------------------------------------------------------------------===//. We do a number of simplifications in simplify libcalls to strength reduce; standard library functions, but we don't currently merge them together. For; example, it is useful to merge memcpy(a,b,strlen(b)) -> strcpy. This can only; be done safely if ""b"" isn't modified between the strlen and memcpy of course. //===---------------------------------------------------------------------===//. We compile this program: (from GCC PR11680); http://gcc.gnu.org/bugzilla/attachment.cgi?id=4487. Into code that runs the same speed in fast/slow modes, but both modes run 2x; slower than when compile with GCC (either 4.0 or 4.2):. $ llvm-g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 1.821u 0.003s 0:01.82 100.0%	0+0k 0+0io 0pf+0w. $ g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 0.821u 0.001s 0:00.82 100.0%	0+0k 0+0io 0pf+0w. It looks like we are making the same inlining decisions, so this may be raw; codegen badness or something else (haven't investigated). //===---------------------------------------------------------------------===//. Divisibility by const",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:19527,Usability,simpl,simplified,19527,"don't currently merge them together. For; example, it is useful to merge memcpy(a,b,strlen(b)) -> strcpy. This can only; be done safely if ""b"" isn't modified between the strlen and memcpy of course. //===---------------------------------------------------------------------===//. We compile this program: (from GCC PR11680); http://gcc.gnu.org/bugzilla/attachment.cgi?id=4487. Into code that runs the same speed in fast/slow modes, but both modes run 2x; slower than when compile with GCC (either 4.0 or 4.2):. $ llvm-g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 1.821u 0.003s 0:01.82 100.0%	0+0k 0+0io 0pf+0w. $ g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 0.821u 0.001s 0:00.82 100.0%	0+0k 0+0io 0pf+0w. It looks like we are making the same inlining decisions, so this may be raw; codegen badness or something else (haven't investigated). //===---------------------------------------------------------------------===//. Divisibility by constant can be simplified (according to GCC PR12849) from; being a mulhi to being a mul lo (cheaper). Testcase:. void bar(unsigned n) {; if (n % 3 == 0); true();; }. This is equivalent to the following, where 2863311531 is the multiplicative; inverse of 3, and 1431655766 is ((2^32)-1)/3+1:; void bar(unsigned n) {; if (n * 2863311531U < 1431655766U); true();; }. The same transformation can work with an even modulo with the addition of a; rotate: rotate the result of the multiply to the right by the number of bits; which need to be zero for the condition to be true, and shrink the compare RHS; by the same amount. Unless the target supports rotates, though, that; transformation probably isn't worthwhile. The transformation can also easily be made to work with non-zero equality; comparisons: just transform, for example, ""n % 3 == 1"" to ""(n-1) % 3 == 0"". //===---------------------------------------------------------------------===//. Better mod/ref analysis for scanf would allow us to eliminate the vtable and a; bunch of other stuf",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:23183,Usability,simpl,simplify,23183,"i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:27969,Usability,simpl,simplified,27969,"mized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) {; int i;; *res = 0;; for (i = 0; i < n; i++); if (*cond); *res ^= 234; /* (*) */; }. On this function GVN hoists the fully redundant value of *res, but nothing; moves the store out. This gives us this code:. bb:		; preds = %bb2, %entry; 	%.rle = phi i32 [ 0, %entry ], [ %.rle6, %bb2 ]	; 	%i.05 = phi i32 [ 0, %entry ], [ %indvar.next, %bb2 ]; 	%1 = load i32* %cond, align 4; 	%2 = icmp eq i32 %1, 0; 	br i1 %2, label %bb2, label %bb1. bb1:		; preds = %bb; 	%3 = xor i32 %.rle, 234	; 	store i32 %3, i32* %res, align 4; 	br label %bb2",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:28051,Usability,simpl,simplifications,28051,"----------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) {; int i;; *res = 0;; for (i = 0; i < n; i++); if (*cond); *res ^= 234; /* (*) */; }. On this function GVN hoists the fully redundant value of *res, but nothing; moves the store out. This gives us this code:. bb:		; preds = %bb2, %entry; 	%.rle = phi i32 [ 0, %entry ], [ %.rle6, %bb2 ]	; 	%i.05 = phi i32 [ 0, %entry ], [ %indvar.next, %bb2 ]; 	%1 = load i32* %cond, align 4; 	%2 = icmp eq i32 %1, 0; 	br i1 %2, label %bb2, label %bb1. bb1:		; preds = %bb; 	%3 = xor i32 %.rle, 234	; 	store i32 %3, i32* %res, align 4; 	br label %bb2. bb2:		; preds = %bb, %bb1; 	%.rle6 = phi i32 [ %3, %bb1 ], [ %.rle, %bb ]	; 	%ind",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:29796,Usability,simpl,simple,29796,"0, %entry ], [ %indvar.next, %bb2 ]; 	%1 = load i32* %cond, align 4; 	%2 = icmp eq i32 %1, 0; 	br i1 %2, label %bb2, label %bb1. bb1:		; preds = %bb; 	%3 = xor i32 %.rle, 234	; 	store i32 %3, i32* %res, align 4; 	br label %bb2. bb2:		; preds = %bb, %bb1; 	%.rle6 = phi i32 [ %3, %bb1 ], [ %.rle, %bb ]	; 	%indvar.next = add i32 %i.05, 1	; 	%exitcond = icmp eq i32 %indvar.next, %n; 	br i1 %exitcond, label %return, label %bb. DSE should sink partially dead stores to get the store out of the loop. Here's another partial dead case:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=12395. //===---------------------------------------------------------------------===//. Scalar PRE hoists the mul in the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * c + g;; return d + e;; }. It would be better to do the mul once to reduce codesize above the if.; This is GCC PR38204. //===---------------------------------------------------------------------===//; This simple function from 179.art:. int winner, numf2s;; struct { double y; int reset; } *Y;. void find_match() {; int i;; winner = 0;; for (i=0;i<numf2s;i++); if (Y[i].y > Y[winner].y); winner =i;; }. Compiles into (with clang TBAA):. for.body: ; preds = %for.inc, %bb.nph; %indvar = phi i64 [ 0, %bb.nph ], [ %indvar.next, %for.inc ]; %i.01718 = phi i32 [ 0, %bb.nph ], [ %i.01719, %for.inc ]; %tmp4 = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i6",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:36779,Usability,simpl,simplifylibcalls,36779,"alloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS. //===---------------------------------------------------------------------===//. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19633; We could eliminate the branch condition here, loading from null is undefined:. struct S { int w, x, y, z; };; struct T { int r; struct S s; };; void bar (struct S, int);; void foo (int a, struct T b); {; struct S *c = 0;; if (a); c = &b.s;; bar (*c, a);; }. //===---------------------------------------------------------------------===//. simplifylibcalls should do several optimizations for strspn/strcspn:. strcspn(x, ""a"") -> inlined loop for up to 3 letters (similarly for strspn):. size_t __strcspn_c3 (__const char *__s, int __reject1, int __reject2,; int __reject3) {; register size_t __result = 0;; while (__s[__result] != '\0' && __s[__result] != __reject1 &&; __s[__result] != __reject2 && __s[__result] != __reject3); ++__result;; return __result;; }. This should turn into a switch on the character. See PR3253 for some notes on; codegen. 456.hmmer apparently uses strcspn and strspn a lot. 471.omnetpp uses strspn. //===---------------------------------------------------------------------===//. simplifylibcalls should turn these snprintf idioms into memcpy (GCC PR47917). char buf1[6], buf2[6], buf3[4], buf4[4];; int i;. int foo (void) {; int ret = snprintf (buf1, sizeof buf1, ""abcde"");; ret += snprintf (buf2, sizeof buf2, ""abcdef"") * 16;; ret += snprintf (buf3, sizeof buf3, ""%s"", i++ < 6 ? ""abc"" : ""def"") * 256;; ret += snprintf (buf4, sizeof buf4, ""%s"",",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:37448,Usability,simpl,simplifylibcalls,37448,"loading from null is undefined:. struct S { int w, x, y, z; };; struct T { int r; struct S s; };; void bar (struct S, int);; void foo (int a, struct T b); {; struct S *c = 0;; if (a); c = &b.s;; bar (*c, a);; }. //===---------------------------------------------------------------------===//. simplifylibcalls should do several optimizations for strspn/strcspn:. strcspn(x, ""a"") -> inlined loop for up to 3 letters (similarly for strspn):. size_t __strcspn_c3 (__const char *__s, int __reject1, int __reject2,; int __reject3) {; register size_t __result = 0;; while (__s[__result] != '\0' && __s[__result] != __reject1 &&; __s[__result] != __reject2 && __s[__result] != __reject3); ++__result;; return __result;; }. This should turn into a switch on the character. See PR3253 for some notes on; codegen. 456.hmmer apparently uses strcspn and strspn a lot. 471.omnetpp uses strspn. //===---------------------------------------------------------------------===//. simplifylibcalls should turn these snprintf idioms into memcpy (GCC PR47917). char buf1[6], buf2[6], buf3[4], buf4[4];; int i;. int foo (void) {; int ret = snprintf (buf1, sizeof buf1, ""abcde"");; ret += snprintf (buf2, sizeof buf2, ""abcdef"") * 16;; ret += snprintf (buf3, sizeof buf3, ""%s"", i++ < 6 ? ""abc"" : ""def"") * 256;; ret += snprintf (buf4, sizeof buf4, ""%s"", i++ > 10 ? ""abcde"" : ""defgh"")*4096;; return ret;; }. //===---------------------------------------------------------------------===//. ""gas"" uses this idiom:; else if (strchr (""+-/*%|&^:[]()~"", *intel_parser.op_string)); ..; else if (strchr (""<>"", *intel_parser.op_string). Those should be turned into a switch. SimplifyLibCalls only gets the second; case. //===---------------------------------------------------------------------===//. 252.eon contains this interesting code:. %3072 = getelementptr [100 x i8]* %tempString, i32 0, i32 0; %3073 = call i8* @strcpy(i8* %3072, i8* %3071) nounwind; %strlen = call i32 @strlen(i8* %3072) ; uses = 1; %endptr = getelementptr [1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:38125,Usability,Simpl,SimplifyLibCalls,38125,"esult] != __reject3); ++__result;; return __result;; }. This should turn into a switch on the character. See PR3253 for some notes on; codegen. 456.hmmer apparently uses strcspn and strspn a lot. 471.omnetpp uses strspn. //===---------------------------------------------------------------------===//. simplifylibcalls should turn these snprintf idioms into memcpy (GCC PR47917). char buf1[6], buf2[6], buf3[4], buf4[4];; int i;. int foo (void) {; int ret = snprintf (buf1, sizeof buf1, ""abcde"");; ret += snprintf (buf2, sizeof buf2, ""abcdef"") * 16;; ret += snprintf (buf3, sizeof buf3, ""%s"", i++ < 6 ? ""abc"" : ""def"") * 256;; ret += snprintf (buf4, sizeof buf4, ""%s"", i++ > 10 ? ""abcde"" : ""defgh"")*4096;; return ret;; }. //===---------------------------------------------------------------------===//. ""gas"" uses this idiom:; else if (strchr (""+-/*%|&^:[]()~"", *intel_parser.op_string)); ..; else if (strchr (""<>"", *intel_parser.op_string). Those should be turned into a switch. SimplifyLibCalls only gets the second; case. //===---------------------------------------------------------------------===//. 252.eon contains this interesting code:. %3072 = getelementptr [100 x i8]* %tempString, i32 0, i32 0; %3073 = call i8* @strcpy(i8* %3072, i8* %3071) nounwind; %strlen = call i32 @strlen(i8* %3072) ; uses = 1; %endptr = getelementptr [100 x i8]* %tempString, i32 0, i32 %strlen; call void @llvm.memcpy.i32(i8* %endptr, ; i8* getelementptr ([5 x i8]* @""\01LC42"", i32 0, i32 0), i32 5, i32 1); %3074 = call i32 @strlen(i8* %endptr) nounwind readonly ; ; This is interesting for a couple reasons. First, in this:. The memcpy+strlen strlen can be replaced with:. %3074 = call i32 @strlen([5 x i8]* @""\01LC42"") nounwind readonly . Because the destination was just copied into the specified memory buffer. This,; in turn, can be constant folded to ""4"". In other code, it contains:. %endptr6978 = bitcast i8* %endptr69 to i32* ; store i32 7107374, i32* %endptr6978, align 1; %3167 = call i32 @strlen(i8* ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:46183,Usability,simpl,simple,46183," ret i8 %2; }. define zeroext i8 @p2(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %.masked = and i8 %a, 64 ; <i8> [#uses=1]; %1 = and i8 %a, -128 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; %3 = or i8 %2, %.masked ; <i8> [#uses=1]; ret i8 %3; }. //===---------------------------------------------------------------------===//. IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------------------------------------------===//. The libcall constant folding stuff should be moved out of SimplifyLibcalls into; libanalysis' constantfolding logic. This would allow IPSCCP to be able to; handle simple things like this:. static int foo(const char *X) { return strlen(X); }; int bar() { return foo(""abcd""); }. //===---------------------------------------------------------------------===//. function-attrs doesn't know much about memcpy/memset. This function should be; marked readnone rather than readonly, since it only twiddles local memory, but; function-attrs doesn't handle memset/memcpy/memmove aggressively:. struct X { int *p; int *q; };; int foo() {; int i = 0, j = 1;; struct X x, y;; int ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:46582,Usability,Simpl,SimplifyLibcalls,46582," IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------------------------------------------===//. The libcall constant folding stuff should be moved out of SimplifyLibcalls into; libanalysis' constantfolding logic. This would allow IPSCCP to be able to; handle simple things like this:. static int foo(const char *X) { return strlen(X); }; int bar() { return foo(""abcd""); }. //===---------------------------------------------------------------------===//. function-attrs doesn't know much about memcpy/memset. This function should be; marked readnone rather than readonly, since it only twiddles local memory, but; function-attrs doesn't handle memset/memcpy/memmove aggressively:. struct X { int *p; int *q; };; int foo() {; int i = 0, j = 1;; struct X x, y;; int **p;; y.p = &i;; x.q = &j;; p = __builtin_memcpy (&x, &y, sizeof (int *));; return **p;; }. This can be seen at:; $ clang t.c -S -o - -mkernel -O0 -emit-llvm | opt -function-attrs -S. //===---------------------------------------------------------------------===//. Missed instcombine transformation:; define i1 @a(i32 %x) nounwind readnone {; entry:; %cmp = icmp eq i32 %x, 30; %sub = add i3",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:46687,Usability,simpl,simple,46687,"oes not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------------------------------------------===//. The libcall constant folding stuff should be moved out of SimplifyLibcalls into; libanalysis' constantfolding logic. This would allow IPSCCP to be able to; handle simple things like this:. static int foo(const char *X) { return strlen(X); }; int bar() { return foo(""abcd""); }. //===---------------------------------------------------------------------===//. function-attrs doesn't know much about memcpy/memset. This function should be; marked readnone rather than readonly, since it only twiddles local memory, but; function-attrs doesn't handle memset/memcpy/memmove aggressively:. struct X { int *p; int *q; };; int foo() {; int i = 0, j = 1;; struct X x, y;; int **p;; y.p = &i;; x.q = &j;; p = __builtin_memcpy (&x, &y, sizeof (int *));; return **p;; }. This can be seen at:; $ clang t.c -S -o - -mkernel -O0 -emit-llvm | opt -function-attrs -S. //===---------------------------------------------------------------------===//. Missed instcombine transformation:; define i1 @a(i32 %x) nounwind readnone {; entry:; %cmp = icmp eq i32 %x, 30; %sub = add i32 %x, -30; %cmp2 = icmp ugt i32 %sub, 9; %or = or i1 %cmp, %cmp2; ret i1 %or; }; This should b",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:52419,Usability,simpl,simplify,52419,"s. //===---------------------------------------------------------------------===//. In this code:. long foo(long x) {; return x > 1 ? x : 1;; }. LLVM emits a comparison with 1 instead of 0. 0 would be equivalent; and cheaper on most targets. LLVM prefers comparisons with zero over non-zero in general, but in this; case it choses instead to keep the max operation obvious. //===---------------------------------------------------------------------===//. define void @a(i32 %x) nounwind {; entry:; switch i32 %x, label %if.end [; i32 0, label %if.then; i32 1, label %if.then; i32 2, label %if.then; i32 3, label %if.then; i32 5, label %if.then; ]; if.then:; tail call void @foo() nounwind; ret void; if.end:; ret void; }; declare void @foo(). Generated code on x86-64 (other platforms give similar results):; a:; 	cmpl	$5, %edi; 	ja	LBB2_2; 	cmpl	$4, %edi; 	jne	LBB2_3; .LBB0_2:; 	ret; .LBB0_3:; 	jmp	foo # TAILCALL. If we wanted to be really clever, we could simplify the whole thing to; something like the following, which eliminates a branch:; 	xorl $1, %edi; 	cmpl	$4, %edi; 	ja	.LBB0_2; 	ret; .LBB0_2:; 	jmp	foo # TAILCALL. //===---------------------------------------------------------------------===//. We compile this:. int foo(int a) { return (a & (~15)) / 16; }. Into:. define i32 @foo(i32 %a) nounwind readnone ssp {; entry:; %and = and i32 %a, -16; %div = sdiv i32 %and, 16; ret i32 %div; }. but this code (X & -A)/A is X >> log2(A) when A is a power of 2, so this case; should be instcombined into just ""a >> 4"". We do get this at the codegen level, so something knows about it, but ; instcombine should catch it earlier:. _foo: ## @foo; ## %bb.0: ## %entry; 	movl	%edi, %eax; 	sarl	$4, %eax; 	ret. //===---------------------------------------------------------------------===//. This code (from GCC PR28685):. int test(int a, int b) {; int lt = a < b;; int eq = a == b;; if (lt); return 1;; return eq;; }. Is compiled to:. define i32 @test(i32 %a, i32 %b) nounwind readnone ssp {; entry",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:58227,Usability,simpl,simplified,58227,"en.i.i.i.i.i, label %_ZNSt12_Vector_baseIiSaIiEEC2EmRKS0_.exit.i.i. if.then.i.i.i.i.i: ; preds = %cond.true.i.i.i.i; call void @_ZSt17__throw_bad_allocv() noreturn nounwind; unreachable. _ZNSt12_Vector_baseIiSaIiEEC2EmRKS0_.exit.i.i: ; preds = %cond.true.i.i.i.i; %mul.i.i.i.i.i = shl i64 %conv, 2; %call3.i.i.i.i.i = call noalias i8* @_Znwm(i64 %mul.i.i.i.i.i) nounwind; %0 = bitcast i8* %call3.i.i.i.i.i to i32*; store i32* %0, i32** %v2.sub, align 8, !tbaa !0; store i32* %0, i32** %tmp3.i.i.i.i.i, align 8, !tbaa !0; %add.ptr.i.i.i = getelementptr inbounds i32* %0, i64 %conv; store i32* %add.ptr.i.i.i, i32** %tmp4.i.i.i.i.i, align 8, !tbaa !0; call void @llvm.memset.p0i8.i64(i8* %call3.i.i.i.i.i, i8 0, i64 %mul.i.i.i.i.i, i32 4, i1 false); br label %_ZNSt6vectorIiSaIiEEC1EmRKiRKS0_.exit. This is just the handling the construction of the vector. Most surprising here; is the fact that all three null stores in %entry are dead (because we do no; cross-block DSE). Also surprising is that %conv isn't simplified to 0 in %....exit.thread.i.i.; This is a because the client of LazyValueInfo doesn't simplify all instruction; operands, just selected ones. //===---------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. void f(char* a, int n) {; __builtin_memset(a, 0, n);; for (int i = 0; i < n; ++i); a[i] = 0;; }. into:. define void @_Z1fPci(i8* nocapture %a, i32 %n) nounwind {; entry:; %conv = sext i32 %n to i64; tail call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %conv, i32 1, i1 false); %cmp8 = icmp sgt i32 %n, 0; br i1 %cmp8, label %for.body.lr.ph, label %for.end. for.body.lr.ph: ; preds = %entry; %tmp10 = add i32 %n, -1; %tmp11 = zext i32 %tmp10 to i64; %tmp12 = add i64 %tmp11, 1; call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %tmp12, i32 1, i1 false); ret void. for.end: ; preds = %entry; ret void; }. This shouldn't need the ((zext (%n - 1)) + 1) game, and it should ideally fold; the two memset's ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:58323,Usability,simpl,simplify,58323,"true.i.i.i.i; call void @_ZSt17__throw_bad_allocv() noreturn nounwind; unreachable. _ZNSt12_Vector_baseIiSaIiEEC2EmRKS0_.exit.i.i: ; preds = %cond.true.i.i.i.i; %mul.i.i.i.i.i = shl i64 %conv, 2; %call3.i.i.i.i.i = call noalias i8* @_Znwm(i64 %mul.i.i.i.i.i) nounwind; %0 = bitcast i8* %call3.i.i.i.i.i to i32*; store i32* %0, i32** %v2.sub, align 8, !tbaa !0; store i32* %0, i32** %tmp3.i.i.i.i.i, align 8, !tbaa !0; %add.ptr.i.i.i = getelementptr inbounds i32* %0, i64 %conv; store i32* %add.ptr.i.i.i, i32** %tmp4.i.i.i.i.i, align 8, !tbaa !0; call void @llvm.memset.p0i8.i64(i8* %call3.i.i.i.i.i, i8 0, i64 %mul.i.i.i.i.i, i32 4, i1 false); br label %_ZNSt6vectorIiSaIiEEC1EmRKiRKS0_.exit. This is just the handling the construction of the vector. Most surprising here; is the fact that all three null stores in %entry are dead (because we do no; cross-block DSE). Also surprising is that %conv isn't simplified to 0 in %....exit.thread.i.i.; This is a because the client of LazyValueInfo doesn't simplify all instruction; operands, just selected ones. //===---------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. void f(char* a, int n) {; __builtin_memset(a, 0, n);; for (int i = 0; i < n; ++i); a[i] = 0;; }. into:. define void @_Z1fPci(i8* nocapture %a, i32 %n) nounwind {; entry:; %conv = sext i32 %n to i64; tail call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %conv, i32 1, i1 false); %cmp8 = icmp sgt i32 %n, 0; br i1 %cmp8, label %for.body.lr.ph, label %for.end. for.body.lr.ph: ; preds = %entry; %tmp10 = add i32 %n, -1; %tmp11 = zext i32 %tmp10 to i64; %tmp12 = add i64 %tmp11, 1; call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %tmp12, i32 1, i1 false); ret void. for.end: ; preds = %entry; ret void; }. This shouldn't need the ((zext (%n - 1)) + 1) game, and it should ideally fold; the two memset's together. The issue with the addition only occurs in 64-bit mode, and appears to be at; least partially ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:64574,Usability,simpl,simplifying,64574,")) unless the two adds only have a single use.; There are a lot of cases that we're refusing to fold in (e.g.) 256.bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this because we don't want to introduce an overlapped live range; of the ivar. However if we can make this more aggressive without causing; performance issues in two ways:. 1. If *either* the LHS or RHS has a single use, we can definitely do the; transformation. In the overlapping liverange case we're trading one register; use for one fewer operation, which is a reasonable trade. Before doing this; we should verify that the llc output actually shrinks for some benchmarks.; 2. If both ops have multiple uses, we can still fold it if the operations are; both sinkable to *after* the icmp (e.g. in a subsequent block) which doesn't; increase register pressure. There are a ton of icmp's we aren't simplifying because of the reg pressure; concern. Care is warranted here though because many of these are induction; variables and other cases that matter a lot to performance, like the above.; Here's a blob of code that you can drop into the bottom of visitICmp to see some; missed cases:. { Value *A, *B, *C, *D;; if (match(Op0, m_Add(m_Value(A), m_Value(B))) && ; match(Op1, m_Add(m_Value(C), m_Value(D))) &&; (A == C || A == D || B == C || B == D)) {; errs() << ""OP0 = "" << *Op0 << "" U="" << Op0->getNumUses() << ""\n"";; errs() << ""OP1 = "" << *Op1 << "" U="" << Op1->getNumUses() << ""\n"";; errs() << ""CMP = "" << I << ""\n\n"";; }; }. //===---------------------------------------------------------------------===//. define i1 @test1(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ult i32 %and, 2; ret i1 %cmp; }. Can be folded to (x & 2) == 0. define i1 @test2(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ugt i32 %and, 1; ret i1 %cmp; }. Can be folded to (x & 2) != 0. SimplifyDemandedBits sh",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:65557,Usability,Simpl,SimplifyDemandedBits,65557," pressure; concern. Care is warranted here though because many of these are induction; variables and other cases that matter a lot to performance, like the above.; Here's a blob of code that you can drop into the bottom of visitICmp to see some; missed cases:. { Value *A, *B, *C, *D;; if (match(Op0, m_Add(m_Value(A), m_Value(B))) && ; match(Op1, m_Add(m_Value(C), m_Value(D))) &&; (A == C || A == D || B == C || B == D)) {; errs() << ""OP0 = "" << *Op0 << "" U="" << Op0->getNumUses() << ""\n"";; errs() << ""OP1 = "" << *Op1 << "" U="" << Op1->getNumUses() << ""\n"";; errs() << ""CMP = "" << I << ""\n\n"";; }; }. //===---------------------------------------------------------------------===//. define i1 @test1(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ult i32 %and, 2; ret i1 %cmp; }. Can be folded to (x & 2) == 0. define i1 @test2(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ugt i32 %and, 1; ret i1 %cmp; }. Can be folded to (x & 2) != 0. SimplifyDemandedBits shrinks the ""and"" constant to 2 but instcombine misses the; icmp transform. //===---------------------------------------------------------------------===//. This code:. typedef struct {; int f1:1;; int f2:1;; int f3:1;; int f4:29;; } t1;. typedef struct {; int f1:1;; int f2:1;; int f3:30;; } t2;. t1 s1;; t2 s2;. void func1(void); {; s1.f1 = s2.f1;; s1.f2 = s2.f2;; }. Compiles into this IR (on x86-64 at least):. %struct.t1 = type { i8, [3 x i8] }; @s2 = global %struct.t1 zeroinitializer, align 4; @s1 = global %struct.t1 zeroinitializer, align 4; define void @func1() nounwind ssp noredzone {; entry:; %0 = load i32* bitcast (%struct.t1* @s2 to i32*), align 4; %bf.val.sext5 = and i32 %0, 1; %1 = load i32* bitcast (%struct.t1* @s1 to i32*), align 4; %2 = and i32 %1, -4; %3 = or i32 %2, %bf.val.sext5; %bf.val.sext26 = and i32 %0, 2; %4 = or i32 %3, %bf.val.sext26; store i32 %4, i32* bitcast (%struct.t1* @s1 to i32*), align 4; ret void; }. The two or/and's should be merged into one each. //===--------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TargetParser/CMakeLists.txt:313,Integrability,depend,dependency,313,"# Ensure that libLLVMTargetParser does not carry any static global initializer.; # ManagedStatic can be used to enable lazy-initialization of globals.; #; # HAS_WERROR_GLOBAL_CTORS and LLVM_HAS_NOGLOBAL_CTOR_MUTEX should have been set; # by llvm/lib/Support/CMakeLists.txt (which provides the required Support; # dependency).; if (HAS_WERROR_GLOBAL_CTORS AND NOT LLVM_HAS_NOGLOBAL_CTOR_MUTEX); SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -Werror=global-constructors""); endif(). # Solaris code uses kstat, so specify dependency explicitly for shared builds.; if (${CMAKE_SYSTEM_NAME} MATCHES ""SunOS""); set(system_libs kstat); endif(). add_llvm_component_library(LLVMTargetParser; AArch64TargetParser.cpp; ARMTargetParserCommon.cpp; ARMTargetParser.cpp; CSKYTargetParser.cpp; Host.cpp; LoongArchTargetParser.cpp; RISCVTargetParser.cpp; SubtargetFeature.cpp; TargetParser.cpp; Triple.cpp; X86TargetParser.cpp. ADDITIONAL_HEADER_DIRS; Unix; Windows. LINK_LIBS; ${system_libs}. LINK_COMPONENTS; Support. DEPENDS; RISCVTargetParserTableGen; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/TargetParser/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TargetParser/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TargetParser/CMakeLists.txt:512,Integrability,depend,dependency,512,"# Ensure that libLLVMTargetParser does not carry any static global initializer.; # ManagedStatic can be used to enable lazy-initialization of globals.; #; # HAS_WERROR_GLOBAL_CTORS and LLVM_HAS_NOGLOBAL_CTOR_MUTEX should have been set; # by llvm/lib/Support/CMakeLists.txt (which provides the required Support; # dependency).; if (HAS_WERROR_GLOBAL_CTORS AND NOT LLVM_HAS_NOGLOBAL_CTOR_MUTEX); SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -Werror=global-constructors""); endif(). # Solaris code uses kstat, so specify dependency explicitly for shared builds.; if (${CMAKE_SYSTEM_NAME} MATCHES ""SunOS""); set(system_libs kstat); endif(). add_llvm_component_library(LLVMTargetParser; AArch64TargetParser.cpp; ARMTargetParserCommon.cpp; ARMTargetParser.cpp; CSKYTargetParser.cpp; Host.cpp; LoongArchTargetParser.cpp; RISCVTargetParser.cpp; SubtargetFeature.cpp; TargetParser.cpp; Triple.cpp; X86TargetParser.cpp. ADDITIONAL_HEADER_DIRS; Unix; Windows. LINK_LIBS; ${system_libs}. LINK_COMPONENTS; Support. DEPENDS; RISCVTargetParserTableGen; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/TargetParser/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TargetParser/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TargetParser/CMakeLists.txt:995,Integrability,DEPEND,DEPENDS,995,"# Ensure that libLLVMTargetParser does not carry any static global initializer.; # ManagedStatic can be used to enable lazy-initialization of globals.; #; # HAS_WERROR_GLOBAL_CTORS and LLVM_HAS_NOGLOBAL_CTOR_MUTEX should have been set; # by llvm/lib/Support/CMakeLists.txt (which provides the required Support; # dependency).; if (HAS_WERROR_GLOBAL_CTORS AND NOT LLVM_HAS_NOGLOBAL_CTOR_MUTEX); SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -Werror=global-constructors""); endif(). # Solaris code uses kstat, so specify dependency explicitly for shared builds.; if (${CMAKE_SYSTEM_NAME} MATCHES ""SunOS""); set(system_libs kstat); endif(). add_llvm_component_library(LLVMTargetParser; AArch64TargetParser.cpp; ARMTargetParserCommon.cpp; ARMTargetParser.cpp; CSKYTargetParser.cpp; Host.cpp; LoongArchTargetParser.cpp; RISCVTargetParser.cpp; SubtargetFeature.cpp; TargetParser.cpp; Triple.cpp; X86TargetParser.cpp. ADDITIONAL_HEADER_DIRS; Unix; Windows. LINK_LIBS; ${system_libs}. LINK_COMPONENTS; Support. DEPENDS; RISCVTargetParserTableGen; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/TargetParser/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TargetParser/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TextAPI/CMakeLists.txt:79,Integrability,Interface,InterfaceFile,79,"add_llvm_component_library(LLVMTextAPI; Architecture.cpp; ArchitectureSet.cpp; InterfaceFile.cpp; TextStubV5.cpp; PackedVersion.cpp; Platform.cpp; RecordsSlice.cpp; RecordVisitor.cpp; Symbol.cpp; SymbolSet.cpp; Target.cpp; TextAPIError.cpp; TextStub.cpp; TextStubCommon.cpp; Utils.cpp. ADDITIONAL_HEADER_DIRS; ""${LLVM_MAIN_INCLUDE_DIR}/llvm/TextAPI"". LINK_COMPONENTS; Support; BinaryFormat; TargetParser; ). add_subdirectory(BinaryReader); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/TextAPI/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TextAPI/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt:538,Deployability,configurat,configuration,538,"include(GetLibraryName). if(LLVM_ENABLE_LIBXML2); set(imported_libs LibXml2::LibXml2); endif(). add_llvm_component_library(LLVMWindowsManifest; WindowsManifestMerger.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/WindowsManifest; ${Backtrace_INCLUDE_DIRS}. LINK_LIBS; ${imported_libs}. LINK_COMPONENTS; Support; ). # This block is only needed for llvm-config. When we deprecate llvm-config and; # move to using CMake export, this block can be removed.; if(LLVM_ENABLE_LIBXML2); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(libxml2_library TARGET LibXml2::LibXml2 PROPERTY LOCATION_${build_type}); endif(); if(NOT libxml2_library); get_property(libxml2_library TARGET LibXml2::LibXml2 PROPERTY LOCATION); endif(); get_library_name(${libxml2_library} libxml2_library); set_property(TARGET LLVMWindowsManifest PROPERTY LLVM_SYSTEM_LIBS ${libxml2_library}); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt:364,Modifiability,config,config,364,"include(GetLibraryName). if(LLVM_ENABLE_LIBXML2); set(imported_libs LibXml2::LibXml2); endif(). add_llvm_component_library(LLVMWindowsManifest; WindowsManifestMerger.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/WindowsManifest; ${Backtrace_INCLUDE_DIRS}. LINK_LIBS; ${imported_libs}. LINK_COMPONENTS; Support; ). # This block is only needed for llvm-config. When we deprecate llvm-config and; # move to using CMake export, this block can be removed.; if(LLVM_ENABLE_LIBXML2); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(libxml2_library TARGET LibXml2::LibXml2 PROPERTY LOCATION_${build_type}); endif(); if(NOT libxml2_library); get_property(libxml2_library TARGET LibXml2::LibXml2 PROPERTY LOCATION); endif(); get_library_name(${libxml2_library} libxml2_library); set_property(TARGET LLVMWindowsManifest PROPERTY LLVM_SYSTEM_LIBS ${libxml2_library}); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt:395,Modifiability,config,config,395,"include(GetLibraryName). if(LLVM_ENABLE_LIBXML2); set(imported_libs LibXml2::LibXml2); endif(). add_llvm_component_library(LLVMWindowsManifest; WindowsManifestMerger.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/WindowsManifest; ${Backtrace_INCLUDE_DIRS}. LINK_LIBS; ${imported_libs}. LINK_COMPONENTS; Support; ). # This block is only needed for llvm-config. When we deprecate llvm-config and; # move to using CMake export, this block can be removed.; if(LLVM_ENABLE_LIBXML2); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(libxml2_library TARGET LibXml2::LibXml2 PROPERTY LOCATION_${build_type}); endif(); if(NOT libxml2_library); get_property(libxml2_library TARGET LibXml2::LibXml2 PROPERTY LOCATION); endif(); get_library_name(${libxml2_library} libxml2_library); set_property(TARGET LLVMWindowsManifest PROPERTY LLVM_SYSTEM_LIBS ${libxml2_library}); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt:538,Modifiability,config,configuration,538,"include(GetLibraryName). if(LLVM_ENABLE_LIBXML2); set(imported_libs LibXml2::LibXml2); endif(). add_llvm_component_library(LLVMWindowsManifest; WindowsManifestMerger.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/WindowsManifest; ${Backtrace_INCLUDE_DIRS}. LINK_LIBS; ${imported_libs}. LINK_COMPONENTS; Support; ). # This block is only needed for llvm-config. When we deprecate llvm-config and; # move to using CMake export, this block can be removed.; if(LLVM_ENABLE_LIBXML2); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(libxml2_library TARGET LibXml2::LibXml2 PROPERTY LOCATION_${build_type}); endif(); if(NOT libxml2_library); get_property(libxml2_library TARGET LibXml2::LibXml2 PROPERTY LOCATION); endif(); get_library_name(${libxml2_library} libxml2_library); set_property(TARGET LLVMWindowsManifest PROPERTY LLVM_SYSTEM_LIBS ${libxml2_library}); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/XRay/CMakeLists.txt:219,Testability,Log,LogBuilderConsumer,219,add_llvm_component_library(LLVMXRay; BlockIndexer.cpp; BlockPrinter.cpp; BlockVerifier.cpp; FDRRecordProducer.cpp; FDRRecords.cpp; FDRTraceExpander.cpp; FDRTraceWriter.cpp; FileHeaderReader.cpp; InstrumentationMap.cpp; LogBuilderConsumer.cpp; Profile.cpp; RecordInitializer.cpp; RecordPrinter.cpp; Trace.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/ADT; ${LLVM_MAIN_INCLUDE_DIR}/llvm/XRay. LINK_COMPONENTS; Support; Object; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/XRay/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/XRay/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Bitcode/Reader/CMakeLists.txt:195,Integrability,DEPEND,DEPENDS,195,add_llvm_component_library(LLVMBitReader; BitcodeAnalyzer.cpp; BitReader.cpp; BitcodeReader.cpp; MetadataLoader.cpp; ValueList.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Bitcode. DEPENDS; intrinsics_gen. LINK_COMPONENTS; BitstreamReader; Core; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Bitcode/Reader/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Bitcode/Reader/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Bitcode/Writer/CMakeLists.txt:120,Integrability,DEPEND,DEPENDS,120,add_llvm_component_library(LLVMBitWriter; BitWriter.cpp; BitcodeWriter.cpp; BitcodeWriterPass.cpp; ValueEnumerator.cpp. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; Core; MC; Object; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Bitcode/Writer/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Bitcode/Writer/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/CMakeLists.txt:556,Integrability,DEPEND,DEPENDS,556,add_llvm_component_library(LLVMAsmPrinter; AccelTable.cpp; AddressPool.cpp; AIXException.cpp; ARMException.cpp; AsmPrinter.cpp; AsmPrinterDwarf.cpp; AsmPrinterInlineAsm.cpp; DbgEntityHistoryCalculator.cpp; DebugHandlerBase.cpp; DebugLocStream.cpp; DIE.cpp; DIEHash.cpp; DwarfCFIException.cpp; DwarfCompileUnit.cpp; DwarfDebug.cpp; DwarfExpression.cpp; DwarfFile.cpp; DwarfStringPool.cpp; DwarfUnit.cpp; EHStreamer.cpp; ErlangGCPrinter.cpp; OcamlGCPrinter.cpp; PseudoProbePrinter.cpp; WinCFGuard.cpp; WinException.cpp; CodeViewDebug.cpp; WasmException.cpp. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; BinaryFormat; CodeGen; CodeGenTypes; Core; DebugInfoCodeView; DebugInfoDWARF; MC; MCParser; Remarks; Support; Target; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CMakeLists.txt:615,Integrability,DEPEND,DEPENDS,615,add_llvm_component_library(LLVMGlobalISel; CSEInfo.cpp; GISelKnownBits.cpp; CSEMIRBuilder.cpp; CallLowering.cpp; GlobalISel.cpp; Combiner.cpp; CombinerHelper.cpp; GIMatchTableExecutor.cpp; GISelChangeObserver.cpp; IRTranslator.cpp; InlineAsmLowering.cpp; InstructionSelect.cpp; InstructionSelector.cpp; LegalityPredicates.cpp; LegalizeMutations.cpp; Legalizer.cpp; LegalizerHelper.cpp; LegalizerInfo.cpp; LegacyLegalizerInfo.cpp; LoadStoreOpt.cpp; Localizer.cpp; LostDebugLocObserver.cpp; MachineIRBuilder.cpp; RegBankSelect.cpp; Utils.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/CodeGen/GlobalISel. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; CodeGen; CodeGenTypes; Core; MC; SelectionDAG; Support; Target; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CMakeLists.txt:430,Performance,Load,LoadStoreOpt,430,add_llvm_component_library(LLVMGlobalISel; CSEInfo.cpp; GISelKnownBits.cpp; CSEMIRBuilder.cpp; CallLowering.cpp; GlobalISel.cpp; Combiner.cpp; CombinerHelper.cpp; GIMatchTableExecutor.cpp; GISelChangeObserver.cpp; IRTranslator.cpp; InlineAsmLowering.cpp; InstructionSelect.cpp; InstructionSelector.cpp; LegalityPredicates.cpp; LegalizeMutations.cpp; Legalizer.cpp; LegalizerHelper.cpp; LegalizerInfo.cpp; LegacyLegalizerInfo.cpp; LoadStoreOpt.cpp; Localizer.cpp; LostDebugLocObserver.cpp; MachineIRBuilder.cpp; RegBankSelect.cpp; Utils.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/CodeGen/GlobalISel. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; CodeGen; CodeGenTypes; Core; MC; SelectionDAG; Support; Target; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MIRParser/CMakeLists.txt:157,Integrability,DEPEND,DEPENDS,157,add_llvm_component_library(LLVMMIRParser; MILexer.cpp; MIParser.cpp; MIRParser.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/CodeGen/MIRParser. DEPENDS; intrinsics_gen. LINK_COMPONENTS; AsmParser; BinaryFormat; CodeGen; CodeGenTypes; Core; MC; Support; Target; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/MIRParser/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MIRParser/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/CMakeLists.txt:307,Energy Efficiency,Schedul,ScheduleDAGFast,307,add_llvm_component_library(LLVMSelectionDAG; DAGCombiner.cpp; FastISel.cpp; FunctionLoweringInfo.cpp; InstrEmitter.cpp; LegalizeDAG.cpp; LegalizeFloatTypes.cpp; LegalizeIntegerTypes.cpp; LegalizeTypes.cpp; LegalizeTypesGeneric.cpp; LegalizeVectorOps.cpp; LegalizeVectorTypes.cpp; ResourcePriorityQueue.cpp; ScheduleDAGFast.cpp; ScheduleDAGRRList.cpp; ScheduleDAGSDNodes.cpp; ScheduleDAGVLIW.cpp; SelectionDAGBuilder.cpp; SelectionDAG.cpp; SelectionDAGAddressAnalysis.cpp; SelectionDAGDumper.cpp; SelectionDAGISel.cpp; SelectionDAGPrinter.cpp; SelectionDAGTargetInfo.cpp; StatepointLowering.cpp; TargetLowering.cpp. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; CodeGen; CodeGenTypes; Core; MC; Support; Target; TargetParser; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/CMakeLists.txt:328,Energy Efficiency,Schedul,ScheduleDAGRRList,328,add_llvm_component_library(LLVMSelectionDAG; DAGCombiner.cpp; FastISel.cpp; FunctionLoweringInfo.cpp; InstrEmitter.cpp; LegalizeDAG.cpp; LegalizeFloatTypes.cpp; LegalizeIntegerTypes.cpp; LegalizeTypes.cpp; LegalizeTypesGeneric.cpp; LegalizeVectorOps.cpp; LegalizeVectorTypes.cpp; ResourcePriorityQueue.cpp; ScheduleDAGFast.cpp; ScheduleDAGRRList.cpp; ScheduleDAGSDNodes.cpp; ScheduleDAGVLIW.cpp; SelectionDAGBuilder.cpp; SelectionDAG.cpp; SelectionDAGAddressAnalysis.cpp; SelectionDAGDumper.cpp; SelectionDAGISel.cpp; SelectionDAGPrinter.cpp; SelectionDAGTargetInfo.cpp; StatepointLowering.cpp; TargetLowering.cpp. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; CodeGen; CodeGenTypes; Core; MC; Support; Target; TargetParser; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/CMakeLists.txt:351,Energy Efficiency,Schedul,ScheduleDAGSDNodes,351,add_llvm_component_library(LLVMSelectionDAG; DAGCombiner.cpp; FastISel.cpp; FunctionLoweringInfo.cpp; InstrEmitter.cpp; LegalizeDAG.cpp; LegalizeFloatTypes.cpp; LegalizeIntegerTypes.cpp; LegalizeTypes.cpp; LegalizeTypesGeneric.cpp; LegalizeVectorOps.cpp; LegalizeVectorTypes.cpp; ResourcePriorityQueue.cpp; ScheduleDAGFast.cpp; ScheduleDAGRRList.cpp; ScheduleDAGSDNodes.cpp; ScheduleDAGVLIW.cpp; SelectionDAGBuilder.cpp; SelectionDAG.cpp; SelectionDAGAddressAnalysis.cpp; SelectionDAGDumper.cpp; SelectionDAGISel.cpp; SelectionDAGPrinter.cpp; SelectionDAGTargetInfo.cpp; StatepointLowering.cpp; TargetLowering.cpp. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; CodeGen; CodeGenTypes; Core; MC; Support; Target; TargetParser; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/CMakeLists.txt:375,Energy Efficiency,Schedul,ScheduleDAGVLIW,375,add_llvm_component_library(LLVMSelectionDAG; DAGCombiner.cpp; FastISel.cpp; FunctionLoweringInfo.cpp; InstrEmitter.cpp; LegalizeDAG.cpp; LegalizeFloatTypes.cpp; LegalizeIntegerTypes.cpp; LegalizeTypes.cpp; LegalizeTypesGeneric.cpp; LegalizeVectorOps.cpp; LegalizeVectorTypes.cpp; ResourcePriorityQueue.cpp; ScheduleDAGFast.cpp; ScheduleDAGRRList.cpp; ScheduleDAGSDNodes.cpp; ScheduleDAGVLIW.cpp; SelectionDAGBuilder.cpp; SelectionDAG.cpp; SelectionDAGAddressAnalysis.cpp; SelectionDAGDumper.cpp; SelectionDAGISel.cpp; SelectionDAGPrinter.cpp; SelectionDAGTargetInfo.cpp; StatepointLowering.cpp; TargetLowering.cpp. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; CodeGen; CodeGenTypes; Core; MC; Support; Target; TargetParser; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/CMakeLists.txt:615,Integrability,DEPEND,DEPENDS,615,add_llvm_component_library(LLVMSelectionDAG; DAGCombiner.cpp; FastISel.cpp; FunctionLoweringInfo.cpp; InstrEmitter.cpp; LegalizeDAG.cpp; LegalizeFloatTypes.cpp; LegalizeIntegerTypes.cpp; LegalizeTypes.cpp; LegalizeTypesGeneric.cpp; LegalizeVectorOps.cpp; LegalizeVectorTypes.cpp; ResourcePriorityQueue.cpp; ScheduleDAGFast.cpp; ScheduleDAGRRList.cpp; ScheduleDAGSDNodes.cpp; ScheduleDAGVLIW.cpp; SelectionDAGBuilder.cpp; SelectionDAG.cpp; SelectionDAGAddressAnalysis.cpp; SelectionDAGDumper.cpp; SelectionDAGISel.cpp; SelectionDAGPrinter.cpp; SelectionDAGTargetInfo.cpp; StatepointLowering.cpp; TargetLowering.cpp. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; CodeGen; CodeGenTypes; Core; MC; Support; Target; TargetParser; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/BTF/CMakeLists.txt:147,Integrability,DEPEND,DEPENDS,147,"add_llvm_component_library(LLVMDebugInfoBTF; BTFParser.cpp; BTFContext.cpp; ADDITIONAL_HEADER_DIRS; ""${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/BTF"". DEPENDS; intrinsics_gen. LINK_COMPONENTS; Support; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/DebugInfo/BTF/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/BTF/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/CodeView/CMakeLists.txt:705,Usability,Simpl,SimpleTypeSerializer,705,add_llvm_component_library(LLVMDebugInfoCodeView; AppendingTypeTableBuilder.cpp; CodeViewError.cpp; CodeViewRecordIO.cpp; ContinuationRecordBuilder.cpp; CVSymbolVisitor.cpp; CVTypeVisitor.cpp; DebugChecksumsSubsection.cpp; DebugCrossExSubsection.cpp; DebugCrossImpSubsection.cpp; DebugFrameDataSubsection.cpp; DebugInlineeLinesSubsection.cpp; DebugLinesSubsection.cpp; DebugStringTableSubsection.cpp; DebugSubsection.cpp; DebugSubsectionRecord.cpp; DebugSubsectionVisitor.cpp; DebugSymbolRVASubsection.cpp; DebugSymbolsSubsection.cpp; EnumTables.cpp; Formatters.cpp; GlobalTypeTableBuilder.cpp; LazyRandomTypeCollection.cpp; Line.cpp; MergingTypeTableBuilder.cpp; RecordName.cpp; RecordSerialization.cpp; SimpleTypeSerializer.cpp; StringsAndChecksums.cpp; SymbolDumper.cpp; SymbolRecordHelpers.cpp; SymbolRecordMapping.cpp; SymbolSerializer.cpp; TypeDumpVisitor.cpp; TypeIndex.cpp; TypeIndexDiscovery.cpp; TypeHashing.cpp; TypeRecordHelpers.cpp; TypeRecordMapping.cpp; TypeStreamMerger.cpp; TypeTableCollection.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/CodeView. LINK_COMPONENTS; Support; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/DebugInfo/CodeView/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/CodeView/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/GSYM/CMakeLists.txt:353,Integrability,DEPEND,DEPENDS,353,add_llvm_component_library(LLVMDebugInfoGSYM; DwarfTransformer.cpp; Header.cpp; FileWriter.cpp; FunctionInfo.cpp; GsymCreator.cpp; GsymReader.cpp; InlineInfo.cpp; LineTable.cpp; LookupResult.cpp; ObjectFileTransformer.cpp; ExtractRanges.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/GSYM; ${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo. DEPENDS; LLVMMC. LINK_COMPONENTS; MC; Object; Support; TargetParser; DebugInfoDWARF; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/DebugInfo/GSYM/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/GSYM/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/CMakeLists.txt:643,Testability,Log,LogicalView,643,"macro(add_lv_impl_folder group); list(APPEND LV_IMPL_SOURCES ${ARGN}); source_group(${group} FILES ${ARGN}); endmacro(). add_lv_impl_folder(Core; Core/LVCompare.cpp; Core/LVElement.cpp; Core/LVLine.cpp; Core/LVLocation.cpp; Core/LVObject.cpp; Core/LVOptions.cpp; Core/LVRange.cpp; Core/LVReader.cpp; Core/LVScope.cpp; Core/LVSort.cpp; Core/LVSupport.cpp; Core/LVSymbol.cpp; Core/LVType.cpp; ). add_lv_impl_folder(Readers; LVReaderHandler.cpp; Readers/LVBinaryReader.cpp; Readers/LVCodeViewReader.cpp; Readers/LVCodeViewVisitor.cpp; Readers/LVELFReader.cpp; ). list(APPEND LIBLV_ADDITIONAL_HEADER_DIRS; ""${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/LogicalView""; ""${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/LogicalView/Core""; ""${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/LogicalView/Readers""; ). add_llvm_component_library(LLVMDebugInfoLogicalView; ${LV_IMPL_SOURCES}. ADDITIONAL_HEADER_DIRS; ${LIBLV_ADDITIONAL_HEADER_DIRS}. LINK_COMPONENTS; BinaryFormat; Demangle; Object; MC; Support; TargetParser; DebugInfoDWARF; DebugInfoCodeView; DebugInfoPDB; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/CMakeLists.txt:698,Testability,Log,LogicalView,698,"macro(add_lv_impl_folder group); list(APPEND LV_IMPL_SOURCES ${ARGN}); source_group(${group} FILES ${ARGN}); endmacro(). add_lv_impl_folder(Core; Core/LVCompare.cpp; Core/LVElement.cpp; Core/LVLine.cpp; Core/LVLocation.cpp; Core/LVObject.cpp; Core/LVOptions.cpp; Core/LVRange.cpp; Core/LVReader.cpp; Core/LVScope.cpp; Core/LVSort.cpp; Core/LVSupport.cpp; Core/LVSymbol.cpp; Core/LVType.cpp; ). add_lv_impl_folder(Readers; LVReaderHandler.cpp; Readers/LVBinaryReader.cpp; Readers/LVCodeViewReader.cpp; Readers/LVCodeViewVisitor.cpp; Readers/LVELFReader.cpp; ). list(APPEND LIBLV_ADDITIONAL_HEADER_DIRS; ""${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/LogicalView""; ""${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/LogicalView/Core""; ""${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/LogicalView/Readers""; ). add_llvm_component_library(LLVMDebugInfoLogicalView; ${LV_IMPL_SOURCES}. ADDITIONAL_HEADER_DIRS; ${LIBLV_ADDITIONAL_HEADER_DIRS}. LINK_COMPONENTS; BinaryFormat; Demangle; Object; MC; Support; TargetParser; DebugInfoDWARF; DebugInfoCodeView; DebugInfoPDB; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/CMakeLists.txt:758,Testability,Log,LogicalView,758,"macro(add_lv_impl_folder group); list(APPEND LV_IMPL_SOURCES ${ARGN}); source_group(${group} FILES ${ARGN}); endmacro(). add_lv_impl_folder(Core; Core/LVCompare.cpp; Core/LVElement.cpp; Core/LVLine.cpp; Core/LVLocation.cpp; Core/LVObject.cpp; Core/LVOptions.cpp; Core/LVRange.cpp; Core/LVReader.cpp; Core/LVScope.cpp; Core/LVSort.cpp; Core/LVSupport.cpp; Core/LVSymbol.cpp; Core/LVType.cpp; ). add_lv_impl_folder(Readers; LVReaderHandler.cpp; Readers/LVBinaryReader.cpp; Readers/LVCodeViewReader.cpp; Readers/LVCodeViewVisitor.cpp; Readers/LVELFReader.cpp; ). list(APPEND LIBLV_ADDITIONAL_HEADER_DIRS; ""${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/LogicalView""; ""${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/LogicalView/Core""; ""${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/LogicalView/Readers""; ). add_llvm_component_library(LLVMDebugInfoLogicalView; ${LV_IMPL_SOURCES}. ADDITIONAL_HEADER_DIRS; ${LIBLV_ADDITIONAL_HEADER_DIRS}. LINK_COMPONENTS; BinaryFormat; Demangle; Object; MC; Support; TargetParser; DebugInfoDWARF; DebugInfoCodeView; DebugInfoPDB; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/CMakeLists.txt:1582,Integrability,Inject,InjectedSourceStream,1582,"_CMAKE_PATH ""${LIBPDB_LINK_FOLDERS}\\diaguids.lib"" LIBPDB_ADDITIONAL_LIBRARIES). add_pdb_impl_folder(DIA; DIA/DIADataStream.cpp; DIA/DIAEnumDebugStreams.cpp; DIA/DIAEnumFrameData.cpp; DIA/DIAEnumInjectedSources.cpp; DIA/DIAEnumLineNumbers.cpp; DIA/DIAEnumSectionContribs.cpp; DIA/DIAEnumSourceFiles.cpp; DIA/DIAEnumSymbols.cpp; DIA/DIAEnumTables.cpp; DIA/DIAError.cpp; DIA/DIAFrameData.cpp; DIA/DIAInjectedSource.cpp; DIA/DIALineNumber.cpp; DIA/DIARawSymbol.cpp; DIA/DIASectionContrib.cpp; DIA/DIASession.cpp; DIA/DIASourceFile.cpp; DIA/DIATable.cpp; ). set(LIBPDB_ADDITIONAL_HEADER_DIRS ""${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/PDB/DIA""); endif(). add_pdb_impl_folder(Native; Native/DbiModuleDescriptor.cpp; Native/DbiModuleDescriptorBuilder.cpp; Native/DbiModuleList.cpp; Native/DbiStream.cpp; Native/DbiStreamBuilder.cpp; Native/EnumTables.cpp; Native/FormatUtil.cpp; Native/GlobalsStream.cpp; Native/Hash.cpp; Native/HashTable.cpp; Native/InfoStream.cpp; Native/InfoStreamBuilder.cpp; Native/InjectedSourceStream.cpp; Native/InputFile.cpp; Native/LinePrinter.cpp; Native/ModuleDebugStream.cpp; Native/NativeCompilandSymbol.cpp; Native/NativeEnumGlobals.cpp; Native/NativeEnumInjectedSources.cpp; Native/NativeEnumLineNumbers.cpp; Native/NativeEnumModules.cpp; Native/NativeEnumTypes.cpp; Native/NativeEnumSymbols.cpp; Native/NativeExeSymbol.cpp; Native/NativeFunctionSymbol.cpp; Native/NativeInlineSiteSymbol.cpp; Native/NativeLineNumber.cpp; Native/NativePublicSymbol.cpp; Native/NativeRawSymbol.cpp; Native/NativeSourceFile.cpp; Native/NativeSymbolEnumerator.cpp; Native/NativeTypeArray.cpp; Native/NativeTypeBuiltin.cpp; Native/NativeTypeEnum.cpp; Native/NativeTypeFunctionSig.cpp; Native/NativeTypePointer.cpp; Native/NativeTypeTypedef.cpp; Native/NativeTypeUDT.cpp; Native/NativeTypeVTShape.cpp; Native/NamedStreamMap.cpp; Native/NativeSession.cpp; Native/PDBFile.cpp; Native/PDBFileBuilder.cpp; Native/PDBStringTable.cpp; Native/PDBStringTableBuilder.cpp; Native/PublicsStream.cpp; Native/GS",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/DebugInfo/PDB/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/CMakeLists.txt:4111,Integrability,INTERFACE,INTERFACE,4111,"Native/NativeTypeArray.cpp; Native/NativeTypeBuiltin.cpp; Native/NativeTypeEnum.cpp; Native/NativeTypeFunctionSig.cpp; Native/NativeTypePointer.cpp; Native/NativeTypeTypedef.cpp; Native/NativeTypeUDT.cpp; Native/NativeTypeVTShape.cpp; Native/NamedStreamMap.cpp; Native/NativeSession.cpp; Native/PDBFile.cpp; Native/PDBFileBuilder.cpp; Native/PDBStringTable.cpp; Native/PDBStringTableBuilder.cpp; Native/PublicsStream.cpp; Native/GSIStreamBuilder.cpp; Native/RawError.cpp; Native/SymbolCache.cpp; Native/SymbolStream.cpp; Native/TpiHashing.cpp; Native/TpiStream.cpp; Native/TpiStreamBuilder.cpp; ). list(APPEND LIBPDB_ADDITIONAL_HEADER_DIRS ""${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/PDB/Native""); list(APPEND LIBPDB_ADDITIONAL_HEADER_DIRS ""${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/PDB""). add_llvm_component_library(LLVMDebugInfoPDB; GenericError.cpp; IPDBSourceFile.cpp; PDB.cpp; PDBContext.cpp; PDBExtras.cpp; PDBInterfaceAnchors.cpp; PDBSymbol.cpp; PDBSymbolAnnotation.cpp; PDBSymbolBlock.cpp; PDBSymbolCompiland.cpp; PDBSymbolCompilandDetails.cpp; PDBSymbolCompilandEnv.cpp; PDBSymbolCustom.cpp; PDBSymbolData.cpp; PDBSymbolExe.cpp; PDBSymbolFunc.cpp; PDBSymbolFuncDebugEnd.cpp; PDBSymbolFuncDebugStart.cpp; PDBSymbolLabel.cpp; PDBSymbolPublicSymbol.cpp; PDBSymbolThunk.cpp; PDBSymbolTypeArray.cpp; PDBSymbolTypeBaseClass.cpp; PDBSymbolTypeBuiltin.cpp; PDBSymbolTypeCustom.cpp; PDBSymbolTypeDimension.cpp; PDBSymbolTypeEnum.cpp; PDBSymbolTypeFriend.cpp; PDBSymbolTypeFunctionArg.cpp; PDBSymbolTypeFunctionSig.cpp; PDBSymbolTypeManaged.cpp; PDBSymbolTypePointer.cpp; PDBSymbolTypeTypedef.cpp; PDBSymbolTypeUDT.cpp; PDBSymbolTypeVTable.cpp; PDBSymbolTypeVTableShape.cpp; PDBSymbolUnknown.cpp; PDBSymbolUsingNamespace.cpp; PDBSymDumper.cpp; UDTLayout.cpp; ${PDB_IMPL_SOURCES}. ADDITIONAL_HEADER_DIRS; ${LIBPDB_ADDITIONAL_HEADER_DIRS}. LINK_COMPONENTS; BinaryFormat; Object; Support; DebugInfoCodeView; DebugInfoMSF; ). target_link_libraries(LLVMDebugInfoPDB INTERFACE ""${LIBPDB_ADDITIONAL_LIBRARIES}""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/DebugInfo/PDB/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/CMakeLists.txt:1490,Security,Hash,Hash,1490,"E_SIZEOF_VOID_P EQUAL 8); set(LIBPDB_LINK_FOLDERS ""${LIBPDB_LINK_FOLDERS}\\amd64""); endif(); file(TO_CMAKE_PATH ""${LIBPDB_LINK_FOLDERS}\\diaguids.lib"" LIBPDB_ADDITIONAL_LIBRARIES). add_pdb_impl_folder(DIA; DIA/DIADataStream.cpp; DIA/DIAEnumDebugStreams.cpp; DIA/DIAEnumFrameData.cpp; DIA/DIAEnumInjectedSources.cpp; DIA/DIAEnumLineNumbers.cpp; DIA/DIAEnumSectionContribs.cpp; DIA/DIAEnumSourceFiles.cpp; DIA/DIAEnumSymbols.cpp; DIA/DIAEnumTables.cpp; DIA/DIAError.cpp; DIA/DIAFrameData.cpp; DIA/DIAInjectedSource.cpp; DIA/DIALineNumber.cpp; DIA/DIARawSymbol.cpp; DIA/DIASectionContrib.cpp; DIA/DIASession.cpp; DIA/DIASourceFile.cpp; DIA/DIATable.cpp; ). set(LIBPDB_ADDITIONAL_HEADER_DIRS ""${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/PDB/DIA""); endif(). add_pdb_impl_folder(Native; Native/DbiModuleDescriptor.cpp; Native/DbiModuleDescriptorBuilder.cpp; Native/DbiModuleList.cpp; Native/DbiStream.cpp; Native/DbiStreamBuilder.cpp; Native/EnumTables.cpp; Native/FormatUtil.cpp; Native/GlobalsStream.cpp; Native/Hash.cpp; Native/HashTable.cpp; Native/InfoStream.cpp; Native/InfoStreamBuilder.cpp; Native/InjectedSourceStream.cpp; Native/InputFile.cpp; Native/LinePrinter.cpp; Native/ModuleDebugStream.cpp; Native/NativeCompilandSymbol.cpp; Native/NativeEnumGlobals.cpp; Native/NativeEnumInjectedSources.cpp; Native/NativeEnumLineNumbers.cpp; Native/NativeEnumModules.cpp; Native/NativeEnumTypes.cpp; Native/NativeEnumSymbols.cpp; Native/NativeExeSymbol.cpp; Native/NativeFunctionSymbol.cpp; Native/NativeInlineSiteSymbol.cpp; Native/NativeLineNumber.cpp; Native/NativePublicSymbol.cpp; Native/NativeRawSymbol.cpp; Native/NativeSourceFile.cpp; Native/NativeSymbolEnumerator.cpp; Native/NativeTypeArray.cpp; Native/NativeTypeBuiltin.cpp; Native/NativeTypeEnum.cpp; Native/NativeTypeFunctionSig.cpp; Native/NativeTypePointer.cpp; Native/NativeTypeTypedef.cpp; Native/NativeTypeUDT.cpp; Native/NativeTypeVTShape.cpp; Native/NamedStreamMap.cpp; Native/NativeSession.cpp; Native/PDBFile.cpp; Native/PDBFileBuilder.c",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/DebugInfo/PDB/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/CMakeLists.txt:1507,Security,Hash,HashTable,1507,"L 8); set(LIBPDB_LINK_FOLDERS ""${LIBPDB_LINK_FOLDERS}\\amd64""); endif(); file(TO_CMAKE_PATH ""${LIBPDB_LINK_FOLDERS}\\diaguids.lib"" LIBPDB_ADDITIONAL_LIBRARIES). add_pdb_impl_folder(DIA; DIA/DIADataStream.cpp; DIA/DIAEnumDebugStreams.cpp; DIA/DIAEnumFrameData.cpp; DIA/DIAEnumInjectedSources.cpp; DIA/DIAEnumLineNumbers.cpp; DIA/DIAEnumSectionContribs.cpp; DIA/DIAEnumSourceFiles.cpp; DIA/DIAEnumSymbols.cpp; DIA/DIAEnumTables.cpp; DIA/DIAError.cpp; DIA/DIAFrameData.cpp; DIA/DIAInjectedSource.cpp; DIA/DIALineNumber.cpp; DIA/DIARawSymbol.cpp; DIA/DIASectionContrib.cpp; DIA/DIASession.cpp; DIA/DIASourceFile.cpp; DIA/DIATable.cpp; ). set(LIBPDB_ADDITIONAL_HEADER_DIRS ""${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/PDB/DIA""); endif(). add_pdb_impl_folder(Native; Native/DbiModuleDescriptor.cpp; Native/DbiModuleDescriptorBuilder.cpp; Native/DbiModuleList.cpp; Native/DbiStream.cpp; Native/DbiStreamBuilder.cpp; Native/EnumTables.cpp; Native/FormatUtil.cpp; Native/GlobalsStream.cpp; Native/Hash.cpp; Native/HashTable.cpp; Native/InfoStream.cpp; Native/InfoStreamBuilder.cpp; Native/InjectedSourceStream.cpp; Native/InputFile.cpp; Native/LinePrinter.cpp; Native/ModuleDebugStream.cpp; Native/NativeCompilandSymbol.cpp; Native/NativeEnumGlobals.cpp; Native/NativeEnumInjectedSources.cpp; Native/NativeEnumLineNumbers.cpp; Native/NativeEnumModules.cpp; Native/NativeEnumTypes.cpp; Native/NativeEnumSymbols.cpp; Native/NativeExeSymbol.cpp; Native/NativeFunctionSymbol.cpp; Native/NativeInlineSiteSymbol.cpp; Native/NativeLineNumber.cpp; Native/NativePublicSymbol.cpp; Native/NativeRawSymbol.cpp; Native/NativeSourceFile.cpp; Native/NativeSymbolEnumerator.cpp; Native/NativeTypeArray.cpp; Native/NativeTypeBuiltin.cpp; Native/NativeTypeEnum.cpp; Native/NativeTypeFunctionSig.cpp; Native/NativeTypePointer.cpp; Native/NativeTypeTypedef.cpp; Native/NativeTypeUDT.cpp; Native/NativeTypeVTShape.cpp; Native/NamedStreamMap.cpp; Native/NativeSession.cpp; Native/PDBFile.cpp; Native/PDBFileBuilder.cpp; Native/PDBStrin",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/DebugInfo/PDB/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/CMakeLists.txt:1582,Security,Inject,InjectedSourceStream,1582,"_CMAKE_PATH ""${LIBPDB_LINK_FOLDERS}\\diaguids.lib"" LIBPDB_ADDITIONAL_LIBRARIES). add_pdb_impl_folder(DIA; DIA/DIADataStream.cpp; DIA/DIAEnumDebugStreams.cpp; DIA/DIAEnumFrameData.cpp; DIA/DIAEnumInjectedSources.cpp; DIA/DIAEnumLineNumbers.cpp; DIA/DIAEnumSectionContribs.cpp; DIA/DIAEnumSourceFiles.cpp; DIA/DIAEnumSymbols.cpp; DIA/DIAEnumTables.cpp; DIA/DIAError.cpp; DIA/DIAFrameData.cpp; DIA/DIAInjectedSource.cpp; DIA/DIALineNumber.cpp; DIA/DIARawSymbol.cpp; DIA/DIASectionContrib.cpp; DIA/DIASession.cpp; DIA/DIASourceFile.cpp; DIA/DIATable.cpp; ). set(LIBPDB_ADDITIONAL_HEADER_DIRS ""${LLVM_MAIN_INCLUDE_DIR}/llvm/DebugInfo/PDB/DIA""); endif(). add_pdb_impl_folder(Native; Native/DbiModuleDescriptor.cpp; Native/DbiModuleDescriptorBuilder.cpp; Native/DbiModuleList.cpp; Native/DbiStream.cpp; Native/DbiStreamBuilder.cpp; Native/EnumTables.cpp; Native/FormatUtil.cpp; Native/GlobalsStream.cpp; Native/Hash.cpp; Native/HashTable.cpp; Native/InfoStream.cpp; Native/InfoStreamBuilder.cpp; Native/InjectedSourceStream.cpp; Native/InputFile.cpp; Native/LinePrinter.cpp; Native/ModuleDebugStream.cpp; Native/NativeCompilandSymbol.cpp; Native/NativeEnumGlobals.cpp; Native/NativeEnumInjectedSources.cpp; Native/NativeEnumLineNumbers.cpp; Native/NativeEnumModules.cpp; Native/NativeEnumTypes.cpp; Native/NativeEnumSymbols.cpp; Native/NativeExeSymbol.cpp; Native/NativeFunctionSymbol.cpp; Native/NativeInlineSiteSymbol.cpp; Native/NativeLineNumber.cpp; Native/NativePublicSymbol.cpp; Native/NativeRawSymbol.cpp; Native/NativeSourceFile.cpp; Native/NativeSymbolEnumerator.cpp; Native/NativeTypeArray.cpp; Native/NativeTypeBuiltin.cpp; Native/NativeTypeEnum.cpp; Native/NativeTypeFunctionSig.cpp; Native/NativeTypePointer.cpp; Native/NativeTypeTypedef.cpp; Native/NativeTypeUDT.cpp; Native/NativeTypeVTShape.cpp; Native/NamedStreamMap.cpp; Native/NativeSession.cpp; Native/PDBFile.cpp; Native/PDBFileBuilder.cpp; Native/PDBStringTable.cpp; Native/PDBStringTableBuilder.cpp; Native/PublicsStream.cpp; Native/GS",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/DebugInfo/PDB/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DWARFLinker/Classic/CMakeLists.txt:210,Integrability,DEPEND,DEPENDS,210,add_llvm_component_library(LLVMDWARFLinkerClassic; DWARFLinkerCompileUnit.cpp; DWARFLinkerDeclContext.cpp; DWARFLinker.cpp; DWARFStreamer.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/DWARFLinker. DEPENDS; intrinsics_gen. LINK_COMPONENTS; AsmPrinter; BinaryFormat; CodeGen; CodeGenTypes; DebugInfoDWARF; DWARFLinker; MC; Object; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/DWARFLinker/Classic/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DWARFLinker/Classic/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DWARFLinker/Parallel/CMakeLists.txt:81,Integrability,Depend,DependencyTracker,81,add_llvm_component_library(LLVMDWARFLinkerParallel; AcceleratorRecordsSaver.cpp; DependencyTracker.cpp; DIEAttributeCloner.cpp; DWARFEmitterImpl.cpp; DWARFLinker.cpp; DWARFLinkerCompileUnit.cpp; DWARFLinkerTypeUnit.cpp; DWARFLinkerImpl.cpp; DWARFLinkerUnit.cpp; OutputSections.cpp; SyntheticTypeNameBuilder.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/DWARFLinkerParallel. DEPENDS; intrinsics_gen. LINK_COMPONENTS; AsmPrinter; BinaryFormat; CodeGen; DebugInfoDWARF; DWARFLinker; MC; Object; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/DWARFLinker/Parallel/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DWARFLinker/Parallel/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DWARFLinker/Parallel/CMakeLists.txt:387,Integrability,DEPEND,DEPENDS,387,add_llvm_component_library(LLVMDWARFLinkerParallel; AcceleratorRecordsSaver.cpp; DependencyTracker.cpp; DIEAttributeCloner.cpp; DWARFEmitterImpl.cpp; DWARFLinker.cpp; DWARFLinkerCompileUnit.cpp; DWARFLinkerTypeUnit.cpp; DWARFLinkerImpl.cpp; DWARFLinkerUnit.cpp; OutputSections.cpp; SyntheticTypeNameBuilder.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/DWARFLinkerParallel. DEPENDS; intrinsics_gen. LINK_COMPONENTS; AsmPrinter; BinaryFormat; CodeGen; DebugInfoDWARF; DWARFLinker; MC; Object; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/DWARFLinker/Parallel/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DWARFLinker/Parallel/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/CMakeLists.txt:557,Integrability,message,message,557,"include_directories( ${CMAKE_CURRENT_SOURCE_DIR}/.. ). if(NOT DEFINED ITTAPI_GIT_REPOSITORY); set(ITTAPI_GIT_REPOSITORY https://github.com/intel/ittapi.git); endif(). if(NOT DEFINED ITTAPI_GIT_TAG); set(ITTAPI_GIT_TAG v3.18.12); endif(). if(NOT DEFINED ITTAPI_SOURCE_DIR); set(ITTAPI_SOURCE_DIR ${PROJECT_BINARY_DIR}); endif(). if(NOT EXISTS ${ITTAPI_SOURCE_DIR}/ittapi); execute_process(COMMAND ${GIT_EXECUTABLE} clone ${ITTAPI_GIT_REPOSITORY}; WORKING_DIRECTORY ${ITTAPI_SOURCE_DIR}; RESULT_VARIABLE GIT_CLONE_RESULT); if(NOT GIT_CLONE_RESULT EQUAL ""0""); message(FATAL_ERROR ""git clone ${ITTAPI_GIT_REPOSITORY} failed with ${GIT_CLONE_RESULT}, please clone ${ITTAPI_GIT_REPOSITORY}""); endif(); endif(). execute_process(COMMAND ${GIT_EXECUTABLE} checkout ${ITTAPI_GIT_TAG}; WORKING_DIRECTORY ${ITTAPI_SOURCE_DIR}/ittapi; RESULT_VARIABLE GIT_CHECKOUT_RESULT); if(NOT GIT_CHECKOUT_RESULT EQUAL ""0""); message(FATAL_ERROR ""git checkout ${ITTAPI_GIT_TAG} failed with ${GIT_CHECKOUT_RESULT}, please checkout ${ITTAPI_GIT_TAG} at ${ITTAPI_SOURCE_DIR}/ittapi""); endif(). include_directories( ${ITTAPI_SOURCE_DIR}/ittapi/include/ ). if( HAVE_LIBDL ); set(LLVM_INTEL_JIT_LIBS ${CMAKE_DL_LIBS}); endif(). set(LLVM_INTEL_JIT_LIBS ${LLVM_PTHREAD_LIB} ${LLVM_INTEL_JIT_LIBS}). add_llvm_component_library(LLVMIntelJITEvents; IntelJITEventListener.cpp; jitprofiling.c; ${ITTAPI_SOURCE_DIR}/ittapi/src/ittnotify/ittnotify_static.c. LINK_LIBS ${LLVM_INTEL_JIT_LIBS}. LINK_COMPONENTS; CodeGen; Core; DebugInfoDWARF; Support; Object; ExecutionEngine; ). add_dependencies(LLVMIntelJITEvents LLVMCodeGen); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/CMakeLists.txt:899,Integrability,message,message,899,"include_directories( ${CMAKE_CURRENT_SOURCE_DIR}/.. ). if(NOT DEFINED ITTAPI_GIT_REPOSITORY); set(ITTAPI_GIT_REPOSITORY https://github.com/intel/ittapi.git); endif(). if(NOT DEFINED ITTAPI_GIT_TAG); set(ITTAPI_GIT_TAG v3.18.12); endif(). if(NOT DEFINED ITTAPI_SOURCE_DIR); set(ITTAPI_SOURCE_DIR ${PROJECT_BINARY_DIR}); endif(). if(NOT EXISTS ${ITTAPI_SOURCE_DIR}/ittapi); execute_process(COMMAND ${GIT_EXECUTABLE} clone ${ITTAPI_GIT_REPOSITORY}; WORKING_DIRECTORY ${ITTAPI_SOURCE_DIR}; RESULT_VARIABLE GIT_CLONE_RESULT); if(NOT GIT_CLONE_RESULT EQUAL ""0""); message(FATAL_ERROR ""git clone ${ITTAPI_GIT_REPOSITORY} failed with ${GIT_CLONE_RESULT}, please clone ${ITTAPI_GIT_REPOSITORY}""); endif(); endif(). execute_process(COMMAND ${GIT_EXECUTABLE} checkout ${ITTAPI_GIT_TAG}; WORKING_DIRECTORY ${ITTAPI_SOURCE_DIR}/ittapi; RESULT_VARIABLE GIT_CHECKOUT_RESULT); if(NOT GIT_CHECKOUT_RESULT EQUAL ""0""); message(FATAL_ERROR ""git checkout ${ITTAPI_GIT_TAG} failed with ${GIT_CHECKOUT_RESULT}, please checkout ${ITTAPI_GIT_TAG} at ${ITTAPI_SOURCE_DIR}/ittapi""); endif(). include_directories( ${ITTAPI_SOURCE_DIR}/ittapi/include/ ). if( HAVE_LIBDL ); set(LLVM_INTEL_JIT_LIBS ${CMAKE_DL_LIBS}); endif(). set(LLVM_INTEL_JIT_LIBS ${LLVM_PTHREAD_LIB} ${LLVM_INTEL_JIT_LIBS}). add_llvm_component_library(LLVMIntelJITEvents; IntelJITEventListener.cpp; jitprofiling.c; ${ITTAPI_SOURCE_DIR}/ittapi/src/ittnotify/ittnotify_static.c. LINK_LIBS ${LLVM_INTEL_JIT_LIBS}. LINK_COMPONENTS; CodeGen; Core; DebugInfoDWARF; Support; Object; ExecutionEngine; ). add_dependencies(LLVMIntelJITEvents LLVMCodeGen); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Interpreter/CMakeLists.txt:99,Integrability,DEPEND,DEPENDS,99,add_llvm_component_library(LLVMInterpreter; Execution.cpp; ExternalFunctions.cpp; Interpreter.cpp. DEPENDS; intrinsics_gen. LINK_COMPONENTS; CodeGen; Core; ExecutionEngine; Support; ). if( LLVM_ENABLE_FFI ); target_link_libraries( LLVMInterpreter PRIVATE FFI::ffi ); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ExecutionEngine/Interpreter/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Interpreter/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/JITLink/CMakeLists.txt:810,Integrability,DEPEND,DEPENDS,810,set(LLVM_TARGET_DEFINITIONS COFFOptions.td); tablegen(LLVM COFFOptions.inc -gen-opt-parser-defs); add_public_tablegen_target(JITLinkTableGen). add_llvm_component_library(LLVMJITLink; DWARFRecordSectionSplitter.cpp; EHFrameSupport.cpp; JITLink.cpp; JITLinkGeneric.cpp; JITLinkMemoryManager.cpp. # Formats:. # MachO; MachO.cpp; MachO_arm64.cpp; MachO_x86_64.cpp; MachOLinkGraphBuilder.cpp. # ELF; ELF.cpp; ELFLinkGraphBuilder.cpp; ELF_aarch32.cpp; ELF_aarch64.cpp; ELF_i386.cpp; ELF_loongarch.cpp; ELF_ppc64.cpp; ELF_riscv.cpp; ELF_x86_64.cpp. # COFF; COFF.cpp; COFFDirectiveParser.cpp; COFFLinkGraphBuilder.cpp; COFF_x86_64.cpp. # Architectures:; aarch32.cpp; aarch64.cpp; i386.cpp; loongarch.cpp; ppc64.cpp; riscv.cpp; x86_64.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/ExecutionEngine/JITLink. DEPENDS; intrinsics_gen; JITLinkTableGen. LINK_COMPONENTS; BinaryFormat; Object; Option; OrcTargetProcess; Support; TargetParser; ). target_link_libraries(LLVMJITLink; PRIVATE; LLVMObject; LLVMOrcShared; LLVMOrcTargetProcess; LLVMSupport; LLVMTargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ExecutionEngine/JITLink/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/JITLink/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/CMakeLists.txt:49,Integrability,DEPEND,DEPENDS,49,add_llvm_component_library(LLVMMCJIT; MCJIT.cpp. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Core; ExecutionEngine; Object; RuntimeDyld; Support; Target; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/CMakeLists.txt:1168,Integrability,DEPEND,DEPENDS,1168,if (NOT HAVE_CXX_ATOMICS64_WITHOUT_LIB); set (atomic_lib atomic); endif(). if( CMAKE_HOST_UNIX AND HAVE_LIBRT ); set(rt_lib rt); endif(). add_llvm_component_library(LLVMOrcJIT; COFFVCRuntimeSupport.cpp; COFFPlatform.cpp; CompileOnDemandLayer.cpp; CompileUtils.cpp; Core.cpp; DebugObjectManagerPlugin.cpp; DebugUtils.cpp; EPCDynamicLibrarySearchGenerator.cpp; EPCDebugObjectRegistrar.cpp; EPCEHFrameRegistrar.cpp; EPCGenericDylibManager.cpp; EPCGenericJITLinkMemoryManager.cpp; EPCGenericRTDyldMemoryManager.cpp; EPCIndirectionUtils.cpp; ExecutionUtils.cpp; ObjectFileInterface.cpp; IndirectionUtils.cpp; IRCompileLayer.cpp; IRTransformLayer.cpp; JITTargetMachineBuilder.cpp; LazyReexports.cpp; Layer.cpp; LookupAndRecordAddrs.cpp; LLJIT.cpp; MachOPlatform.cpp; MapperJITLinkMemoryManager.cpp; MemoryMapper.cpp; ELFNixPlatform.cpp; Mangling.cpp; ObjectLinkingLayer.cpp; ObjectTransformLayer.cpp; OrcABISupport.cpp; OrcV2CBindings.cpp; RTDyldObjectLinkingLayer.cpp; SimpleRemoteEPC.cpp; Speculation.cpp; SpeculateAnalyses.cpp; ExecutorProcessControl.cpp; TaskDispatch.cpp; ThreadSafeModule.cpp; ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/ExecutionEngine/Orc. DEPENDS; intrinsics_gen. LINK_LIBS; ${LLVM_PTHREAD_LIB}; ${rt_lib}; ${atomic_lib}. LINK_COMPONENTS; Core; ExecutionEngine; JITLink; Object; OrcShared; OrcTargetProcess; WindowsDriver; MC; Passes; RuntimeDyld; Support; Target; TargetParser; TransformUtils; ). add_subdirectory(Debugging); add_subdirectory(Shared); add_subdirectory(TargetProcess). target_link_libraries(LLVMOrcJIT; PRIVATE; LLVMAnalysis; LLVMBitReader; LLVMBitWriter; LLVMPasses; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/CMakeLists.txt:964,Usability,Simpl,SimpleRemoteEPC,964,if (NOT HAVE_CXX_ATOMICS64_WITHOUT_LIB); set (atomic_lib atomic); endif(). if( CMAKE_HOST_UNIX AND HAVE_LIBRT ); set(rt_lib rt); endif(). add_llvm_component_library(LLVMOrcJIT; COFFVCRuntimeSupport.cpp; COFFPlatform.cpp; CompileOnDemandLayer.cpp; CompileUtils.cpp; Core.cpp; DebugObjectManagerPlugin.cpp; DebugUtils.cpp; EPCDynamicLibrarySearchGenerator.cpp; EPCDebugObjectRegistrar.cpp; EPCEHFrameRegistrar.cpp; EPCGenericDylibManager.cpp; EPCGenericJITLinkMemoryManager.cpp; EPCGenericRTDyldMemoryManager.cpp; EPCIndirectionUtils.cpp; ExecutionUtils.cpp; ObjectFileInterface.cpp; IndirectionUtils.cpp; IRCompileLayer.cpp; IRTransformLayer.cpp; JITTargetMachineBuilder.cpp; LazyReexports.cpp; Layer.cpp; LookupAndRecordAddrs.cpp; LLJIT.cpp; MachOPlatform.cpp; MapperJITLinkMemoryManager.cpp; MemoryMapper.cpp; ELFNixPlatform.cpp; Mangling.cpp; ObjectLinkingLayer.cpp; ObjectTransformLayer.cpp; OrcABISupport.cpp; OrcV2CBindings.cpp; RTDyldObjectLinkingLayer.cpp; SimpleRemoteEPC.cpp; Speculation.cpp; SpeculateAnalyses.cpp; ExecutorProcessControl.cpp; TaskDispatch.cpp; ThreadSafeModule.cpp; ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/ExecutionEngine/Orc. DEPENDS; intrinsics_gen. LINK_LIBS; ${LLVM_PTHREAD_LIB}; ${rt_lib}; ${atomic_lib}. LINK_COMPONENTS; Core; ExecutionEngine; JITLink; Object; OrcShared; OrcTargetProcess; WindowsDriver; MC; Passes; RuntimeDyld; Support; Target; TargetParser; TransformUtils; ). add_subdirectory(Debugging); add_subdirectory(Shared); add_subdirectory(TargetProcess). target_link_libraries(LLVMOrcJIT; PRIVATE; LLVMAnalysis; LLVMBitReader; LLVMBitWriter; LLVMPasses; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/CMakeLists.txt:220,Integrability,DEPEND,DEPENDS,220,add_llvm_component_library(LLVMRuntimeDyld; JITSymbol.cpp; RTDyldMemoryManager.cpp; RuntimeDyld.cpp; RuntimeDyldChecker.cpp; RuntimeDyldCOFF.cpp; RuntimeDyldELF.cpp; RuntimeDyldMachO.cpp; Targets/RuntimeDyldELFMips.cpp. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Core; MC; Object; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/Shared/CMakeLists.txt:216,Integrability,DEPEND,DEPENDS,216,add_llvm_component_library(LLVMOrcShared; AllocationActions.cpp; ObjectFormats.cpp; OrcError.cpp; OrcRTBridge.cpp; SimpleRemoteEPCUtils.cpp; ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/ExecutionEngine/Orc. DEPENDS; intrinsics_gen. LINK_LIBS; ${LLVM_PTHREAD_LIB}. LINK_COMPONENTS; Support; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/Shared/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/Shared/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/Shared/CMakeLists.txt:115,Usability,Simpl,SimpleRemoteEPCUtils,115,add_llvm_component_library(LLVMOrcShared; AllocationActions.cpp; ObjectFormats.cpp; OrcError.cpp; OrcRTBridge.cpp; SimpleRemoteEPCUtils.cpp; ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/ExecutionEngine/Orc. DEPENDS; intrinsics_gen. LINK_LIBS; ${LLVM_PTHREAD_LIB}. LINK_COMPONENTS; Support; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/Shared/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/Shared/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/TargetProcess/CMakeLists.txt:230,Usability,Simpl,SimpleExecutorDylibManager,230,if( CMAKE_HOST_UNIX AND HAVE_LIBRT ); set(rt_lib rt); endif(). add_llvm_component_library(LLVMOrcTargetProcess; ExecutorSharedMemoryMapperService.cpp; JITLoaderGDB.cpp; JITLoaderPerf.cpp; OrcRTBootstrap.cpp; RegisterEHFrames.cpp; SimpleExecutorDylibManager.cpp; SimpleExecutorMemoryManager.cpp; SimpleRemoteEPCServer.cpp; TargetExecutionUtils.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/ExecutionEngine/Orc. LINK_LIBS; ${LLVM_PTHREAD_LIB}; ${rt_lib}. LINK_COMPONENTS; OrcShared; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/TargetProcess/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/TargetProcess/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/TargetProcess/CMakeLists.txt:262,Usability,Simpl,SimpleExecutorMemoryManager,262,if( CMAKE_HOST_UNIX AND HAVE_LIBRT ); set(rt_lib rt); endif(). add_llvm_component_library(LLVMOrcTargetProcess; ExecutorSharedMemoryMapperService.cpp; JITLoaderGDB.cpp; JITLoaderPerf.cpp; OrcRTBootstrap.cpp; RegisterEHFrames.cpp; SimpleExecutorDylibManager.cpp; SimpleExecutorMemoryManager.cpp; SimpleRemoteEPCServer.cpp; TargetExecutionUtils.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/ExecutionEngine/Orc. LINK_LIBS; ${LLVM_PTHREAD_LIB}; ${rt_lib}. LINK_COMPONENTS; OrcShared; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/TargetProcess/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/TargetProcess/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/TargetProcess/CMakeLists.txt:295,Usability,Simpl,SimpleRemoteEPCServer,295,if( CMAKE_HOST_UNIX AND HAVE_LIBRT ); set(rt_lib rt); endif(). add_llvm_component_library(LLVMOrcTargetProcess; ExecutorSharedMemoryMapperService.cpp; JITLoaderGDB.cpp; JITLoaderPerf.cpp; OrcRTBootstrap.cpp; RegisterEHFrames.cpp; SimpleExecutorDylibManager.cpp; SimpleExecutorMemoryManager.cpp; SimpleRemoteEPCServer.cpp; TargetExecutionUtils.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/ExecutionEngine/Orc. LINK_LIBS; ${LLVM_PTHREAD_LIB}; ${rt_lib}. LINK_COMPONENTS; OrcShared; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/TargetProcess/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/TargetProcess/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Frontend/Driver/CMakeLists.txt:138,Integrability,DEPEND,DEPENDS,138,add_llvm_component_library(LLVMFrontendDriver; CodeGenOptions.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Frontend/Driver. DEPENDS; LLVMAnalysis; LLVMTargetParser. LINK_COMPONENTS; Core; Support; Analysis; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Frontend/Driver/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Frontend/Driver/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Frontend/HLSL/CMakeLists.txt:172,Integrability,DEPEND,DEPENDS,172,add_llvm_component_library(LLVMFrontendHLSL; HLSLResource.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Frontend; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Frontend/HLSL. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Core; Support; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Frontend/HLSL/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Frontend/HLSL/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Frontend/Offloading/CMakeLists.txt:148,Integrability,DEPEND,DEPENDS,148,add_llvm_component_library(LLVMFrontendOffloading; Utility.cpp; OffloadWrapper.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Frontend. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Core; BinaryFormat; Support; TransformUtils; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Frontend/Offloading/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Frontend/Offloading/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Frontend/OpenACC/CMakeLists.txt:169,Integrability,DEPEND,DEPENDS,169,add_llvm_component_library(LLVMFrontendOpenACC; ACC.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Frontend; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Frontend/OpenACC. DEPENDS; acc_gen; ). target_link_libraries(LLVMFrontendOpenACC LLVMSupport). ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Frontend/OpenACC/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Frontend/OpenACC/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Frontend/OpenMP/CMakeLists.txt:201,Integrability,DEPEND,DEPENDS,201,add_llvm_component_library(LLVMFrontendOpenMP; OMP.cpp; OMPContext.cpp; OMPIRBuilder.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Frontend; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Frontend/OpenMP. DEPENDS; intrinsics_gen; omp_gen. LINK_COMPONENTS; Core; Support; TargetParser; TransformUtils; Analysis; MC; Scalar; BitReader; FrontendOffloading; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Frontend/OpenMP/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Frontend/OpenMP/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ProfileData/Coverage/CMakeLists.txt:192,Integrability,DEPEND,DEPENDS,192,add_llvm_component_library(LLVMCoverage; CoverageMapping.cpp; CoverageMappingWriter.cpp; CoverageMappingReader.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/ProfileData/Coverage. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Core; Object; ProfileData; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ProfileData/Coverage/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ProfileData/Coverage/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt:475,Availability,avail,available,475,"set(LLVM_BLAKE3_FILES; blake3.c; blake3_dispatch.c; blake3_portable.c; blake3_neon.c; ). if (LLVM_DISABLE_ASSEMBLY_FILES); set(CAN_USE_ASSEMBLER FALSE); else(); set(CAN_USE_ASSEMBLER TRUE); endif(). macro(disable_blake3_x86_simd); add_compile_definitions(BLAKE3_NO_AVX512 BLAKE3_NO_AVX2 BLAKE3_NO_SSE41 BLAKE3_NO_SSE2); endmacro(). # The BLAKE3 team recommends using the assembly versions, from the README:; #; # ""For each of the x86 SIMD instruction sets, four versions are available:; # three flavors of assembly (Unix, Windows MSVC, and Windows GNU) and one; # version using C intrinsics. The assembly versions are generally; # preferred. They perform better, they perform more consistently across; # different compilers, and they build more quickly."". if (CAN_USE_ASSEMBLER); if (MSVC); check_symbol_exists(_M_X64 """" IS_X64); if (IS_X64); enable_language(ASM_MASM); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_msvc.asm; blake3_sse41_x86-64_windows_msvc.asm; blake3_avx2_x86-64_windows_msvc.asm; blake3_avx512_x86-64_windows_msvc.asm; ); else(); disable_blake3_x86_simd(); endif(); elseif(WIN32 OR CYGWIN); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_gnu.S; blake3_sse41_x86-64_windows_gnu.S; blake3_avx2_x86-64_windows_gnu.S; blake3_avx512_x86-64_windows_gnu.S; ); # Clang before 7 needs -mavx512vl to assemble some instructions.; set_source_files_properties(blake3_avx512_x86-64_windows_gnu.S; PROPERTIES COMPILE_OPTIONS ""-mavx512vl""); else(); disable_blake3_x86_simd(); endif(); else(); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64 OR CMAKE_OSX_ARCHITECTURES MATCHES ""x86_64""); # In a macOS Universal build (setting CMAKE_OSX_ARCHITECTURES to multiple; # values), compilation of the source files will target multiple architectures; # (each source file is internally compiled once for each architecture).; # To accomodate this configuration we include these assembly files without a; # CMake check but t",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt:1925,Deployability,configurat,configuration,1925,"nerally; # preferred. They perform better, they perform more consistently across; # different compilers, and they build more quickly."". if (CAN_USE_ASSEMBLER); if (MSVC); check_symbol_exists(_M_X64 """" IS_X64); if (IS_X64); enable_language(ASM_MASM); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_msvc.asm; blake3_sse41_x86-64_windows_msvc.asm; blake3_avx2_x86-64_windows_msvc.asm; blake3_avx512_x86-64_windows_msvc.asm; ); else(); disable_blake3_x86_simd(); endif(); elseif(WIN32 OR CYGWIN); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_gnu.S; blake3_sse41_x86-64_windows_gnu.S; blake3_avx2_x86-64_windows_gnu.S; blake3_avx512_x86-64_windows_gnu.S; ); # Clang before 7 needs -mavx512vl to assemble some instructions.; set_source_files_properties(blake3_avx512_x86-64_windows_gnu.S; PROPERTIES COMPILE_OPTIONS ""-mavx512vl""); else(); disable_blake3_x86_simd(); endif(); else(); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64 OR CMAKE_OSX_ARCHITECTURES MATCHES ""x86_64""); # In a macOS Universal build (setting CMAKE_OSX_ARCHITECTURES to multiple; # values), compilation of the source files will target multiple architectures; # (each source file is internally compiled once for each architecture).; # To accomodate this configuration we include these assembly files without a; # CMake check but their source is guarded with architecture ""#ifdef"" checks.; list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_unix.S; blake3_sse41_x86-64_unix.S; blake3_avx2_x86-64_unix.S; blake3_avx512_x86-64_unix.S; ); # Clang before 7 needs -mavx512vl to assemble some instructions.; set_source_files_properties(blake3_avx512_x86-64_unix.S; PROPERTIES COMPILE_OPTIONS ""-mavx512vl""); else(); disable_blake3_x86_simd(); endif(); endif(); else(); # CAN_USE_ASSEMBLER == FALSE; disable_blake3_x86_simd(); endif(). add_library(LLVMSupportBlake3 OBJECT EXCLUDE_FROM_ALL ${LLVM_BLAKE3_FILES}); llvm_update_compile_flags(LLVMSupportBlake3); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt:1925,Modifiability,config,configuration,1925,"nerally; # preferred. They perform better, they perform more consistently across; # different compilers, and they build more quickly."". if (CAN_USE_ASSEMBLER); if (MSVC); check_symbol_exists(_M_X64 """" IS_X64); if (IS_X64); enable_language(ASM_MASM); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_msvc.asm; blake3_sse41_x86-64_windows_msvc.asm; blake3_avx2_x86-64_windows_msvc.asm; blake3_avx512_x86-64_windows_msvc.asm; ); else(); disable_blake3_x86_simd(); endif(); elseif(WIN32 OR CYGWIN); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_gnu.S; blake3_sse41_x86-64_windows_gnu.S; blake3_avx2_x86-64_windows_gnu.S; blake3_avx512_x86-64_windows_gnu.S; ); # Clang before 7 needs -mavx512vl to assemble some instructions.; set_source_files_properties(blake3_avx512_x86-64_windows_gnu.S; PROPERTIES COMPILE_OPTIONS ""-mavx512vl""); else(); disable_blake3_x86_simd(); endif(); else(); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64 OR CMAKE_OSX_ARCHITECTURES MATCHES ""x86_64""); # In a macOS Universal build (setting CMAKE_OSX_ARCHITECTURES to multiple; # values), compilation of the source files will target multiple architectures; # (each source file is internally compiled once for each architecture).; # To accomodate this configuration we include these assembly files without a; # CMake check but their source is guarded with architecture ""#ifdef"" checks.; list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_unix.S; blake3_sse41_x86-64_unix.S; blake3_avx2_x86-64_unix.S; blake3_avx512_x86-64_unix.S; ); # Clang before 7 needs -mavx512vl to assemble some instructions.; set_source_files_properties(blake3_avx512_x86-64_unix.S; PROPERTIES COMPILE_OPTIONS ""-mavx512vl""); else(); disable_blake3_x86_simd(); endif(); endif(); else(); # CAN_USE_ASSEMBLER == FALSE; disable_blake3_x86_simd(); endif(). add_library(LLVMSupportBlake3 OBJECT EXCLUDE_FROM_ALL ${LLVM_BLAKE3_FILES}); llvm_update_compile_flags(LLVMSupportBlake3); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt:647,Performance,perform,perform,647,"set(LLVM_BLAKE3_FILES; blake3.c; blake3_dispatch.c; blake3_portable.c; blake3_neon.c; ). if (LLVM_DISABLE_ASSEMBLY_FILES); set(CAN_USE_ASSEMBLER FALSE); else(); set(CAN_USE_ASSEMBLER TRUE); endif(). macro(disable_blake3_x86_simd); add_compile_definitions(BLAKE3_NO_AVX512 BLAKE3_NO_AVX2 BLAKE3_NO_SSE41 BLAKE3_NO_SSE2); endmacro(). # The BLAKE3 team recommends using the assembly versions, from the README:; #; # ""For each of the x86 SIMD instruction sets, four versions are available:; # three flavors of assembly (Unix, Windows MSVC, and Windows GNU) and one; # version using C intrinsics. The assembly versions are generally; # preferred. They perform better, they perform more consistently across; # different compilers, and they build more quickly."". if (CAN_USE_ASSEMBLER); if (MSVC); check_symbol_exists(_M_X64 """" IS_X64); if (IS_X64); enable_language(ASM_MASM); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_msvc.asm; blake3_sse41_x86-64_windows_msvc.asm; blake3_avx2_x86-64_windows_msvc.asm; blake3_avx512_x86-64_windows_msvc.asm; ); else(); disable_blake3_x86_simd(); endif(); elseif(WIN32 OR CYGWIN); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_gnu.S; blake3_sse41_x86-64_windows_gnu.S; blake3_avx2_x86-64_windows_gnu.S; blake3_avx512_x86-64_windows_gnu.S; ); # Clang before 7 needs -mavx512vl to assemble some instructions.; set_source_files_properties(blake3_avx512_x86-64_windows_gnu.S; PROPERTIES COMPILE_OPTIONS ""-mavx512vl""); else(); disable_blake3_x86_simd(); endif(); else(); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64 OR CMAKE_OSX_ARCHITECTURES MATCHES ""x86_64""); # In a macOS Universal build (setting CMAKE_OSX_ARCHITECTURES to multiple; # values), compilation of the source files will target multiple architectures; # (each source file is internally compiled once for each architecture).; # To accomodate this configuration we include these assembly files without a; # CMake check but t",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt:668,Performance,perform,perform,668,"set(LLVM_BLAKE3_FILES; blake3.c; blake3_dispatch.c; blake3_portable.c; blake3_neon.c; ). if (LLVM_DISABLE_ASSEMBLY_FILES); set(CAN_USE_ASSEMBLER FALSE); else(); set(CAN_USE_ASSEMBLER TRUE); endif(). macro(disable_blake3_x86_simd); add_compile_definitions(BLAKE3_NO_AVX512 BLAKE3_NO_AVX2 BLAKE3_NO_SSE41 BLAKE3_NO_SSE2); endmacro(). # The BLAKE3 team recommends using the assembly versions, from the README:; #; # ""For each of the x86 SIMD instruction sets, four versions are available:; # three flavors of assembly (Unix, Windows MSVC, and Windows GNU) and one; # version using C intrinsics. The assembly versions are generally; # preferred. They perform better, they perform more consistently across; # different compilers, and they build more quickly."". if (CAN_USE_ASSEMBLER); if (MSVC); check_symbol_exists(_M_X64 """" IS_X64); if (IS_X64); enable_language(ASM_MASM); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_msvc.asm; blake3_sse41_x86-64_windows_msvc.asm; blake3_avx2_x86-64_windows_msvc.asm; blake3_avx512_x86-64_windows_msvc.asm; ); else(); disable_blake3_x86_simd(); endif(); elseif(WIN32 OR CYGWIN); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_gnu.S; blake3_sse41_x86-64_windows_gnu.S; blake3_avx2_x86-64_windows_gnu.S; blake3_avx512_x86-64_windows_gnu.S; ); # Clang before 7 needs -mavx512vl to assemble some instructions.; set_source_files_properties(blake3_avx512_x86-64_windows_gnu.S; PROPERTIES COMPILE_OPTIONS ""-mavx512vl""); else(); disable_blake3_x86_simd(); endif(); else(); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64 OR CMAKE_OSX_ARCHITECTURES MATCHES ""x86_64""); # In a macOS Universal build (setting CMAKE_OSX_ARCHITECTURES to multiple; # values), compilation of the source files will target multiple architectures; # (each source file is internally compiled once for each architecture).; # To accomodate this configuration we include these assembly files without a; # CMake check but t",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/CMakeLists.txt:3347,Integrability,DEPEND,DEPENDS,3347,GISel/AArch64O0PreLegalizerCombiner.cpp; GISel/AArch64PreLegalizerCombiner.cpp; GISel/AArch64PostLegalizerCombiner.cpp; GISel/AArch64PostLegalizerLowering.cpp; GISel/AArch64PostSelectOptimize.cpp; GISel/AArch64RegisterBankInfo.cpp; AArch64A57FPLoadBalancing.cpp; AArch64AdvSIMDScalarPass.cpp; AArch64Arm64ECCallLowering.cpp; AArch64AsmPrinter.cpp; AArch64BranchTargets.cpp; AArch64CallingConvention.cpp; AArch64CleanupLocalDynamicTLSPass.cpp; AArch64CollectLOH.cpp; AArch64CondBrTuning.cpp; AArch64ConditionalCompares.cpp; AArch64DeadRegisterDefinitionsPass.cpp; AArch64ExpandImm.cpp; AArch64ExpandPseudoInsts.cpp; AArch64FalkorHWPFFix.cpp; AArch64FastISel.cpp; AArch64A53Fix835769.cpp; AArch64FrameLowering.cpp; AArch64GlobalsTagging.cpp; AArch64CompressJumpTables.cpp; AArch64ConditionOptimizer.cpp; AArch64RedundantCopyElimination.cpp; AArch64ISelDAGToDAG.cpp; AArch64ISelLowering.cpp; AArch64InstrInfo.cpp; AArch64LoadStoreOptimizer.cpp; AArch64LoopIdiomTransform.cpp; AArch64LowerHomogeneousPrologEpilog.cpp; AArch64MachineFunctionInfo.cpp; AArch64MachineScheduler.cpp; AArch64MacroFusion.cpp; AArch64MIPeepholeOpt.cpp; AArch64MCInstLower.cpp; AArch64PointerAuth.cpp; AArch64PromoteConstant.cpp; AArch64PBQPRegAlloc.cpp; AArch64RegisterInfo.cpp; AArch64SLSHardening.cpp; AArch64SelectionDAGInfo.cpp; AArch64SpeculationHardening.cpp; AArch64StackTagging.cpp; AArch64StackTaggingPreRA.cpp; AArch64StorePairSuppress.cpp; AArch64Subtarget.cpp; AArch64TargetMachine.cpp; AArch64TargetObjectFile.cpp; AArch64TargetTransformInfo.cpp; SMEABIPass.cpp; SVEIntrinsicOpts.cpp; AArch64SIMDInstrOpt.cpp. DEPENDS; intrinsics_gen. LINK_COMPONENTS; AArch64Desc; AArch64Info; AArch64Utils; Analysis; AsmPrinter; CFGuard; CodeGen; CodeGenTypes; Core; GlobalISel; MC; Scalar; SelectionDAG; Support; Target; TargetParser; TransformUtils. ADD_TO_COMPONENT; AArch64; ). add_subdirectory(AsmParser); add_subdirectory(Disassembler); add_subdirectory(MCTargetDesc); add_subdirectory(TargetInfo); add_subdirectory(Utils); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/AArch64/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:473,Availability,failure,failure,473,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend (Thumb specific).; //===---------------------------------------------------------------------===//. * Add support for compiling functions in both ARM and Thumb mode, then taking; the smallest. * Add support for compiling individual basic blocks in thumb mode, when in a ; larger ARM function. This can be used for presumed cold code, like paths; to abort (failure path of asserts), EH handling code, etc. * Thumb doesn't have normal pre/post increment addressing modes, but you can; load/store 32-bit integers with pre/postinc by using load/store multiple; instrs with a single register. * Make better use of high registers r8, r10, r11, r12 (ip). Some variants of add; and cmp instructions can use high registers. Also, we can use them as; temporaries to spill values into. * In thumb mode, short, byte, and bool preferred alignments are currently set; to 4 to accommodate ISA restriction (i.e. add sp, #imm, imm must be multiple; of 4). //===---------------------------------------------------------------------===//. Potential jumptable improvements:. * If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit; jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the; function is even smaller. This also applies to ARM. * Thumb jumptable codegen can improve given some help from the assembler. This; is what we generate right now:. 	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4)); LPCRELL0:; 	mov r1, #PCRELV0; 	add r1, pc; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3; ... Note there is another pc relative add that we can take advantage of.; add r1, pc, #imm_8 * 4. We should be able to generate:. LPCRELL0:; 	add r1, LJTI1_0_0; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3. if the assembler can translate the add to:; add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc). Note the assembler also doe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:5674,Deployability,toggle,toggle,5674,"-----------------------------------------===//. Poor codegen test/CodeGen/ARM/select.ll f7:. 	ldr r5, LCPI1_0; LPC0:; 	add r5, pc; 	ldr r6, LCPI1_1; 	ldr r2, LCPI1_2; 	mov r3, r6; 	mov lr, pc; 	bx r5. //===---------------------------------------------------------------------===//. Make register allocator / spiller smarter so we can re-materialize ""mov r, imm"",; etc. Almost all Thumb instructions clobber condition code. //===---------------------------------------------------------------------===//. Thumb load / store address mode offsets are scaled. The values kept in the; instruction operands are pre-scale values. This probably ought to be changed; to avoid extra work when we convert Thumb2 instructions to Thumb1 instructions. //===---------------------------------------------------------------------===//. We need to make (some of the) Thumb1 instructions predicable. That will allow; shrinking of predicated Thumb2 instructions. To allow this, we need to be able; to toggle the 's' bit since they do not set CPSR when they are inside IT blocks. //===---------------------------------------------------------------------===//. Make use of hi register variants of cmp: tCMPhir / tCMPZhir. //===---------------------------------------------------------------------===//. Thumb1 immediate field sometimes keep pre-scaled values. See; ThumbRegisterInfo::eliminateFrameIndex. This is inconsistent from ARM and; Thumb2. //===---------------------------------------------------------------------===//. Rather than having tBR_JTr print a "".align 2"" and constant island pass pad it,; add a target specific ALIGN instruction instead. That way, getInstSizeInBytes; won't have to over-estimate. It can also be used for loop alignment pass. //===---------------------------------------------------------------------===//. We generate conditional code for icmp when we don't need to. This code:. int foo(int s) {; return s == 1;; }. produces:. foo:; cmp r0, #1; mov.w r0, #0; it eq; moveq r0, #1; bx l",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:600,Performance,load,load,600,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend (Thumb specific).; //===---------------------------------------------------------------------===//. * Add support for compiling functions in both ARM and Thumb mode, then taking; the smallest. * Add support for compiling individual basic blocks in thumb mode, when in a ; larger ARM function. This can be used for presumed cold code, like paths; to abort (failure path of asserts), EH handling code, etc. * Thumb doesn't have normal pre/post increment addressing modes, but you can; load/store 32-bit integers with pre/postinc by using load/store multiple; instrs with a single register. * Make better use of high registers r8, r10, r11, r12 (ip). Some variants of add; and cmp instructions can use high registers. Also, we can use them as; temporaries to spill values into. * In thumb mode, short, byte, and bool preferred alignments are currently set; to 4 to accommodate ISA restriction (i.e. add sp, #imm, imm must be multiple; of 4). //===---------------------------------------------------------------------===//. Potential jumptable improvements:. * If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit; jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the; function is even smaller. This also applies to ARM. * Thumb jumptable codegen can improve given some help from the assembler. This; is what we generate right now:. 	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4)); LPCRELL0:; 	mov r1, #PCRELV0; 	add r1, pc; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3; ... Note there is another pc relative add that we can take advantage of.; add r1, pc, #imm_8 * 4. We should be able to generate:. LPCRELL0:; 	add r1, LJTI1_0_0; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3. if the assembler can translate the add to:; add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc). Note the assembler also doe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:653,Performance,load,load,653,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend (Thumb specific).; //===---------------------------------------------------------------------===//. * Add support for compiling functions in both ARM and Thumb mode, then taking; the smallest. * Add support for compiling individual basic blocks in thumb mode, when in a ; larger ARM function. This can be used for presumed cold code, like paths; to abort (failure path of asserts), EH handling code, etc. * Thumb doesn't have normal pre/post increment addressing modes, but you can; load/store 32-bit integers with pre/postinc by using load/store multiple; instrs with a single register. * Make better use of high registers r8, r10, r11, r12 (ip). Some variants of add; and cmp instructions can use high registers. Also, we can use them as; temporaries to spill values into. * In thumb mode, short, byte, and bool preferred alignments are currently set; to 4 to accommodate ISA restriction (i.e. add sp, #imm, imm must be multiple; of 4). //===---------------------------------------------------------------------===//. Potential jumptable improvements:. * If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit; jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the; function is even smaller. This also applies to ARM. * Thumb jumptable codegen can improve given some help from the assembler. This; is what we generate right now:. 	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4)); LPCRELL0:; 	mov r1, #PCRELV0; 	add r1, pc; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3; ... Note there is another pc relative add that we can take advantage of.; add r1, pc, #imm_8 * 4. We should be able to generate:. LPCRELL0:; 	add r1, LJTI1_0_0; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3. if the assembler can translate the add to:; add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc). Note the assembler also doe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:2034,Performance,load,load,2034,"; of 4). //===---------------------------------------------------------------------===//. Potential jumptable improvements:. * If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit; jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the; function is even smaller. This also applies to ARM. * Thumb jumptable codegen can improve given some help from the assembler. This; is what we generate right now:. 	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4)); LPCRELL0:; 	mov r1, #PCRELV0; 	add r1, pc; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3; ... Note there is another pc relative add that we can take advantage of.; add r1, pc, #imm_8 * 4. We should be able to generate:. LPCRELL0:; 	add r1, LJTI1_0_0; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3. if the assembler can translate the add to:; add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc). Note the assembler also does something similar to constpool load:; LPCRELL0:; ldr r0, LCPI1_0; =>; ldr r0, pc, #((LCPI1_0-(LPCRELL0+4))&0xfffffffc). //===---------------------------------------------------------------------===//. We compile the following:. define i16 @func_entry_2E_ce(i32 %i) {; switch i32 %i, label %bb12.exitStub [; i32 0, label %bb4.exitStub; i32 1, label %bb9.exitStub; i32 2, label %bb4.exitStub; i32 3, label %bb4.exitStub; i32 7, label %bb9.exitStub; i32 8, label %bb.exitStub; i32 9, label %bb9.exitStub; ]. bb12.exitStub:; ret i16 0. bb4.exitStub:; ret i16 1. bb9.exitStub:; ret i16 2. bb.exitStub:; ret i16 3; }. into:. _func_entry_2E_ce:; mov r2, #1; lsl r2, r0; cmp r0, #9; bhi LBB1_4 @bb12.exitStub; LBB1_1: @newFuncRoot; mov r1, #13; tst r2, r1; bne LBB1_5 @bb4.exitStub; LBB1_2: @newFuncRoot; ldr r1, LCPI1_0; tst r2, r1; bne LBB1_6 @bb9.exitStub; LBB1_3: @newFuncRoot; mov r1, #1; lsl r1, r1, #8; tst r2, r1; bne LBB1_7 @bb.exitStub; LBB1_4: @bb12.exitStub; mov r0, #0; bx lr; LBB1_5: @bb4.exitStub; mov r0, #1; bx lr; LBB1_6: @bb9.exitStub",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:4100,Performance,load,load,4100,"@bb.exitStub; mov r0, #3; bx lr; LBB1_8:; .align 2; LCPI1_0:; .long 642. gcc compiles to:. 	cmp	r0, #9; 	@ lr needed for prologue; 	bhi	L2; 	ldr	r3, L11; 	mov	r2, #1; 	mov	r1, r2, asl r0; 	ands	r0, r3, r2, asl r0; 	movne	r0, #2; 	bxne	lr; 	tst	r1, #13; 	beq	L9; L3:; 	mov	r0, r2; 	bx	lr; L9:; 	tst	r1, #256; 	movne	r0, #3; 	bxne	lr; L2:; 	mov	r0, #0; 	bx	lr; L12:; 	.align 2; L11:; 	.long	642; . GCC is doing a couple of clever things here:; 1. It is predicating one of the returns. This isn't a clear win though: in; cases where that return isn't taken, it is replacing one condbranch with; two 'ne' predicated instructions.; 2. It is sinking the shift of ""1 << i"" into the tst, and using ands instead of; tst. This will probably require whole function isel.; 3. GCC emits:; 	tst	r1, #256; we emit:; mov r1, #1; lsl r1, r1, #8; tst r2, r1. //===---------------------------------------------------------------------===//. When spilling in thumb mode and the sp offset is too large to fit in the ldr /; str offset field, we load the offset from a constpool entry and add it to sp:. ldr r2, LCPI; add r2, sp; ldr r2, [r2]. These instructions preserve the condition code which is important if the spill; is between a cmp and a bcc instruction. However, we can use the (potentially); cheaper sequence if we know it's ok to clobber the condition register. add r2, sp, #255 * 4; add r2, #132; ldr r2, [r2, #7 * 4]. This is especially bad when dynamic alloca is used. The all fixed size stack; objects are referenced off the frame pointer with negative offsets. See; oggenc for an example. //===---------------------------------------------------------------------===//. Poor codegen test/CodeGen/ARM/select.ll f7:. 	ldr r5, LCPI1_0; LPC0:; 	add r5, pc; 	ldr r6, LCPI1_1; 	ldr r2, LCPI1_2; 	mov r3, r6; 	mov lr, pc; 	bx r5. //===---------------------------------------------------------------------===//. Make register allocator / spiller smarter so we can re-materialize ""mov r, imm"",; etc. Almost all Thumb",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:5203,Performance,load,load,5203,"rve the condition code which is important if the spill; is between a cmp and a bcc instruction. However, we can use the (potentially); cheaper sequence if we know it's ok to clobber the condition register. add r2, sp, #255 * 4; add r2, #132; ldr r2, [r2, #7 * 4]. This is especially bad when dynamic alloca is used. The all fixed size stack; objects are referenced off the frame pointer with negative offsets. See; oggenc for an example. //===---------------------------------------------------------------------===//. Poor codegen test/CodeGen/ARM/select.ll f7:. 	ldr r5, LCPI1_0; LPC0:; 	add r5, pc; 	ldr r6, LCPI1_1; 	ldr r2, LCPI1_2; 	mov r3, r6; 	mov lr, pc; 	bx r5. //===---------------------------------------------------------------------===//. Make register allocator / spiller smarter so we can re-materialize ""mov r, imm"",; etc. Almost all Thumb instructions clobber condition code. //===---------------------------------------------------------------------===//. Thumb load / store address mode offsets are scaled. The values kept in the; instruction operands are pre-scale values. This probably ought to be changed; to avoid extra work when we convert Thumb2 instructions to Thumb1 instructions. //===---------------------------------------------------------------------===//. We need to make (some of the) Thumb1 instructions predicable. That will allow; shrinking of predicated Thumb2 instructions. To allow this, we need to be able; to toggle the 's' bit since they do not set CPSR when they are inside IT blocks. //===---------------------------------------------------------------------===//. Make use of hi register variants of cmp: tCMPhir / tCMPZhir. //===---------------------------------------------------------------------===//. Thumb1 immediate field sometimes keep pre-scaled values. See; ThumbRegisterInfo::eliminateFrameIndex. This is inconsistent from ARM and; Thumb2. //===---------------------------------------------------------------------===//. Rather than having tB",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:466,Safety,abort,abort,466,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend (Thumb specific).; //===---------------------------------------------------------------------===//. * Add support for compiling functions in both ARM and Thumb mode, then taking; the smallest. * Add support for compiling individual basic blocks in thumb mode, when in a ; larger ARM function. This can be used for presumed cold code, like paths; to abort (failure path of asserts), EH handling code, etc. * Thumb doesn't have normal pre/post increment addressing modes, but you can; load/store 32-bit integers with pre/postinc by using load/store multiple; instrs with a single register. * Make better use of high registers r8, r10, r11, r12 (ip). Some variants of add; and cmp instructions can use high registers. Also, we can use them as; temporaries to spill values into. * In thumb mode, short, byte, and bool preferred alignments are currently set; to 4 to accommodate ISA restriction (i.e. add sp, #imm, imm must be multiple; of 4). //===---------------------------------------------------------------------===//. Potential jumptable improvements:. * If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit; jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the; function is even smaller. This also applies to ARM. * Thumb jumptable codegen can improve given some help from the assembler. This; is what we generate right now:. 	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4)); LPCRELL0:; 	mov r1, #PCRELV0; 	add r1, pc; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3; ... Note there is another pc relative add that we can take advantage of.; add r1, pc, #imm_8 * 4. We should be able to generate:. LPCRELL0:; 	add r1, LJTI1_0_0; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3. if the assembler can translate the add to:; add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc). Note the assembler also doe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:5354,Safety,avoid,avoid,5354,"e if we know it's ok to clobber the condition register. add r2, sp, #255 * 4; add r2, #132; ldr r2, [r2, #7 * 4]. This is especially bad when dynamic alloca is used. The all fixed size stack; objects are referenced off the frame pointer with negative offsets. See; oggenc for an example. //===---------------------------------------------------------------------===//. Poor codegen test/CodeGen/ARM/select.ll f7:. 	ldr r5, LCPI1_0; LPC0:; 	add r5, pc; 	ldr r6, LCPI1_1; 	ldr r2, LCPI1_2; 	mov r3, r6; 	mov lr, pc; 	bx r5. //===---------------------------------------------------------------------===//. Make register allocator / spiller smarter so we can re-materialize ""mov r, imm"",; etc. Almost all Thumb instructions clobber condition code. //===---------------------------------------------------------------------===//. Thumb load / store address mode offsets are scaled. The values kept in the; instruction operands are pre-scale values. This probably ought to be changed; to avoid extra work when we convert Thumb2 instructions to Thumb1 instructions. //===---------------------------------------------------------------------===//. We need to make (some of the) Thumb1 instructions predicable. That will allow; shrinking of predicated Thumb2 instructions. To allow this, we need to be able; to toggle the 's' bit since they do not set CPSR when they are inside IT blocks. //===---------------------------------------------------------------------===//. Make use of hi register variants of cmp: tCMPhir / tCMPZhir. //===---------------------------------------------------------------------===//. Thumb1 immediate field sometimes keep pre-scaled values. See; ThumbRegisterInfo::eliminateFrameIndex. This is inconsistent from ARM and; Thumb2. //===---------------------------------------------------------------------===//. Rather than having tBR_JTr print a "".align 2"" and constant island pass pad it,; add a target specific ALIGN instruction instead. That way, getInstSizeInBytes; won't have to",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:489,Testability,assert,asserts,489,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend (Thumb specific).; //===---------------------------------------------------------------------===//. * Add support for compiling functions in both ARM and Thumb mode, then taking; the smallest. * Add support for compiling individual basic blocks in thumb mode, when in a ; larger ARM function. This can be used for presumed cold code, like paths; to abort (failure path of asserts), EH handling code, etc. * Thumb doesn't have normal pre/post increment addressing modes, but you can; load/store 32-bit integers with pre/postinc by using load/store multiple; instrs with a single register. * Make better use of high registers r8, r10, r11, r12 (ip). Some variants of add; and cmp instructions can use high registers. Also, we can use them as; temporaries to spill values into. * In thumb mode, short, byte, and bool preferred alignments are currently set; to 4 to accommodate ISA restriction (i.e. add sp, #imm, imm must be multiple; of 4). //===---------------------------------------------------------------------===//. Potential jumptable improvements:. * If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit; jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the; function is even smaller. This also applies to ARM. * Thumb jumptable codegen can improve given some help from the assembler. This; is what we generate right now:. 	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4)); LPCRELL0:; 	mov r1, #PCRELV0; 	add r1, pc; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3; ... Note there is another pc relative add that we can take advantage of.; add r1, pc, #imm_8 * 4. We should be able to generate:. LPCRELL0:; 	add r1, LJTI1_0_0; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3. if the assembler can translate the add to:; add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc). Note the assembler also doe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:4754,Testability,test,test,4754,"d using ands instead of; tst. This will probably require whole function isel.; 3. GCC emits:; 	tst	r1, #256; we emit:; mov r1, #1; lsl r1, r1, #8; tst r2, r1. //===---------------------------------------------------------------------===//. When spilling in thumb mode and the sp offset is too large to fit in the ldr /; str offset field, we load the offset from a constpool entry and add it to sp:. ldr r2, LCPI; add r2, sp; ldr r2, [r2]. These instructions preserve the condition code which is important if the spill; is between a cmp and a bcc instruction. However, we can use the (potentially); cheaper sequence if we know it's ok to clobber the condition register. add r2, sp, #255 * 4; add r2, #132; ldr r2, [r2, #7 * 4]. This is especially bad when dynamic alloca is used. The all fixed size stack; objects are referenced off the frame pointer with negative offsets. See; oggenc for an example. //===---------------------------------------------------------------------===//. Poor codegen test/CodeGen/ARM/select.ll f7:. 	ldr r5, LCPI1_0; LPC0:; 	add r5, pc; 	ldr r6, LCPI1_1; 	ldr r2, LCPI1_2; 	mov r3, r6; 	mov lr, pc; 	bx r5. //===---------------------------------------------------------------------===//. Make register allocator / spiller smarter so we can re-materialize ""mov r, imm"",; etc. Almost all Thumb instructions clobber condition code. //===---------------------------------------------------------------------===//. Thumb load / store address mode offsets are scaled. The values kept in the; instruction operands are pre-scale values. This probably ought to be changed; to avoid extra work when we convert Thumb2 instructions to Thumb1 instructions. //===---------------------------------------------------------------------===//. We need to make (some of the) Thumb1 instructions predicable. That will allow; shrinking of predicated Thumb2 instructions. To allow this, we need to be able; to toggle the 's' bit since they do not set CPSR when they are inside IT blocks. //===--",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:3573,Usability,clear,clear,3573,"try_2E_ce:; mov r2, #1; lsl r2, r0; cmp r0, #9; bhi LBB1_4 @bb12.exitStub; LBB1_1: @newFuncRoot; mov r1, #13; tst r2, r1; bne LBB1_5 @bb4.exitStub; LBB1_2: @newFuncRoot; ldr r1, LCPI1_0; tst r2, r1; bne LBB1_6 @bb9.exitStub; LBB1_3: @newFuncRoot; mov r1, #1; lsl r1, r1, #8; tst r2, r1; bne LBB1_7 @bb.exitStub; LBB1_4: @bb12.exitStub; mov r0, #0; bx lr; LBB1_5: @bb4.exitStub; mov r0, #1; bx lr; LBB1_6: @bb9.exitStub; mov r0, #2; bx lr; LBB1_7: @bb.exitStub; mov r0, #3; bx lr; LBB1_8:; .align 2; LCPI1_0:; .long 642. gcc compiles to:. 	cmp	r0, #9; 	@ lr needed for prologue; 	bhi	L2; 	ldr	r3, L11; 	mov	r2, #1; 	mov	r1, r2, asl r0; 	ands	r0, r3, r2, asl r0; 	movne	r0, #2; 	bxne	lr; 	tst	r1, #13; 	beq	L9; L3:; 	mov	r0, r2; 	bx	lr; L9:; 	tst	r1, #256; 	movne	r0, #3; 	bxne	lr; L2:; 	mov	r0, #0; 	bx	lr; L12:; 	.align 2; L11:; 	.long	642; . GCC is doing a couple of clever things here:; 1. It is predicating one of the returns. This isn't a clear win though: in; cases where that return isn't taken, it is replacing one condbranch with; two 'ne' predicated instructions.; 2. It is sinking the shift of ""1 << i"" into the tst, and using ands instead of; tst. This will probably require whole function isel.; 3. GCC emits:; 	tst	r1, #256; we emit:; mov r1, #1; lsl r1, r1, #8; tst r2, r1. //===---------------------------------------------------------------------===//. When spilling in thumb mode and the sp offset is too large to fit in the ldr /; str offset field, we load the offset from a constpool entry and add it to sp:. ldr r2, LCPI; add r2, sp; ldr r2, [r2]. These instructions preserve the condition code which is important if the spill; is between a cmp and a bcc instruction. However, we can use the (potentially); cheaper sequence if we know it's ok to clobber the condition register. add r2, sp, #255 * 4; add r2, #132; ldr r2, [r2, #7 * 4]. This is especially bad when dynamic alloca is used. The all fixed size stack; objects are referenced off the frame pointer with negative offsets.",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:19313,Availability,redundant,redundant,19313,"--------------------===//. The code generated for bswap on armv4/5 (CPUs without rev) is less than ideal:. int a(int x) { return __builtin_bswap32(x); }. a:; 	mov	r1, #255, 24; 	mov	r2, #255, 16; 	and	r1, r1, r0, lsr #8; 	and	r2, r2, r0, lsl #8; 	orr	r1, r1, r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16; 	bic r1, r1, #0xff0000; 	mov r1, r1, lsr #8; 	eor r0, r1, r0, ror #8; 	bx	lr. A custom Thumb version would also be a slight improvement over the generic; version. //===---------------------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb r0, [r0]; ldrb r1, [r1]; orr r0, r1, r0; tst r0, #255; moveq r0, #0; streq r0, [r2]; bx lr. Note that both ""tst"" and ""moveq"" are redundant. //===---------------------------------------------------------------------===//. When loading immediate constants with movt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:21536,Availability,robust,robust,21536,"----------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===---------------------------------------------------------------------===//. Clean up the test/MC/ARM files to have more robust register choices. R0 should not be used as a register operand in the assembler tests as it's then; not possible to distinguish between a correct encoding and a missing operand; encoding, as zero is the default value for the binary encoder.; e.g.,; add r0, r0 // bad; add r3, r5 // good. Register operands should be distinct. That is, when the encoding does not; require two syntactical operands to refer to the same register, two different; registers should be used in the test so as to catch errors where the; operands are swapped in the encoding.; e.g.,; subs.w r1, r1, r1 // bad; subs.w r1, r2, r3 // good. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:22036,Availability,error,errors,22036,"----------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===---------------------------------------------------------------------===//. Clean up the test/MC/ARM files to have more robust register choices. R0 should not be used as a register operand in the assembler tests as it's then; not possible to distinguish between a correct encoding and a missing operand; encoding, as zero is the default value for the binary encoder.; e.g.,; add r0, r0 // bad; add r3, r5 // good. Register operands should be distinct. That is, when the encoding does not; require two syntactical operands to refer to the same register, two different; registers should be used in the test so as to catch errors where the; operands are swapped in the encoding.; e.g.,; subs.w r1, r1, r1 // bad; subs.w r1, r2, r3 // good. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:9553,Deployability,update,update,9553," However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly better code for this function. int foo(int StackPtr, unsigned char *Line, unsigned char *Stack, int LineLen) {; int i = 0;. if (StackPtr != 0) {; while (StackPtr != 0 && i < (((LineLen) < (32768))? (LineLen) : (32768))); Line[i++] = Stack[--StackPtr];; if (LineLen > 32768); {; while (StackPtr != 0 && i < LineLen); {; i++;; --StackPtr;; }; }; }; return StackPtr;; }. //===---------------------------------------------------------------------===//. This should compile to the mlas instruction:; int mlas(int x, int y, int z) { return ((x * y + z) < 0) ? 7 : 13; }. //===---------------------------------------------------------------------===//. At some point, we should triage these to see if they still apply to us",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:16878,Deployability,patch,patches,16878,":. _foo:; 	and r1, r0, #127; 	ldr r2, LCPI1_0; 	ldr r2, [r2]; 	ldr r1, [r2, +r1, lsl #2]; 	mov r2, r1, lsr #2; 	tst r0, #128; 	moveq r2, r1; 	ldr r0, LCPI1_1; 	and r0, r2, r0; 	bx lr. It would be better to do something like this, to fold the shift into the; conditional move:. 	and r1, r0, #127; 	ldr r2, LCPI1_0; 	ldr r2, [r2]; 	ldr r1, [r2, +r1, lsl #2]; 	tst r0, #128; 	movne r1, r1, lsr #2; 	ldr r0, LCPI1_1; 	and r0, r1, r0; 	bx lr. it saves an instruction and a register. //===---------------------------------------------------------------------===//. It might be profitable to cse MOVi16 if there are lots of 32-bit immediates; with the same bottom half. //===---------------------------------------------------------------------===//. Robert Muth started working on an alternate jump table implementation that; does not put the tables in-line in the text. This is more like the llvm; default jump table implementation. This might be useful sometime. Several; revisions of patches are on the mailing list, beginning at:; http://lists.llvm.org/pipermail/llvm-dev/2009-June/022763.html. //===---------------------------------------------------------------------===//. Make use of the ""rbit"" instruction. //===---------------------------------------------------------------------===//. Take a look at test/CodeGen/Thumb2/machine-licm.ll. ARM should be taught how; to licm and cse the unnecessary load from cp#1. //===---------------------------------------------------------------------===//. The CMN instruction sets the flags like an ADD instruction, while CMP sets; them like a subtract. Therefore to be able to use CMN for comparisons other; than the Z bit, we'll need additional logic to reverse the conditionals; associated with the comparison. Perhaps a pseudo-instruction for the comparison,; with a post-codegen pass to clean up and handle the condition codes?; See PR5694 for testcase. //===---------------------------------------------------------------------===//. Given the followin",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:1097,Energy Efficiency,reduce,reduces,1097,"------------===//. Reimplement 'select' in terms of 'SEL'. * We would really like to support UXTAB16, but we need to prove that the; add doesn't need to overflow between the two 16-bit chunks. * Implement pre/post increment support. (e.g. PR935); * Implement smarter constant generation for binops with large immediates. A few ARMv6T2 ops should be pattern matched: BFI, SBFX, and UBFX. Interesting optimization for PIC codegen on arm-linux:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43129. //===---------------------------------------------------------------------===//. Crazy idea: Consider code that uses lots of 8-bit or 16-bit values. By the; time regalloc happens, these values are now in a 32-bit register, usually with; the top-bits known to be sign or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===-------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:2916,Energy Efficiency,power,power,2916,"rocess the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===---------------------------------------------------------------------===//. Eliminate one instruction from:. define i32 @_Z6slow4bii(i32 %x, i32 %y) {; %tmp = icmp sgt i32 %x, %y; %retval = select i1 %tmp, i32 %x, i32 %y; ret i32 %retval; }. __Z6slow4bii:; cmp r0, r1; movgt r1, r0; mov r0, r1; bx lr; =>. __Z6slow4bii:; cmp r0, r1; movle r0, r1; bx lr. //===---------------------------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:; 1) Better representation for block transfer? This is from Olden/power:. 	fldd d0, [r4]; 	fstd d0, [r4, #+32]; 	fldd d0, [r4, #+8]; 	fstd d0, [r4, #+40]; 	fldd d0, [r4, #+16]; 	fstd d0, [r4, #+48]; 	fldd d0, [r4, #+24]; 	fstd d0, [r4, #+56]. If we can spare the registers, it would be better to use fldm and fstm here.; Need major register allocator enhancement though. 2) Can we recognize the relative position of constantpool entries? i.e. Treat. 	ldr r0, LCPI17_3; 	ldr r1, LCPI17_4; 	ldr r2, LCPI17_5. as; 	ldr r0, LCPI17; 	ldr r1, LCPI17+4; 	ldr r2, LCPI17+8. Then the ldr's can be combined into a single ldm. See Olden/power. Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a; double 64-bit FP constant:. 	adr	r0, L6; 	ldmia	r0, {r0-r1}. 	.align 2; L6:; 	.long	-858993459; 	.long	1074318540. 3) struct copies appear to be done field by field; instead of by words, at least sometimes:. struct foo { int x; short s; char c1; char c2; };; void cpy(struct foo*a, struct foo*b) { *a = *b; }",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:3476,Energy Efficiency,power,power,3476,"i:; cmp r0, r1; movle r0, r1; bx lr. //===---------------------------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:; 1) Better representation for block transfer? This is from Olden/power:. 	fldd d0, [r4]; 	fstd d0, [r4, #+32]; 	fldd d0, [r4, #+8]; 	fstd d0, [r4, #+40]; 	fldd d0, [r4, #+16]; 	fstd d0, [r4, #+48]; 	fldd d0, [r4, #+24]; 	fstd d0, [r4, #+56]. If we can spare the registers, it would be better to use fldm and fstm here.; Need major register allocator enhancement though. 2) Can we recognize the relative position of constantpool entries? i.e. Treat. 	ldr r0, LCPI17_3; 	ldr r1, LCPI17_4; 	ldr r2, LCPI17_5. as; 	ldr r0, LCPI17; 	ldr r1, LCPI17+4; 	ldr r2, LCPI17+8. Then the ldr's can be combined into a single ldm. See Olden/power. Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a; double 64-bit FP constant:. 	adr	r0, L6; 	ldmia	r0, {r0-r1}. 	.align 2; L6:; 	.long	-858993459; 	.long	1074318540. 3) struct copies appear to be done field by field; instead of by words, at least sometimes:. struct foo { int x; short s; char c1; char c2; };; void cpy(struct foo*a, struct foo*b) { *a = *b; }. llvm code (-O2); ldrb r3, [r1, #+6]; ldr r2, [r1]; ldrb r12, [r1, #+7]; ldrh r1, [r1, #+4]; str r2, [r0]; strh r1, [r0, #+4]; strb r3, [r0, #+6]; strb r12, [r0, #+7]; gcc code (-O2); ldmia r1, {r1-r2}; stmia r0, {r1-r2}. In this benchmark poor handling of aggregate copies has shown up as; having a large effect on size, and possibly speed as well (we don't have; a good way to measure on ARM). //===---------------------------------------------------------------------===//. * Consider this silly example:. double bar(double x) {; double r = foo(3.1);; return x+r;; }. _bar:; stmfd sp!, {r4, r5, r7, l",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:4904,Energy Efficiency,schedul,scheduled,4904,"]; ldrb r12, [r1, #+7]; ldrh r1, [r1, #+4]; str r2, [r0]; strh r1, [r0, #+4]; strb r3, [r0, #+6]; strb r12, [r0, #+7]; gcc code (-O2); ldmia r1, {r1-r2}; stmia r0, {r1-r2}. In this benchmark poor handling of aggregate copies has shown up as; having a large effect on size, and possibly speed as well (we don't have; a good way to measure on ARM). //===---------------------------------------------------------------------===//. * Consider this silly example:. double bar(double x) {; double r = foo(3.1);; return x+r;; }. _bar:; stmfd sp!, {r4, r5, r7, lr}; add r7, sp, #8; mov r4, r0; mov r5, r1; fldd d0, LCPI1_0; fmrrd r0, r1, d0; bl _foo; fmdrr d0, r4, r5; fmsr s2, r0; fsitod d1, s2; faddd d0, d1, d0; fmrrd r0, r1, d0; ldmfd sp!, {r4, r5, r7, pc}. Ignore the prologue and epilogue stuff for a second. Note; 	mov r4, r0; 	mov r5, r1; the copys to callee-save registers and the fact they are only being used by the; fmdrr instruction. It would have been better had the fmdrr been scheduled; before the call and place the result in a callee-save DPR register. The two; mov ops would not have been necessary. //===---------------------------------------------------------------------===//. Calling convention related stuff:. * gcc's parameter passing implementation is terrible and we suffer as a result:. e.g.; struct s {; double d1;; int s1;; };. void foo(struct s S) {; printf(""%g, %d\n"", S.d1, S.s1);; }. 'S' is passed via registers r0, r1, r2. But gcc stores them to the stack, and; then reload them to r1, r2, and r3 before issuing the call (r0 contains the; address of the format string):. 	stmfd	sp!, {r7, lr}; 	add	r7, sp, #0; 	sub	sp, sp, #12; 	stmia	sp, {r0, r1, r2}; 	ldmia	sp, {r1-r2}; 	ldr	r0, L5; 	ldr	r3, [sp, #8]; L2:; 	add	r0, pc, r0; 	bl	L_printf$stub. Instead of a stmia, ldmia, and a ldr, wouldn't it be better to do three moves?. * Return an aggregate type is even worse:. e.g.; struct s foo(void) {; struct s S = {1.1, 2};; return S;; }. 	mov	ip, r0; 	ldr	r0, L5; 	sub	sp, sp",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:7368,Energy Efficiency,allocate,allocate,7368," some stack location. The store is dead. The llvm-gcc generated code looks like this:. csretcc void %foo(%struct.s* %agg.result) {; entry:; 	%S = alloca %struct.s, align 4		; <%struct.s*> [#uses=1]; 	%memtmp = alloca %struct.s		; <%struct.s*> [#uses=1]; 	cast %struct.s* %S to sbyte*		; <sbyte*>:0 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %0, sbyte* cast ({ double, int }* %C.0.904 to sbyte*), uint 12, uint 4 ); 	cast %struct.s* %agg.result to sbyte*		; <sbyte*>:1 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %1, sbyte* %0, uint 12, uint 0 ); 	cast %struct.s* %memtmp to sbyte*		; <sbyte*>:2 [#uses=1]; 	call void %llvm.memcpy.i32( sbyte* %2, sbyte* %1, uint 12, uint 0 ); 	ret void; }. llc ends up issuing two memcpy's (the first memcpy becomes 3 loads from; constantpool). Perhaps we should 1) fix llvm-gcc so the memcpy is translated; into a number of load and stores, or 2) custom lower memcpy (of small size) to; be ldmia / stmia. I think option 2 is better but the current register; allocator cannot allocate a chunk of registers at a time. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/st",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8704,Energy Efficiency,schedul,scheduling,8704,"--------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single imm",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:950,Modifiability,extend,extended,950,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend.; //===---------------------------------------------------------------------===//. Reimplement 'select' in terms of 'SEL'. * We would really like to support UXTAB16, but we need to prove that the; add doesn't need to overflow between the two 16-bit chunks. * Implement pre/post increment support. (e.g. PR935); * Implement smarter constant generation for binops with large immediates. A few ARMv6T2 ops should be pattern matched: BFI, SBFX, and UBFX. Interesting optimization for PIC codegen on arm-linux:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43129. //===---------------------------------------------------------------------===//. Crazy idea: Consider code that uses lots of 8-bit or 16-bit values. By the; time regalloc happens, these values are now in a 32-bit register, usually with; the top-bits known to be sign or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===----------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:1052,Modifiability,extend,extending,1052,"----------------------------------------------------===//; // Random ideas for the ARM backend.; //===---------------------------------------------------------------------===//. Reimplement 'select' in terms of 'SEL'. * We would really like to support UXTAB16, but we need to prove that the; add doesn't need to overflow between the two 16-bit chunks. * Implement pre/post increment support. (e.g. PR935); * Implement smarter constant generation for binops with large immediates. A few ARMv6T2 ops should be pattern matched: BFI, SBFX, and UBFX. Interesting optimization for PIC codegen on arm-linux:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43129. //===---------------------------------------------------------------------===//. Crazy idea: Consider code that uses lots of 8-bit or 16-bit values. By the; time regalloc happens, these values are now in a 32-bit register, usually with; the top-bits known to be sign or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===-------------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:3201,Modifiability,enhance,enhancement,3201,"-------------------------------===//. Eliminate one instruction from:. define i32 @_Z6slow4bii(i32 %x, i32 %y) {; %tmp = icmp sgt i32 %x, %y; %retval = select i1 %tmp, i32 %x, i32 %y; ret i32 %retval; }. __Z6slow4bii:; cmp r0, r1; movgt r1, r0; mov r0, r1; bx lr; =>. __Z6slow4bii:; cmp r0, r1; movle r0, r1; bx lr. //===---------------------------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:; 1) Better representation for block transfer? This is from Olden/power:. 	fldd d0, [r4]; 	fstd d0, [r4, #+32]; 	fldd d0, [r4, #+8]; 	fstd d0, [r4, #+40]; 	fldd d0, [r4, #+16]; 	fstd d0, [r4, #+48]; 	fldd d0, [r4, #+24]; 	fstd d0, [r4, #+56]. If we can spare the registers, it would be better to use fldm and fstm here.; Need major register allocator enhancement though. 2) Can we recognize the relative position of constantpool entries? i.e. Treat. 	ldr r0, LCPI17_3; 	ldr r1, LCPI17_4; 	ldr r2, LCPI17_5. as; 	ldr r0, LCPI17; 	ldr r1, LCPI17+4; 	ldr r2, LCPI17+8. Then the ldr's can be combined into a single ldm. See Olden/power. Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a; double 64-bit FP constant:. 	adr	r0, L6; 	ldmia	r0, {r0-r1}. 	.align 2; L6:; 	.long	-858993459; 	.long	1074318540. 3) struct copies appear to be done field by field; instead of by words, at least sometimes:. struct foo { int x; short s; char c1; char c2; };; void cpy(struct foo*a, struct foo*b) { *a = *b; }. llvm code (-O2); ldrb r3, [r1, #+6]; ldr r2, [r1]; ldrb r12, [r1, #+7]; ldrh r1, [r1, #+4]; str r2, [r0]; strh r1, [r0, #+4]; strb r3, [r0, #+6]; strb r12, [r0, #+7]; gcc code (-O2); ldmia r1, {r1-r2}; stmia r0, {r1-r2}. In this benchmark poor handling of aggregate copies has shown up as; having a large effect on size, an",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8934,Modifiability,Enhance,Enhance,8934,"it BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly better code for this function. int foo(int StackPtr, unsigned char *Line, unsigned char *Stack, int",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:12173,Modifiability,extend,extend,12173,".cgi?id=9760; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=9759; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=9703; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=9702; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=9663. http://www.inf.u-szeged.hu/gcc-arm/; http://citeseer.ist.psu.edu/debus04linktime.html. //===---------------------------------------------------------------------===//. gcc generates smaller code for this function at -O2 or -Os:. void foo(signed char* p) {; if (*p == 3); bar();; else if (*p == 4); baz();; else if (*p == 5); quux();; }. llvm decides it's a good idea to turn the repeated if...else into a; binary tree, as if it were a switch; the resulting code requires -1; compare-and-branches when *p<=2 or *p==5, the same number if *p==4; or *p>6, and +1 if *p==3. So it should be a speed win; (on balance). However, the revised code is larger, with 4 conditional; branches instead of 3. More seriously, there is a byte->word extend before; each comparison, where there should be only one, and the condition codes; are not remembered when the same two values are compared twice. //===---------------------------------------------------------------------===//. More LSR enhancements possible:. 1. Teach LSR about pre- and post- indexed ops to allow iv increment be merged; in a load / store.; 2. Allow iv reuse even when a type conversion is required. For example, i8; and i32 load / store addressing modes are identical. //===---------------------------------------------------------------------===//. This:. int foo(int a, int b, int c, int d) {; long long acc = (long long)a * (long long)b;; acc += (long long)c * (long long)d;; return (int)(acc >> 32);; }. Should compile to use SMLAL (Signed Multiply Accumulate Long) which multiplies; two signed 32-bit values to produce a 64-bit value, and accumulates this with; a 64-bit value. We currently get this with both v4 and v6:. _foo:; smull r1, r0, r1, r0; smull r3, r2, r3, r2; adds r3, r3, r1; adc r0, r2, r0; bx lr. //===--------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:12416,Modifiability,enhance,enhancements,12416,"how_bug.cgi?id=9663. http://www.inf.u-szeged.hu/gcc-arm/; http://citeseer.ist.psu.edu/debus04linktime.html. //===---------------------------------------------------------------------===//. gcc generates smaller code for this function at -O2 or -Os:. void foo(signed char* p) {; if (*p == 3); bar();; else if (*p == 4); baz();; else if (*p == 5); quux();; }. llvm decides it's a good idea to turn the repeated if...else into a; binary tree, as if it were a switch; the resulting code requires -1; compare-and-branches when *p<=2 or *p==5, the same number if *p==4; or *p>6, and +1 if *p==3. So it should be a speed win; (on balance). However, the revised code is larger, with 4 conditional; branches instead of 3. More seriously, there is a byte->word extend before; each comparison, where there should be only one, and the condition codes; are not remembered when the same two values are compared twice. //===---------------------------------------------------------------------===//. More LSR enhancements possible:. 1. Teach LSR about pre- and post- indexed ops to allow iv increment be merged; in a load / store.; 2. Allow iv reuse even when a type conversion is required. For example, i8; and i32 load / store addressing modes are identical. //===---------------------------------------------------------------------===//. This:. int foo(int a, int b, int c, int d) {; long long acc = (long long)a * (long long)b;; acc += (long long)c * (long long)d;; return (int)(acc >> 32);; }. Should compile to use SMLAL (Signed Multiply Accumulate Long) which multiplies; two signed 32-bit values to produce a 64-bit value, and accumulates this with; a 64-bit value. We currently get this with both v4 and v6:. _foo:; smull r1, r0, r1, r0; smull r3, r2, r3, r2; adds r3, r3, r1; adc r0, r2, r0; bx lr. //===---------------------------------------------------------------------===//. This:; #include <algorithm>; std::pair<unsigned, bool> full_add(unsigned a, unsigned b); { return std::make_pair(a + b, a + b",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:19668,Modifiability,rewrite,rewrite,19668," r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16; 	bic r1, r1, #0xff0000; 	mov r1, r1, lsr #8; 	eor r0, r1, r0, ror #8; 	bx	lr. A custom Thumb version would also be a slight improvement over the generic; version. //===---------------------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb r0, [r0]; ldrb r1, [r1]; orr r0, r1, r0; tst r0, #255; moveq r0, #0; streq r0, [r2]; bx lr. Note that both ""tst"" and ""moveq"" are redundant. //===---------------------------------------------------------------------===//. When loading immediate constants with movt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. W",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:20811,Modifiability,portab,portable,20811,"ile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===---------------------------------------------------------------------===//. Clean up the test/MC/ARM files to have more robust register choices. R0 should not be used as a register operand in the assembler tests as it's then; not possible to distinguish between a correct encoding and a missing operand; encoding, as zero is the default value for the binary encoder.; e.g.,; add r0, r0 // bad; add r",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:580,Performance,optimiz,optimization,580,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend.; //===---------------------------------------------------------------------===//. Reimplement 'select' in terms of 'SEL'. * We would really like to support UXTAB16, but we need to prove that the; add doesn't need to overflow between the two 16-bit chunks. * Implement pre/post increment support. (e.g. PR935); * Implement smarter constant generation for binops with large immediates. A few ARMv6T2 ops should be pattern matched: BFI, SBFX, and UBFX. Interesting optimization for PIC codegen on arm-linux:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43129. //===---------------------------------------------------------------------===//. Crazy idea: Consider code that uses lots of 8-bit or 16-bit values. By the; time regalloc happens, these values are now in a 32-bit register, usually with; the top-bits known to be sign or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===----------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:1272,Performance,load,load,1272,"------------===//. Reimplement 'select' in terms of 'SEL'. * We would really like to support UXTAB16, but we need to prove that the; add doesn't need to overflow between the two 16-bit chunks. * Implement pre/post increment support. (e.g. PR935); * Implement smarter constant generation for binops with large immediates. A few ARMv6T2 ops should be pattern matched: BFI, SBFX, and UBFX. Interesting optimization for PIC codegen on arm-linux:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43129. //===---------------------------------------------------------------------===//. Crazy idea: Consider code that uses lots of 8-bit or 16-bit values. By the; time regalloc happens, these values are now in a 32-bit register, usually with; the top-bits known to be sign or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===-------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:1792,Performance,queue,queue,1792,"pens, these values are now in a 32-bit register, usually with; the top-bits known to be sign or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===---------------------------------------------------------------------===//. Eliminate one instruction from:. define i32 @_Z6slow4bii(i32 %x, i32 %y) {; %tmp = icmp sgt i32 %x, %y; %retval = select i1 %tmp, i32 %x, i32 %y; ret i32 %retval; }. __Z6slow4bii:; cmp r0, r1; movgt r1, r0; mov r0, r1; bx lr; =>. __Z6slow4bii:; cmp r0, r1; movle r0, r1; bx lr. //===---------------------------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:2823,Performance,load,load,2823,"rocess the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===---------------------------------------------------------------------===//. Eliminate one instruction from:. define i32 @_Z6slow4bii(i32 %x, i32 %y) {; %tmp = icmp sgt i32 %x, %y; %retval = select i1 %tmp, i32 %x, i32 %y; ret i32 %retval; }. __Z6slow4bii:; cmp r0, r1; movgt r1, r0; mov r0, r1; bx lr; =>. __Z6slow4bii:; cmp r0, r1; movle r0, r1; bx lr. //===---------------------------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:; 1) Better representation for block transfer? This is from Olden/power:. 	fldd d0, [r4]; 	fstd d0, [r4, #+32]; 	fldd d0, [r4, #+8]; 	fstd d0, [r4, #+40]; 	fldd d0, [r4, #+16]; 	fstd d0, [r4, #+48]; 	fldd d0, [r4, #+24]; 	fstd d0, [r4, #+56]. If we can spare the registers, it would be better to use fldm and fstm here.; Need major register allocator enhancement though. 2) Can we recognize the relative position of constantpool entries? i.e. Treat. 	ldr r0, LCPI17_3; 	ldr r1, LCPI17_4; 	ldr r2, LCPI17_5. as; 	ldr r0, LCPI17; 	ldr r1, LCPI17+4; 	ldr r2, LCPI17+8. Then the ldr's can be combined into a single ldm. See Olden/power. Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a; double 64-bit FP constant:. 	adr	r0, L6; 	ldmia	r0, {r0-r1}. 	.align 2; L6:; 	.long	-858993459; 	.long	1074318540. 3) struct copies appear to be done field by field; instead of by words, at least sometimes:. struct foo { int x; short s; char c1; char c2; };; void cpy(struct foo*a, struct foo*b) { *a = *b; }",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:2836,Performance,optimiz,optimizations,2836,"rocess the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===---------------------------------------------------------------------===//. Eliminate one instruction from:. define i32 @_Z6slow4bii(i32 %x, i32 %y) {; %tmp = icmp sgt i32 %x, %y; %retval = select i1 %tmp, i32 %x, i32 %y; ret i32 %retval; }. __Z6slow4bii:; cmp r0, r1; movgt r1, r0; mov r0, r1; bx lr; =>. __Z6slow4bii:; cmp r0, r1; movle r0, r1; bx lr. //===---------------------------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:; 1) Better representation for block transfer? This is from Olden/power:. 	fldd d0, [r4]; 	fstd d0, [r4, #+32]; 	fldd d0, [r4, #+8]; 	fstd d0, [r4, #+40]; 	fldd d0, [r4, #+16]; 	fstd d0, [r4, #+48]; 	fldd d0, [r4, #+24]; 	fstd d0, [r4, #+56]. If we can spare the registers, it would be better to use fldm and fstm here.; Need major register allocator enhancement though. 2) Can we recognize the relative position of constantpool entries? i.e. Treat. 	ldr r0, LCPI17_3; 	ldr r1, LCPI17_4; 	ldr r2, LCPI17_5. as; 	ldr r0, LCPI17; 	ldr r1, LCPI17+4; 	ldr r2, LCPI17+8. Then the ldr's can be combined into a single ldm. See Olden/power. Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a; double 64-bit FP constant:. 	adr	r0, L6; 	ldmia	r0, {r0-r1}. 	.align 2; L6:; 	.long	-858993459; 	.long	1074318540. 3) struct copies appear to be done field by field; instead of by words, at least sometimes:. struct foo { int x; short s; char c1; char c2; };; void cpy(struct foo*a, struct foo*b) { *a = *b; }",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:3517,Performance,load,load,3517,"-------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:; 1) Better representation for block transfer? This is from Olden/power:. 	fldd d0, [r4]; 	fstd d0, [r4, #+32]; 	fldd d0, [r4, #+8]; 	fstd d0, [r4, #+40]; 	fldd d0, [r4, #+16]; 	fstd d0, [r4, #+48]; 	fldd d0, [r4, #+24]; 	fstd d0, [r4, #+56]. If we can spare the registers, it would be better to use fldm and fstm here.; Need major register allocator enhancement though. 2) Can we recognize the relative position of constantpool entries? i.e. Treat. 	ldr r0, LCPI17_3; 	ldr r1, LCPI17_4; 	ldr r2, LCPI17_5. as; 	ldr r0, LCPI17; 	ldr r1, LCPI17+4; 	ldr r2, LCPI17+8. Then the ldr's can be combined into a single ldm. See Olden/power. Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a; double 64-bit FP constant:. 	adr	r0, L6; 	ldmia	r0, {r0-r1}. 	.align 2; L6:; 	.long	-858993459; 	.long	1074318540. 3) struct copies appear to be done field by field; instead of by words, at least sometimes:. struct foo { int x; short s; char c1; char c2; };; void cpy(struct foo*a, struct foo*b) { *a = *b; }. llvm code (-O2); ldrb r3, [r1, #+6]; ldr r2, [r1]; ldrb r12, [r1, #+7]; ldrh r1, [r1, #+4]; str r2, [r0]; strh r1, [r0, #+4]; strb r3, [r0, #+6]; strb r12, [r0, #+7]; gcc code (-O2); ldmia r1, {r1-r2}; stmia r0, {r1-r2}. In this benchmark poor handling of aggregate copies has shown up as; having a large effect on size, and possibly speed as well (we don't have; a good way to measure on ARM). //===---------------------------------------------------------------------===//. * Consider this silly example:. double bar(double x) {; double r = foo(3.1);; return x+r;; }. _bar:; stmfd sp!, {r4, r5, r7, lr}; add r7, sp, #8; mov r4, r0; mov r5, r1; fldd d0, LCPI1_0; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:6184,Performance,load,loads,6184,"rrible and we suffer as a result:. e.g.; struct s {; double d1;; int s1;; };. void foo(struct s S) {; printf(""%g, %d\n"", S.d1, S.s1);; }. 'S' is passed via registers r0, r1, r2. But gcc stores them to the stack, and; then reload them to r1, r2, and r3 before issuing the call (r0 contains the; address of the format string):. 	stmfd	sp!, {r7, lr}; 	add	r7, sp, #0; 	sub	sp, sp, #12; 	stmia	sp, {r0, r1, r2}; 	ldmia	sp, {r1-r2}; 	ldr	r0, L5; 	ldr	r3, [sp, #8]; L2:; 	add	r0, pc, r0; 	bl	L_printf$stub. Instead of a stmia, ldmia, and a ldr, wouldn't it be better to do three moves?. * Return an aggregate type is even worse:. e.g.; struct s foo(void) {; struct s S = {1.1, 2};; return S;; }. 	mov	ip, r0; 	ldr	r0, L5; 	sub	sp, sp, #12; L2:; 	add	r0, pc, r0; 	@ lr needed for prologue; 	ldmia	r0, {r0, r1, r2}; 	stmia	sp, {r0, r1, r2}; 	stmia	ip, {r0, r1, r2}; 	mov	r0, ip; 	add	sp, sp, #12; 	bx	lr. r0 (and later ip) is the hidden parameter from caller to store the value in. The; first ldmia loads the constants into r0, r1, r2. The last stmia stores r0, r1,; r2 into the address passed in. However, there is one additional stmia that; stores r0, r1, and r2 to some stack location. The store is dead. The llvm-gcc generated code looks like this:. csretcc void %foo(%struct.s* %agg.result) {; entry:; 	%S = alloca %struct.s, align 4		; <%struct.s*> [#uses=1]; 	%memtmp = alloca %struct.s		; <%struct.s*> [#uses=1]; 	cast %struct.s* %S to sbyte*		; <sbyte*>:0 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %0, sbyte* cast ({ double, int }* %C.0.904 to sbyte*), uint 12, uint 4 ); 	cast %struct.s* %agg.result to sbyte*		; <sbyte*>:1 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %1, sbyte* %0, uint 12, uint 0 ); 	cast %struct.s* %memtmp to sbyte*		; <sbyte*>:2 [#uses=1]; 	call void %llvm.memcpy.i32( sbyte* %2, sbyte* %1, uint 12, uint 0 ); 	ret void; }. llc ends up issuing two memcpy's (the first memcpy becomes 3 loads from; constantpool). Perhaps we should 1) fix llvm-gcc so the memcpy is trans",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:7110,Performance,load,loads,7110," (and later ip) is the hidden parameter from caller to store the value in. The; first ldmia loads the constants into r0, r1, r2. The last stmia stores r0, r1,; r2 into the address passed in. However, there is one additional stmia that; stores r0, r1, and r2 to some stack location. The store is dead. The llvm-gcc generated code looks like this:. csretcc void %foo(%struct.s* %agg.result) {; entry:; 	%S = alloca %struct.s, align 4		; <%struct.s*> [#uses=1]; 	%memtmp = alloca %struct.s		; <%struct.s*> [#uses=1]; 	cast %struct.s* %S to sbyte*		; <sbyte*>:0 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %0, sbyte* cast ({ double, int }* %C.0.904 to sbyte*), uint 12, uint 4 ); 	cast %struct.s* %agg.result to sbyte*		; <sbyte*>:1 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %1, sbyte* %0, uint 12, uint 0 ); 	cast %struct.s* %memtmp to sbyte*		; <sbyte*>:2 [#uses=1]; 	call void %llvm.memcpy.i32( sbyte* %2, sbyte* %1, uint 12, uint 0 ); 	ret void; }. llc ends up issuing two memcpy's (the first memcpy becomes 3 loads from; constantpool). Perhaps we should 1) fix llvm-gcc so the memcpy is translated; into a number of load and stores, or 2) custom lower memcpy (of small size) to; be ldmia / stmia. I think option 2 is better but the current register; allocator cannot allocate a chunk of registers at a time. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:7217,Performance,load,load,7217," r2. The last stmia stores r0, r1,; r2 into the address passed in. However, there is one additional stmia that; stores r0, r1, and r2 to some stack location. The store is dead. The llvm-gcc generated code looks like this:. csretcc void %foo(%struct.s* %agg.result) {; entry:; 	%S = alloca %struct.s, align 4		; <%struct.s*> [#uses=1]; 	%memtmp = alloca %struct.s		; <%struct.s*> [#uses=1]; 	cast %struct.s* %S to sbyte*		; <sbyte*>:0 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %0, sbyte* cast ({ double, int }* %C.0.904 to sbyte*), uint 12, uint 4 ); 	cast %struct.s* %agg.result to sbyte*		; <sbyte*>:1 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %1, sbyte* %0, uint 12, uint 0 ); 	cast %struct.s* %memtmp to sbyte*		; <sbyte*>:2 [#uses=1]; 	call void %llvm.memcpy.i32( sbyte* %2, sbyte* %1, uint 12, uint 0 ); 	ret void; }. llc ends up issuing two memcpy's (the first memcpy becomes 3 loads from; constantpool). Perhaps we should 1) fix llvm-gcc so the memcpy is translated; into a number of load and stores, or 2) custom lower memcpy (of small size) to; be ldmia / stmia. I think option 2 is better but the current register; allocator cannot allocate a chunk of registers at a time. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8212,Performance,load,load,8212,"mber of load and stores, or 2) custom lower memcpy (of small size) to; be ldmia / stmia. I think option 2 is better but the current register; allocator cannot allocate a chunk of registers at a time. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===----------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8272,Performance,load,load,8272,"tmia. I think option 2 is better but the current register; allocator cannot allocate a chunk of registers at a time. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tr",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8346,Performance,load,load,8346,"tmia. I think option 2 is better but the current register; allocator cannot allocate a chunk of registers at a time. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tr",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8423,Performance,perform,performing,8423,"ime. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9,",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8438,Performance,optimiz,optimization,8438,"ime. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9,",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8566,Performance,optimiz,optimization,8566,"ling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to u",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8629,Performance,perform,performance,8629,"--------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single imm",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8833,Performance,load,load,8833,":. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8871,Performance,load,load,8871,":. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:9063,Performance,load,loads,9063," See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly better code for this function. int foo(int StackPtr, unsigned char *Line, unsigned char *Stack, int LineLen) {; int i = 0;. if (StackPtr != 0) {; while (StackPtr != 0 && i < (((LineLen) < (32768",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:9163,Performance,load,load,9163,"--------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly better code for this function. int foo(int StackPtr, unsigned char *Line, unsigned char *Stack, int LineLen) {; int i = 0;. if (StackPtr != 0) {; while (StackPtr != 0 && i < (((LineLen) < (32768))? (LineLen) : (32768))); Line[i++] = Stack[--StackPtr];; if (LineLen > 32768); {; while (S",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:9709,Performance,load,load,9709,"duling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly better code for this function. int foo(int StackPtr, unsigned char *Line, unsigned char *Stack, int LineLen) {; int i = 0;. if (StackPtr != 0) {; while (StackPtr != 0 && i < (((LineLen) < (32768))? (LineLen) : (32768))); Line[i++] = Stack[--StackPtr];; if (LineLen > 32768); {; while (StackPtr != 0 && i < LineLen); {; i++;; --StackPtr;; }; }; }; return StackPtr;; }. //===---------------------------------------------------------------------===//. This should compile to the mlas instruction:; int mlas(int x, int y, int z) { return ((x * y + z) < 0) ? 7 : 13; }. //===---------------------------------------------------------------------===//. At some point, we should triage these to see if they still apply to us:. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19598; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=18560; http://gcc.gnu.org/bugzi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:12524,Performance,load,load,12524,"teseer.ist.psu.edu/debus04linktime.html. //===---------------------------------------------------------------------===//. gcc generates smaller code for this function at -O2 or -Os:. void foo(signed char* p) {; if (*p == 3); bar();; else if (*p == 4); baz();; else if (*p == 5); quux();; }. llvm decides it's a good idea to turn the repeated if...else into a; binary tree, as if it were a switch; the resulting code requires -1; compare-and-branches when *p<=2 or *p==5, the same number if *p==4; or *p>6, and +1 if *p==3. So it should be a speed win; (on balance). However, the revised code is larger, with 4 conditional; branches instead of 3. More seriously, there is a byte->word extend before; each comparison, where there should be only one, and the condition codes; are not remembered when the same two values are compared twice. //===---------------------------------------------------------------------===//. More LSR enhancements possible:. 1. Teach LSR about pre- and post- indexed ops to allow iv increment be merged; in a load / store.; 2. Allow iv reuse even when a type conversion is required. For example, i8; and i32 load / store addressing modes are identical. //===---------------------------------------------------------------------===//. This:. int foo(int a, int b, int c, int d) {; long long acc = (long long)a * (long long)b;; acc += (long long)c * (long long)d;; return (int)(acc >> 32);; }. Should compile to use SMLAL (Signed Multiply Accumulate Long) which multiplies; two signed 32-bit values to produce a 64-bit value, and accumulates this with; a 64-bit value. We currently get this with both v4 and v6:. _foo:; smull r1, r0, r1, r0; smull r3, r2, r3, r2; adds r3, r3, r1; adc r0, r2, r0; bx lr. //===---------------------------------------------------------------------===//. This:; #include <algorithm>; std::pair<unsigned, bool> full_add(unsigned a, unsigned b); { return std::make_pair(a + b, a + b < a); }; bool no_overflow(unsigned a, unsigned b); { return !full_",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:12623,Performance,load,load,12623," code for this function at -O2 or -Os:. void foo(signed char* p) {; if (*p == 3); bar();; else if (*p == 4); baz();; else if (*p == 5); quux();; }. llvm decides it's a good idea to turn the repeated if...else into a; binary tree, as if it were a switch; the resulting code requires -1; compare-and-branches when *p<=2 or *p==5, the same number if *p==4; or *p>6, and +1 if *p==3. So it should be a speed win; (on balance). However, the revised code is larger, with 4 conditional; branches instead of 3. More seriously, there is a byte->word extend before; each comparison, where there should be only one, and the condition codes; are not remembered when the same two values are compared twice. //===---------------------------------------------------------------------===//. More LSR enhancements possible:. 1. Teach LSR about pre- and post- indexed ops to allow iv increment be merged; in a load / store.; 2. Allow iv reuse even when a type conversion is required. For example, i8; and i32 load / store addressing modes are identical. //===---------------------------------------------------------------------===//. This:. int foo(int a, int b, int c, int d) {; long long acc = (long long)a * (long long)b;; acc += (long long)c * (long long)d;; return (int)(acc >> 32);; }. Should compile to use SMLAL (Signed Multiply Accumulate Long) which multiplies; two signed 32-bit values to produce a 64-bit value, and accumulates this with; a 64-bit value. We currently get this with both v4 and v6:. _foo:; smull r1, r0, r1, r0; smull r3, r2, r3, r2; adds r3, r3, r1; adc r0, r2, r0; bx lr. //===---------------------------------------------------------------------===//. This:; #include <algorithm>; std::pair<unsigned, bool> full_add(unsigned a, unsigned b); { return std::make_pair(a + b, a + b < a); }; bool no_overflow(unsigned a, unsigned b); { return !full_add(a, b).second; }. Should compile to:. _Z8full_addjj:; 	adds	r2, r1, r2; 	movcc	r1, #0; 	movcs	r1, #1; 	str	r2, [r0, #0]; 	strb	r1, [r0, #4]",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:17298,Performance,load,load,17298,"r1, r1, lsr #2; 	ldr r0, LCPI1_1; 	and r0, r1, r0; 	bx lr. it saves an instruction and a register. //===---------------------------------------------------------------------===//. It might be profitable to cse MOVi16 if there are lots of 32-bit immediates; with the same bottom half. //===---------------------------------------------------------------------===//. Robert Muth started working on an alternate jump table implementation that; does not put the tables in-line in the text. This is more like the llvm; default jump table implementation. This might be useful sometime. Several; revisions of patches are on the mailing list, beginning at:; http://lists.llvm.org/pipermail/llvm-dev/2009-June/022763.html. //===---------------------------------------------------------------------===//. Make use of the ""rbit"" instruction. //===---------------------------------------------------------------------===//. Take a look at test/CodeGen/Thumb2/machine-licm.ll. ARM should be taught how; to licm and cse the unnecessary load from cp#1. //===---------------------------------------------------------------------===//. The CMN instruction sets the flags like an ADD instruction, while CMP sets; them like a subtract. Therefore to be able to use CMN for comparisons other; than the Z bit, we'll need additional logic to reverse the conditionals; associated with the comparison. Perhaps a pseudo-instruction for the comparison,; with a post-codegen pass to clean up and handle the condition codes?; See PR5694 for testcase. //===---------------------------------------------------------------------===//. Given the following on armv5:; int test1(int A, int B) {; return (A&-8388481)|(B&8388480);; }. We currently generate:; 	ldr	r2, .LCPI0_0; 	and	r0, r0, r2; 	ldr	r2, .LCPI0_1; 	and	r1, r1, r2; 	orr	r0, r1, r0; 	bx	lr. We should be able to replace the second ldr+and with a bic (i.e. reuse the; constant which was already loaded). Not sure what's necessary to do that. //===--------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:18198,Performance,load,loaded,18198,"==//. Take a look at test/CodeGen/Thumb2/machine-licm.ll. ARM should be taught how; to licm and cse the unnecessary load from cp#1. //===---------------------------------------------------------------------===//. The CMN instruction sets the flags like an ADD instruction, while CMP sets; them like a subtract. Therefore to be able to use CMN for comparisons other; than the Z bit, we'll need additional logic to reverse the conditionals; associated with the comparison. Perhaps a pseudo-instruction for the comparison,; with a post-codegen pass to clean up and handle the condition codes?; See PR5694 for testcase. //===---------------------------------------------------------------------===//. Given the following on armv5:; int test1(int A, int B) {; return (A&-8388481)|(B&8388480);; }. We currently generate:; 	ldr	r2, .LCPI0_0; 	and	r0, r0, r2; 	ldr	r2, .LCPI0_1; 	and	r1, r1, r2; 	orr	r0, r1, r0; 	bx	lr. We should be able to replace the second ldr+and with a bic (i.e. reuse the; constant which was already loaded). Not sure what's necessary to do that. //===---------------------------------------------------------------------===//. The code generated for bswap on armv4/5 (CPUs without rev) is less than ideal:. int a(int x) { return __builtin_bswap32(x); }. a:; 	mov	r1, #255, 24; 	mov	r2, #255, 16; 	and	r1, r1, r0, lsr #8; 	and	r2, r2, r0, lsl #8; 	orr	r1, r1, r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16; 	bic r1, r1, #0xff0000; 	mov r1, r1, lsr #8; 	eor r0, r1, r0, ror #8; 	bx	lr. A custom Thumb version would also be a slight improvement over the generic; version. //===---------------------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:19410,Performance,load,loading,19410," r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16; 	bic r1, r1, #0xff0000; 	mov r1, r1, lsr #8; 	eor r0, r1, r0, ror #8; 	bx	lr. A custom Thumb version would also be a slight improvement over the generic; version. //===---------------------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb r0, [r0]; ldrb r1, [r1]; orr r0, r1, r0; tst r0, #255; moveq r0, #0; streq r0, [r2]; bx lr. Note that both ""tst"" and ""moveq"" are redundant. //===---------------------------------------------------------------------===//. When loading immediate constants with movt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. W",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:19910,Performance,optimiz,optimized,19910,"-------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb r0, [r0]; ldrb r1, [r1]; orr r0, r1, r0; tst r0, #255; moveq r0, #0; streq r0, [r2]; bx lr. Note that both ""tst"" and ""moveq"" are redundant. //===---------------------------------------------------------------------===//. When loading immediate constants with movt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM whe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:20692,Performance,Optimiz,Optimize,20692,"le:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===---------------------------------------------------------------------===//. Clean up the test/MC/ARM files to have more robust register choices. R0 should not be used as a register operand in the assembler tests as it's then; not possible to distinguish between a correct encoding and a missing operand; en",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:19313,Safety,redund,redundant,19313,"--------------------===//. The code generated for bswap on armv4/5 (CPUs without rev) is less than ideal:. int a(int x) { return __builtin_bswap32(x); }. a:; 	mov	r1, #255, 24; 	mov	r2, #255, 16; 	and	r1, r1, r0, lsr #8; 	and	r2, r2, r0, lsl #8; 	orr	r1, r1, r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16; 	bic r1, r1, #0xff0000; 	mov r1, r1, lsr #8; 	eor r0, r1, r0, ror #8; 	bx	lr. A custom Thumb version would also be a slight improvement over the generic; version. //===---------------------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb r0, [r0]; ldrb r1, [r1]; orr r0, r1, r0; tst r0, #255; moveq r0, #0; streq r0, [r2]; bx lr. Note that both ""tst"" and ""moveq"" are redundant. //===---------------------------------------------------------------------===//. When loading immediate constants with movt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:4101,Testability,benchmark,benchmark,4101,"or register allocator enhancement though. 2) Can we recognize the relative position of constantpool entries? i.e. Treat. 	ldr r0, LCPI17_3; 	ldr r1, LCPI17_4; 	ldr r2, LCPI17_5. as; 	ldr r0, LCPI17; 	ldr r1, LCPI17+4; 	ldr r2, LCPI17+8. Then the ldr's can be combined into a single ldm. See Olden/power. Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a; double 64-bit FP constant:. 	adr	r0, L6; 	ldmia	r0, {r0-r1}. 	.align 2; L6:; 	.long	-858993459; 	.long	1074318540. 3) struct copies appear to be done field by field; instead of by words, at least sometimes:. struct foo { int x; short s; char c1; char c2; };; void cpy(struct foo*a, struct foo*b) { *a = *b; }. llvm code (-O2); ldrb r3, [r1, #+6]; ldr r2, [r1]; ldrb r12, [r1, #+7]; ldrh r1, [r1, #+4]; str r2, [r0]; strh r1, [r0, #+4]; strb r3, [r0, #+6]; strb r12, [r0, #+7]; gcc code (-O2); ldmia r1, {r1-r2}; stmia r0, {r1-r2}. In this benchmark poor handling of aggregate copies has shown up as; having a large effect on size, and possibly speed as well (we don't have; a good way to measure on ARM). //===---------------------------------------------------------------------===//. * Consider this silly example:. double bar(double x) {; double r = foo(3.1);; return x+r;; }. _bar:; stmfd sp!, {r4, r5, r7, lr}; add r7, sp, #8; mov r4, r0; mov r5, r1; fldd d0, LCPI1_0; fmrrd r0, r1, d0; bl _foo; fmdrr d0, r4, r5; fmsr s2, r0; fsitod d1, s2; faddd d0, d1, d0; fmrrd r0, r1, d0; ldmfd sp!, {r4, r5, r7, pc}. Ignore the prologue and epilogue stuff for a second. Note; 	mov r4, r0; 	mov r5, r1; the copys to callee-save registers and the fact they are only being used by the; fmdrr instruction. It would have been better had the fmdrr been scheduled; before the call and place the result in a callee-save DPR register. The two; mov ops would not have been necessary. //===---------------------------------------------------------------------===//. Calling convention related stuff:. * gcc's parameter passing implem",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:5688,Testability,stub,stub,5688,"s2; faddd d0, d1, d0; fmrrd r0, r1, d0; ldmfd sp!, {r4, r5, r7, pc}. Ignore the prologue and epilogue stuff for a second. Note; 	mov r4, r0; 	mov r5, r1; the copys to callee-save registers and the fact they are only being used by the; fmdrr instruction. It would have been better had the fmdrr been scheduled; before the call and place the result in a callee-save DPR register. The two; mov ops would not have been necessary. //===---------------------------------------------------------------------===//. Calling convention related stuff:. * gcc's parameter passing implementation is terrible and we suffer as a result:. e.g.; struct s {; double d1;; int s1;; };. void foo(struct s S) {; printf(""%g, %d\n"", S.d1, S.s1);; }. 'S' is passed via registers r0, r1, r2. But gcc stores them to the stack, and; then reload them to r1, r2, and r3 before issuing the call (r0 contains the; address of the format string):. 	stmfd	sp!, {r7, lr}; 	add	r7, sp, #0; 	sub	sp, sp, #12; 	stmia	sp, {r0, r1, r2}; 	ldmia	sp, {r1-r2}; 	ldr	r0, L5; 	ldr	r3, [sp, #8]; L2:; 	add	r0, pc, r0; 	bl	L_printf$stub. Instead of a stmia, ldmia, and a ldr, wouldn't it be better to do three moves?. * Return an aggregate type is even worse:. e.g.; struct s foo(void) {; struct s S = {1.1, 2};; return S;; }. 	mov	ip, r0; 	ldr	r0, L5; 	sub	sp, sp, #12; L2:; 	add	r0, pc, r0; 	@ lr needed for prologue; 	ldmia	r0, {r0, r1, r2}; 	stmia	sp, {r0, r1, r2}; 	stmia	ip, {r0, r1, r2}; 	mov	r0, ip; 	add	sp, sp, #12; 	bx	lr. r0 (and later ip) is the hidden parameter from caller to store the value in. The; first ldmia loads the constants into r0, r1, r2. The last stmia stores r0, r1,; r2 into the address passed in. However, there is one additional stmia that; stores r0, r1, and r2 to some stack location. The store is dead. The llvm-gcc generated code looks like this:. csretcc void %foo(%struct.s* %agg.result) {; entry:; 	%S = alloca %struct.s, align 4		; <%struct.s*> [#uses=1]; 	%memtmp = alloca %struct.s		; <%struct.s*> [#uses=1]; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:17203,Testability,test,test,17203,"	ldr r2, [r2]; 	ldr r1, [r2, +r1, lsl #2]; 	tst r0, #128; 	movne r1, r1, lsr #2; 	ldr r0, LCPI1_1; 	and r0, r1, r0; 	bx lr. it saves an instruction and a register. //===---------------------------------------------------------------------===//. It might be profitable to cse MOVi16 if there are lots of 32-bit immediates; with the same bottom half. //===---------------------------------------------------------------------===//. Robert Muth started working on an alternate jump table implementation that; does not put the tables in-line in the text. This is more like the llvm; default jump table implementation. This might be useful sometime. Several; revisions of patches are on the mailing list, beginning at:; http://lists.llvm.org/pipermail/llvm-dev/2009-June/022763.html. //===---------------------------------------------------------------------===//. Make use of the ""rbit"" instruction. //===---------------------------------------------------------------------===//. Take a look at test/CodeGen/Thumb2/machine-licm.ll. ARM should be taught how; to licm and cse the unnecessary load from cp#1. //===---------------------------------------------------------------------===//. The CMN instruction sets the flags like an ADD instruction, while CMP sets; them like a subtract. Therefore to be able to use CMN for comparisons other; than the Z bit, we'll need additional logic to reverse the conditionals; associated with the comparison. Perhaps a pseudo-instruction for the comparison,; with a post-codegen pass to clean up and handle the condition codes?; See PR5694 for testcase. //===---------------------------------------------------------------------===//. Given the following on armv5:; int test1(int A, int B) {; return (A&-8388481)|(B&8388480);; }. We currently generate:; 	ldr	r2, .LCPI0_0; 	and	r0, r0, r2; 	ldr	r2, .LCPI0_1; 	and	r1, r1, r2; 	orr	r0, r1, r0; 	bx	lr. We should be able to replace the second ldr+and with a bic (i.e. reuse the; constant which was already loaded). Not s",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:17586,Testability,log,logic,17586,"--------------------------------------------------------------===//. Robert Muth started working on an alternate jump table implementation that; does not put the tables in-line in the text. This is more like the llvm; default jump table implementation. This might be useful sometime. Several; revisions of patches are on the mailing list, beginning at:; http://lists.llvm.org/pipermail/llvm-dev/2009-June/022763.html. //===---------------------------------------------------------------------===//. Make use of the ""rbit"" instruction. //===---------------------------------------------------------------------===//. Take a look at test/CodeGen/Thumb2/machine-licm.ll. ARM should be taught how; to licm and cse the unnecessary load from cp#1. //===---------------------------------------------------------------------===//. The CMN instruction sets the flags like an ADD instruction, while CMP sets; them like a subtract. Therefore to be able to use CMN for comparisons other; than the Z bit, we'll need additional logic to reverse the conditionals; associated with the comparison. Perhaps a pseudo-instruction for the comparison,; with a post-codegen pass to clean up and handle the condition codes?; See PR5694 for testcase. //===---------------------------------------------------------------------===//. Given the following on armv5:; int test1(int A, int B) {; return (A&-8388481)|(B&8388480);; }. We currently generate:; 	ldr	r2, .LCPI0_0; 	and	r0, r0, r2; 	ldr	r2, .LCPI0_1; 	and	r1, r1, r2; 	orr	r0, r1, r0; 	bx	lr. We should be able to replace the second ldr+and with a bic (i.e. reuse the; constant which was already loaded). Not sure what's necessary to do that. //===---------------------------------------------------------------------===//. The code generated for bswap on armv4/5 (CPUs without rev) is less than ideal:. int a(int x) { return __builtin_bswap32(x); }. a:; 	mov	r1, #255, 24; 	mov	r2, #255, 16; 	and	r1, r1, r0, lsr #8; 	and	r2, r2, r0, lsl #8; 	orr	r1, r1, r0, lsr #24; 	",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:17788,Testability,test,testcase,17788,"t put the tables in-line in the text. This is more like the llvm; default jump table implementation. This might be useful sometime. Several; revisions of patches are on the mailing list, beginning at:; http://lists.llvm.org/pipermail/llvm-dev/2009-June/022763.html. //===---------------------------------------------------------------------===//. Make use of the ""rbit"" instruction. //===---------------------------------------------------------------------===//. Take a look at test/CodeGen/Thumb2/machine-licm.ll. ARM should be taught how; to licm and cse the unnecessary load from cp#1. //===---------------------------------------------------------------------===//. The CMN instruction sets the flags like an ADD instruction, while CMP sets; them like a subtract. Therefore to be able to use CMN for comparisons other; than the Z bit, we'll need additional logic to reverse the conditionals; associated with the comparison. Perhaps a pseudo-instruction for the comparison,; with a post-codegen pass to clean up and handle the condition codes?; See PR5694 for testcase. //===---------------------------------------------------------------------===//. Given the following on armv5:; int test1(int A, int B) {; return (A&-8388481)|(B&8388480);; }. We currently generate:; 	ldr	r2, .LCPI0_0; 	and	r0, r0, r2; 	ldr	r2, .LCPI0_1; 	and	r1, r1, r2; 	orr	r0, r1, r0; 	bx	lr. We should be able to replace the second ldr+and with a bic (i.e. reuse the; constant which was already loaded). Not sure what's necessary to do that. //===---------------------------------------------------------------------===//. The code generated for bswap on armv4/5 (CPUs without rev) is less than ideal:. int a(int x) { return __builtin_bswap32(x); }. a:; 	mov	r1, #255, 24; 	mov	r2, #255, 16; 	and	r1, r1, r0, lsr #8; 	and	r2, r2, r0, lsl #8; 	orr	r1, r1, r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:20407,Testability,test,test,20407,"vt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===-----------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:20424,Testability,test,test,20424,"vt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===-----------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:21505,Testability,test,test,21505,"----------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===---------------------------------------------------------------------===//. Clean up the test/MC/ARM files to have more robust register choices. R0 should not be used as a register operand in the assembler tests as it's then; not possible to distinguish between a correct encoding and a missing operand; encoding, as zero is the default value for the binary encoder.; e.g.,; add r0, r0 // bad; add r3, r5 // good. Register operands should be distinct. That is, when the encoding does not; require two syntactical operands to refer to the same register, two different; registers should be used in the test so as to catch errors where the; operands are swapped in the encoding.; e.g.,; subs.w r1, r1, r1 // bad; subs.w r1, r2, r3 // good. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:21622,Testability,test,tests,21622,"----------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===---------------------------------------------------------------------===//. Clean up the test/MC/ARM files to have more robust register choices. R0 should not be used as a register operand in the assembler tests as it's then; not possible to distinguish between a correct encoding and a missing operand; encoding, as zero is the default value for the binary encoder.; e.g.,; add r0, r0 // bad; add r3, r5 // good. Register operands should be distinct. That is, when the encoding does not; require two syntactical operands to refer to the same register, two different; registers should be used in the test so as to catch errors where the; operands are swapped in the encoding.; e.g.,; subs.w r1, r1, r1 // bad; subs.w r1, r2, r3 // good. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:22016,Testability,test,test,22016,"----------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===---------------------------------------------------------------------===//. Clean up the test/MC/ARM files to have more robust register choices. R0 should not be used as a register operand in the assembler tests as it's then; not possible to distinguish between a correct encoding and a missing operand; encoding, as zero is the default value for the binary encoder.; e.g.,; add r0, r0 // bad; add r3, r5 // good. Register operands should be distinct. That is, when the encoding does not; require two syntactical operands to refer to the same register, two different; registers should be used in the test so as to catch errors where the; operands are swapped in the encoding.; e.g.,; subs.w r1, r1, r1 // bad; subs.w r1, r2, r3 // good. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:1933,Usability,simpl,simply,1933,"gn or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===---------------------------------------------------------------------===//. Eliminate one instruction from:. define i32 @_Z6slow4bii(i32 %x, i32 %y) {; %tmp = icmp sgt i32 %x, %y; %retval = select i1 %tmp, i32 %x, i32 %y; ret i32 %retval; }. __Z6slow4bii:; cmp r0, r1; movgt r1, r0; mov r0, r1; bx lr; =>. __Z6slow4bii:; cmp r0, r1; movle r0, r1; bx lr. //===---------------------------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:; 1) Better representation for block transfer? This is from Olden/power:. 	fldd d0, [r4];",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:18993,Usability,simpl,simple,18993,"enerate:; 	ldr	r2, .LCPI0_0; 	and	r0, r0, r2; 	ldr	r2, .LCPI0_1; 	and	r1, r1, r2; 	orr	r0, r1, r0; 	bx	lr. We should be able to replace the second ldr+and with a bic (i.e. reuse the; constant which was already loaded). Not sure what's necessary to do that. //===---------------------------------------------------------------------===//. The code generated for bswap on armv4/5 (CPUs without rev) is less than ideal:. int a(int x) { return __builtin_bswap32(x); }. a:; 	mov	r1, #255, 24; 	mov	r2, #255, 16; 	and	r1, r1, r0, lsr #8; 	and	r2, r2, r0, lsl #8; 	orr	r1, r1, r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16; 	bic r1, r1, #0xff0000; 	mov r1, r1, lsr #8; 	eor r0, r1, r0, ror #8; 	bx	lr. A custom Thumb version would also be a slight improvement over the generic; version. //===---------------------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb r0, [r0]; ldrb r1, [r1]; orr r0, r1, r0; tst r0, #255; moveq r0, #0; streq r0, [r2]; bx lr. Note that both ""tst"" and ""moveq"" are redundant. //===---------------------------------------------------------------------===//. When loading immediate constants with movt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; m",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/CMakeLists.txt:886,Integrability,DEPEND,DEPENDS,886,add_llvm_component_group(AVR). set(LLVM_TARGET_DEFINITIONS AVR.td). tablegen(LLVM AVRGenAsmMatcher.inc -gen-asm-matcher); tablegen(LLVM AVRGenAsmWriter.inc -gen-asm-writer); tablegen(LLVM AVRGenCallingConv.inc -gen-callingconv); tablegen(LLVM AVRGenDAGISel.inc -gen-dag-isel); tablegen(LLVM AVRGenDisassemblerTables.inc -gen-disassembler); tablegen(LLVM AVRGenInstrInfo.inc -gen-instr-info); tablegen(LLVM AVRGenMCCodeEmitter.inc -gen-emitter); tablegen(LLVM AVRGenRegisterInfo.inc -gen-register-info); tablegen(LLVM AVRGenSubtargetInfo.inc -gen-subtarget). add_public_tablegen_target(AVRCommonTableGen). add_llvm_target(AVRCodeGen; AVRAsmPrinter.cpp; AVRExpandPseudoInsts.cpp; AVRFrameLowering.cpp; AVRInstrInfo.cpp; AVRISelDAGToDAG.cpp; AVRISelLowering.cpp; AVRMCInstLower.cpp; AVRRegisterInfo.cpp; AVRShiftExpand.cpp; AVRSubtarget.cpp; AVRTargetMachine.cpp; AVRTargetObjectFile.cpp. DEPENDS; intrinsics_gen. LINK_COMPONENTS; AVRDesc; AVRInfo; AsmPrinter; CodeGen; CodeGenTypes; Core; MC; SelectionDAG; Support; Target. ADD_TO_COMPONENT; AVR; ). add_subdirectory(AsmParser); add_subdirectory(Disassembler); add_subdirectory(MCTargetDesc); add_subdirectory(TargetInfo); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/AVR/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt:2368,Availability,mask,masks,2368,"MipsISD node. ilvl.d, pckev.d:; It is not possible to emit ilvl.d, or pckev.d since ilvev.d covers the; same shuffle. ilvev.d will be emitted instead. ilvr.d, ilvod.d, pckod.d:; It is not possible to emit ilvr.d, or pckod.d since ilvod.d covers the; same shuffle. ilvod.d will be emitted instead. splat.[bhwd]; The intrinsic will work as expected. However, unlike other intrinsics; it lowers directly to MipsISD::VSHF instead of using common IR. splati.w:; It is not possible to emit splati.w since shf.w covers the same cases.; shf.w will be emitted instead. copy_s.w:; On MIPS32, the copy_u.d intrinsic will emit this instruction instead of; copy_u.w. This is semantically equivalent since the general-purpose; register file is 32-bits wide. binsri.[bhwd], binsli.[bhwd]:; These two operations are equivalent to each other with the operands; swapped and condition inverted. The compiler may use either one as; appropriate.; Furthermore, the compiler may use bsel.[bhwd] for some masks that do; not survive the legalization process (this is a bug and will be fixed). bmnz.v, bmz.v, bsel.v:; These three operations differ only in the operand that is tied to the; result and the order of the operands.; It is (currently) not possible to emit bmz.v, or bsel.v since bmnz.v is; the same operation and will be emitted instead.; In future, the compiler may choose between these three instructions; according to register allocation.; These three operations can be very confusing so here is a mapping; between the instructions and the vselect node in one place:; bmz.v wd, ws, wt/i8 -> (vselect wt/i8, wd, ws); bmnz.v wd, ws, wt/i8 -> (vselect wt/i8, ws, wd); bsel.v wd, ws, wt/i8 -> (vselect wd, wt/i8, ws). bmnzi.b, bmzi.b:; Like their non-immediate counterparts, bmnzi.v and bmzi.v are the same; operation with the operands swapped. bmnzi.v will (currently) be emitted; for both cases. bseli.v:; Unlike the non-immediate versions, bseli.v is distinguishable from; bmnzi.b and bmzi.b and can be emitted.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt:156,Energy Efficiency,reduce,reduce,156,"Code Generation Notes for MSA; =============================. Intrinsics are lowered to SelectionDAG nodes where possible in order to enable; optimisation, reduce the size of the ISel matcher, and reduce repetition in; the implementation. In a small number of cases, this can cause different; (semantically equivalent) instructions to be used in place of the requested; instruction, even when no optimisation has taken place. Instructions; ============. This section describes any quirks of instruction selection for MSA. For; example, two instructions might be equally valid for some given IR and one is; chosen in preference to the other. bclri.b:; It is not possible to emit bclri.b since andi.b covers exactly the; same cases. andi.b should use fractionally less power than bclri.b in; most hardware implementations so it is used in preference to bclri.b. vshf.w:; It is not possible to emit vshf.w when the shuffle description is; constant since shf.w covers exactly the same cases. shf.w is used; instead. It is also impossible for the shuffle description to be; unknown at compile-time due to the definition of shufflevector in; LLVM IR. vshf.[bhwd]; When the shuffle description describes a splat operation, splat.[bhwd]; instructions will be selected instead of vshf.[bhwd]. Unlike the ilv*,; and pck* instructions, this is matched from MipsISD::VSHF instead of; a special-case MipsISD node. ilvl.d, pckev.d:; It is not possible to emit ilvl.d, or pckev.d since ilvev.d covers the; same shuffle. ilvev.d will be emitted instead. ilvr.d, ilvod.d, pckod.d:; It is not possible to emit ilvr.d, or pckod.d since ilvod.d covers the; same shuffle. ilvod.d will be emitted instead. splat.[bhwd]; The intrinsic will work as expected. However, unlike other intrinsics; it lowers directly to MipsISD::VSHF instead of using common IR. splati.w:; It is not possible to emit splati.w since shf.w covers the same cases.; shf.w will be emitted instead. copy_s.w:; On MIPS32, the copy_u.d intrinsic will emit",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt:197,Energy Efficiency,reduce,reduce,197,"Code Generation Notes for MSA; =============================. Intrinsics are lowered to SelectionDAG nodes where possible in order to enable; optimisation, reduce the size of the ISel matcher, and reduce repetition in; the implementation. In a small number of cases, this can cause different; (semantically equivalent) instructions to be used in place of the requested; instruction, even when no optimisation has taken place. Instructions; ============. This section describes any quirks of instruction selection for MSA. For; example, two instructions might be equally valid for some given IR and one is; chosen in preference to the other. bclri.b:; It is not possible to emit bclri.b since andi.b covers exactly the; same cases. andi.b should use fractionally less power than bclri.b in; most hardware implementations so it is used in preference to bclri.b. vshf.w:; It is not possible to emit vshf.w when the shuffle description is; constant since shf.w covers exactly the same cases. shf.w is used; instead. It is also impossible for the shuffle description to be; unknown at compile-time due to the definition of shufflevector in; LLVM IR. vshf.[bhwd]; When the shuffle description describes a splat operation, splat.[bhwd]; instructions will be selected instead of vshf.[bhwd]. Unlike the ilv*,; and pck* instructions, this is matched from MipsISD::VSHF instead of; a special-case MipsISD node. ilvl.d, pckev.d:; It is not possible to emit ilvl.d, or pckev.d since ilvev.d covers the; same shuffle. ilvev.d will be emitted instead. ilvr.d, ilvod.d, pckod.d:; It is not possible to emit ilvr.d, or pckod.d since ilvod.d covers the; same shuffle. ilvod.d will be emitted instead. splat.[bhwd]; The intrinsic will work as expected. However, unlike other intrinsics; it lowers directly to MipsISD::VSHF instead of using common IR. splati.w:; It is not possible to emit splati.w since shf.w covers the same cases.; shf.w will be emitted instead. copy_s.w:; On MIPS32, the copy_u.d intrinsic will emit",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt:767,Energy Efficiency,power,power,767,"Code Generation Notes for MSA; =============================. Intrinsics are lowered to SelectionDAG nodes where possible in order to enable; optimisation, reduce the size of the ISel matcher, and reduce repetition in; the implementation. In a small number of cases, this can cause different; (semantically equivalent) instructions to be used in place of the requested; instruction, even when no optimisation has taken place. Instructions; ============. This section describes any quirks of instruction selection for MSA. For; example, two instructions might be equally valid for some given IR and one is; chosen in preference to the other. bclri.b:; It is not possible to emit bclri.b since andi.b covers exactly the; same cases. andi.b should use fractionally less power than bclri.b in; most hardware implementations so it is used in preference to bclri.b. vshf.w:; It is not possible to emit vshf.w when the shuffle description is; constant since shf.w covers exactly the same cases. shf.w is used; instead. It is also impossible for the shuffle description to be; unknown at compile-time due to the definition of shufflevector in; LLVM IR. vshf.[bhwd]; When the shuffle description describes a splat operation, splat.[bhwd]; instructions will be selected instead of vshf.[bhwd]. Unlike the ilv*,; and pck* instructions, this is matched from MipsISD::VSHF instead of; a special-case MipsISD node. ilvl.d, pckev.d:; It is not possible to emit ilvl.d, or pckev.d since ilvev.d covers the; same shuffle. ilvev.d will be emitted instead. ilvr.d, ilvod.d, pckod.d:; It is not possible to emit ilvr.d, or pckod.d since ilvod.d covers the; same shuffle. ilvod.d will be emitted instead. splat.[bhwd]; The intrinsic will work as expected. However, unlike other intrinsics; it lowers directly to MipsISD::VSHF instead of using common IR. splati.w:; It is not possible to emit splati.w since shf.w covers the same cases.; shf.w will be emitted instead. copy_s.w:; On MIPS32, the copy_u.d intrinsic will emit",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:416,Integrability,depend,dependent,416,"MIPS Relocation Principles. In LLVM, there are several elements of the llvm::ISD::NodeType enum; that deal with addresses and/or relocations. These are defined in; include/llvm/Target/TargetSelectionDAG.td, namely:; GlobalAddress, GlobalTLSAddress, JumpTable, ConstantPool,; ExternalSymbol, BlockAddress; The MIPS backend uses several principles to handle these. 1. Code for lowering addresses references to machine dependent code is; factored into common code for generating different address forms and; is called by the relocation model specific lowering function, using; templated functions. For example:. // lib/Target/Mips/MipsISelLowering.cpp; SDValue MipsTargetLowering::; lowerJumpTable(SDValue Op, SelectionDAG &DAG) const. calls. template <class NodeTy> // lib/Target/Mips/MipsISelLowering.h; SDValue getAddrLocal(NodeTy *N, const SDLoc &DL, EVT Ty,; SelectionDAG &DAG, bool IsN32OrN64) const. which calls the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and r",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:1234,Integrability,depend,depending,1234,"rgetSelectionDAG.td, namely:; GlobalAddress, GlobalTLSAddress, JumpTable, ConstantPool,; ExternalSymbol, BlockAddress; The MIPS backend uses several principles to handle these. 1. Code for lowering addresses references to machine dependent code is; factored into common code for generating different address forms and; is called by the relocation model specific lowering function, using; templated functions. For example:. // lib/Target/Mips/MipsISelLowering.cpp; SDValue MipsTargetLowering::; lowerJumpTable(SDValue Op, SelectionDAG &DAG) const. calls. template <class NodeTy> // lib/Target/Mips/MipsISelLowering.h; SDValue getAddrLocal(NodeTy *N, const SDLoc &DL, EVT Ty,; SelectionDAG &DAG, bool IsN32OrN64) const. which calls the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:1536,Integrability,wrap,wrapper,1536," lowering function, using; templated functions. For example:. // lib/Target/Mips/MipsISelLowering.cpp; SDValue MipsTargetLowering::; lowerJumpTable(SDValue Op, SelectionDAG &DAG) const. calls. template <class NodeTy> // lib/Target/Mips/MipsISelLowering.h; SDValue getAddrLocal(NodeTy *N, const SDLoc &DL, EVT Ty,; SelectionDAG &DAG, bool IsN32OrN64) const. which calls the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:1649,Integrability,wrap,wrapper,1649,"argetLowering::; lowerJumpTable(SDValue Op, SelectionDAG &DAG) const. calls. template <class NodeTy> // lib/Target/Mips/MipsISelLowering.h; SDValue getAddrLocal(NodeTy *N, const SDLoc &DL, EVT Ty,; SelectionDAG &DAG, bool IsN32OrN64) const. which calls the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoR",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:1694,Integrability,Wrap,Wrapper,1694,"argetLowering::; lowerJumpTable(SDValue Op, SelectionDAG &DAG) const. calls. template <class NodeTy> // lib/Target/Mips/MipsISelLowering.h; SDValue getAddrLocal(NodeTy *N, const SDLoc &DL, EVT Ty,; SelectionDAG &DAG, bool IsN32OrN64) const. which calls the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoR",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:1801,Integrability,depend,dependent,1801,"SelLowering.h; SDValue getAddrLocal(NodeTy *N, const SDLoc &DL, EVT Ty,; SelectionDAG &DAG, bool IsN32OrN64) const. which calls the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:1904,Modifiability,parameteriz,parameterized,1904,"the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:3386,Modifiability,extend,extends,3386,"template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details on how multiclasses in TableGen work can be found in the; section ""Multiclass definitions and instances"" in the document; ""TableGen Language Introduction"". 4. Instruction definitions are multiply defined to cover the different; register classes. In some cases, such as LW/LW64, this also accounts; for the difference in the results of instruction execution. On MIPS32,; ""lw"" loads a 32 bit value from memory. On MIPS64, ""lw"" loads a 32 bit; value from memory and sign extends the value to 64 bits. // lib/Target/Mips/MipsInstrInfo.td; def LUi : MMRel, LoadUpper<""lui"", GPR32Opnd, uimm16_relaxed>, LUI_FM;; // lib/Target/Mips/Mips64InstrInfo.td; def LUi64 : LoadUpper<""lui"", GPR64Opnd, uimm16_64_relaxed>, LUI_FM;. defines two names ""LUi"" and ""LUi64"" with two different register; classes, but with the same encoding---""LUI_FM"". These instructions load a; 16-bit immediate into bits 31-16 and clear the lower 15 bits. On MIPS64,; the result is sign-extended to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:3865,Modifiability,extend,extended,3865,"template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details on how multiclasses in TableGen work can be found in the; section ""Multiclass definitions and instances"" in the document; ""TableGen Language Introduction"". 4. Instruction definitions are multiply defined to cover the different; register classes. In some cases, such as LW/LW64, this also accounts; for the difference in the results of instruction execution. On MIPS32,; ""lw"" loads a 32 bit value from memory. On MIPS64, ""lw"" loads a 32 bit; value from memory and sign extends the value to 64 bits. // lib/Target/Mips/MipsInstrInfo.td; def LUi : MMRel, LoadUpper<""lui"", GPR32Opnd, uimm16_relaxed>, LUI_FM;; // lib/Target/Mips/Mips64InstrInfo.td; def LUi64 : LoadUpper<""lui"", GPR64Opnd, uimm16_64_relaxed>, LUI_FM;. defines two names ""LUi"" and ""LUi64"" with two different register; classes, but with the same encoding---""LUI_FM"". These instructions load a; 16-bit immediate into bits 31-16 and clear the lower 15 bits. On MIPS64,; the result is sign-extended to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:1530,Performance,load,load,1530," lowering function, using; templated functions. For example:. // lib/Target/Mips/MipsISelLowering.cpp; SDValue MipsTargetLowering::; lowerJumpTable(SDValue Op, SelectionDAG &DAG) const. calls. template <class NodeTy> // lib/Target/Mips/MipsISelLowering.h; SDValue getAddrLocal(NodeTy *N, const SDLoc &DL, EVT Ty,; SelectionDAG &DAG, bool IsN32OrN64) const. which calls the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:1927,Performance,load,load,1927,"the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:3293,Performance,load,loads,3293,"template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details on how multiclasses in TableGen work can be found in the; section ""Multiclass definitions and instances"" in the document; ""TableGen Language Introduction"". 4. Instruction definitions are multiply defined to cover the different; register classes. In some cases, such as LW/LW64, this also accounts; for the difference in the results of instruction execution. On MIPS32,; ""lw"" loads a 32 bit value from memory. On MIPS64, ""lw"" loads a 32 bit; value from memory and sign extends the value to 64 bits. // lib/Target/Mips/MipsInstrInfo.td; def LUi : MMRel, LoadUpper<""lui"", GPR32Opnd, uimm16_relaxed>, LUI_FM;; // lib/Target/Mips/Mips64InstrInfo.td; def LUi64 : LoadUpper<""lui"", GPR64Opnd, uimm16_64_relaxed>, LUI_FM;. defines two names ""LUi"" and ""LUi64"" with two different register; classes, but with the same encoding---""LUI_FM"". These instructions load a; 16-bit immediate into bits 31-16 and clear the lower 15 bits. On MIPS64,; the result is sign-extended to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:3343,Performance,load,loads,3343,"template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details on how multiclasses in TableGen work can be found in the; section ""Multiclass definitions and instances"" in the document; ""TableGen Language Introduction"". 4. Instruction definitions are multiply defined to cover the different; register classes. In some cases, such as LW/LW64, this also accounts; for the difference in the results of instruction execution. On MIPS32,; ""lw"" loads a 32 bit value from memory. On MIPS64, ""lw"" loads a 32 bit; value from memory and sign extends the value to 64 bits. // lib/Target/Mips/MipsInstrInfo.td; def LUi : MMRel, LoadUpper<""lui"", GPR32Opnd, uimm16_relaxed>, LUI_FM;; // lib/Target/Mips/Mips64InstrInfo.td; def LUi64 : LoadUpper<""lui"", GPR64Opnd, uimm16_64_relaxed>, LUI_FM;. defines two names ""LUi"" and ""LUi64"" with two different register; classes, but with the same encoding---""LUI_FM"". These instructions load a; 16-bit immediate into bits 31-16 and clear the lower 15 bits. On MIPS64,; the result is sign-extended to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:3470,Performance,Load,LoadUpper,3470,"template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details on how multiclasses in TableGen work can be found in the; section ""Multiclass definitions and instances"" in the document; ""TableGen Language Introduction"". 4. Instruction definitions are multiply defined to cover the different; register classes. In some cases, such as LW/LW64, this also accounts; for the difference in the results of instruction execution. On MIPS32,; ""lw"" loads a 32 bit value from memory. On MIPS64, ""lw"" loads a 32 bit; value from memory and sign extends the value to 64 bits. // lib/Target/Mips/MipsInstrInfo.td; def LUi : MMRel, LoadUpper<""lui"", GPR32Opnd, uimm16_relaxed>, LUI_FM;; // lib/Target/Mips/Mips64InstrInfo.td; def LUi64 : LoadUpper<""lui"", GPR64Opnd, uimm16_64_relaxed>, LUI_FM;. defines two names ""LUi"" and ""LUi64"" with two different register; classes, but with the same encoding---""LUI_FM"". These instructions load a; 16-bit immediate into bits 31-16 and clear the lower 15 bits. On MIPS64,; the result is sign-extended to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:3575,Performance,Load,LoadUpper,3575,"template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details on how multiclasses in TableGen work can be found in the; section ""Multiclass definitions and instances"" in the document; ""TableGen Language Introduction"". 4. Instruction definitions are multiply defined to cover the different; register classes. In some cases, such as LW/LW64, this also accounts; for the difference in the results of instruction execution. On MIPS32,; ""lw"" loads a 32 bit value from memory. On MIPS64, ""lw"" loads a 32 bit; value from memory and sign extends the value to 64 bits. // lib/Target/Mips/MipsInstrInfo.td; def LUi : MMRel, LoadUpper<""lui"", GPR32Opnd, uimm16_relaxed>, LUI_FM;; // lib/Target/Mips/Mips64InstrInfo.td; def LUi64 : LoadUpper<""lui"", GPR64Opnd, uimm16_64_relaxed>, LUI_FM;. defines two names ""LUi"" and ""LUi64"" with two different register; classes, but with the same encoding---""LUI_FM"". These instructions load a; 16-bit immediate into bits 31-16 and clear the lower 15 bits. On MIPS64,; the result is sign-extended to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:3764,Performance,load,load,3764,"template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details on how multiclasses in TableGen work can be found in the; section ""Multiclass definitions and instances"" in the document; ""TableGen Language Introduction"". 4. Instruction definitions are multiply defined to cover the different; register classes. In some cases, such as LW/LW64, this also accounts; for the difference in the results of instruction execution. On MIPS32,; ""lw"" loads a 32 bit value from memory. On MIPS64, ""lw"" loads a 32 bit; value from memory and sign extends the value to 64 bits. // lib/Target/Mips/MipsInstrInfo.td; def LUi : MMRel, LoadUpper<""lui"", GPR32Opnd, uimm16_relaxed>, LUI_FM;; // lib/Target/Mips/Mips64InstrInfo.td; def LUi64 : LoadUpper<""lui"", GPR64Opnd, uimm16_64_relaxed>, LUI_FM;. defines two names ""LUi"" and ""LUi64"" with two different register; classes, but with the same encoding---""LUI_FM"". These instructions load a; 16-bit immediate into bits 31-16 and clear the lower 15 bits. On MIPS64,; the result is sign-extended to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:3809,Usability,clear,clear,3809,"template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details on how multiclasses in TableGen work can be found in the; section ""Multiclass definitions and instances"" in the document; ""TableGen Language Introduction"". 4. Instruction definitions are multiply defined to cover the different; register classes. In some cases, such as LW/LW64, this also accounts; for the difference in the results of instruction execution. On MIPS32,; ""lw"" loads a 32 bit value from memory. On MIPS64, ""lw"" loads a 32 bit; value from memory and sign extends the value to 64 bits. // lib/Target/Mips/MipsInstrInfo.td; def LUi : MMRel, LoadUpper<""lui"", GPR32Opnd, uimm16_relaxed>, LUI_FM;; // lib/Target/Mips/Mips64InstrInfo.td; def LUi64 : LoadUpper<""lui"", GPR64Opnd, uimm16_64_relaxed>, LUI_FM;. defines two names ""LUi"" and ""LUi64"" with two different register; classes, but with the same encoding---""LUI_FM"". These instructions load a; 16-bit immediate into bits 31-16 and clear the lower 15 bits. On MIPS64,; the result is sign-extended to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/MSP430/README.txt:493,Availability,avail,available,493,"//===---------------------------------------------------------------------===//; // MSP430 backend.; //===---------------------------------------------------------------------===//. DISCLAIMER: This backend should be considered as highly experimental. I never; seen nor worked with this MCU, all information was gathered from datasheet; only. The original intention of making this backend was to write documentation; of form ""How to write backend for dummies"" :) Thes notes hopefully will be; available pretty soon. Some things are incomplete / not implemented yet (this list surely is not; complete as well):. 1. Verify, how stuff is handling implicit zext with 8 bit operands (this might; be modelled currently in improper way - should we need to mark the superreg as; def for every 8 bit instruction?). 2. Libcalls: multiplication, division, remainder. Note, that calling convention; for libcalls is incomptible with calling convention of libcalls of msp430-gcc; (these cannot be used though due to license restriction). 3. Implement multiplication / division by constant (dag combiner hook?). 4. Implement non-constant shifts. 5. Implement varargs stuff. 6. Verify and fix (if needed) how's stuff playing with i32 / i64. 7. Implement floating point stuff (softfp?). 8. Implement instruction encoding for (possible) direct code emission in the; future. 9. Since almost all instructions set flags - implement brcond / select in better; way (currently they emit explicit comparison). 10. Handle imm in comparisons in better way (see comment in MSP430InstrInfo.td). 11. Implement hooks for better memory op folding, etc. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/MSP430/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/MSP430/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt:25,Energy Efficiency,Power,PowerPC,25,add_llvm_component_group(PowerPC HAS_JIT). set(LLVM_TARGET_DEFINITIONS PPC.td). tablegen(LLVM PPCGenAsmMatcher.inc -gen-asm-matcher); tablegen(LLVM PPCGenAsmWriter.inc -gen-asm-writer); tablegen(LLVM PPCGenCallingConv.inc -gen-callingconv); tablegen(LLVM PPCGenDAGISel.inc -gen-dag-isel); tablegen(LLVM PPCGenDisassemblerTables.inc -gen-disassembler); tablegen(LLVM PPCGenFastISel.inc -gen-fast-isel); tablegen(LLVM PPCGenInstrInfo.inc -gen-instr-info); tablegen(LLVM PPCGenMCCodeEmitter.inc -gen-emitter); tablegen(LLVM PPCGenRegisterInfo.inc -gen-register-info); tablegen(LLVM PPCGenSubtargetInfo.inc -gen-subtarget); tablegen(LLVM PPCGenExegesis.inc -gen-exegesis); tablegen(LLVM PPCGenRegisterBank.inc -gen-register-bank); tablegen(LLVM PPCGenGlobalISel.inc -gen-global-isel). add_public_tablegen_target(PowerPCCommonTableGen). add_llvm_target(PowerPCCodeGen; GISel/PPCInstructionSelector.cpp; PPCBoolRetToInt.cpp; PPCAsmPrinter.cpp; PPCBranchSelector.cpp; PPCBranchCoalescing.cpp; PPCCallingConv.cpp; PPCCCState.cpp; PPCCTRLoops.cpp; PPCCTRLoopsVerify.cpp; PPCExpandAtomicPseudoInsts.cpp; PPCHazardRecognizers.cpp; PPCInstrInfo.cpp; PPCISelDAGToDAG.cpp; PPCISelLowering.cpp; PPCEarlyReturn.cpp; PPCFastISel.cpp; PPCFrameLowering.cpp; PPCLoopInstrFormPrep.cpp; PPCMCInstLower.cpp; PPCMachineFunctionInfo.cpp; PPCMachineScheduler.cpp; PPCMacroFusion.cpp; PPCMergeStringPool.cpp; PPCMIPeephole.cpp; PPCRegisterInfo.cpp; PPCSubtarget.cpp; PPCTargetMachine.cpp; PPCTargetObjectFile.cpp; PPCTargetTransformInfo.cpp; PPCTOCRegDeps.cpp; PPCTLSDynamicCall.cpp; PPCVSXCopy.cpp; PPCReduceCRLogicals.cpp; PPCVSXFMAMutate.cpp; PPCVSXSwapRemoval.cpp; PPCExpandISEL.cpp; PPCPreEmitPeephole.cpp; PPCLowerMASSVEntries.cpp; PPCGenScalarMASSEntries.cpp; GISel/PPCCallLowering.cpp; GISel/PPCRegisterBankInfo.cpp; GISel/PPCLegalizerInfo.cpp. LINK_COMPONENTS; Analysis; AsmPrinter; BinaryFormat; CodeGen; CodeGenTypes; Core; GlobalISel; MC; PowerPCDesc; PowerPCInfo; Scalar; SelectionDAG; Support; Target; TargetParser,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt:808,Energy Efficiency,Power,PowerPCCommonTableGen,808,add_llvm_component_group(PowerPC HAS_JIT). set(LLVM_TARGET_DEFINITIONS PPC.td). tablegen(LLVM PPCGenAsmMatcher.inc -gen-asm-matcher); tablegen(LLVM PPCGenAsmWriter.inc -gen-asm-writer); tablegen(LLVM PPCGenCallingConv.inc -gen-callingconv); tablegen(LLVM PPCGenDAGISel.inc -gen-dag-isel); tablegen(LLVM PPCGenDisassemblerTables.inc -gen-disassembler); tablegen(LLVM PPCGenFastISel.inc -gen-fast-isel); tablegen(LLVM PPCGenInstrInfo.inc -gen-instr-info); tablegen(LLVM PPCGenMCCodeEmitter.inc -gen-emitter); tablegen(LLVM PPCGenRegisterInfo.inc -gen-register-info); tablegen(LLVM PPCGenSubtargetInfo.inc -gen-subtarget); tablegen(LLVM PPCGenExegesis.inc -gen-exegesis); tablegen(LLVM PPCGenRegisterBank.inc -gen-register-bank); tablegen(LLVM PPCGenGlobalISel.inc -gen-global-isel). add_public_tablegen_target(PowerPCCommonTableGen). add_llvm_target(PowerPCCodeGen; GISel/PPCInstructionSelector.cpp; PPCBoolRetToInt.cpp; PPCAsmPrinter.cpp; PPCBranchSelector.cpp; PPCBranchCoalescing.cpp; PPCCallingConv.cpp; PPCCCState.cpp; PPCCTRLoops.cpp; PPCCTRLoopsVerify.cpp; PPCExpandAtomicPseudoInsts.cpp; PPCHazardRecognizers.cpp; PPCInstrInfo.cpp; PPCISelDAGToDAG.cpp; PPCISelLowering.cpp; PPCEarlyReturn.cpp; PPCFastISel.cpp; PPCFrameLowering.cpp; PPCLoopInstrFormPrep.cpp; PPCMCInstLower.cpp; PPCMachineFunctionInfo.cpp; PPCMachineScheduler.cpp; PPCMacroFusion.cpp; PPCMergeStringPool.cpp; PPCMIPeephole.cpp; PPCRegisterInfo.cpp; PPCSubtarget.cpp; PPCTargetMachine.cpp; PPCTargetObjectFile.cpp; PPCTargetTransformInfo.cpp; PPCTOCRegDeps.cpp; PPCTLSDynamicCall.cpp; PPCVSXCopy.cpp; PPCReduceCRLogicals.cpp; PPCVSXFMAMutate.cpp; PPCVSXSwapRemoval.cpp; PPCExpandISEL.cpp; PPCPreEmitPeephole.cpp; PPCLowerMASSVEntries.cpp; PPCGenScalarMASSEntries.cpp; GISel/PPCCallLowering.cpp; GISel/PPCRegisterBankInfo.cpp; GISel/PPCLegalizerInfo.cpp. LINK_COMPONENTS; Analysis; AsmPrinter; BinaryFormat; CodeGen; CodeGenTypes; Core; GlobalISel; MC; PowerPCDesc; PowerPCInfo; Scalar; SelectionDAG; Support; Target; TargetParser,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt:848,Energy Efficiency,Power,PowerPCCodeGen,848,add_llvm_component_group(PowerPC HAS_JIT). set(LLVM_TARGET_DEFINITIONS PPC.td). tablegen(LLVM PPCGenAsmMatcher.inc -gen-asm-matcher); tablegen(LLVM PPCGenAsmWriter.inc -gen-asm-writer); tablegen(LLVM PPCGenCallingConv.inc -gen-callingconv); tablegen(LLVM PPCGenDAGISel.inc -gen-dag-isel); tablegen(LLVM PPCGenDisassemblerTables.inc -gen-disassembler); tablegen(LLVM PPCGenFastISel.inc -gen-fast-isel); tablegen(LLVM PPCGenInstrInfo.inc -gen-instr-info); tablegen(LLVM PPCGenMCCodeEmitter.inc -gen-emitter); tablegen(LLVM PPCGenRegisterInfo.inc -gen-register-info); tablegen(LLVM PPCGenSubtargetInfo.inc -gen-subtarget); tablegen(LLVM PPCGenExegesis.inc -gen-exegesis); tablegen(LLVM PPCGenRegisterBank.inc -gen-register-bank); tablegen(LLVM PPCGenGlobalISel.inc -gen-global-isel). add_public_tablegen_target(PowerPCCommonTableGen). add_llvm_target(PowerPCCodeGen; GISel/PPCInstructionSelector.cpp; PPCBoolRetToInt.cpp; PPCAsmPrinter.cpp; PPCBranchSelector.cpp; PPCBranchCoalescing.cpp; PPCCallingConv.cpp; PPCCCState.cpp; PPCCTRLoops.cpp; PPCCTRLoopsVerify.cpp; PPCExpandAtomicPseudoInsts.cpp; PPCHazardRecognizers.cpp; PPCInstrInfo.cpp; PPCISelDAGToDAG.cpp; PPCISelLowering.cpp; PPCEarlyReturn.cpp; PPCFastISel.cpp; PPCFrameLowering.cpp; PPCLoopInstrFormPrep.cpp; PPCMCInstLower.cpp; PPCMachineFunctionInfo.cpp; PPCMachineScheduler.cpp; PPCMacroFusion.cpp; PPCMergeStringPool.cpp; PPCMIPeephole.cpp; PPCRegisterInfo.cpp; PPCSubtarget.cpp; PPCTargetMachine.cpp; PPCTargetObjectFile.cpp; PPCTargetTransformInfo.cpp; PPCTOCRegDeps.cpp; PPCTLSDynamicCall.cpp; PPCVSXCopy.cpp; PPCReduceCRLogicals.cpp; PPCVSXFMAMutate.cpp; PPCVSXSwapRemoval.cpp; PPCExpandISEL.cpp; PPCPreEmitPeephole.cpp; PPCLowerMASSVEntries.cpp; PPCGenScalarMASSEntries.cpp; GISel/PPCCallLowering.cpp; GISel/PPCRegisterBankInfo.cpp; GISel/PPCLegalizerInfo.cpp. LINK_COMPONENTS; Analysis; AsmPrinter; BinaryFormat; CodeGen; CodeGenTypes; Core; GlobalISel; MC; PowerPCDesc; PowerPCInfo; Scalar; SelectionDAG; Support; Target; TargetParser,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt:1924,Energy Efficiency,Power,PowerPCDesc,1924,-asm-writer); tablegen(LLVM PPCGenCallingConv.inc -gen-callingconv); tablegen(LLVM PPCGenDAGISel.inc -gen-dag-isel); tablegen(LLVM PPCGenDisassemblerTables.inc -gen-disassembler); tablegen(LLVM PPCGenFastISel.inc -gen-fast-isel); tablegen(LLVM PPCGenInstrInfo.inc -gen-instr-info); tablegen(LLVM PPCGenMCCodeEmitter.inc -gen-emitter); tablegen(LLVM PPCGenRegisterInfo.inc -gen-register-info); tablegen(LLVM PPCGenSubtargetInfo.inc -gen-subtarget); tablegen(LLVM PPCGenExegesis.inc -gen-exegesis); tablegen(LLVM PPCGenRegisterBank.inc -gen-register-bank); tablegen(LLVM PPCGenGlobalISel.inc -gen-global-isel). add_public_tablegen_target(PowerPCCommonTableGen). add_llvm_target(PowerPCCodeGen; GISel/PPCInstructionSelector.cpp; PPCBoolRetToInt.cpp; PPCAsmPrinter.cpp; PPCBranchSelector.cpp; PPCBranchCoalescing.cpp; PPCCallingConv.cpp; PPCCCState.cpp; PPCCTRLoops.cpp; PPCCTRLoopsVerify.cpp; PPCExpandAtomicPseudoInsts.cpp; PPCHazardRecognizers.cpp; PPCInstrInfo.cpp; PPCISelDAGToDAG.cpp; PPCISelLowering.cpp; PPCEarlyReturn.cpp; PPCFastISel.cpp; PPCFrameLowering.cpp; PPCLoopInstrFormPrep.cpp; PPCMCInstLower.cpp; PPCMachineFunctionInfo.cpp; PPCMachineScheduler.cpp; PPCMacroFusion.cpp; PPCMergeStringPool.cpp; PPCMIPeephole.cpp; PPCRegisterInfo.cpp; PPCSubtarget.cpp; PPCTargetMachine.cpp; PPCTargetObjectFile.cpp; PPCTargetTransformInfo.cpp; PPCTOCRegDeps.cpp; PPCTLSDynamicCall.cpp; PPCVSXCopy.cpp; PPCReduceCRLogicals.cpp; PPCVSXFMAMutate.cpp; PPCVSXSwapRemoval.cpp; PPCExpandISEL.cpp; PPCPreEmitPeephole.cpp; PPCLowerMASSVEntries.cpp; PPCGenScalarMASSEntries.cpp; GISel/PPCCallLowering.cpp; GISel/PPCRegisterBankInfo.cpp; GISel/PPCLegalizerInfo.cpp. LINK_COMPONENTS; Analysis; AsmPrinter; BinaryFormat; CodeGen; CodeGenTypes; Core; GlobalISel; MC; PowerPCDesc; PowerPCInfo; Scalar; SelectionDAG; Support; Target; TargetParser; TransformUtils. ADD_TO_COMPONENT; PowerPC; ). add_subdirectory(AsmParser); add_subdirectory(Disassembler); add_subdirectory(MCTargetDesc); add_subdirectory(TargetInfo); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt:1937,Energy Efficiency,Power,PowerPCInfo,1937,-asm-writer); tablegen(LLVM PPCGenCallingConv.inc -gen-callingconv); tablegen(LLVM PPCGenDAGISel.inc -gen-dag-isel); tablegen(LLVM PPCGenDisassemblerTables.inc -gen-disassembler); tablegen(LLVM PPCGenFastISel.inc -gen-fast-isel); tablegen(LLVM PPCGenInstrInfo.inc -gen-instr-info); tablegen(LLVM PPCGenMCCodeEmitter.inc -gen-emitter); tablegen(LLVM PPCGenRegisterInfo.inc -gen-register-info); tablegen(LLVM PPCGenSubtargetInfo.inc -gen-subtarget); tablegen(LLVM PPCGenExegesis.inc -gen-exegesis); tablegen(LLVM PPCGenRegisterBank.inc -gen-register-bank); tablegen(LLVM PPCGenGlobalISel.inc -gen-global-isel). add_public_tablegen_target(PowerPCCommonTableGen). add_llvm_target(PowerPCCodeGen; GISel/PPCInstructionSelector.cpp; PPCBoolRetToInt.cpp; PPCAsmPrinter.cpp; PPCBranchSelector.cpp; PPCBranchCoalescing.cpp; PPCCallingConv.cpp; PPCCCState.cpp; PPCCTRLoops.cpp; PPCCTRLoopsVerify.cpp; PPCExpandAtomicPseudoInsts.cpp; PPCHazardRecognizers.cpp; PPCInstrInfo.cpp; PPCISelDAGToDAG.cpp; PPCISelLowering.cpp; PPCEarlyReturn.cpp; PPCFastISel.cpp; PPCFrameLowering.cpp; PPCLoopInstrFormPrep.cpp; PPCMCInstLower.cpp; PPCMachineFunctionInfo.cpp; PPCMachineScheduler.cpp; PPCMacroFusion.cpp; PPCMergeStringPool.cpp; PPCMIPeephole.cpp; PPCRegisterInfo.cpp; PPCSubtarget.cpp; PPCTargetMachine.cpp; PPCTargetObjectFile.cpp; PPCTargetTransformInfo.cpp; PPCTOCRegDeps.cpp; PPCTLSDynamicCall.cpp; PPCVSXCopy.cpp; PPCReduceCRLogicals.cpp; PPCVSXFMAMutate.cpp; PPCVSXSwapRemoval.cpp; PPCExpandISEL.cpp; PPCPreEmitPeephole.cpp; PPCLowerMASSVEntries.cpp; PPCGenScalarMASSEntries.cpp; GISel/PPCCallLowering.cpp; GISel/PPCRegisterBankInfo.cpp; GISel/PPCLegalizerInfo.cpp. LINK_COMPONENTS; Analysis; AsmPrinter; BinaryFormat; CodeGen; CodeGenTypes; Core; GlobalISel; MC; PowerPCDesc; PowerPCInfo; Scalar; SelectionDAG; Support; Target; TargetParser; TransformUtils. ADD_TO_COMPONENT; PowerPC; ). add_subdirectory(AsmParser); add_subdirectory(Disassembler); add_subdirectory(MCTargetDesc); add_subdirectory(TargetInfo); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt:2037,Energy Efficiency,Power,PowerPC,2037,-asm-writer); tablegen(LLVM PPCGenCallingConv.inc -gen-callingconv); tablegen(LLVM PPCGenDAGISel.inc -gen-dag-isel); tablegen(LLVM PPCGenDisassemblerTables.inc -gen-disassembler); tablegen(LLVM PPCGenFastISel.inc -gen-fast-isel); tablegen(LLVM PPCGenInstrInfo.inc -gen-instr-info); tablegen(LLVM PPCGenMCCodeEmitter.inc -gen-emitter); tablegen(LLVM PPCGenRegisterInfo.inc -gen-register-info); tablegen(LLVM PPCGenSubtargetInfo.inc -gen-subtarget); tablegen(LLVM PPCGenExegesis.inc -gen-exegesis); tablegen(LLVM PPCGenRegisterBank.inc -gen-register-bank); tablegen(LLVM PPCGenGlobalISel.inc -gen-global-isel). add_public_tablegen_target(PowerPCCommonTableGen). add_llvm_target(PowerPCCodeGen; GISel/PPCInstructionSelector.cpp; PPCBoolRetToInt.cpp; PPCAsmPrinter.cpp; PPCBranchSelector.cpp; PPCBranchCoalescing.cpp; PPCCallingConv.cpp; PPCCCState.cpp; PPCCTRLoops.cpp; PPCCTRLoopsVerify.cpp; PPCExpandAtomicPseudoInsts.cpp; PPCHazardRecognizers.cpp; PPCInstrInfo.cpp; PPCISelDAGToDAG.cpp; PPCISelLowering.cpp; PPCEarlyReturn.cpp; PPCFastISel.cpp; PPCFrameLowering.cpp; PPCLoopInstrFormPrep.cpp; PPCMCInstLower.cpp; PPCMachineFunctionInfo.cpp; PPCMachineScheduler.cpp; PPCMacroFusion.cpp; PPCMergeStringPool.cpp; PPCMIPeephole.cpp; PPCRegisterInfo.cpp; PPCSubtarget.cpp; PPCTargetMachine.cpp; PPCTargetObjectFile.cpp; PPCTargetTransformInfo.cpp; PPCTOCRegDeps.cpp; PPCTLSDynamicCall.cpp; PPCVSXCopy.cpp; PPCReduceCRLogicals.cpp; PPCVSXFMAMutate.cpp; PPCVSXSwapRemoval.cpp; PPCExpandISEL.cpp; PPCPreEmitPeephole.cpp; PPCLowerMASSVEntries.cpp; PPCGenScalarMASSEntries.cpp; GISel/PPCCallLowering.cpp; GISel/PPCRegisterBankInfo.cpp; GISel/PPCLegalizerInfo.cpp. LINK_COMPONENTS; Analysis; AsmPrinter; BinaryFormat; CodeGen; CodeGenTypes; Core; GlobalISel; MC; PowerPCDesc; PowerPCInfo; Scalar; SelectionDAG; Support; Target; TargetParser; TransformUtils. ADD_TO_COMPONENT; PowerPC; ). add_subdirectory(AsmParser); add_subdirectory(Disassembler); add_subdirectory(MCTargetDesc); add_subdirectory(TargetInfo); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:5762,Availability,mask,masked,5762,"-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* %b) {; entry:; %tmp = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp = load i32* %tmp ; <uint> [#uses=1]; %tmp3 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp4 = load i32* %tmp3 ; <uint> [#uses=1]; %tmp8 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=2]; %tmp9 = load i32* %tmp8 ; <uint> [#uses=1]; %tmp4.mask17 = shl i32 %tmp4, i8 1 ; <uint> [#uses=1]; %tmp1415 = and i32 %tmp4.mask17, 2147483648 ; <uint> [#uses=1]; %tmp.masked = and i32 %tmp, 2147483648 ; <uint> [#uses=1]; %tmp11 = or i32 %tmp1415, %tmp.masked ; <uint> [#uses=1]; %tmp12 = and i32 %tmp9, 2147483647 ; <uint> [#uses=1]; %tmp13 = or i32 %tmp12, %tmp11 ; <uint> [#uses=1]; store i32 %tmp13, i32* %tmp8; ret void; }. We emit:. _foo:; lwz r2, 0(r3); slwi r4, r2, 1; or r4, r4, r2; rlwimi r2, r4, 0, 0, 0; stw r2, 0(r3); blr. We could collapse a bunch of those ORs and ANDs and generate the following; equivalent code:. _foo:; lwz r2, 0(r3); rlwinm r4, r2, 1, 0, 0; or r2, r2, r4; stw r2, 0(r3); blr. ===-------------------------------------------------------------------------===. Consider a function like this:. float foo(float X) { return X + 1234.4123f; }. The FP constant ends up in the constant pool, so we need to get the LR register.; This ends up producing code like this:. _foo:; .LBB_foo_0: ; entry; mflr r11; *** stw r11, 8(r1); bl ""L00000$pb""; ""L00000$pb"":; mflr r2; addis r2, r2, ha16(.CPI_foo_0-""L00000$pb""); lfs f0, lo16(.CPI_foo_0-""L00000$pb"")(r2); fadds f1, f1, f0; *** lwz r11, 8(r1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:5847,Availability,mask,masked,5847,"int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* %b) {; entry:; %tmp = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp = load i32* %tmp ; <uint> [#uses=1]; %tmp3 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp4 = load i32* %tmp3 ; <uint> [#uses=1]; %tmp8 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=2]; %tmp9 = load i32* %tmp8 ; <uint> [#uses=1]; %tmp4.mask17 = shl i32 %tmp4, i8 1 ; <uint> [#uses=1]; %tmp1415 = and i32 %tmp4.mask17, 2147483648 ; <uint> [#uses=1]; %tmp.masked = and i32 %tmp, 2147483648 ; <uint> [#uses=1]; %tmp11 = or i32 %tmp1415, %tmp.masked ; <uint> [#uses=1]; %tmp12 = and i32 %tmp9, 2147483647 ; <uint> [#uses=1]; %tmp13 = or i32 %tmp12, %tmp11 ; <uint> [#uses=1]; store i32 %tmp13, i32* %tmp8; ret void; }. We emit:. _foo:; lwz r2, 0(r3); slwi r4, r2, 1; or r4, r4, r2; rlwimi r2, r4, 0, 0, 0; stw r2, 0(r3); blr. We could collapse a bunch of those ORs and ANDs and generate the following; equivalent code:. _foo:; lwz r2, 0(r3); rlwinm r4, r2, 1, 0, 0; or r2, r2, r4; stw r2, 0(r3); blr. ===-------------------------------------------------------------------------===. Consider a function like this:. float foo(float X) { return X + 1234.4123f; }. The FP constant ends up in the constant pool, so we need to get the LR register.; This ends up producing code like this:. _foo:; .LBB_foo_0: ; entry; mflr r11; *** stw r11, 8(r1); bl ""L00000$pb""; ""L00000$pb"":; mflr r2; addis r2, r2, ha16(.CPI_foo_0-""L00000$pb""); lfs f0, lo16(.CPI_foo_0-""L00000$pb"")(r2); fadds f1, f1, f0; *** lwz r11, 8(r1); mtlr r11; blr. This is functional, but there is no reason to spill the LR register all the way; to the stack (the two marked in",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:11187,Availability,recover,recover,11187,",1; 	blr; ; which is more efficient and can use mfocr. See PR642 for some more context. //===---------------------------------------------------------------------===//. void foo(float *data, float d) {; long i;; for (i = 0; i < 8000; i++); data[i] = d;; }; void foo2(float *data, float d) {; long i;; data--;; for (i = 0; i < 8000; i++) {; data[1] = d;; data++;; }; }. These compile to:. _foo:; 	li r2, 0; LBB1_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB1_1	; bb; 	blr ; _foo2:; 	li r2, 0; LBB2_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB2_1	; bb; 	blr . The 'mr' could be eliminated to folding the add into the cmp better. //===---------------------------------------------------------------------===//; Codegen for the following (low-probability) case deteriorated considerably ; when the correctness fixes for unordered comparisons went in (PR 642, 58871).; It should be possible to recover the code quality described in the comments. ; RUN: llvm-as < %s | llc -march=ppc32 | grep or | count 3; ; This should produce one 'or' or 'cror' instruction per function. ; RUN: llvm-as < %s | llc -march=ppc32 | grep mfcr | count 3; ; PR2964. define i32 @test(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ole double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test2(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to G",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:2005,Deployability,patch,patches,2005,"nz instead of bdz) but it still beats us. If we; produced this with bdnz, the loop would be a single dispatch group. ===-------------------------------------------------------------------------===. Lump the constant pool for each function into ONE pic object, and reference; pieces of it as offsets from the start. For functions like this (contrived; to have lots of constants obviously):. double X(double Y) { return (Y*1.23 + 4.512)*2.34 + 14.38; }. We generate:. _X:; lis r2, ha16(.CPI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-weak addresses so this:. static int a;; void bar(int b) { a = b; }; v",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:2779,Deployability,patch,patches,2779,"a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-weak addresses so this:. static int a;; void bar(int b) { a = b; }; void foo(unsigned char *c) {; *c = a;; }. So that . _foo:; lis r2, ha16(_a); la r2, lo16(_a)(r2); lbz r2, 3(r2); stb r2, 0(r3); blr. Becomes. _foo:; lis r2, ha16(_a+3); lbz r2, lo16(_a+3)(r2); stb r2, 0(r3); blr. ===-------------------------------------------------------------------------===. We should compile these two functions to the same thing:. #include <stdlib.h>; void f(int a, int b, int *P) {; *P = (a-b)>=0?(a-b):(b-a);; }; void g(int a, int b, int *P) {; *P = abs(a-b);; }. Further, they should compile to something better than:. _g:; subf r2, r4, r3; subfic r3, r2, 0; cmpwi cr0, r2, -1; bgt cr0, LBB2_2 ; entry; LBB2_1: ; entry; mr r2, r3; LBB2_2: ; entry; stw r2, 0(r5); blr. GCC produces:. _g:; subf r4,r4,r3; srawi r2,r4,31; xor r0,r2,r4; subf r0,r2,r0; stw",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:40,Energy Efficiency,Power,PowerPC-specific,40,"//===- README.txt - Notes for improving PowerPC-specific code gen ---------===//. TODO:; * lmw/stmw pass a la arm load store optimizer for prolog/epilog. ===-------------------------------------------------------------------------===. This code:. unsigned add32carry(unsigned sum, unsigned x) {; unsigned z = sum + x;; if (sum + x < x); z++;; return z;; }. Should compile to something like:. 	addc r3,r3,r4; 	addze r3,r3. instead we get:. 	add r3, r4, r3; 	cmplw cr7, r3, r4; 	mfcr r4 ; 1; 	rlwinm r4, r4, 29, 31, 31; 	add r3, r3, r4. Ick. ===-------------------------------------------------------------------------===. We compile the hottest inner loop of viterbi to:. li r6, 0; b LBB1_84 ;bb432.i; LBB1_83: ;bb420.i; lbzx r8, r5, r7; addi r6, r7, 1; stbx r8, r4, r7; LBB1_84: ;bb432.i; mr r7, r6; cmplwi cr0, r7, 143; bne cr0, LBB1_83 ;bb420.i. The CBE manages to produce:. 	li r0, 143; 	mtctr r0; loop:; 	lbzx r2, r2, r11; 	stbx r0, r2, r9; 	addi r2, r2, 1; 	bdz later; 	b loop. This could be much better (bdnz instead of bdz) but it still beats us. If we; produced this with bdnz, the loop would be a single dispatch group. ===-------------------------------------------------------------------------===. Lump the constant pool for each function into ONE pic object, and reference; pieces of it as offsets from the start. For functions like this (contrived; to have lots of constants obviously):. double X(double Y) { return (Y*1.23 + 4.512)*2.34 + 14.38; }. We generate:. _X:; lis r2, ha16(.CPI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:2659,Energy Efficiency,reduce,reduces,2659,"s r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-weak addresses so this:. static int a;; void bar(int b) { a = b; }; void foo(unsigned char *c) {; *c = a;; }. So that . _foo:; lis r2, ha16(_a); la r2, lo16(_a)(r2); lbz r2, 3(r2); stb r2, 0(r3); blr. Becomes. _foo:; lis r2, ha16(_a+3); lbz r2, lo16(_a+3)(r2); stb r2, 0(r3); blr. ===-------------------------------------------------------------------------===. We should compile these two functions to the same thing:. #include <stdlib.h>; void f(int a, int b, int *P) {; *P = (a-b)>=0?(a-b):(b-a);; }; void g(int a, int b, int *P) {; *P = abs(a-b);; }. Further, they should compile to something better than:. _g:; subf r2, r4, r3; subfic r3, r2, 0; cmpwi cr0, r2, -1;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:4149,Energy Efficiency,Power,PowerPC,4149,"omes. _foo:; lis r2, ha16(_a+3); lbz r2, lo16(_a+3)(r2); stb r2, 0(r3); blr. ===-------------------------------------------------------------------------===. We should compile these two functions to the same thing:. #include <stdlib.h>; void f(int a, int b, int *P) {; *P = (a-b)>=0?(a-b):(b-a);; }; void g(int a, int b, int *P) {; *P = abs(a-b);; }. Further, they should compile to something better than:. _g:; subf r2, r4, r3; subfic r3, r2, 0; cmpwi cr0, r2, -1; bgt cr0, LBB2_2 ; entry; LBB2_1: ; entry; mr r2, r3; LBB2_2: ; entry; stw r2, 0(r5); blr. GCC produces:. _g:; subf r4,r4,r3; srawi r2,r4,31; xor r0,r2,r4; subf r0,r2,r0; stw r0,0(r5); blr. ... which is much nicer. This theoretically may help improve twolf slightly (used in dimbox.c:142?). ===-------------------------------------------------------------------------===. PR5945: This: ; define i32 @clamp0g(i32 %a) {; entry:; %cmp = icmp slt i32 %a, 0; %sel = select i1 %cmp, i32 0, i32 %a; ret i32 %sel; }. Is compile to this with the PowerPC (32-bit) backend:. _clamp0g:; cmpwi cr0, r3, 0; li r2, 0; blt cr0, LBB1_2; ; %bb.1: ; %entry; mr r2, r3; LBB1_2: ; %entry; mr r3, r2; blr. This could be reduced to the much simpler:. _clamp0g:; srawi r2, r3, 31; andc r3, r3, r2; blr. ===-------------------------------------------------------------------------===. int foo(int N, int ***W, int **TK, int X) {; int t, i;; ; for (t = 0; t < N; ++t); for (i = 0; i < 4; ++i); W[t / X][i][t % X] = TK[i][t];; ; return 5;; }. We generate relatively atrocious code for this loop compared to gcc. We could also strength reduce the rem and the div:; http://www.lcs.mit.edu/pubs/pdf/MIT-LCS-TM-600.pdf. ===-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:4310,Energy Efficiency,reduce,reduced,4310,"mpile these two functions to the same thing:. #include <stdlib.h>; void f(int a, int b, int *P) {; *P = (a-b)>=0?(a-b):(b-a);; }; void g(int a, int b, int *P) {; *P = abs(a-b);; }. Further, they should compile to something better than:. _g:; subf r2, r4, r3; subfic r3, r2, 0; cmpwi cr0, r2, -1; bgt cr0, LBB2_2 ; entry; LBB2_1: ; entry; mr r2, r3; LBB2_2: ; entry; stw r2, 0(r5); blr. GCC produces:. _g:; subf r4,r4,r3; srawi r2,r4,31; xor r0,r2,r4; subf r0,r2,r0; stw r0,0(r5); blr. ... which is much nicer. This theoretically may help improve twolf slightly (used in dimbox.c:142?). ===-------------------------------------------------------------------------===. PR5945: This: ; define i32 @clamp0g(i32 %a) {; entry:; %cmp = icmp slt i32 %a, 0; %sel = select i1 %cmp, i32 0, i32 %a; ret i32 %sel; }. Is compile to this with the PowerPC (32-bit) backend:. _clamp0g:; cmpwi cr0, r3, 0; li r2, 0; blt cr0, LBB1_2; ; %bb.1: ; %entry; mr r2, r3; LBB1_2: ; %entry; mr r3, r2; blr. This could be reduced to the much simpler:. _clamp0g:; srawi r2, r3, 31; andc r3, r3, r2; blr. ===-------------------------------------------------------------------------===. int foo(int N, int ***W, int **TK, int X) {; int t, i;; ; for (t = 0; t < N; ++t); for (i = 0; i < 4; ++i); W[t / X][i][t % X] = TK[i][t];; ; return 5;; }. We generate relatively atrocious code for this loop compared to gcc. We could also strength reduce the rem and the div:; http://www.lcs.mit.edu/pubs/pdf/MIT-LCS-TM-600.pdf. ===-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:4720,Energy Efficiency,reduce,reduce,4720,"r4,r4,r3; srawi r2,r4,31; xor r0,r2,r4; subf r0,r2,r0; stw r0,0(r5); blr. ... which is much nicer. This theoretically may help improve twolf slightly (used in dimbox.c:142?). ===-------------------------------------------------------------------------===. PR5945: This: ; define i32 @clamp0g(i32 %a) {; entry:; %cmp = icmp slt i32 %a, 0; %sel = select i1 %cmp, i32 0, i32 %a; ret i32 %sel; }. Is compile to this with the PowerPC (32-bit) backend:. _clamp0g:; cmpwi cr0, r3, 0; li r2, 0; blt cr0, LBB1_2; ; %bb.1: ; %entry; mr r2, r3; LBB1_2: ; %entry; mr r3, r2; blr. This could be reduced to the much simpler:. _clamp0g:; srawi r2, r3, 31; andc r3, r3, r2; blr. ===-------------------------------------------------------------------------===. int foo(int N, int ***W, int **TK, int X) {; int t, i;; ; for (t = 0; t < N; ++t); for (i = 0; i < 4; ++i); W[t / X][i][t % X] = TK[i][t];; ; return 5;; }. We generate relatively atrocious code for this loop compared to gcc. We could also strength reduce the rem and the div:; http://www.lcs.mit.edu/pubs/pdf/MIT-LCS-TM-600.pdf. ===-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* %b) {; entry:; %tmp = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp = load i32* %tmp ; <uint> [#uses=1]; %tmp3 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp4 = load i32* %tmp3 ; <uint> [#uses=1]; %tmp8 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=2]; %tmp9 = load i32* %tmp8 ; <uint> [#uses=1]; %tmp4.mask17 = shl i32 %tmp4, i8 1 ; <uint> [#uses=1]; %tmp1415 = and i32 %tmp4.mask17, 21",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:10225,Energy Efficiency,efficient,efficient,10225,"nsigned b); { return std::make_pair(a + b, a + b < a); }; bool no_overflow(unsigned a, unsigned b); { return !full_add(a, b).second; }. Should compile to:. __Z11no_overflowjj:; add r4,r3,r4; subfc r3,r3,r4; li r3,0; adde r3,r3,r3; blr. (or better) not:. __Z11no_overflowjj:; add r2, r4, r3; cmplw cr7, r2, r3; mfcr r2; rlwinm r2, r2, 29, 31, 31; xori r3, r2, 1; blr . //===---------------------------------------------------------------------===//. We compile some FP comparisons into an mfcr with two rlwinms and an or. For; example:; #include <math.h>; int test(double x, double y) { return islessequal(x, y);}; int test2(double x, double y) { return islessgreater(x, y);}; int test3(double x, double y) { return !islessequal(x, y);}. Compiles into (all three are similar, but the bits differ):. _test:; 	fcmpu cr7, f1, f2; 	mfcr r2; 	rlwinm r3, r2, 29, 31, 31; 	rlwinm r2, r2, 31, 31, 31; 	or r3, r2, r3; 	blr . GCC compiles this into:. _test:; 	fcmpu cr7,f1,f2; 	cror 30,28,30; 	mfcr r3; 	rlwinm r3,r3,31,1; 	blr; ; which is more efficient and can use mfocr. See PR642 for some more context. //===---------------------------------------------------------------------===//. void foo(float *data, float d) {; long i;; for (i = 0; i < 8000; i++); data[i] = d;; }; void foo2(float *data, float d) {; long i;; data--;; for (i = 0; i < 8000; i++) {; data[1] = d;; data++;; }; }. These compile to:. _foo:; 	li r2, 0; LBB1_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB1_1	; bb; 	blr ; _foo2:; 	li r2, 0; LBB2_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB2_1	; bb; 	blr . The 'mr' could be eliminated to folding the add into the cmp better. //===---------------------------------------------------------------------===//; Codegen for the following (low-probability) case deteriorated considerably ; when the correctness fixes for unordered comparisons went in (PR 642, 58871).; It should be possible to recov",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:14856,Energy Efficiency,schedul,scheduled,14856,"---------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather than doing one save as a unit.; - On Darwin, the save is done after the decrement of SP, which means the offset; from SP of the save slot can be too big for a store instruction, which means we; need an additional register (currently hacked in 96015+96020; the solution there; is correct, but poor).; - On SVR4 the same thing can happen, and I don't think saving before the SP; decrement is safe on that target, as there is no red zone. This is currently; broken AFAIK, although it's not a target I can exercise.; The following demonstrates the problem:; extern void bar(char *p);; void foo() {; char x[100000];; bar(x);; __asm__("""" ::: ""cr2"");; }. //===-------------------------------------------------------------------------===; Naming convention for instruction formats is very haphazard.; We have agreed on a naming scheme as follows:. <INST_form>{_<OP_type><OP_len>}+. Where:; INST_form is the instruction format (X-form, etc.); OP_type is the operand type - one of OPC (opcode), RD (register destination),; RS (register source),; RDp (destination register pair),; RSp (source register pair), IM (immediate),; XO (extended opcode); OP_len is the length of the operand in bits. VSX register operands would be of length 6 (split across two fields),; condition register fields of length 3.; We would not need denote reserved fields in names of instruction formats. //===----------------------------------------------------------------------===//. Instruction fusion was introduced in ISA 2.06 and more opportunities added in; ISA 2.07. LLVM needs to add infrastructure to recognize fusion opportunities; and force instruction pairs to be scheduled together. -----------------------------------------------------------------------------. More general handling of any_extend and zero_extend:. See https://reviews.llvm.org/D24924#555306; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:1933,Modifiability,variab,variable,1933,"	addi r2, r2, 1; 	bdz later; 	b loop. This could be much better (bdnz instead of bdz) but it still beats us. If we; produced this with bdnz, the loop would be a single dispatch group. ===-------------------------------------------------------------------------===. Lump the constant pool for each function into ONE pic object, and reference; pieces of it as offsets from the start. For functions like this (contrived; to have lots of constants obviously):. double X(double Y) { return (Y*1.23 + 4.512)*2.34 + 14.38; }. We generate:. _X:; lis r2, ha16(.CPI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-wea",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:14335,Modifiability,extend,extended,14335,"---------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather than doing one save as a unit.; - On Darwin, the save is done after the decrement of SP, which means the offset; from SP of the save slot can be too big for a store instruction, which means we; need an additional register (currently hacked in 96015+96020; the solution there; is correct, but poor).; - On SVR4 the same thing can happen, and I don't think saving before the SP; decrement is safe on that target, as there is no red zone. This is currently; broken AFAIK, although it's not a target I can exercise.; The following demonstrates the problem:; extern void bar(char *p);; void foo() {; char x[100000];; bar(x);; __asm__("""" ::: ""cr2"");; }. //===-------------------------------------------------------------------------===; Naming convention for instruction formats is very haphazard.; We have agreed on a naming scheme as follows:. <INST_form>{_<OP_type><OP_len>}+. Where:; INST_form is the instruction format (X-form, etc.); OP_type is the operand type - one of OPC (opcode), RD (register destination),; RS (register source),; RDp (destination register pair),; RSp (source register pair), IM (immediate),; XO (extended opcode); OP_len is the length of the operand in bits. VSX register operands would be of length 6 (split across two fields),; condition register fields of length 3.; We would not need denote reserved fields in names of instruction formats. //===----------------------------------------------------------------------===//. Instruction fusion was introduced in ISA 2.06 and more opportunities added in; ISA 2.07. LLVM needs to add infrastructure to recognize fusion opportunities; and force instruction pairs to be scheduled together. -----------------------------------------------------------------------------. More general handling of any_extend and zero_extend:. See https://reviews.llvm.org/D24924#555306; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:114,Performance,load,load,114,"//===- README.txt - Notes for improving PowerPC-specific code gen ---------===//. TODO:; * lmw/stmw pass a la arm load store optimizer for prolog/epilog. ===-------------------------------------------------------------------------===. This code:. unsigned add32carry(unsigned sum, unsigned x) {; unsigned z = sum + x;; if (sum + x < x); z++;; return z;; }. Should compile to something like:. 	addc r3,r3,r4; 	addze r3,r3. instead we get:. 	add r3, r4, r3; 	cmplw cr7, r3, r4; 	mfcr r4 ; 1; 	rlwinm r4, r4, 29, 31, 31; 	add r3, r3, r4. Ick. ===-------------------------------------------------------------------------===. We compile the hottest inner loop of viterbi to:. li r6, 0; b LBB1_84 ;bb432.i; LBB1_83: ;bb420.i; lbzx r8, r5, r7; addi r6, r7, 1; stbx r8, r4, r7; LBB1_84: ;bb432.i; mr r7, r6; cmplwi cr0, r7, 143; bne cr0, LBB1_83 ;bb420.i. The CBE manages to produce:. 	li r0, 143; 	mtctr r0; loop:; 	lbzx r2, r2, r11; 	stbx r0, r2, r9; 	addi r2, r2, 1; 	bdz later; 	b loop. This could be much better (bdnz instead of bdz) but it still beats us. If we; produced this with bdnz, the loop would be a single dispatch group. ===-------------------------------------------------------------------------===. Lump the constant pool for each function into ONE pic object, and reference; pieces of it as offsets from the start. For functions like this (contrived; to have lots of constants obviously):. double X(double Y) { return (Y*1.23 + 4.512)*2.34 + 14.38; }. We generate:. _X:; lis r2, ha16(.CPI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:125,Performance,optimiz,optimizer,125,"//===- README.txt - Notes for improving PowerPC-specific code gen ---------===//. TODO:; * lmw/stmw pass a la arm load store optimizer for prolog/epilog. ===-------------------------------------------------------------------------===. This code:. unsigned add32carry(unsigned sum, unsigned x) {; unsigned z = sum + x;; if (sum + x < x); z++;; return z;; }. Should compile to something like:. 	addc r3,r3,r4; 	addze r3,r3. instead we get:. 	add r3, r4, r3; 	cmplw cr7, r3, r4; 	mfcr r4 ; 1; 	rlwinm r4, r4, 29, 31, 31; 	add r3, r3, r4. Ick. ===-------------------------------------------------------------------------===. We compile the hottest inner loop of viterbi to:. li r6, 0; b LBB1_84 ;bb432.i; LBB1_83: ;bb420.i; lbzx r8, r5, r7; addi r6, r7, 1; stbx r8, r4, r7; LBB1_84: ;bb432.i; mr r7, r6; cmplwi cr0, r7, 143; bne cr0, LBB1_83 ;bb420.i. The CBE manages to produce:. 	li r0, 143; 	mtctr r0; loop:; 	lbzx r2, r2, r11; 	stbx r0, r2, r9; 	addi r2, r2, 1; 	bdz later; 	b loop. This could be much better (bdnz instead of bdz) but it still beats us. If we; produced this with bdnz, the loop would be a single dispatch group. ===-------------------------------------------------------------------------===. Lump the constant pool for each function into ONE pic object, and reference; pieces of it as offsets from the start. For functions like this (contrived; to have lots of constants obviously):. double X(double Y) { return (Y*1.23 + 4.512)*2.34 + 14.38; }. We generate:. _X:; lis r2, ha16(.CPI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:2500,Performance,optimiz,optimization,2500,"PI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-weak addresses so this:. static int a;; void bar(int b) { a = b; }; void foo(unsigned char *c) {; *c = a;; }. So that . _foo:; lis r2, ha16(_a); la r2, lo16(_a)(r2); lbz r2, 3(r2); stb r2, 0(r3); blr. Becomes. _foo:; lis r2, ha16(_a+3); lbz r2, lo16(_a+3)(r2); stb r2, 0(r3); blr. ===-------------------------------------------------------------------------===. We should compile these two functions to the same thing:. #include <stdlib.h>; void f(int a, int b, int *P) {; *P = (a-b)>=0?(a-b):(b-a);; }; void g(int a, int b, int *P) {; *P = abs(a-b);; }. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:5397,Performance,load,load,5397,"--------------------------------------------------------------===. int foo(int N, int ***W, int **TK, int X) {; int t, i;; ; for (t = 0; t < N; ++t); for (i = 0; i < 4; ++i); W[t / X][i][t % X] = TK[i][t];; ; return 5;; }. We generate relatively atrocious code for this loop compared to gcc. We could also strength reduce the rem and the div:; http://www.lcs.mit.edu/pubs/pdf/MIT-LCS-TM-600.pdf. ===-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* %b) {; entry:; %tmp = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp = load i32* %tmp ; <uint> [#uses=1]; %tmp3 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp4 = load i32* %tmp3 ; <uint> [#uses=1]; %tmp8 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=2]; %tmp9 = load i32* %tmp8 ; <uint> [#uses=1]; %tmp4.mask17 = shl i32 %tmp4, i8 1 ; <uint> [#uses=1]; %tmp1415 = and i32 %tmp4.mask17, 2147483648 ; <uint> [#uses=1]; %tmp.masked = and i32 %tmp, 2147483648 ; <uint> [#uses=1]; %tmp11 = or i32 %tmp1415, %tmp.masked ; <uint> [#uses=1]; %tmp12 = and i32 %tmp9, 2147483647 ; <uint> [#uses=1]; %tmp13 = or i32 %tmp12, %tmp11 ; <uint> [#uses=1]; store i32 %tmp13, i32* %tmp8; ret void; }. We emit:. _foo:; lwz r2, 0(r3); slwi r4, r2, 1; or r4, r4, r2; rlwimi r2, r4, 0, 0, 0; stw r2, 0(r3); blr. We could collapse a bunch of those ORs and ANDs and generate the following; equivalent code:. _foo:; lwz r2, 0(r3); rlwinm r4, r2, 1, 0, 0; or r2, r2, r4; stw r2, 0(r3); blr. ===-------------------------------------------------------------------------===. Consider a function ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:5499,Performance,load,load,5499,"int X) {; int t, i;; ; for (t = 0; t < N; ++t); for (i = 0; i < 4; ++i); W[t / X][i][t % X] = TK[i][t];; ; return 5;; }. We generate relatively atrocious code for this loop compared to gcc. We could also strength reduce the rem and the div:; http://www.lcs.mit.edu/pubs/pdf/MIT-LCS-TM-600.pdf. ===-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* %b) {; entry:; %tmp = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp = load i32* %tmp ; <uint> [#uses=1]; %tmp3 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp4 = load i32* %tmp3 ; <uint> [#uses=1]; %tmp8 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=2]; %tmp9 = load i32* %tmp8 ; <uint> [#uses=1]; %tmp4.mask17 = shl i32 %tmp4, i8 1 ; <uint> [#uses=1]; %tmp1415 = and i32 %tmp4.mask17, 2147483648 ; <uint> [#uses=1]; %tmp.masked = and i32 %tmp, 2147483648 ; <uint> [#uses=1]; %tmp11 = or i32 %tmp1415, %tmp.masked ; <uint> [#uses=1]; %tmp12 = and i32 %tmp9, 2147483647 ; <uint> [#uses=1]; %tmp13 = or i32 %tmp12, %tmp11 ; <uint> [#uses=1]; store i32 %tmp13, i32* %tmp8; ret void; }. We emit:. _foo:; lwz r2, 0(r3); slwi r4, r2, 1; or r4, r4, r2; rlwimi r2, r4, 0, 0, 0; stw r2, 0(r3); blr. We could collapse a bunch of those ORs and ANDs and generate the following; equivalent code:. _foo:; lwz r2, 0(r3); rlwinm r4, r2, 1, 0, 0; or r2, r2, r4; stw r2, 0(r3); blr. ===-------------------------------------------------------------------------===. Consider a function like this:. float foo(float X) { return X + 1234.4123f; }. The FP constant ends up in the constant poo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:5602,Performance,load,load,5602,"TK[i][t];; ; return 5;; }. We generate relatively atrocious code for this loop compared to gcc. We could also strength reduce the rem and the div:; http://www.lcs.mit.edu/pubs/pdf/MIT-LCS-TM-600.pdf. ===-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* %b) {; entry:; %tmp = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp = load i32* %tmp ; <uint> [#uses=1]; %tmp3 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp4 = load i32* %tmp3 ; <uint> [#uses=1]; %tmp8 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=2]; %tmp9 = load i32* %tmp8 ; <uint> [#uses=1]; %tmp4.mask17 = shl i32 %tmp4, i8 1 ; <uint> [#uses=1]; %tmp1415 = and i32 %tmp4.mask17, 2147483648 ; <uint> [#uses=1]; %tmp.masked = and i32 %tmp, 2147483648 ; <uint> [#uses=1]; %tmp11 = or i32 %tmp1415, %tmp.masked ; <uint> [#uses=1]; %tmp12 = and i32 %tmp9, 2147483647 ; <uint> [#uses=1]; %tmp13 = or i32 %tmp12, %tmp11 ; <uint> [#uses=1]; store i32 %tmp13, i32* %tmp8; ret void; }. We emit:. _foo:; lwz r2, 0(r3); slwi r4, r2, 1; or r4, r4, r2; rlwimi r2, r4, 0, 0, 0; stw r2, 0(r3); blr. We could collapse a bunch of those ORs and ANDs and generate the following; equivalent code:. _foo:; lwz r2, 0(r3); rlwinm r4, r2, 1, 0, 0; or r2, r2, r4; stw r2, 0(r3); blr. ===-------------------------------------------------------------------------===. Consider a function like this:. float foo(float X) { return X + 1234.4123f; }. The FP constant ends up in the constant pool, so we need to get the LR register.; This ends up producing code like this:. _foo:; .LBB_foo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:7159,Performance,optimiz,optimization,7159,"); blr. We could collapse a bunch of those ORs and ANDs and generate the following; equivalent code:. _foo:; lwz r2, 0(r3); rlwinm r4, r2, 1, 0, 0; or r2, r2, r4; stw r2, 0(r3); blr. ===-------------------------------------------------------------------------===. Consider a function like this:. float foo(float X) { return X + 1234.4123f; }. The FP constant ends up in the constant pool, so we need to get the LR register.; This ends up producing code like this:. _foo:; .LBB_foo_0: ; entry; mflr r11; *** stw r11, 8(r1); bl ""L00000$pb""; ""L00000$pb"":; mflr r2; addis r2, r2, ha16(.CPI_foo_0-""L00000$pb""); lfs f0, lo16(.CPI_foo_0-""L00000$pb"")(r2); fadds f1, f1, f0; *** lwz r11, 8(r1); mtlr r11; blr. This is functional, but there is no reason to spill the LR register all the way; to the stack (the two marked instrs): spilling it to a GPR is quite enough. Implementing this will require some codegen improvements. Nate writes:. ""So basically what we need to support the ""no stack frame save and restore"" is a; generalization of the LR optimization to ""callee-save regs"". Currently, we have LR marked as a callee-save reg. The register allocator sees; that it's callee save, and spills it directly to the stack. Ideally, something like this would happen:. LR would be in a separate register class from the GPRs. The class of LR would be; marked ""unspillable"". When the register allocator came across an unspillable; reg, it would ask ""what is the best class to copy this into that I *can* spill""; If it gets a class back, which it will in this case (the gprs), it grabs a free; register of that class. If it is then later necessary to spill that reg, so be; it. ===-------------------------------------------------------------------------===. We compile this:; int test(_Bool X) {; return X ? 524288 : 0;; }. to: ; _test:; cmplwi cr0, r3, 0; lis r2, 8; li r3, 0; beq cr0, LBB1_2 ;entry; LBB1_1: ;entry; mr r3, r2; LBB1_2: ;entry; blr . instead of:; _test:; addic r2,r3,-1; subfe r0,r2,r3; slwi r3,r0,",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:12185,Performance,load,load,12185,"cribed in the comments. ; RUN: llvm-as < %s | llc -march=ppc32 | grep or | count 3; ; This should produce one 'or' or 'cror' instruction per function. ; RUN: llvm-as < %s | llc -march=ppc32 | grep mfcr | count 3; ; PR2964. define i32 @test(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ole double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test2(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to GPR, then move it VSX register and convert it float. We should ; use vsx scalar integer load instructions to avoid direct moves. //===----------------------------------------------------------------------===//; ; RUN: llvm-as < %s | llc -march=ppc32 | not grep fneg. ; This could generate FSEL with appropriate flags (FSEL is not IEEE-safe, and ; ; should not be generated except with -enable-finite-only-fp-math or the like).; ; With the correctness fixes for PR642 (58871) LowerSELECT_CC would need to; ; recognize a more elaborate tree than a simple SETxx. define double @test_FNEG_sel(double %A, double %B, double %C) {; %D = fsub double -0.000000e+00, %A ; <double> [#uses=1]; %Cond = fcmp ugt double %D, -0.000000e+00 ; <i1> [#uses=1]; %E = select i1 %Cond, double %B, double %C ; <double> [#uses=1]; ret double %E; }. //===----------------------------------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:12286,Performance,load,load,12286,"| count 3; ; This should produce one 'or' or 'cror' instruction per function. ; RUN: llvm-as < %s | llc -march=ppc32 | grep mfcr | count 3; ; PR2964. define i32 @test(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ole double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test2(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to GPR, then move it VSX register and convert it float. We should ; use vsx scalar integer load instructions to avoid direct moves. //===----------------------------------------------------------------------===//; ; RUN: llvm-as < %s | llc -march=ppc32 | not grep fneg. ; This could generate FSEL with appropriate flags (FSEL is not IEEE-safe, and ; ; should not be generated except with -enable-finite-only-fp-math or the like).; ; With the correctness fixes for PR642 (58871) LowerSELECT_CC would need to; ; recognize a more elaborate tree than a simple SETxx. define double @test_FNEG_sel(double %A, double %B, double %C) {; %D = fsub double -0.000000e+00, %A ; <double> [#uses=1]; %Cond = fcmp ugt double %D, -0.000000e+00 ; <i1> [#uses=1]; %E = select i1 %Cond, double %B, double %C ; <double> [#uses=1]; ret double %E; }. //===----------------------------------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather than doing one save as a unit.; - On Darwin, the save is done after the ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:1842,Safety,avoid,avoid,1842,"143; bne cr0, LBB1_83 ;bb420.i. The CBE manages to produce:. 	li r0, 143; 	mtctr r0; loop:; 	lbzx r2, r2, r11; 	stbx r0, r2, r9; 	addi r2, r2, 1; 	bdz later; 	b loop. This could be much better (bdnz instead of bdz) but it still beats us. If we; produced this with bdnz, the loop would be a single dispatch group. ===-------------------------------------------------------------------------===. Lump the constant pool for each function into ONE pic object, and reference; pieces of it as offsets from the start. For functions like this (contrived; to have lots of constants obviously):. double X(double Y) { return (Y*1.23 + 4.512)*2.34 + 14.38; }. We generate:. _X:; lis r2, ha16(.CPI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===----",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:2631,Safety,avoid,avoiding,2631,"s r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-weak addresses so this:. static int a;; void bar(int b) { a = b; }; void foo(unsigned char *c) {; *c = a;; }. So that . _foo:; lis r2, ha16(_a); la r2, lo16(_a)(r2); lbz r2, 3(r2); stb r2, 0(r3); blr. Becomes. _foo:; lis r2, ha16(_a+3); lbz r2, lo16(_a+3)(r2); stb r2, 0(r3); blr. ===-------------------------------------------------------------------------===. We should compile these two functions to the same thing:. #include <stdlib.h>; void f(int a, int b, int *P) {; *P = (a-b)>=0?(a-b):(b-a);; }; void g(int a, int b, int *P) {; *P = abs(a-b);; }. Further, they should compile to something better than:. _g:; subf r2, r4, r3; subfic r3, r2, 0; cmpwi cr0, r2, -1;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:11187,Safety,recover,recover,11187,",1; 	blr; ; which is more efficient and can use mfocr. See PR642 for some more context. //===---------------------------------------------------------------------===//. void foo(float *data, float d) {; long i;; for (i = 0; i < 8000; i++); data[i] = d;; }; void foo2(float *data, float d) {; long i;; data--;; for (i = 0; i < 8000; i++) {; data[1] = d;; data++;; }; }. These compile to:. _foo:; 	li r2, 0; LBB1_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB1_1	; bb; 	blr ; _foo2:; 	li r2, 0; LBB2_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB2_1	; bb; 	blr . The 'mr' could be eliminated to folding the add into the cmp better. //===---------------------------------------------------------------------===//; Codegen for the following (low-probability) case deteriorated considerably ; when the correctness fixes for unordered comparisons went in (PR 642, 58871).; It should be possible to recover the code quality described in the comments. ; RUN: llvm-as < %s | llc -march=ppc32 | grep or | count 3; ; This should produce one 'or' or 'cror' instruction per function. ; RUN: llvm-as < %s | llc -march=ppc32 | grep mfcr | count 3; ; PR2964. define i32 @test(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ole double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test2(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to G",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:12307,Safety,avoid,avoid,12307,"| count 3; ; This should produce one 'or' or 'cror' instruction per function. ; RUN: llvm-as < %s | llc -march=ppc32 | grep mfcr | count 3; ; PR2964. define i32 @test(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ole double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test2(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to GPR, then move it VSX register and convert it float. We should ; use vsx scalar integer load instructions to avoid direct moves. //===----------------------------------------------------------------------===//; ; RUN: llvm-as < %s | llc -march=ppc32 | not grep fneg. ; This could generate FSEL with appropriate flags (FSEL is not IEEE-safe, and ; ; should not be generated except with -enable-finite-only-fp-math or the like).; ; With the correctness fixes for PR642 (58871) LowerSELECT_CC would need to; ; recognize a more elaborate tree than a simple SETxx. define double @test_FNEG_sel(double %A, double %B, double %C) {; %D = fsub double -0.000000e+00, %A ; <double> [#uses=1]; %Cond = fcmp ugt double %D, -0.000000e+00 ; <i1> [#uses=1]; %E = select i1 %Cond, double %B, double %C ; <double> [#uses=1]; ret double %E; }. //===----------------------------------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather than doing one save as a unit.; - On Darwin, the save is done after the ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:12533,Safety,safe,safe,12533,"1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test2(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to GPR, then move it VSX register and convert it float. We should ; use vsx scalar integer load instructions to avoid direct moves. //===----------------------------------------------------------------------===//; ; RUN: llvm-as < %s | llc -march=ppc32 | not grep fneg. ; This could generate FSEL with appropriate flags (FSEL is not IEEE-safe, and ; ; should not be generated except with -enable-finite-only-fp-math or the like).; ; With the correctness fixes for PR642 (58871) LowerSELECT_CC would need to; ; recognize a more elaborate tree than a simple SETxx. define double @test_FNEG_sel(double %A, double %B, double %C) {; %D = fsub double -0.000000e+00, %A ; <double> [#uses=1]; %Cond = fcmp ugt double %D, -0.000000e+00 ; <i1> [#uses=1]; %E = select i1 %Cond, double %B, double %C ; <double> [#uses=1]; ret double %E; }. //===----------------------------------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather than doing one save as a unit.; - On Darwin, the save is done after the decrement of SP, which means the offset; from SP of the save slot can be too big for a store instruction, which means we; need an additional register (currently hacked in 96015+96020; the solution there; is correct, but poor).; - On SVR4 the same thing can",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:13606,Safety,safe,safe,13606," -enable-finite-only-fp-math or the like).; ; With the correctness fixes for PR642 (58871) LowerSELECT_CC would need to; ; recognize a more elaborate tree than a simple SETxx. define double @test_FNEG_sel(double %A, double %B, double %C) {; %D = fsub double -0.000000e+00, %A ; <double> [#uses=1]; %Cond = fcmp ugt double %D, -0.000000e+00 ; <i1> [#uses=1]; %E = select i1 %Cond, double %B, double %C ; <double> [#uses=1]; ret double %E; }. //===----------------------------------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather than doing one save as a unit.; - On Darwin, the save is done after the decrement of SP, which means the offset; from SP of the save slot can be too big for a store instruction, which means we; need an additional register (currently hacked in 96015+96020; the solution there; is correct, but poor).; - On SVR4 the same thing can happen, and I don't think saving before the SP; decrement is safe on that target, as there is no red zone. This is currently; broken AFAIK, although it's not a target I can exercise.; The following demonstrates the problem:; extern void bar(char *p);; void foo() {; char x[100000];; bar(x);; __asm__("""" ::: ""cr2"");; }. //===-------------------------------------------------------------------------===; Naming convention for instruction formats is very haphazard.; We have agreed on a naming scheme as follows:. <INST_form>{_<OP_type><OP_len>}+. Where:; INST_form is the instruction format (X-form, etc.); OP_type is the operand type - one of OPC (opcode), RD (register destination),; RS (register source),; RDp (destination register pair),; RSp (source register pair), IM (immediate),; XO (extended opcode); OP_len is the length of the operand in bits. VSX register operands would be of length 6 (split across two fields),; condition register fields of length 3.; We would not need denote reserved fields in names of instruction formats. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:2644,Security,access,accesses,2644,"s r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-weak addresses so this:. static int a;; void bar(int b) { a = b; }; void foo(unsigned char *c) {; *c = a;; }. So that . _foo:; lis r2, ha16(_a); la r2, lo16(_a)(r2); lbz r2, 3(r2); stb r2, 0(r3); blr. Becomes. _foo:; lis r2, ha16(_a+3); lbz r2, lo16(_a+3)(r2); stb r2, 0(r3); blr. ===-------------------------------------------------------------------------===. We should compile these two functions to the same thing:. #include <stdlib.h>; void f(int a, int b, int *P) {; *P = (a-b)>=0?(a-b):(b-a);; }; void g(int a, int b, int *P) {; *P = abs(a-b);; }. Further, they should compile to something better than:. _g:; subf r2, r4, r3; subfic r3, r2, 0; cmpwi cr0, r2, -1;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:2087,Testability,test,testf,2087,"th bdnz, the loop would be a single dispatch group. ===-------------------------------------------------------------------------===. Lump the constant pool for each function into ONE pic object, and reference; pieces of it as offsets from the start. For functions like this (contrived; to have lots of constants obviously):. double X(double Y) { return (Y*1.23 + 4.512)*2.34 + 14.38; }. We generate:. _X:; lis r2, ha16(.CPI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-weak addresses so this:. static int a;; void bar(int b) { a = b; }; void foo(unsigned char *c) {; *c = a;; }. So that . _foo:; lis r2, ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:7888,Testability,test,test,7888," way; to the stack (the two marked instrs): spilling it to a GPR is quite enough. Implementing this will require some codegen improvements. Nate writes:. ""So basically what we need to support the ""no stack frame save and restore"" is a; generalization of the LR optimization to ""callee-save regs"". Currently, we have LR marked as a callee-save reg. The register allocator sees; that it's callee save, and spills it directly to the stack. Ideally, something like this would happen:. LR would be in a separate register class from the GPRs. The class of LR would be; marked ""unspillable"". When the register allocator came across an unspillable; reg, it would ask ""what is the best class to copy this into that I *can* spill""; If it gets a class back, which it will in this case (the gprs), it grabs a free; register of that class. If it is then later necessary to spill that reg, so be; it. ===-------------------------------------------------------------------------===. We compile this:; int test(_Bool X) {; return X ? 524288 : 0;; }. to: ; _test:; cmplwi cr0, r3, 0; lis r2, 8; li r3, 0; beq cr0, LBB1_2 ;entry; LBB1_1: ;entry; mr r3, r2; LBB1_2: ;entry; blr . instead of:; _test:; addic r2,r3,-1; subfe r0,r2,r3; slwi r3,r0,19; blr. This sort of thing occurs a lot due to globalopt. ===-------------------------------------------------------------------------===. We compile:. define i32 @bar(i32 %x) nounwind readnone ssp {; entry:; %0 = icmp eq i32 %x, 0 ; <i1> [#uses=1]; %neg = sext i1 %0 to i32 ; <i32> [#uses=1]; ret i32 %neg; }. to:. _bar:; 	cntlzw r2, r3; 	slwi r2, r2, 26; 	srawi r3, r2, 31; 	blr . it would be better to produce:. _bar: ; addic r3,r3,-1; subfe r3,r3,r3; blr. ===-------------------------------------------------------------------------===. We generate horrible ppc code for this:. #define N 2000000; double a[N],c[N];; void simpleloop() {; int j;; for (j=0; j<N; j++); c[j] = a[j];; }. LBB1_1: ;bb; lfdx f0, r3, r4; addi r5, r5, 1 ;; Extra IV for the exit value compare.; s",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:9750,Testability,test,test,9750,"3, r4; addi r5, r5, 1 ;; Extra IV for the exit value compare.; stfdx f0, r2, r4; addi r4, r4, 8. xoris r6, r5, 30 ;; This is due to a large immediate.; cmplwi cr0, r6, 33920; bne cr0, LBB1_1. //===---------------------------------------------------------------------===//. This:; #include <algorithm>; inline std::pair<unsigned, bool> full_add(unsigned a, unsigned b); { return std::make_pair(a + b, a + b < a); }; bool no_overflow(unsigned a, unsigned b); { return !full_add(a, b).second; }. Should compile to:. __Z11no_overflowjj:; add r4,r3,r4; subfc r3,r3,r4; li r3,0; adde r3,r3,r3; blr. (or better) not:. __Z11no_overflowjj:; add r2, r4, r3; cmplw cr7, r2, r3; mfcr r2; rlwinm r2, r2, 29, 31, 31; xori r3, r2, 1; blr . //===---------------------------------------------------------------------===//. We compile some FP comparisons into an mfcr with two rlwinms and an or. For; example:; #include <math.h>; int test(double x, double y) { return islessequal(x, y);}; int test2(double x, double y) { return islessgreater(x, y);}; int test3(double x, double y) { return !islessequal(x, y);}. Compiles into (all three are similar, but the bits differ):. _test:; 	fcmpu cr7, f1, f2; 	mfcr r2; 	rlwinm r3, r2, 29, 31, 31; 	rlwinm r2, r2, 31, 31, 31; 	or r3, r2, r3; 	blr . GCC compiles this into:. _test:; 	fcmpu cr7,f1,f2; 	cror 30,28,30; 	mfcr r3; 	rlwinm r3,r3,31,1; 	blr; ; which is more efficient and can use mfocr. See PR642 for some more context. //===---------------------------------------------------------------------===//. void foo(float *data, float d) {; long i;; for (i = 0; i < 8000; i++); data[i] = d;; }; void foo2(float *data, float d) {; long i;; data--;; for (i = 0; i < 8000; i++) {; data[1] = d;; data++;; }; }. These compile to:. _foo:; 	li r2, 0; LBB1_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB1_1	; bb; 	blr ; _foo2:; 	li r2, 0; LBB2_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0,",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:11450,Testability,test,test,11450,"0; i++) {; data[1] = d;; data++;; }; }. These compile to:. _foo:; 	li r2, 0; LBB1_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB1_1	; bb; 	blr ; _foo2:; 	li r2, 0; LBB2_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB2_1	; bb; 	blr . The 'mr' could be eliminated to folding the add into the cmp better. //===---------------------------------------------------------------------===//; Codegen for the following (low-probability) case deteriorated considerably ; when the correctness fixes for unordered comparisons went in (PR 642, 58871).; It should be possible to recover the code quality described in the comments. ; RUN: llvm-as < %s | llc -march=ppc32 | grep or | count 3; ; This should produce one 'or' or 'cror' instruction per function. ; RUN: llvm-as < %s | llc -march=ppc32 | grep mfcr | count 3; ; PR2964. define i32 @test(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ole double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test2(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to GPR, then move it VSX register and convert it float. We should ; use vsx scalar integer load instructions to avoid direct moves. //===----------------------------------------------------------------------===//; ; RUN: llvm-as < %s | llc -march=ppc32 | not grep fneg. ; This could generate FSEL with appropriate flags (FSEL is not I",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:4330,Usability,simpl,simpler,4330,"mpile these two functions to the same thing:. #include <stdlib.h>; void f(int a, int b, int *P) {; *P = (a-b)>=0?(a-b):(b-a);; }; void g(int a, int b, int *P) {; *P = abs(a-b);; }. Further, they should compile to something better than:. _g:; subf r2, r4, r3; subfic r3, r2, 0; cmpwi cr0, r2, -1; bgt cr0, LBB2_2 ; entry; LBB2_1: ; entry; mr r2, r3; LBB2_2: ; entry; stw r2, 0(r5); blr. GCC produces:. _g:; subf r4,r4,r3; srawi r2,r4,31; xor r0,r2,r4; subf r0,r2,r0; stw r0,0(r5); blr. ... which is much nicer. This theoretically may help improve twolf slightly (used in dimbox.c:142?). ===-------------------------------------------------------------------------===. PR5945: This: ; define i32 @clamp0g(i32 %a) {; entry:; %cmp = icmp slt i32 %a, 0; %sel = select i1 %cmp, i32 0, i32 %a; ret i32 %sel; }. Is compile to this with the PowerPC (32-bit) backend:. _clamp0g:; cmpwi cr0, r3, 0; li r2, 0; blt cr0, LBB1_2; ; %bb.1: ; %entry; mr r2, r3; LBB1_2: ; %entry; mr r3, r2; blr. This could be reduced to the much simpler:. _clamp0g:; srawi r2, r3, 31; andc r3, r3, r2; blr. ===-------------------------------------------------------------------------===. int foo(int N, int ***W, int **TK, int X) {; int t, i;; ; for (t = 0; t < N; ++t); for (i = 0; i < 4; ++i); W[t / X][i][t % X] = TK[i][t];; ; return 5;; }. We generate relatively atrocious code for this loop compared to gcc. We could also strength reduce the rem and the div:; http://www.lcs.mit.edu/pubs/pdf/MIT-LCS-TM-600.pdf. ===-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:8749,Usability,simpl,simpleloop,8749," spill that reg, so be; it. ===-------------------------------------------------------------------------===. We compile this:; int test(_Bool X) {; return X ? 524288 : 0;; }. to: ; _test:; cmplwi cr0, r3, 0; lis r2, 8; li r3, 0; beq cr0, LBB1_2 ;entry; LBB1_1: ;entry; mr r3, r2; LBB1_2: ;entry; blr . instead of:; _test:; addic r2,r3,-1; subfe r0,r2,r3; slwi r3,r0,19; blr. This sort of thing occurs a lot due to globalopt. ===-------------------------------------------------------------------------===. We compile:. define i32 @bar(i32 %x) nounwind readnone ssp {; entry:; %0 = icmp eq i32 %x, 0 ; <i1> [#uses=1]; %neg = sext i1 %0 to i32 ; <i32> [#uses=1]; ret i32 %neg; }. to:. _bar:; 	cntlzw r2, r3; 	slwi r2, r2, 26; 	srawi r3, r2, 31; 	blr . it would be better to produce:. _bar: ; addic r3,r3,-1; subfe r3,r3,r3; blr. ===-------------------------------------------------------------------------===. We generate horrible ppc code for this:. #define N 2000000; double a[N],c[N];; void simpleloop() {; int j;; for (j=0; j<N; j++); c[j] = a[j];; }. LBB1_1: ;bb; lfdx f0, r3, r4; addi r5, r5, 1 ;; Extra IV for the exit value compare.; stfdx f0, r2, r4; addi r4, r4, 8. xoris r6, r5, 30 ;; This is due to a large immediate.; cmplwi cr0, r6, 33920; bne cr0, LBB1_1. //===---------------------------------------------------------------------===//. This:; #include <algorithm>; inline std::pair<unsigned, bool> full_add(unsigned a, unsigned b); { return std::make_pair(a + b, a + b < a); }; bool no_overflow(unsigned a, unsigned b); { return !full_add(a, b).second; }. Should compile to:. __Z11no_overflowjj:; add r4,r3,r4; subfc r3,r3,r4; li r3,0; adde r3,r3,r3; blr. (or better) not:. __Z11no_overflowjj:; add r2, r4, r3; cmplw cr7, r2, r3; mfcr r2; rlwinm r2, r2, 29, 31, 31; xori r3, r2, 1; blr . //===---------------------------------------------------------------------===//. We compile some FP comparisons into an mfcr with two rlwinms and an or. For; example:; #include <math.h>; int test(dou",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:12744,Usability,simpl,simple,12744,"fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to GPR, then move it VSX register and convert it float. We should ; use vsx scalar integer load instructions to avoid direct moves. //===----------------------------------------------------------------------===//; ; RUN: llvm-as < %s | llc -march=ppc32 | not grep fneg. ; This could generate FSEL with appropriate flags (FSEL is not IEEE-safe, and ; ; should not be generated except with -enable-finite-only-fp-math or the like).; ; With the correctness fixes for PR642 (58871) LowerSELECT_CC would need to; ; recognize a more elaborate tree than a simple SETxx. define double @test_FNEG_sel(double %A, double %B, double %C) {; %D = fsub double -0.000000e+00, %A ; <double> [#uses=1]; %Cond = fcmp ugt double %D, -0.000000e+00 ; <i1> [#uses=1]; %E = select i1 %Cond, double %B, double %C ; <double> [#uses=1]; ret double %E; }. //===----------------------------------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather than doing one save as a unit.; - On Darwin, the save is done after the decrement of SP, which means the offset; from SP of the save slot can be too big for a store instruction, which means we; need an additional register (currently hacked in 96015+96020; the solution there; is correct, but poor).; - On SVR4 the same thing can happen, and I don't think saving before the SP; decrement is safe on that target, as there is no red zone. This is currently; broken AFAIK, althou",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:5323,Availability,mask,mask,5323,"the same peephole; applied to it as a predicate compare used by a br_cc. There should be no; mfcr here:. _foo:; mfspr r2, 256; oris r5, r2, 12288; mtspr 256, r5; li r5, 12; li r6, 3245; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp. v2, v3, v2; mfcr r3, 2; rlwinm r3, r3, 25, 31, 31; cmpwi cr0, r3, 0; bne cr0, LBB1_2 ; entry; LBB1_1: ; entry; mr r6, r5; LBB1_2: ; entry; mr r3, r6; mtspr 256, r2; blr. //===----------------------------------------------------------------------===//. CodeGen/PowerPC/vec_constants.ll has an and operation that should be; codegen'd to andc. The issue is that the 'all ones' build vector is; SelectNodeTo'd a VSPLTISB instruction node before the and/xor is selected; which prevents the vnot pattern from matching. //===----------------------------------------------------------------------===//. An alternative to the store/store/load approach for illegal insert element ; lowering would be:. 1. store element to any ol' slot; 2. lvx the slot; 3. lvsl 0; splat index; vcmpeq to generate a select mask; 4. lvsl slot + x; vperm to rotate result into correct slot; 5. vsel result together. //===----------------------------------------------------------------------===//. Should codegen branches on vec_any/vec_all to avoid mfcr. Two examples:. #include <altivec.h>; int f(vector float a, vector float b); {; int aa = 0;; if (vec_all_ge(a, b)); aa |= 0x1;; if (vec_any_ge(a,b)); aa |= 0x2;; return aa;; }. vector float f(vector float a, vector float b) { ; if (vec_any_eq(a, b)) ; return a; ; else ; return b; ; }. //===----------------------------------------------------------------------===//. We should do a little better with eliminating dead stores.; The stores to the stack are dead since %a and %b are not needed. ; Function Attrs: nounwind; define <16 x i8> @test_vpmsumb() #0 {; entry:; %a = alloca <16 x i8>, align 16; %b = alloca <16 x i8>, align 16; store <16 x i8> <i8 1, i8 2, i8 3, i8 4, i8 5, i8 6, i8 7, i8 8, i8 9, i8 10, i8 11, i8 12, i8 13, i8 14, i8 15, i8",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:10169,Availability,avail,available,10169,"nce for implementing a shufflevector operation on; PowerPC. However, this was designed for big-endian code generation. We could; modify this program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===----------------------------------------------------------------------===//. Opportunies to use instructions from PPCInstrVSX.td during code gen; - Conversion instructions (Sections 7.6.1.5 and 7.6.1.6 of ISA 2.07); - Scalar comparisons (xscmpodp and xscmpudp); - Min and max (xsmaxdp, xsmindp, xvmaxdp, xvmindp, xvmaxsp, xvminsp). Related to this: we currently do not generate the lxvw4x instruction for either; v4f32 or v4i32, probably because adding a dag pattern to the recognizer requires; a single target type. This should probably be addressed in the PPCISelDAGToDAG logic. //===----------------------------------------------------------------------===//. Currently EXTRACT_VECTOR_ELT and INSERT_VECTOR_ELT are type-legal only; for v2f64 with VSX available. We should create custom lowering; support for the other vector types. Without this support, we generate; sequences with load-hit-store hazards. v4f32 can be supported with VSX by shifting the correct element into; big-endian lane 0, using xscvspdpn to produce a double-precision; representation of the single-precision value in big-endian; double-precision lane 0, and reinterpreting lane 0 as an FPR or; vector-scalar register. v2i64 can be supported with VSX and P8Vector in the same manner as; v2f64, followed by a direct move to a GPR. v4i32 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 1, using a direct move to a GPR, and; sign-extending the 32-bit result to 64 bits. v8i16 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 3, using a direct move to a GPR, and; sign-extending the 16-bit result to 64 bits. v16i8 can be supported with VSX and P8Vector",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:2613,Energy Efficiency,schedul,scheduler,2613,"------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===----------------------------------------------------------------------===//. If we want to tie instruction selection into the scheduler, we can do some; constant formation with different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-pass scheduler. //===----------------------------------------------------------------------===//. For this function:. void test(vector float *A, vector float *B) {; vector float C = (vector float)vec_cmpeq(*A, *B);; if (!vec_any_eq(*A, *B)); *B = (vector float){0,0,0,0};; *A = C;; }. we get the following basic block:. 	...; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp v4, v3, v2; vcmpeqfp. v2, v3, v2; bne cr6, LBB1_2 ; cond_next. The vcmpeqfp/vcmpeqfp. instructions currently cannot be merged when the; vcmpeqfp. result is used by a branch. This can be improved. //===----------------------------------------------------------------------===//. The code generated for this is truly awefu",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:2884,Energy Efficiency,schedul,scheduling,2884,"aint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===----------------------------------------------------------------------===//. If we want to tie instruction selection into the scheduler, we can do some; constant formation with different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-pass scheduler. //===----------------------------------------------------------------------===//. For this function:. void test(vector float *A, vector float *B) {; vector float C = (vector float)vec_cmpeq(*A, *B);; if (!vec_any_eq(*A, *B)); *B = (vector float){0,0,0,0};; *A = C;; }. we get the following basic block:. 	...; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp v4, v3, v2; vcmpeqfp. v2, v3, v2; bne cr6, LBB1_2 ; cond_next. The vcmpeqfp/vcmpeqfp. instructions currently cannot be merged when the; vcmpeqfp. result is used by a branch. This can be improved. //===----------------------------------------------------------------------===//. The code generated for this is truly aweful:. vector float test(float a, float b) {; return (vector float){ 0.0, a, 0.0, 0.0}; ; }. LCPI1_0: ; float; .space 4; .text; .globl _test; .align 4; _test:; mfspr r2,",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:2945,Energy Efficiency,schedul,scheduler,2945,"re of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===----------------------------------------------------------------------===//. If we want to tie instruction selection into the scheduler, we can do some; constant formation with different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-pass scheduler. //===----------------------------------------------------------------------===//. For this function:. void test(vector float *A, vector float *B) {; vector float C = (vector float)vec_cmpeq(*A, *B);; if (!vec_any_eq(*A, *B)); *B = (vector float){0,0,0,0};; *A = C;; }. we get the following basic block:. 	...; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp v4, v3, v2; vcmpeqfp. v2, v3, v2; bne cr6, LBB1_2 ; cond_next. The vcmpeqfp/vcmpeqfp. instructions currently cannot be merged when the; vcmpeqfp. result is used by a branch. This can be improved. //===----------------------------------------------------------------------===//. The code generated for this is truly aweful:. vector float test(float a, float b) {; return (vector float){ 0.0, a, 0.0, 0.0}; ; }. LCPI1_0: ; float; .space 4; .text; .globl _test; .align 4; _test:; mfspr r2, 256; oris r3, r2, 4096; mtspr 256, r3; lis r3, ha16(LCPI1_0); addi r4, r1, -32; stfs f1, -16(r1); addi r5, r1, -16; lfs f0, lo16(LCPI",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:4788,Energy Efficiency,Power,PowerPC,4788,"6; oris r3, r2, 4096; mtspr 256, r3; lis r3, ha16(LCPI1_0); addi r4, r1, -32; stfs f1, -16(r1); addi r5, r1, -16; lfs f0, lo16(LCPI1_0)(r3); stfs f0, -32(r1); lvx v2, 0, r4; lvx v3, 0, r5; vmrghw v3, v3, v2; vspltw v2, v2, 0; vmrghw v2, v2, v3; mtspr 256, r2; blr. //===----------------------------------------------------------------------===//. int foo(vector float *x, vector float *y) {; if (vec_all_eq(*x,*y)) return 3245; ; else return 12;; }. A predicate compare being used in a select_cc should have the same peephole; applied to it as a predicate compare used by a br_cc. There should be no; mfcr here:. _foo:; mfspr r2, 256; oris r5, r2, 12288; mtspr 256, r5; li r5, 12; li r6, 3245; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp. v2, v3, v2; mfcr r3, 2; rlwinm r3, r3, 25, 31, 31; cmpwi cr0, r3, 0; bne cr0, LBB1_2 ; entry; LBB1_1: ; entry; mr r6, r5; LBB1_2: ; entry; mr r3, r6; mtspr 256, r2; blr. //===----------------------------------------------------------------------===//. CodeGen/PowerPC/vec_constants.ll has an and operation that should be; codegen'd to andc. The issue is that the 'all ones' build vector is; SelectNodeTo'd a VSPLTISB instruction node before the and/xor is selected; which prevents the vnot pattern from matching. //===----------------------------------------------------------------------===//. An alternative to the store/store/load approach for illegal insert element ; lowering would be:. 1. store element to any ol' slot; 2. lvx the slot; 3. lvsl 0; splat index; vcmpeq to generate a select mask; 4. lvsl slot + x; vperm to rotate result into correct slot; 5. vsel result together. //===----------------------------------------------------------------------===//. Should codegen branches on vec_any/vec_all to avoid mfcr. Two examples:. #include <altivec.h>; int f(vector float a, vector float b); {; int aa = 0;; if (vec_all_ge(a, b)); aa |= 0x1;; if (vec_any_ge(a,b)); aa |= 0x2;; return aa;; }. vector float f(vector float a, vector float b) { ; if (vec_any_eq",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:7426,Energy Efficiency,Power,PowerPC,7426,"121, i8 122, i8 123, i8 124, i8 125, i8 126, i8 127, i8 112>, <16 x i8>* %b, align 16; %0 = load <16 x i8>* %a, align 16; %1 = load <16 x i8>* %b, align 16; %2 = call <16 x i8> @llvm.ppc.altivec.crypto.vpmsumb(<16 x i8> %0, <16 x i8> %1); ret <16 x i8> %2; }. ; Function Attrs: nounwind readnone; declare <16 x i8> @llvm.ppc.altivec.crypto.vpmsumb(<16 x i8>, <16 x i8>) #1. Produces the following code with -mtriple=powerpc64-unknown-linux-gnu:; # %bb.0: # %entry; addis 3, 2, .LCPI0_0@toc@ha; addis 4, 2, .LCPI0_1@toc@ha; addi 3, 3, .LCPI0_0@toc@l; addi 4, 4, .LCPI0_1@toc@l; lxvw4x 0, 0, 3; addi 3, 1, -16; lxvw4x 35, 0, 4; stxvw4x 0, 0, 3; ori 2, 2, 0; lxvw4x 34, 0, 3; addi 3, 1, -32; stxvw4x 35, 0, 3; vpmsumb 2, 2, 3; blr; .long 0; .quad 0. The two stxvw4x instructions are not needed.; With -mtriple=powerpc64le-unknown-linux-gnu, the associated permutes; are present too. //===----------------------------------------------------------------------===//. The following example is found in test/CodeGen/PowerPC/vec_add_sub_doubleword.ll:. define <2 x i64> @increment_by_val(<2 x i64> %x, i64 %val) nounwind {; %tmpvec = insertelement <2 x i64> <i64 0, i64 0>, i64 %val, i32 0; %tmpvec2 = insertelement <2 x i64> %tmpvec, i64 %val, i32 1; %result = add <2 x i64> %x, %tmpvec2; ret <2 x i64> %result. This will generate the following instruction sequence:; std 5, -8(1); std 5, -16(1); addi 3, 1, -16; ori 2, 2, 0; lxvd2x 35, 0, 3; vaddudm 2, 2, 3; blr. This will almost certainly cause a load-hit-store hazard. ; Since val is a value parameter, it should not need to be saved onto; the stack, unless it's being done set up the vector register. Instead,; it would be better to splat the value into a vector register, and then; remove the (dead) stores to the stack. //===----------------------------------------------------------------------===//. At the moment we always generate a lxsdx in preference to lfd, or stxsdx in; preference to stfd. When we have a reg-immediate addressing mode, this i",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:9179,Energy Efficiency,Power,PowerPC,9179,"the value into a vector register, and then; remove the (dead) stores to the stack. //===----------------------------------------------------------------------===//. At the moment we always generate a lxsdx in preference to lfd, or stxsdx in; preference to stfd. When we have a reg-immediate addressing mode, this is a; poor choice, since we have to load the address into an index register. This; should be fixed for P7/P8. . //===----------------------------------------------------------------------===//. Right now, ShuffleKind 0 is supported only on BE, and ShuffleKind 2 only on LE.; However, we could actually support both kinds on either endianness, if we check; for the appropriate shufflevector pattern for each case ... this would cause; some additional shufflevectors to be recognized and implemented via the; ""swapped"" form. //===----------------------------------------------------------------------===//. There is a utility program called PerfectShuffle that generates a table of the; shortest instruction sequence for implementing a shufflevector operation on; PowerPC. However, this was designed for big-endian code generation. We could; modify this program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===----------------------------------------------------------------------===//. Opportunies to use instructions from PPCInstrVSX.td during code gen; - Conversion instructions (Sections 7.6.1.5 and 7.6.1.6 of ISA 2.07); - Scalar comparisons (xscmpodp and xscmpudp); - Min and max (xsmaxdp, xsmindp, xvmaxdp, xvmindp, xvmaxsp, xvminsp). Related to this: we currently do not generate the lxvw4x instruction for either; v4f32 or v4i32, probably because adding a dag pattern to the recognizer requires; a single target type. This should probably be addressed in the PPCISelDAGToDAG logic. //===----------------------------------------------------------------------===//. Currently EXTRACT_VECTOR_E",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:1486,Modifiability,Variab,Variable,1486,"id) {; int x[8] __attribute__((aligned(128)));; memset (x, 0, sizeof (x));; bar (x);; }. //===----------------------------------------------------------------------===//. Altivec: Codegen'ing MUL with vector FMADD should add -0.0, not 0.0:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=8763. When -ffast-math is on, we can use 0.0. //===----------------------------------------------------------------------===//. Consider this:; v4f32 Vector;; v4f32 Vector2 = { Vector.X, Vector.X, Vector.X, Vector.X };. Since we know that ""Vector"" is 16-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===---------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:1564,Modifiability,Variab,Variable,1564,"id) {; int x[8] __attribute__((aligned(128)));; memset (x, 0, sizeof (x));; bar (x);; }. //===----------------------------------------------------------------------===//. Altivec: Codegen'ing MUL with vector FMADD should add -0.0, not 0.0:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=8763. When -ffast-math is on, we can use 0.0. //===----------------------------------------------------------------------===//. Consider this:; v4f32 Vector;; v4f32 Vector2 = { Vector.X, Vector.X, Vector.X, Vector.X };. Since we know that ""Vector"" is 16-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===---------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:10862,Modifiability,extend,extending,10862,"is program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===----------------------------------------------------------------------===//. Opportunies to use instructions from PPCInstrVSX.td during code gen; - Conversion instructions (Sections 7.6.1.5 and 7.6.1.6 of ISA 2.07); - Scalar comparisons (xscmpodp and xscmpudp); - Min and max (xsmaxdp, xsmindp, xvmaxdp, xvmindp, xvmaxsp, xvminsp). Related to this: we currently do not generate the lxvw4x instruction for either; v4f32 or v4i32, probably because adding a dag pattern to the recognizer requires; a single target type. This should probably be addressed in the PPCISelDAGToDAG logic. //===----------------------------------------------------------------------===//. Currently EXTRACT_VECTOR_ELT and INSERT_VECTOR_ELT are type-legal only; for v2f64 with VSX available. We should create custom lowering; support for the other vector types. Without this support, we generate; sequences with load-hit-store hazards. v4f32 can be supported with VSX by shifting the correct element into; big-endian lane 0, using xscvspdpn to produce a double-precision; representation of the single-precision value in big-endian; double-precision lane 0, and reinterpreting lane 0 as an FPR or; vector-scalar register. v2i64 can be supported with VSX and P8Vector in the same manner as; v2f64, followed by a direct move to a GPR. v4i32 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 1, using a direct move to a GPR, and; sign-extending the 32-bit result to 64 bits. v8i16 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 3, using a direct move to a GPR, and; sign-extending the 16-bit result to 64 bits. v16i8 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 7, using a direct move to a GPR, and; sign-extending the 8-bit result to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:11044,Modifiability,extend,extending,11044,"is program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===----------------------------------------------------------------------===//. Opportunies to use instructions from PPCInstrVSX.td during code gen; - Conversion instructions (Sections 7.6.1.5 and 7.6.1.6 of ISA 2.07); - Scalar comparisons (xscmpodp and xscmpudp); - Min and max (xsmaxdp, xsmindp, xvmaxdp, xvmindp, xvmaxsp, xvminsp). Related to this: we currently do not generate the lxvw4x instruction for either; v4f32 or v4i32, probably because adding a dag pattern to the recognizer requires; a single target type. This should probably be addressed in the PPCISelDAGToDAG logic. //===----------------------------------------------------------------------===//. Currently EXTRACT_VECTOR_ELT and INSERT_VECTOR_ELT are type-legal only; for v2f64 with VSX available. We should create custom lowering; support for the other vector types. Without this support, we generate; sequences with load-hit-store hazards. v4f32 can be supported with VSX by shifting the correct element into; big-endian lane 0, using xscvspdpn to produce a double-precision; representation of the single-precision value in big-endian; double-precision lane 0, and reinterpreting lane 0 as an FPR or; vector-scalar register. v2i64 can be supported with VSX and P8Vector in the same manner as; v2f64, followed by a direct move to a GPR. v4i32 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 1, using a direct move to a GPR, and; sign-extending the 32-bit result to 64 bits. v8i16 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 3, using a direct move to a GPR, and; sign-extending the 16-bit result to 64 bits. v16i8 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 7, using a direct move to a GPR, and; sign-extending the 8-bit result to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:11226,Modifiability,extend,extending,11226,"is program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===----------------------------------------------------------------------===//. Opportunies to use instructions from PPCInstrVSX.td during code gen; - Conversion instructions (Sections 7.6.1.5 and 7.6.1.6 of ISA 2.07); - Scalar comparisons (xscmpodp and xscmpudp); - Min and max (xsmaxdp, xsmindp, xvmaxdp, xvmindp, xvmaxsp, xvminsp). Related to this: we currently do not generate the lxvw4x instruction for either; v4f32 or v4i32, probably because adding a dag pattern to the recognizer requires; a single target type. This should probably be addressed in the PPCISelDAGToDAG logic. //===----------------------------------------------------------------------===//. Currently EXTRACT_VECTOR_ELT and INSERT_VECTOR_ELT are type-legal only; for v2f64 with VSX available. We should create custom lowering; support for the other vector types. Without this support, we generate; sequences with load-hit-store hazards. v4f32 can be supported with VSX by shifting the correct element into; big-endian lane 0, using xscvspdpn to produce a double-precision; representation of the single-precision value in big-endian; double-precision lane 0, and reinterpreting lane 0 as an FPR or; vector-scalar register. v2i64 can be supported with VSX and P8Vector in the same manner as; v2f64, followed by a direct move to a GPR. v4i32 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 1, using a direct move to a GPR, and; sign-extending the 32-bit result to 64 bits. v8i16 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 3, using a direct move to a GPR, and; sign-extending the 16-bit result to 64 bits. v16i8 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 7, using a direct move to a GPR, and; sign-extending the 8-bit result to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:1123,Performance,load,load,1123,"vector; registers, to generate better spill code. //===----------------------------------------------------------------------===//. The first should be a single lvx from the constant pool, the second should be ; a xor/stvx:. void foo(void) {; int x[8] __attribute__((aligned(128))) = { 1, 1, 1, 17, 1, 1, 1, 1 };; bar (x);; }. #include <string.h>; void foo(void) {; int x[8] __attribute__((aligned(128)));; memset (x, 0, sizeof (x));; bar (x);; }. //===----------------------------------------------------------------------===//. Altivec: Codegen'ing MUL with vector FMADD should add -0.0, not 0.0:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=8763. When -ffast-math is on, we can use 0.0. //===----------------------------------------------------------------------===//. Consider this:; v4f32 Vector;; v4f32 Vector2 = { Vector.X, Vector.X, Vector.X, Vector.X };. Since we know that ""Vector"" is 16-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //=",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:1174,Performance,load,load,1174,"vector; registers, to generate better spill code. //===----------------------------------------------------------------------===//. The first should be a single lvx from the constant pool, the second should be ; a xor/stvx:. void foo(void) {; int x[8] __attribute__((aligned(128))) = { 1, 1, 1, 17, 1, 1, 1, 1 };; bar (x);; }. #include <string.h>; void foo(void) {; int x[8] __attribute__((aligned(128)));; memset (x, 0, sizeof (x));; bar (x);; }. //===----------------------------------------------------------------------===//. Altivec: Codegen'ing MUL with vector FMADD should add -0.0, not 0.0:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=8763. When -ffast-math is on, we can use 0.0. //===----------------------------------------------------------------------===//. Consider this:; v4f32 Vector;; v4f32 Vector2 = { Vector.X, Vector.X, Vector.X, Vector.X };. Since we know that ""Vector"" is 16-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //=",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:1520,Performance,load,load,1520,"id) {; int x[8] __attribute__((aligned(128)));; memset (x, 0, sizeof (x));; bar (x);; }. //===----------------------------------------------------------------------===//. Altivec: Codegen'ing MUL with vector FMADD should add -0.0, not 0.0:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=8763. When -ffast-math is on, we can use 0.0. //===----------------------------------------------------------------------===//. Consider this:; v4f32 Vector;; v4f32 Vector2 = { Vector.X, Vector.X, Vector.X, Vector.X };. Since we know that ""Vector"" is 16-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===---------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:1546,Performance,load,load,1546,"id) {; int x[8] __attribute__((aligned(128)));; memset (x, 0, sizeof (x));; bar (x);; }. //===----------------------------------------------------------------------===//. Altivec: Codegen'ing MUL with vector FMADD should add -0.0, not 0.0:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=8763. When -ffast-math is on, we can use 0.0. //===----------------------------------------------------------------------===//. Consider this:; v4f32 Vector;; v4f32 Vector2 = { Vector.X, Vector.X, Vector.X, Vector.X };. Since we know that ""Vector"" is 16-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===---------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:1990,Performance,load,load,1990,"f32 Vector;; v4f32 Vector2 = { Vector.X, Vector.X, Vector.X, Vector.X };. Since we know that ""Vector"" is 16-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===----------------------------------------------------------------------===//. If we want to tie instruction selection into the scheduler, we can do some; constant formation with different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:2083,Performance,load,load,2083,"-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===----------------------------------------------------------------------===//. If we want to tie instruction selection into the scheduler, we can do some; constant formation with different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-pass scheduler. //===----------------------------------------------------------------------===//. For this",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:5157,Performance,load,load,5157,"or float *y) {; if (vec_all_eq(*x,*y)) return 3245; ; else return 12;; }. A predicate compare being used in a select_cc should have the same peephole; applied to it as a predicate compare used by a br_cc. There should be no; mfcr here:. _foo:; mfspr r2, 256; oris r5, r2, 12288; mtspr 256, r5; li r5, 12; li r6, 3245; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp. v2, v3, v2; mfcr r3, 2; rlwinm r3, r3, 25, 31, 31; cmpwi cr0, r3, 0; bne cr0, LBB1_2 ; entry; LBB1_1: ; entry; mr r6, r5; LBB1_2: ; entry; mr r3, r6; mtspr 256, r2; blr. //===----------------------------------------------------------------------===//. CodeGen/PowerPC/vec_constants.ll has an and operation that should be; codegen'd to andc. The issue is that the 'all ones' build vector is; SelectNodeTo'd a VSPLTISB instruction node before the and/xor is selected; which prevents the vnot pattern from matching. //===----------------------------------------------------------------------===//. An alternative to the store/store/load approach for illegal insert element ; lowering would be:. 1. store element to any ol' slot; 2. lvx the slot; 3. lvsl 0; splat index; vcmpeq to generate a select mask; 4. lvsl slot + x; vperm to rotate result into correct slot; 5. vsel result together. //===----------------------------------------------------------------------===//. Should codegen branches on vec_any/vec_all to avoid mfcr. Two examples:. #include <altivec.h>; int f(vector float a, vector float b); {; int aa = 0;; if (vec_all_ge(a, b)); aa |= 0x1;; if (vec_any_ge(a,b)); aa |= 0x2;; return aa;; }. vector float f(vector float a, vector float b) { ; if (vec_any_eq(a, b)) ; return a; ; else ; return b; ; }. //===----------------------------------------------------------------------===//. We should do a little better with eliminating dead stores.; The stores to the stack are dead since %a and %b are not needed. ; Function Attrs: nounwind; define <16 x i8> @test_vpmsumb() #0 {; entry:; %a = alloca <16 x i8>, align 16; %b = alloca <16 x",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:6509,Performance,load,load,6509," mask; 4. lvsl slot + x; vperm to rotate result into correct slot; 5. vsel result together. //===----------------------------------------------------------------------===//. Should codegen branches on vec_any/vec_all to avoid mfcr. Two examples:. #include <altivec.h>; int f(vector float a, vector float b); {; int aa = 0;; if (vec_all_ge(a, b)); aa |= 0x1;; if (vec_any_ge(a,b)); aa |= 0x2;; return aa;; }. vector float f(vector float a, vector float b) { ; if (vec_any_eq(a, b)) ; return a; ; else ; return b; ; }. //===----------------------------------------------------------------------===//. We should do a little better with eliminating dead stores.; The stores to the stack are dead since %a and %b are not needed. ; Function Attrs: nounwind; define <16 x i8> @test_vpmsumb() #0 {; entry:; %a = alloca <16 x i8>, align 16; %b = alloca <16 x i8>, align 16; store <16 x i8> <i8 1, i8 2, i8 3, i8 4, i8 5, i8 6, i8 7, i8 8, i8 9, i8 10, i8 11, i8 12, i8 13, i8 14, i8 15, i8 16>, <16 x i8>* %a, align 16; store <16 x i8> <i8 113, i8 114, i8 115, i8 116, i8 117, i8 118, i8 119, i8 120, i8 121, i8 122, i8 123, i8 124, i8 125, i8 126, i8 127, i8 112>, <16 x i8>* %b, align 16; %0 = load <16 x i8>* %a, align 16; %1 = load <16 x i8>* %b, align 16; %2 = call <16 x i8> @llvm.ppc.altivec.crypto.vpmsumb(<16 x i8> %0, <16 x i8> %1); ret <16 x i8> %2; }. ; Function Attrs: nounwind readnone; declare <16 x i8> @llvm.ppc.altivec.crypto.vpmsumb(<16 x i8>, <16 x i8>) #1. Produces the following code with -mtriple=powerpc64-unknown-linux-gnu:; # %bb.0: # %entry; addis 3, 2, .LCPI0_0@toc@ha; addis 4, 2, .LCPI0_1@toc@ha; addi 3, 3, .LCPI0_0@toc@l; addi 4, 4, .LCPI0_1@toc@l; lxvw4x 0, 0, 3; addi 3, 1, -16; lxvw4x 35, 0, 4; stxvw4x 0, 0, 3; ori 2, 2, 0; lxvw4x 34, 0, 3; addi 3, 1, -32; stxvw4x 35, 0, 3; vpmsumb 2, 2, 3; blr; .long 0; .quad 0. The two stxvw4x instructions are not needed.; With -mtriple=powerpc64le-unknown-linux-gnu, the associated permutes; are present too. //===---------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:6544,Performance,load,load,6544," mask; 4. lvsl slot + x; vperm to rotate result into correct slot; 5. vsel result together. //===----------------------------------------------------------------------===//. Should codegen branches on vec_any/vec_all to avoid mfcr. Two examples:. #include <altivec.h>; int f(vector float a, vector float b); {; int aa = 0;; if (vec_all_ge(a, b)); aa |= 0x1;; if (vec_any_ge(a,b)); aa |= 0x2;; return aa;; }. vector float f(vector float a, vector float b) { ; if (vec_any_eq(a, b)) ; return a; ; else ; return b; ; }. //===----------------------------------------------------------------------===//. We should do a little better with eliminating dead stores.; The stores to the stack are dead since %a and %b are not needed. ; Function Attrs: nounwind; define <16 x i8> @test_vpmsumb() #0 {; entry:; %a = alloca <16 x i8>, align 16; %b = alloca <16 x i8>, align 16; store <16 x i8> <i8 1, i8 2, i8 3, i8 4, i8 5, i8 6, i8 7, i8 8, i8 9, i8 10, i8 11, i8 12, i8 13, i8 14, i8 15, i8 16>, <16 x i8>* %a, align 16; store <16 x i8> <i8 113, i8 114, i8 115, i8 116, i8 117, i8 118, i8 119, i8 120, i8 121, i8 122, i8 123, i8 124, i8 125, i8 126, i8 127, i8 112>, <16 x i8>* %b, align 16; %0 = load <16 x i8>* %a, align 16; %1 = load <16 x i8>* %b, align 16; %2 = call <16 x i8> @llvm.ppc.altivec.crypto.vpmsumb(<16 x i8> %0, <16 x i8> %1); ret <16 x i8> %2; }. ; Function Attrs: nounwind readnone; declare <16 x i8> @llvm.ppc.altivec.crypto.vpmsumb(<16 x i8>, <16 x i8>) #1. Produces the following code with -mtriple=powerpc64-unknown-linux-gnu:; # %bb.0: # %entry; addis 3, 2, .LCPI0_0@toc@ha; addis 4, 2, .LCPI0_1@toc@ha; addi 3, 3, .LCPI0_0@toc@l; addi 4, 4, .LCPI0_1@toc@l; lxvw4x 0, 0, 3; addi 3, 1, -16; lxvw4x 35, 0, 4; stxvw4x 0, 0, 3; ori 2, 2, 0; lxvw4x 34, 0, 3; addi 3, 1, -32; stxvw4x 35, 0, 3; vpmsumb 2, 2, 3; blr; .long 0; .quad 0. The two stxvw4x instructions are not needed.; With -mtriple=powerpc64le-unknown-linux-gnu, the associated permutes; are present too. //===---------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:7910,Performance,load,load-hit-store,7910,"toc@ha; addis 4, 2, .LCPI0_1@toc@ha; addi 3, 3, .LCPI0_0@toc@l; addi 4, 4, .LCPI0_1@toc@l; lxvw4x 0, 0, 3; addi 3, 1, -16; lxvw4x 35, 0, 4; stxvw4x 0, 0, 3; ori 2, 2, 0; lxvw4x 34, 0, 3; addi 3, 1, -32; stxvw4x 35, 0, 3; vpmsumb 2, 2, 3; blr; .long 0; .quad 0. The two stxvw4x instructions are not needed.; With -mtriple=powerpc64le-unknown-linux-gnu, the associated permutes; are present too. //===----------------------------------------------------------------------===//. The following example is found in test/CodeGen/PowerPC/vec_add_sub_doubleword.ll:. define <2 x i64> @increment_by_val(<2 x i64> %x, i64 %val) nounwind {; %tmpvec = insertelement <2 x i64> <i64 0, i64 0>, i64 %val, i32 0; %tmpvec2 = insertelement <2 x i64> %tmpvec, i64 %val, i32 1; %result = add <2 x i64> %x, %tmpvec2; ret <2 x i64> %result. This will generate the following instruction sequence:; std 5, -8(1); std 5, -16(1); addi 3, 1, -16; ori 2, 2, 0; lxvd2x 35, 0, 3; vaddudm 2, 2, 3; blr. This will almost certainly cause a load-hit-store hazard. ; Since val is a value parameter, it should not need to be saved onto; the stack, unless it's being done set up the vector register. Instead,; it would be better to splat the value into a vector register, and then; remove the (dead) stores to the stack. //===----------------------------------------------------------------------===//. At the moment we always generate a lxsdx in preference to lfd, or stxsdx in; preference to stfd. When we have a reg-immediate addressing mode, this is a; poor choice, since we have to load the address into an index register. This; should be fixed for P7/P8. . //===----------------------------------------------------------------------===//. Right now, ShuffleKind 0 is supported only on BE, and ShuffleKind 2 only on LE.; However, we could actually support both kinds on either endianness, if we check; for the appropriate shufflevector pattern for each case ... this would cause; some additional shufflevectors to be recognized and ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:8453,Performance,load,load,8453,"erPC/vec_add_sub_doubleword.ll:. define <2 x i64> @increment_by_val(<2 x i64> %x, i64 %val) nounwind {; %tmpvec = insertelement <2 x i64> <i64 0, i64 0>, i64 %val, i32 0; %tmpvec2 = insertelement <2 x i64> %tmpvec, i64 %val, i32 1; %result = add <2 x i64> %x, %tmpvec2; ret <2 x i64> %result. This will generate the following instruction sequence:; std 5, -8(1); std 5, -16(1); addi 3, 1, -16; ori 2, 2, 0; lxvd2x 35, 0, 3; vaddudm 2, 2, 3; blr. This will almost certainly cause a load-hit-store hazard. ; Since val is a value parameter, it should not need to be saved onto; the stack, unless it's being done set up the vector register. Instead,; it would be better to splat the value into a vector register, and then; remove the (dead) stores to the stack. //===----------------------------------------------------------------------===//. At the moment we always generate a lxsdx in preference to lfd, or stxsdx in; preference to stfd. When we have a reg-immediate addressing mode, this is a; poor choice, since we have to load the address into an index register. This; should be fixed for P7/P8. . //===----------------------------------------------------------------------===//. Right now, ShuffleKind 0 is supported only on BE, and ShuffleKind 2 only on LE.; However, we could actually support both kinds on either endianness, if we check; for the appropriate shufflevector pattern for each case ... this would cause; some additional shufflevectors to be recognized and implemented via the; ""swapped"" form. //===----------------------------------------------------------------------===//. There is a utility program called PerfectShuffle that generates a table of the; shortest instruction sequence for implementing a shufflevector operation on; PowerPC. However, this was designed for big-endian code generation. We could; modify this program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===--------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:10300,Performance,load,load-hit-store,10300,"is program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===----------------------------------------------------------------------===//. Opportunies to use instructions from PPCInstrVSX.td during code gen; - Conversion instructions (Sections 7.6.1.5 and 7.6.1.6 of ISA 2.07); - Scalar comparisons (xscmpodp and xscmpudp); - Min and max (xsmaxdp, xsmindp, xvmaxdp, xvmindp, xvmaxsp, xvminsp). Related to this: we currently do not generate the lxvw4x instruction for either; v4f32 or v4i32, probably because adding a dag pattern to the recognizer requires; a single target type. This should probably be addressed in the PPCISelDAGToDAG logic. //===----------------------------------------------------------------------===//. Currently EXTRACT_VECTOR_ELT and INSERT_VECTOR_ELT are type-legal only; for v2f64 with VSX available. We should create custom lowering; support for the other vector types. Without this support, we generate; sequences with load-hit-store hazards. v4f32 can be supported with VSX by shifting the correct element into; big-endian lane 0, using xscvspdpn to produce a double-precision; representation of the single-precision value in big-endian; double-precision lane 0, and reinterpreting lane 0 as an FPR or; vector-scalar register. v2i64 can be supported with VSX and P8Vector in the same manner as; v2f64, followed by a direct move to a GPR. v4i32 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 1, using a direct move to a GPR, and; sign-extending the 32-bit result to 64 bits. v8i16 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 3, using a direct move to a GPR, and; sign-extending the 16-bit result to 64 bits. v16i8 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 7, using a direct move to a GPR, and; sign-extending the 8-bit result to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:5542,Safety,avoid,avoid,5542,"fp. v2, v3, v2; mfcr r3, 2; rlwinm r3, r3, 25, 31, 31; cmpwi cr0, r3, 0; bne cr0, LBB1_2 ; entry; LBB1_1: ; entry; mr r6, r5; LBB1_2: ; entry; mr r3, r6; mtspr 256, r2; blr. //===----------------------------------------------------------------------===//. CodeGen/PowerPC/vec_constants.ll has an and operation that should be; codegen'd to andc. The issue is that the 'all ones' build vector is; SelectNodeTo'd a VSPLTISB instruction node before the and/xor is selected; which prevents the vnot pattern from matching. //===----------------------------------------------------------------------===//. An alternative to the store/store/load approach for illegal insert element ; lowering would be:. 1. store element to any ol' slot; 2. lvx the slot; 3. lvsl 0; splat index; vcmpeq to generate a select mask; 4. lvsl slot + x; vperm to rotate result into correct slot; 5. vsel result together. //===----------------------------------------------------------------------===//. Should codegen branches on vec_any/vec_all to avoid mfcr. Two examples:. #include <altivec.h>; int f(vector float a, vector float b); {; int aa = 0;; if (vec_all_ge(a, b)); aa |= 0x1;; if (vec_any_ge(a,b)); aa |= 0x2;; return aa;; }. vector float f(vector float a, vector float b) { ; if (vec_any_eq(a, b)) ; return a; ; else ; return b; ; }. //===----------------------------------------------------------------------===//. We should do a little better with eliminating dead stores.; The stores to the stack are dead since %a and %b are not needed. ; Function Attrs: nounwind; define <16 x i8> @test_vpmsumb() #0 {; entry:; %a = alloca <16 x i8>, align 16; %b = alloca <16 x i8>, align 16; store <16 x i8> <i8 1, i8 2, i8 3, i8 4, i8 5, i8 6, i8 7, i8 8, i8 9, i8 10, i8 11, i8 12, i8 13, i8 14, i8 15, i8 16>, <16 x i8>* %a, align 16; store <16 x i8> <i8 113, i8 114, i8 115, i8 116, i8 117, i8 118, i8 119, i8 120, i8 121, i8 122, i8 123, i8 124, i8 125, i8 126, i8 127, i8 112>, <16 x i8>* %b, align 16; %0 = load <16 x i8>*",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:7925,Safety,hazard,hazard,7925,"toc@ha; addis 4, 2, .LCPI0_1@toc@ha; addi 3, 3, .LCPI0_0@toc@l; addi 4, 4, .LCPI0_1@toc@l; lxvw4x 0, 0, 3; addi 3, 1, -16; lxvw4x 35, 0, 4; stxvw4x 0, 0, 3; ori 2, 2, 0; lxvw4x 34, 0, 3; addi 3, 1, -32; stxvw4x 35, 0, 3; vpmsumb 2, 2, 3; blr; .long 0; .quad 0. The two stxvw4x instructions are not needed.; With -mtriple=powerpc64le-unknown-linux-gnu, the associated permutes; are present too. //===----------------------------------------------------------------------===//. The following example is found in test/CodeGen/PowerPC/vec_add_sub_doubleword.ll:. define <2 x i64> @increment_by_val(<2 x i64> %x, i64 %val) nounwind {; %tmpvec = insertelement <2 x i64> <i64 0, i64 0>, i64 %val, i32 0; %tmpvec2 = insertelement <2 x i64> %tmpvec, i64 %val, i32 1; %result = add <2 x i64> %x, %tmpvec2; ret <2 x i64> %result. This will generate the following instruction sequence:; std 5, -8(1); std 5, -16(1); addi 3, 1, -16; ori 2, 2, 0; lxvd2x 35, 0, 3; vaddudm 2, 2, 3; blr. This will almost certainly cause a load-hit-store hazard. ; Since val is a value parameter, it should not need to be saved onto; the stack, unless it's being done set up the vector register. Instead,; it would be better to splat the value into a vector register, and then; remove the (dead) stores to the stack. //===----------------------------------------------------------------------===//. At the moment we always generate a lxsdx in preference to lfd, or stxsdx in; preference to stfd. When we have a reg-immediate addressing mode, this is a; poor choice, since we have to load the address into an index register. This; should be fixed for P7/P8. . //===----------------------------------------------------------------------===//. Right now, ShuffleKind 0 is supported only on BE, and ShuffleKind 2 only on LE.; However, we could actually support both kinds on either endianness, if we check; for the appropriate shufflevector pattern for each case ... this would cause; some additional shufflevectors to be recognized and ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:10315,Safety,hazard,hazards,10315,"is program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===----------------------------------------------------------------------===//. Opportunies to use instructions from PPCInstrVSX.td during code gen; - Conversion instructions (Sections 7.6.1.5 and 7.6.1.6 of ISA 2.07); - Scalar comparisons (xscmpodp and xscmpudp); - Min and max (xsmaxdp, xsmindp, xvmaxdp, xvmindp, xvmaxsp, xvminsp). Related to this: we currently do not generate the lxvw4x instruction for either; v4f32 or v4i32, probably because adding a dag pattern to the recognizer requires; a single target type. This should probably be addressed in the PPCISelDAGToDAG logic. //===----------------------------------------------------------------------===//. Currently EXTRACT_VECTOR_ELT and INSERT_VECTOR_ELT are type-legal only; for v2f64 with VSX available. We should create custom lowering; support for the other vector types. Without this support, we generate; sequences with load-hit-store hazards. v4f32 can be supported with VSX by shifting the correct element into; big-endian lane 0, using xscvspdpn to produce a double-precision; representation of the single-precision value in big-endian; double-precision lane 0, and reinterpreting lane 0 as an FPR or; vector-scalar register. v2i64 can be supported with VSX and P8Vector in the same manner as; v2f64, followed by a direct move to a GPR. v4i32 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 1, using a direct move to a GPR, and; sign-extending the 32-bit result to 64 bits. v8i16 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 3, using a direct move to a GPR, and; sign-extending the 16-bit result to 64 bits. v16i8 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 7, using a direct move to a GPR, and; sign-extending the 8-bit result to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:3063,Testability,test,test,3063," win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===----------------------------------------------------------------------===//. If we want to tie instruction selection into the scheduler, we can do some; constant formation with different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-pass scheduler. //===----------------------------------------------------------------------===//. For this function:. void test(vector float *A, vector float *B) {; vector float C = (vector float)vec_cmpeq(*A, *B);; if (!vec_any_eq(*A, *B)); *B = (vector float){0,0,0,0};; *A = C;; }. we get the following basic block:. 	...; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp v4, v3, v2; vcmpeqfp. v2, v3, v2; bne cr6, LBB1_2 ; cond_next. The vcmpeqfp/vcmpeqfp. instructions currently cannot be merged when the; vcmpeqfp. result is used by a branch. This can be improved. //===----------------------------------------------------------------------===//. The code generated for this is truly aweful:. vector float test(float a, float b) {; return (vector float){ 0.0, a, 0.0, 0.0}; ; }. LCPI1_0: ; float; .space 4; .text; .globl _test; .align 4; _test:; mfspr r2, 256; oris r3, r2, 4096; mtspr 256, r3; lis r3, ha16(LCPI1_0); addi r4, r1, -32; stfs f1, -16(r1); addi r5, r1, -16; lfs f0, lo16(LCPI1_0)(r3); stfs f0, -32(r1); lvx v2, 0, r4; lvx v3, 0, r5; vmrghw v3, v3, v2; vspltw v2, v2, 0; vmrghw v2, v2, v3; mtspr 256, r2; blr. //===----------------------------------------------------------------------===//. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:3642,Testability,test,test,3642,"ith different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-pass scheduler. //===----------------------------------------------------------------------===//. For this function:. void test(vector float *A, vector float *B) {; vector float C = (vector float)vec_cmpeq(*A, *B);; if (!vec_any_eq(*A, *B)); *B = (vector float){0,0,0,0};; *A = C;; }. we get the following basic block:. 	...; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp v4, v3, v2; vcmpeqfp. v2, v3, v2; bne cr6, LBB1_2 ; cond_next. The vcmpeqfp/vcmpeqfp. instructions currently cannot be merged when the; vcmpeqfp. result is used by a branch. This can be improved. //===----------------------------------------------------------------------===//. The code generated for this is truly aweful:. vector float test(float a, float b) {; return (vector float){ 0.0, a, 0.0, 0.0}; ; }. LCPI1_0: ; float; .space 4; .text; .globl _test; .align 4; _test:; mfspr r2, 256; oris r3, r2, 4096; mtspr 256, r3; lis r3, ha16(LCPI1_0); addi r4, r1, -32; stfs f1, -16(r1); addi r5, r1, -16; lfs f0, lo16(LCPI1_0)(r3); stfs f0, -32(r1); lvx v2, 0, r4; lvx v3, 0, r5; vmrghw v3, v3, v2; vspltw v2, v2, 0; vmrghw v2, v2, v3; mtspr 256, r2; blr. //===----------------------------------------------------------------------===//. int foo(vector float *x, vector float *y) {; if (vec_all_eq(*x,*y)) return 3245; ; else return 12;; }. A predicate compare being used in a select_cc should have the same peephole; applied to it as a predicate compare used by a br_cc. There should be no; mfcr here:. _foo:; mfspr r2, 256; oris r5, r2, 12288; mtspr 256, r5; li r5, 12; li r6, 3245; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp. v2, v3, v2; mfcr r3, 2; rlwinm r3, r3, 25, 31, 31; cmpwi cr0, r3, 0; bne cr0, LBB1_2 ; entry; LBB1_1: ; entry; mr r6, r5; LBB1_2: ; e",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:7413,Testability,test,test,7413,"121, i8 122, i8 123, i8 124, i8 125, i8 126, i8 127, i8 112>, <16 x i8>* %b, align 16; %0 = load <16 x i8>* %a, align 16; %1 = load <16 x i8>* %b, align 16; %2 = call <16 x i8> @llvm.ppc.altivec.crypto.vpmsumb(<16 x i8> %0, <16 x i8> %1); ret <16 x i8> %2; }. ; Function Attrs: nounwind readnone; declare <16 x i8> @llvm.ppc.altivec.crypto.vpmsumb(<16 x i8>, <16 x i8>) #1. Produces the following code with -mtriple=powerpc64-unknown-linux-gnu:; # %bb.0: # %entry; addis 3, 2, .LCPI0_0@toc@ha; addis 4, 2, .LCPI0_1@toc@ha; addi 3, 3, .LCPI0_0@toc@l; addi 4, 4, .LCPI0_1@toc@l; lxvw4x 0, 0, 3; addi 3, 1, -16; lxvw4x 35, 0, 4; stxvw4x 0, 0, 3; ori 2, 2, 0; lxvw4x 34, 0, 3; addi 3, 1, -32; stxvw4x 35, 0, 3; vpmsumb 2, 2, 3; blr; .long 0; .quad 0. The two stxvw4x instructions are not needed.; With -mtriple=powerpc64le-unknown-linux-gnu, the associated permutes; are present too. //===----------------------------------------------------------------------===//. The following example is found in test/CodeGen/PowerPC/vec_add_sub_doubleword.ll:. define <2 x i64> @increment_by_val(<2 x i64> %x, i64 %val) nounwind {; %tmpvec = insertelement <2 x i64> <i64 0, i64 0>, i64 %val, i32 0; %tmpvec2 = insertelement <2 x i64> %tmpvec, i64 %val, i32 1; %result = add <2 x i64> %x, %tmpvec2; ret <2 x i64> %result. This will generate the following instruction sequence:; std 5, -8(1); std 5, -16(1); addi 3, 1, -16; ori 2, 2, 0; lxvd2x 35, 0, 3; vaddudm 2, 2, 3; blr. This will almost certainly cause a load-hit-store hazard. ; Since val is a value parameter, it should not need to be saved onto; the stack, unless it's being done set up the vector register. Instead,; it would be better to splat the value into a vector register, and then; remove the (dead) stores to the stack. //===----------------------------------------------------------------------===//. At the moment we always generate a lxsdx in preference to lfd, or stxsdx in; preference to stfd. When we have a reg-immediate addressing mode, this i",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:9989,Testability,log,logic,9989,"----------------------------------------------------===//. There is a utility program called PerfectShuffle that generates a table of the; shortest instruction sequence for implementing a shufflevector operation on; PowerPC. However, this was designed for big-endian code generation. We could; modify this program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===----------------------------------------------------------------------===//. Opportunies to use instructions from PPCInstrVSX.td during code gen; - Conversion instructions (Sections 7.6.1.5 and 7.6.1.6 of ISA 2.07); - Scalar comparisons (xscmpodp and xscmpudp); - Min and max (xsmaxdp, xsmindp, xvmaxdp, xvmindp, xvmaxsp, xvminsp). Related to this: we currently do not generate the lxvw4x instruction for either; v4f32 or v4i32, probably because adding a dag pattern to the recognizer requires; a single target type. This should probably be addressed in the PPCISelDAGToDAG logic. //===----------------------------------------------------------------------===//. Currently EXTRACT_VECTOR_ELT and INSERT_VECTOR_ELT are type-legal only; for v2f64 with VSX available. We should create custom lowering; support for the other vector types. Without this support, we generate; sequences with load-hit-store hazards. v4f32 can be supported with VSX by shifting the correct element into; big-endian lane 0, using xscvspdpn to produce a double-precision; representation of the single-precision value in big-endian; double-precision lane 0, and reinterpreting lane 0 as an FPR or; vector-scalar register. v2i64 can be supported with VSX and P8Vector in the same manner as; v2f64, followed by a direct move to a GPR. v4i32 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 1, using a direct move to a GPR, and; sign-extending the 32-bit result to 64 bits. v8i16 can be supported with VSX and P8Vector by shifting the ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:3524,Availability,Mask,Mask,3524,".dword[i] ← EXTS64(VR[VRB].dword[i].byte[7]); end. . vextsh2d; (set v2i64:$vD, (sext v2i16:$vB)). // PowerISA_V3.0:; do i = 0 to 1; VR[VRT].dword[i] ← EXTS64(VR[VRB].dword[i].hword[3]); end. . vextsw2d; (set v2i64:$vD, (sext v2i32:$vB)). // PowerISA_V3.0:; do i = 0 to 1; VR[VRT].dword[i] ← EXTS64(VR[VRB].dword[i].word[1]); end. - Vector Integer Negate: vnegw vnegd; . Map to llvm ineg; (set v4i32:$rT, (ineg v4i32:$rA)) // vnegw; (set v2i64:$rT, (ineg v2i64:$rA)) // vnegd. - Vector Parity Byte: vprtybw vprtybd vprtybq; . Use intrinsic:; (set v4i32:$rD, (int_ppc_altivec_vprtybw v4i32:$vB)); (set v2i64:$rD, (int_ppc_altivec_vprtybd v2i64:$vB)); (set v1i128:$rD, (int_ppc_altivec_vprtybq v1i128:$vB)). - Vector (Bit) Permute (Right-indexed):; . vbpermd: Same as ""vbpermq"", use VX1_Int_Ty2:; VX1_Int_Ty2<1484, ""vbpermd"", int_ppc_altivec_vbpermd, v2i64, v2i64>;. . vpermr: use VA1a_Int_Ty3; VA1a_Int_Ty3<59, ""vpermr"", int_ppc_altivec_vpermr, v16i8, v16i8, v16i8>;. - Vector Rotate Left Mask/Mask-Insert: vrlwnm vrlwmi vrldnm vrldmi; . Use intrinsic:; VX1_Int_Ty<389, ""vrlwnm"", int_ppc_altivec_vrlwnm, v4i32>;; VX1_Int_Ty<133, ""vrlwmi"", int_ppc_altivec_vrlwmi, v4i32>;; VX1_Int_Ty<453, ""vrldnm"", int_ppc_altivec_vrldnm, v2i64>;; VX1_Int_Ty<197, ""vrldmi"", int_ppc_altivec_vrldmi, v2i64>;. - Vector Shift Left/Right: vslv vsrv; . Use intrinsic, don't map to llvm shl and lshr, because they have different; semantics, e.g. vslv:. do i = 0 to 15; sh ← VR[VRB].byte[i].bit[5:7]; VR[VRT].byte[i] ← src.byte[i:i+1].bit[sh:sh+7]; end. VR[VRT].byte[i] is composed of 2 bytes from src.byte[i:i+1]. . VX1_Int_Ty<1860, ""vslv"", int_ppc_altivec_vslv, v16i8>;; VX1_Int_Ty<1796, ""vsrv"", int_ppc_altivec_vsrv, v16i8>;. - Vector Multiply-by-10 (& Write Carry) Unsigned Quadword:; vmul10uq vmul10cuq; . Use intrinsic:; VX1_Int_Ty<513, ""vmul10uq"", int_ppc_altivec_vmul10uq, v1i128>;; VX1_Int_Ty< 1, ""vmul10cuq"", int_ppc_altivec_vmul10cuq, v1i128>;. - Vector Multiply-by-10 Extended (& Write Carry) Unsigned Quadword:; vm",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:3529,Availability,Mask,Mask-Insert,3529,".dword[i] ← EXTS64(VR[VRB].dword[i].byte[7]); end. . vextsh2d; (set v2i64:$vD, (sext v2i16:$vB)). // PowerISA_V3.0:; do i = 0 to 1; VR[VRT].dword[i] ← EXTS64(VR[VRB].dword[i].hword[3]); end. . vextsw2d; (set v2i64:$vD, (sext v2i32:$vB)). // PowerISA_V3.0:; do i = 0 to 1; VR[VRT].dword[i] ← EXTS64(VR[VRB].dword[i].word[1]); end. - Vector Integer Negate: vnegw vnegd; . Map to llvm ineg; (set v4i32:$rT, (ineg v4i32:$rA)) // vnegw; (set v2i64:$rT, (ineg v2i64:$rA)) // vnegd. - Vector Parity Byte: vprtybw vprtybd vprtybq; . Use intrinsic:; (set v4i32:$rD, (int_ppc_altivec_vprtybw v4i32:$vB)); (set v2i64:$rD, (int_ppc_altivec_vprtybd v2i64:$vB)); (set v1i128:$rD, (int_ppc_altivec_vprtybq v1i128:$vB)). - Vector (Bit) Permute (Right-indexed):; . vbpermd: Same as ""vbpermq"", use VX1_Int_Ty2:; VX1_Int_Ty2<1484, ""vbpermd"", int_ppc_altivec_vbpermd, v2i64, v2i64>;. . vpermr: use VA1a_Int_Ty3; VA1a_Int_Ty3<59, ""vpermr"", int_ppc_altivec_vpermr, v16i8, v16i8, v16i8>;. - Vector Rotate Left Mask/Mask-Insert: vrlwnm vrlwmi vrldnm vrldmi; . Use intrinsic:; VX1_Int_Ty<389, ""vrlwnm"", int_ppc_altivec_vrlwnm, v4i32>;; VX1_Int_Ty<133, ""vrlwmi"", int_ppc_altivec_vrlwmi, v4i32>;; VX1_Int_Ty<453, ""vrldnm"", int_ppc_altivec_vrldnm, v2i64>;; VX1_Int_Ty<197, ""vrldmi"", int_ppc_altivec_vrldmi, v2i64>;. - Vector Shift Left/Right: vslv vsrv; . Use intrinsic, don't map to llvm shl and lshr, because they have different; semantics, e.g. vslv:. do i = 0 to 15; sh ← VR[VRB].byte[i].bit[5:7]; VR[VRT].byte[i] ← src.byte[i:i+1].bit[sh:sh+7]; end. VR[VRT].byte[i] is composed of 2 bytes from src.byte[i:i+1]. . VX1_Int_Ty<1860, ""vslv"", int_ppc_altivec_vslv, v16i8>;; VX1_Int_Ty<1796, ""vsrv"", int_ppc_altivec_vsrv, v16i8>;. - Vector Multiply-by-10 (& Write Carry) Unsigned Quadword:; vmul10uq vmul10cuq; . Use intrinsic:; VX1_Int_Ty<513, ""vmul10uq"", int_ppc_altivec_vmul10uq, v1i128>;; VX1_Int_Ty< 1, ""vmul10cuq"", int_ppc_altivec_vmul10cuq, v1i128>;. - Vector Multiply-by-10 Extended (& Write Carry) Unsigned Quadword:; vm",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:20494,Integrability,Message,Message,20494,"xsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (outs vsrc:$XT), (ins memrr:$src),; ""lxvd2x $XT, $src"", IIC_LdStLFD,; [(set v2f64:$XT, (int_ppc_vsx_lxvd2x xoaddr:$src))]>;. . (set v8i16:$XT, (int_ppc_vsx_lxvh8x xoaddr:$src)); (set v16i8:$XT, (int_ppc_vsx_lxvb16x xoaddr:$src)). - Store Vector Halfword*8/Byte*16 Indexed: stxvh8x stxvb16x; . Similar to stxvd2x/stxvw4x:; def STXVD2X : XX1Form<31, 972,; (outs), (ins vsrc:$XT, memrr:$dst),; ""stxvd2x $XT, $dst"", IIC_LdStSTFD,; [(store v2f64:$XT, xoaddr:$dst)]>;. . (store v8i16:$XT, xoaddr:$dst); (store v16i8:$XT, xoaddr:$dst). - Load/Store Vector (Left-justified) with Length: lxvl lxvll stxvl stxvll; . Likely needs an intrinsic; . (set v?:$XT, (int_ppc_vsx_lxvl xoaddr:$src)); (set v?:$XT, (int_ppc_vsx_lxvll xoaddr:$src)). . (int_ppc_vsx_stxvl xoaddr:$dst)); (int_ppc_vsx_stxvll xoaddr:$dst)). - Load Vector Word & Splat Indexed: lxvwsx; . Likely needs an intrinsic; . (set v?:$XT, (int_ppc_vsx_lxvwsx xoaddr:$src)). Atomic operations (l[dw]at, st[dw]at):; - Provide custom lowering for common atomic operations to use these; instructions with the correct Function Code; - Ensure the operands are in the correct register (i.e. RT+1, RT+2); - Provide builtins since not all FC's necessarily have an existing LLVM; atomic operation. Move to CR from XER Extended (mcrxrx):; - Is there a use for this in LLVM?. Fixed Point Facility:. - Copy-Paste Facility: copy copy_first cp_abort paste paste. paste_last; . Use instrinstics:; (int_ppc_copy_first i32:$rA, i32:$rB); (int_ppc_copy i32:$rA, i32:$rB). (int_ppc_paste i32:$rA, i32:$rB); (int_ppc_paste_last i32:$rA, i32:$rB). (int_cp_abort). - Message Synchronize: msgsync; - SLB*: slbieg slbsync; - stop; . No instrinstics; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:20502,Integrability,Synchroniz,Synchronize,20502,"xsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (outs vsrc:$XT), (ins memrr:$src),; ""lxvd2x $XT, $src"", IIC_LdStLFD,; [(set v2f64:$XT, (int_ppc_vsx_lxvd2x xoaddr:$src))]>;. . (set v8i16:$XT, (int_ppc_vsx_lxvh8x xoaddr:$src)); (set v16i8:$XT, (int_ppc_vsx_lxvb16x xoaddr:$src)). - Store Vector Halfword*8/Byte*16 Indexed: stxvh8x stxvb16x; . Similar to stxvd2x/stxvw4x:; def STXVD2X : XX1Form<31, 972,; (outs), (ins vsrc:$XT, memrr:$dst),; ""stxvd2x $XT, $dst"", IIC_LdStSTFD,; [(store v2f64:$XT, xoaddr:$dst)]>;. . (store v8i16:$XT, xoaddr:$dst); (store v16i8:$XT, xoaddr:$dst). - Load/Store Vector (Left-justified) with Length: lxvl lxvll stxvl stxvll; . Likely needs an intrinsic; . (set v?:$XT, (int_ppc_vsx_lxvl xoaddr:$src)); (set v?:$XT, (int_ppc_vsx_lxvll xoaddr:$src)). . (int_ppc_vsx_stxvl xoaddr:$dst)); (int_ppc_vsx_stxvll xoaddr:$dst)). - Load Vector Word & Splat Indexed: lxvwsx; . Likely needs an intrinsic; . (set v?:$XT, (int_ppc_vsx_lxvwsx xoaddr:$src)). Atomic operations (l[dw]at, st[dw]at):; - Provide custom lowering for common atomic operations to use these; instructions with the correct Function Code; - Ensure the operands are in the correct register (i.e. RT+1, RT+2); - Provide builtins since not all FC's necessarily have an existing LLVM; atomic operation. Move to CR from XER Extended (mcrxrx):; - Is there a use for this in LLVM?. Fixed Point Facility:. - Copy-Paste Facility: copy copy_first cp_abort paste paste. paste_last; . Use instrinstics:; (int_ppc_copy_first i32:$rA, i32:$rB); (int_ppc_copy i32:$rA, i32:$rB). (int_ppc_paste i32:$rA, i32:$rB); (int_ppc_paste_last i32:$rA, i32:$rB). (int_cp_abort). - Message Synchronize: msgsync; - SLB*: slbieg slbsync; - stop; . No instrinstics; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:2115,Modifiability,Extend,Extend,2115,"ec_vextubrx i64:$rA, v16i8:$vB)); (set i64:$rD, (int_ppc_altivec_vextuhrx i64:$rA, v8i16:$vB)); (set i64:$rD, (int_ppc_altivec_vextuwrx i64:$rA, v4i32:$vB)). - Vector Insert Element Instructions: vinsertb vinsertd vinserth vinsertw; (set v16i8:$vD, (int_ppc_altivec_vinsertb v16i8:$vA, imm:$UIMM)); (set v8i16:$vD, (int_ppc_altivec_vinsertd v8i16:$vA, imm:$UIMM)); (set v4i32:$vD, (int_ppc_altivec_vinserth v4i32:$vA, imm:$UIMM)); (set v2i64:$vD, (int_ppc_altivec_vinsertw v2i64:$vA, imm:$UIMM)). - Vector Count Leading/Trailing Zero LSB. Result is placed into GPR[rD]:; vclzlsbb vctzlsbb; . Use intrinsic:; (set i64:$rD, (int_ppc_altivec_vclzlsbb v16i8:$vB)); (set i64:$rD, (int_ppc_altivec_vctzlsbb v16i8:$vB)). - Vector Count Trailing Zeros: vctzb vctzh vctzw vctzd; . Map to llvm cttz; (set v16i8:$vD, (cttz v16i8:$vB)) // vctzb; (set v8i16:$vD, (cttz v8i16:$vB)) // vctzh; (set v4i32:$vD, (cttz v4i32:$vB)) // vctzw; (set v2i64:$vD, (cttz v2i64:$vB)) // vctzd. - Vector Extend Sign: vextsb2w vextsh2w vextsb2d vextsh2d vextsw2d; . vextsb2w:; (set v4i32:$vD, (sext v4i8:$vB)). // PowerISA_V3.0:; do i = 0 to 3; VR[VRT].word[i] ← EXTS32(VR[VRB].word[i].byte[3]); end. . vextsh2w:; (set v4i32:$vD, (sext v4i16:$vB)). // PowerISA_V3.0:; do i = 0 to 3; VR[VRT].word[i] ← EXTS32(VR[VRB].word[i].hword[1]); end. . vextsb2d; (set v2i64:$vD, (sext v2i8:$vB)). // PowerISA_V3.0:; do i = 0 to 1; VR[VRT].dword[i] ← EXTS64(VR[VRB].dword[i].byte[7]); end. . vextsh2d; (set v2i64:$vD, (sext v2i16:$vB)). // PowerISA_V3.0:; do i = 0 to 1; VR[VRT].dword[i] ← EXTS64(VR[VRB].dword[i].hword[3]); end. . vextsw2d; (set v2i64:$vD, (sext v2i32:$vB)). // PowerISA_V3.0:; do i = 0 to 1; VR[VRT].dword[i] ← EXTS64(VR[VRB].dword[i].word[1]); end. - Vector Integer Negate: vnegw vnegd; . Map to llvm ineg; (set v4i32:$rT, (ineg v4i32:$rA)) // vnegw; (set v2i64:$rT, (ineg v2i64:$rA)) // vnegd. - Vector Parity Byte: vprtybw vprtybd vprtybq; . Use intrinsic:; (set v4i32:$rD, (int_ppc_altivec_vprtybw v4i32:$vB)); (set v2i",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:4490,Modifiability,Extend,Extended,4490," Rotate Left Mask/Mask-Insert: vrlwnm vrlwmi vrldnm vrldmi; . Use intrinsic:; VX1_Int_Ty<389, ""vrlwnm"", int_ppc_altivec_vrlwnm, v4i32>;; VX1_Int_Ty<133, ""vrlwmi"", int_ppc_altivec_vrlwmi, v4i32>;; VX1_Int_Ty<453, ""vrldnm"", int_ppc_altivec_vrldnm, v2i64>;; VX1_Int_Ty<197, ""vrldmi"", int_ppc_altivec_vrldmi, v2i64>;. - Vector Shift Left/Right: vslv vsrv; . Use intrinsic, don't map to llvm shl and lshr, because they have different; semantics, e.g. vslv:. do i = 0 to 15; sh ← VR[VRB].byte[i].bit[5:7]; VR[VRT].byte[i] ← src.byte[i:i+1].bit[sh:sh+7]; end. VR[VRT].byte[i] is composed of 2 bytes from src.byte[i:i+1]. . VX1_Int_Ty<1860, ""vslv"", int_ppc_altivec_vslv, v16i8>;; VX1_Int_Ty<1796, ""vsrv"", int_ppc_altivec_vsrv, v16i8>;. - Vector Multiply-by-10 (& Write Carry) Unsigned Quadword:; vmul10uq vmul10cuq; . Use intrinsic:; VX1_Int_Ty<513, ""vmul10uq"", int_ppc_altivec_vmul10uq, v1i128>;; VX1_Int_Ty< 1, ""vmul10cuq"", int_ppc_altivec_vmul10cuq, v1i128>;. - Vector Multiply-by-10 Extended (& Write Carry) Unsigned Quadword:; vmul10euq vmul10ecuq; . Use intrinsic:; VX1_Int_Ty<577, ""vmul10euq"", int_ppc_altivec_vmul10euq, v1i128>;; VX1_Int_Ty< 65, ""vmul10ecuq"", int_ppc_altivec_vmul10ecuq, v1i128>;. - Decimal Convert From/to National/Zoned/Signed-QWord:; bcdcfn. bcdcfz. bcdctn. bcdctz. bcdcfsq. bcdctsq.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdcfno v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcdcfzo v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcdctno v1i128:$vB)); (set v1i128:$vD, (int_ppc_altivec_bcdctzo v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcdcfsqo v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcdctsqo v1i128:$vB)). - Decimal Copy-Sign/Set-Sign: bcdcpsgn. bcdsetsgn.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdcpsgno v1i128:$vA, v1i128:$vB)); (set v1i128:$vD, (int_ppc_altivec_bcdsetsgno v1i128:$vB, i1:$PS)). - Decimal Shift/Unsigned-Shift/Shift-and-Round: bcds. bcdus. bcdsr.; . Use instrinstics:; (set ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:12955,Modifiability,Extend,Extended,12955,"// xscvqpsdz; (set f128:$XT, (PPCfctiwz f128:$XB)) // xscvqpswz; (set f128:$XT, (PPCfctiduz f128:$XB)) // xscvqpudz; (set f128:$XT, (PPCfctiwuz f128:$XB)) // xscvqpuwz. - Convert (Un)Signed DWord -> QP: xscvsdqp xscvudqp; . Similar to XSCVSXDSP; . (set f128:$XT, (PPCfcfids f64:$XB)) // xscvsdqp; (set f128:$XT, (PPCfcfidus f64:$XB)) // xscvudqp. - (Round &) Convert DP <-> HP: xscvdphp xscvhpdp; . Similar to XSCVDPSP; . No SDAG, intrinsic, builtin are required??. - Vector HP -> SP: xvcvhpsp xvcvsphp; . Similar to XVCVDPSP:; def XVCVDPSP : XX2Form<60, 393,; (outs vsrc:$XT), (ins vsrc:$XB),; ""xvcvdpsp $XT, $XB"", IIC_VecFP, []>;; . No SDAG, intrinsic, builtin are required??. - Round to Quad-Precision Integer: xsrqpi xsrqpix; . These are combination of ""XSRDPI"", ""XSRDPIC"", ""XSRDPIM"", .., because you; need to assign rounding mode in instruction; . Provide builtin?; (set f128:$vT, (int_ppc_vsx_xsrqpi f128:$vB)); (set f128:$vT, (int_ppc_vsx_xsrqpix f128:$vB)). - Round Quad-Precision to Double-Extended Precision (fp80): xsrqpxp; . Provide builtin?; (set f128:$vT, (int_ppc_vsx_xsrqpxp f128:$vB)). Fixed Point Facility:. - Exploit cmprb and cmpeqb (perhaps for something like; isalpha/isdigit/isupper/islower and isspace respectivelly). This can; perhaps be done through a builtin. - Provide testing for cnttz[dw]; - Insert Exponent DP/QP: xsiexpdp xsiexpqp; . Use intrinsic?; . xsiexpdp:; // Note: rA and rB are the unsigned integer value.; (set f128:$XT, (int_ppc_vsx_xsiexpdp i64:$rA, i64:$rB)). . xsiexpqp:; (set f128:$vT, (int_ppc_vsx_xsiexpqp f128:$vA, f64:$vB)). - Extract Exponent/Significand DP/QP: xsxexpdp xsxsigdp xsxexpqp xsxsigqp; . Use intrinsic?; . (set i64:$rT, (int_ppc_vsx_xsxexpdp f64$XB)) // xsxexpdp; (set i64:$rT, (int_ppc_vsx_xsxsigdp f64$XB)) // xsxsigdp; (set f128:$vT, (int_ppc_vsx_xsxexpqp f128$vB)) // xsxexpqp; (set f128:$vT, (int_ppc_vsx_xsxsigqp f128$vB)) // xsxsigqp. - Vector Insert Word: xxinsertw; - Useful for inserting f32/i32 elements into vectors (the elem",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:20158,Modifiability,Extend,Extended,20158,"xsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (outs vsrc:$XT), (ins memrr:$src),; ""lxvd2x $XT, $src"", IIC_LdStLFD,; [(set v2f64:$XT, (int_ppc_vsx_lxvd2x xoaddr:$src))]>;. . (set v8i16:$XT, (int_ppc_vsx_lxvh8x xoaddr:$src)); (set v16i8:$XT, (int_ppc_vsx_lxvb16x xoaddr:$src)). - Store Vector Halfword*8/Byte*16 Indexed: stxvh8x stxvb16x; . Similar to stxvd2x/stxvw4x:; def STXVD2X : XX1Form<31, 972,; (outs), (ins vsrc:$XT, memrr:$dst),; ""stxvd2x $XT, $dst"", IIC_LdStSTFD,; [(store v2f64:$XT, xoaddr:$dst)]>;. . (store v8i16:$XT, xoaddr:$dst); (store v16i8:$XT, xoaddr:$dst). - Load/Store Vector (Left-justified) with Length: lxvl lxvll stxvl stxvll; . Likely needs an intrinsic; . (set v?:$XT, (int_ppc_vsx_lxvl xoaddr:$src)); (set v?:$XT, (int_ppc_vsx_lxvll xoaddr:$src)). . (int_ppc_vsx_stxvl xoaddr:$dst)); (int_ppc_vsx_stxvll xoaddr:$dst)). - Load Vector Word & Splat Indexed: lxvwsx; . Likely needs an intrinsic; . (set v?:$XT, (int_ppc_vsx_lxvwsx xoaddr:$src)). Atomic operations (l[dw]at, st[dw]at):; - Provide custom lowering for common atomic operations to use these; instructions with the correct Function Code; - Ensure the operands are in the correct register (i.e. RT+1, RT+2); - Provide builtins since not all FC's necessarily have an existing LLVM; atomic operation. Move to CR from XER Extended (mcrxrx):; - Is there a use for this in LLVM?. Fixed Point Facility:. - Copy-Paste Facility: copy copy_first cp_abort paste paste. paste_last; . Use instrinstics:; (int_ppc_copy_first i32:$rA, i32:$rB); (int_ppc_copy i32:$rA, i32:$rB). (int_ppc_paste i32:$rA, i32:$rB); (int_ppc_paste_last i32:$rA, i32:$rB). (int_cp_abort). - Message Synchronize: msgsync; - SLB*: slbieg slbsync; - stop; . No instrinstics; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:8927,Performance,perform,perform,8927,"snmsubadp. . isCommutable = 1; // xsmaddqp; [(set f128:$vT, (fma f128:$vA, f128:$vB, f128:$vTi))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsmsubqp; [(set f128:$vT, (fma f128:$vA, f128:$vB, (fneg f128:$vTi)))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsnmaddqp; [(set f128:$vT, (fneg (fma f128:$vA, f128:$vB, f128:$vTi)))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsnmsubqp; [(set f128:$vT, (fneg (fma f128:$vA, f128:$vB, (fneg f128:$vTi))))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. - Round to Odd of QP (Negative) Multiply-{Add/Subtract}:; xsmaddqpo xsmsubqpo xsnmaddqpo xsnmsubqpo; . Similar to xsrsqrtedp??. . Define DAG Node in PPCInstrInfo.td:; def PPCfmarto: SDNode<""PPCISD::FMARTO"", SDTFPTernaryOp, []>;. It looks like we only need to define ""PPCfmarto"" for these instructions,; because according to PowerISA_V3.0, these instructions perform RTO on; fma's result:; xsmaddqp(o); v ← bfp_MULTIPLY_ADD(src1, src3, src2); rnd ← bfp_ROUND_TO_BFP128(RO, FPSCR.RN, v); result ← bfp_CONVERT_TO_BFP128(rnd). xsmsubqp(o); v ← bfp_MULTIPLY_ADD(src1, src3, bfp_NEGATE(src2)); rnd ← bfp_ROUND_TO_BFP128(RO, FPSCR.RN, v); result ← bfp_CONVERT_TO_BFP128(rnd). xsnmaddqp(o); v ← bfp_MULTIPLY_ADD(src1,src3,src2); rnd ← bfp_NEGATE(bfp_ROUND_TO_BFP128(RO, FPSCR.RN, v)); result ← bfp_CONVERT_TO_BFP128(rnd). xsnmsubqp(o); v ← bfp_MULTIPLY_ADD(src1, src3, bfp_NEGATE(src2)); rnd ← bfp_NEGATE(bfp_ROUND_TO_BFP128(RO, FPSCR.RN, v)); result ← bfp_CONVERT_TO_BFP128(rnd). DAG patterns of each instruction (PPCInstrVSX.td):; . isCommutable = 1; // xsmaddqpo; [(set f128:$vT, (PPCfmarto f128:$vA, f128:$vB, f128:$vTi))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsmsubqpo; [(set f128:$vT, (PPCfmarto f128:$vA, f128:$vB, (fneg f128:$vTi)))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsnmaddqpo; [(set f128:$vT, (fneg (PPCfmarto f128:$vA, f128:$vB, f128:$v",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:17296,Performance,Load,Load,17296,"xcdp xsmincdp; (set f64:$XT, (fmaxnum f64:$XA, f64:$XB)); (set f64:$XT, (fminnum f64:$XA, f64:$XB)). . xsmaxjdp xsminjdp; (set f64:$XT, (int_ppc_vsx_xsmaxjdp f64:$XA, f64:$XB)); (set f64:$XT, (int_ppc_vsx_xsminjdp f64:$XA, f64:$XB)). - Vector Byte-Reverse H/W/D/Q Word: xxbrh xxbrw xxbrd xxbrq; . Use intrinsic; (set v8i16:$XT, (int_ppc_vsx_xxbrh v8i16:$XB)); (set v4i32:$XT, (int_ppc_vsx_xxbrw v4i32:$XB)); (set v2i64:$XT, (int_ppc_vsx_xxbrd v2i64:$XB)); (set v1i128:$XT, (int_ppc_vsx_xxbrq v1i128:$XB)). - Vector Permute: xxperm xxpermr; . I have checked ""PPCxxswapd"" in PPCInstrVSX.td, but they are different; . Use intrinsic; (set v16i8:$XT, (int_ppc_vsx_xxperm v16i8:$XA, v16i8:$XB)); (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB)). - Vector Splat Immediate Byte: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:17365,Performance,load,load,17365,"4:$XT, (fminnum f64:$XA, f64:$XB)). . xsmaxjdp xsminjdp; (set f64:$XT, (int_ppc_vsx_xsmaxjdp f64:$XA, f64:$XB)); (set f64:$XT, (int_ppc_vsx_xsminjdp f64:$XA, f64:$XB)). - Vector Byte-Reverse H/W/D/Q Word: xxbrh xxbrw xxbrd xxbrq; . Use intrinsic; (set v8i16:$XT, (int_ppc_vsx_xxbrh v8i16:$XB)); (set v4i32:$XT, (int_ppc_vsx_xxbrw v4i32:$XB)); (set v2i64:$XT, (int_ppc_vsx_xxbrd v2i64:$XB)); (set v1i128:$XT, (int_ppc_vsx_xxbrq v1i128:$XB)). - Vector Permute: xxperm xxpermr; . I have checked ""PPCxxswapd"" in PPCInstrVSX.td, but they are different; . Use intrinsic; (set v16i8:$XT, (int_ppc_vsx_xxperm v16i8:$XA, v16i8:$XB)); (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB)). - Vector Splat Immediate Byte: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfr",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:17535,Performance,Load,Load,17535,"yte-Reverse H/W/D/Q Word: xxbrh xxbrw xxbrd xxbrq; . Use intrinsic; (set v8i16:$XT, (int_ppc_vsx_xxbrh v8i16:$XB)); (set v4i32:$XT, (int_ppc_vsx_xxbrw v4i32:$XB)); (set v2i64:$XT, (int_ppc_vsx_xxbrd v2i64:$XB)); (set v1i128:$XT, (int_ppc_vsx_xxbrq v1i128:$XB)). - Vector Permute: xxperm xxpermr; . I have checked ""PPCxxswapd"" in PPCInstrVSX.td, but they are different; . Use intrinsic; (set v16i8:$XT, (int_ppc_vsx_xxperm v16i8:$XA, v16i8:$XB)); (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB)). - Vector Splat Immediate Byte: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfwor",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:17614,Performance,load,load,17614,"(set v8i16:$XT, (int_ppc_vsx_xxbrh v8i16:$XB)); (set v4i32:$XT, (int_ppc_vsx_xxbrw v4i32:$XB)); (set v2i64:$XT, (int_ppc_vsx_xxbrd v2i64:$XB)); (set v1i128:$XT, (int_ppc_vsx_xxbrq v1i128:$XB)). - Vector Permute: xxperm xxpermr; . I have checked ""PPCxxswapd"" in PPCInstrVSX.td, but they are different; . Use intrinsic; (set v16i8:$XT, (int_ppc_vsx_xxperm v16i8:$XA, v16i8:$XB)); (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB)). - Vector Splat Immediate Byte: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:17671,Performance,Load,Load,17671,"(int_ppc_vsx_xxbrw v4i32:$XB)); (set v2i64:$XT, (int_ppc_vsx_xxbrd v2i64:$XB)); (set v1i128:$XT, (int_ppc_vsx_xxbrq v1i128:$XB)). - Vector Permute: xxperm xxpermr; . I have checked ""PPCxxswapd"" in PPCInstrVSX.td, but they are different; . Use intrinsic; (set v16i8:$XT, (int_ppc_vsx_xxperm v16i8:$XA, v16i8:$XB)); (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB)). - Vector Splat Immediate Byte: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $X",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:17845,Performance,load,load,17845,"nt_ppc_vsx_xxbrq v1i128:$XB)). - Vector Permute: xxperm xxpermr; . I have checked ""PPCxxswapd"" in PPCInstrVSX.td, but they are different; . Use intrinsic; (set v16i8:$XT, (int_ppc_vsx_xxperm v16i8:$XA, v16i8:$XB)); (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB)). - Vector Splat Immediate Byte: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). -",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:17885,Performance,load,load,17885,"y are different; . Use intrinsic; (set v16i8:$XT, (int_ppc_vsx_xxperm v16i8:$XA, v16i8:$XB)); (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB)). - Vector Splat Immediate Byte: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (out",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:17945,Performance,Load,Load,17945,"rm v16i8:$XA, v16i8:$XB)); (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB)). - Vector Splat Immediate Byte: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (outs vsrc:$XT), (ins memrr:$src),; ""lxvd2x $XT, $src"", IIC_LdStLFD,; [",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:18149,Performance,load,load,18149,"te: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (outs vsrc:$XT), (ins memrr:$src),; ""lxvd2x $XT, $src"", IIC_LdStLFD,; [(set v2f64:$XT, (int_ppc_vsx_lxvd2x xoaddr:$src))]>;. . (set v8i16:$XT, (int_ppc_vsx_lxvh8x xoaddr:$src)); (set v1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:18189,Performance,load,load,18189,"ltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (outs vsrc:$XT), (ins memrr:$src),; ""lxvd2x $XT, $src"", IIC_LdStLFD,; [(set v2f64:$XT, (int_ppc_vsx_lxvd2x xoaddr:$src))]>;. . (set v8i16:$XT, (int_ppc_vsx_lxvh8x xoaddr:$src)); (set v16i8:$XT, (int_ppc_vsx_lxvb16x xoaddr:$src)). - Store Vector Halfword*8/Byte*16 Indexed: stxvh8x stxvb16x; . Similar to stxv",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:18249,Performance,Load,Load,18249,"are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (outs vsrc:$XT), (ins memrr:$src),; ""lxvd2x $XT, $src"", IIC_LdStLFD,; [(set v2f64:$XT, (int_ppc_vsx_lxvd2x xoaddr:$src))]>;. . (set v8i16:$XT, (int_ppc_vsx_lxvh8x xoaddr:$src)); (set v16i8:$XT, (int_ppc_vsx_lxvb16x xoaddr:$src)). - Store Vector Halfword*8/Byte*16 Indexed: stxvh8x stxvb16x; . Similar to stxvd2x/stxvw4x:; def STXVD2X : XX1Form<31, 972,; (outs), (ins vsrc:$XT, m",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:18786,Performance,Load,Load,18786,", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (outs vsrc:$XT), (ins memrr:$src),; ""lxvd2x $XT, $src"", IIC_LdStLFD,; [(set v2f64:$XT, (int_ppc_vsx_lxvd2x xoaddr:$src))]>;. . (set v8i16:$XT, (int_ppc_vsx_lxvh8x xoaddr:$src)); (set v16i8:$XT, (int_ppc_vsx_lxvb16x xoaddr:$src)). - Store Vector Halfword*8/Byte*16 Indexed: stxvh8x stxvb16x; . Similar to stxvd2x/stxvw4x:; def STXVD2X : XX1Form<31, 972,; (outs), (ins vsrc:$XT, memrr:$dst),; ""stxvd2x $XT, $dst"", IIC_LdStSTFD,; [(store v2f64:$XT, xoaddr:$dst)]>;. . (store v8i16:$XT, xoaddr:$dst); (store v16i8:$XT, xoaddr:$dst). - Load/Store Vector (Left-justified) with Length: lxvl lxvll stxvl stxvll; . Likely needs an intrinsic; . (set v?:$XT, (int_ppc_vsx_lxvl xoaddr:$src)); (set v?:$XT, (int_ppc_vsx_lxvll xoaddr:$src)). . (int_ppc_vsx_stxvl xoaddr:$dst)); (int_ppc_vsx_stxvll xoaddr:$dst)). - Load Vector Word & Splat Indexed: lxvwsx; . Likely needs an intrinsic; . (set v?:$XT, (int_ppc_vsx_lxvwsx xoad",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:19433,Performance,Load,Load,19433,"r:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (outs vsrc:$XT), (ins memrr:$src),; ""lxvd2x $XT, $src"", IIC_LdStLFD,; [(set v2f64:$XT, (int_ppc_vsx_lxvd2x xoaddr:$src))]>;. . (set v8i16:$XT, (int_ppc_vsx_lxvh8x xoaddr:$src)); (set v16i8:$XT, (int_ppc_vsx_lxvb16x xoaddr:$src)). - Store Vector Halfword*8/Byte*16 Indexed: stxvh8x stxvb16x; . Similar to stxvd2x/stxvw4x:; def STXVD2X : XX1Form<31, 972,; (outs), (ins vsrc:$XT, memrr:$dst),; ""stxvd2x $XT, $dst"", IIC_LdStSTFD,; [(store v2f64:$XT, xoaddr:$dst)]>;. . (store v8i16:$XT, xoaddr:$dst); (store v16i8:$XT, xoaddr:$dst). - Load/Store Vector (Left-justified) with Length: lxvl lxvll stxvl stxvll; . Likely needs an intrinsic; . (set v?:$XT, (int_ppc_vsx_lxvl xoaddr:$src)); (set v?:$XT, (int_ppc_vsx_lxvll xoaddr:$src)). . (int_ppc_vsx_stxvl xoaddr:$dst)); (int_ppc_vsx_stxvll xoaddr:$dst)). - Load Vector Word & Splat Indexed: lxvwsx; . Likely needs an intrinsic; . (set v?:$XT, (int_ppc_vsx_lxvwsx xoaddr:$src)). Atomic operations (l[dw]at, st[dw]at):; - Provide custom lowering for common atomic operations to use these; instructions with the correct Function Code; - Ensure the operands are in the correct register (i.e. RT+1, RT+2); - Provide builtins since not all FC's necessarily have an existing LLVM; atomic operation. Move to CR from XER Extended (mcrxrx):; - Is there a use for this in LLVM?. Fixed Point Facility:. - Copy-Paste Facility: copy copy_first cp_abort paste paste. paste_last; . Use instrinstics:; (int_ppc_copy_first i32:$rA, i32:$rB); (int_ppc_copy i32:$rA, i32:$rB). (int_ppc_paste i32:$rA, i32:$rB); (int_ppc_paste_last i32:$rA, i32",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:19703,Performance,Load,Load,19703,"xsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (outs vsrc:$XT), (ins memrr:$src),; ""lxvd2x $XT, $src"", IIC_LdStLFD,; [(set v2f64:$XT, (int_ppc_vsx_lxvd2x xoaddr:$src))]>;. . (set v8i16:$XT, (int_ppc_vsx_lxvh8x xoaddr:$src)); (set v16i8:$XT, (int_ppc_vsx_lxvb16x xoaddr:$src)). - Store Vector Halfword*8/Byte*16 Indexed: stxvh8x stxvb16x; . Similar to stxvd2x/stxvw4x:; def STXVD2X : XX1Form<31, 972,; (outs), (ins vsrc:$XT, memrr:$dst),; ""stxvd2x $XT, $dst"", IIC_LdStSTFD,; [(store v2f64:$XT, xoaddr:$dst)]>;. . (store v8i16:$XT, xoaddr:$dst); (store v16i8:$XT, xoaddr:$dst). - Load/Store Vector (Left-justified) with Length: lxvl lxvll stxvl stxvll; . Likely needs an intrinsic; . (set v?:$XT, (int_ppc_vsx_lxvl xoaddr:$src)); (set v?:$XT, (int_ppc_vsx_lxvll xoaddr:$src)). . (int_ppc_vsx_stxvl xoaddr:$dst)); (int_ppc_vsx_stxvll xoaddr:$dst)). - Load Vector Word & Splat Indexed: lxvwsx; . Likely needs an intrinsic; . (set v?:$XT, (int_ppc_vsx_lxvwsx xoaddr:$src)). Atomic operations (l[dw]at, st[dw]at):; - Provide custom lowering for common atomic operations to use these; instructions with the correct Function Code; - Ensure the operands are in the correct register (i.e. RT+1, RT+2); - Provide builtins since not all FC's necessarily have an existing LLVM; atomic operation. Move to CR from XER Extended (mcrxrx):; - Is there a use for this in LLVM?. Fixed Point Facility:. - Copy-Paste Facility: copy copy_first cp_abort paste paste. paste_last; . Use instrinstics:; (int_ppc_copy_first i32:$rA, i32:$rB); (int_ppc_copy i32:$rA, i32:$rB). (int_ppc_paste i32:$rA, i32:$rB); (int_ppc_paste_last i32:$rA, i32:$rB). (int_cp_abort). - Message Synchronize: msgsync; - SLB*: slbieg slbsync; - stop; . No instrinstics; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:5743,Security,access,accessed,5743,"oned/Signed-QWord:; bcdcfn. bcdcfz. bcdctn. bcdctz. bcdcfsq. bcdctsq.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdcfno v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcdcfzo v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcdctno v1i128:$vB)); (set v1i128:$vD, (int_ppc_altivec_bcdctzo v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcdcfsqo v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcdctsqo v1i128:$vB)). - Decimal Copy-Sign/Set-Sign: bcdcpsgn. bcdsetsgn.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdcpsgno v1i128:$vA, v1i128:$vB)); (set v1i128:$vD, (int_ppc_altivec_bcdsetsgno v1i128:$vB, i1:$PS)). - Decimal Shift/Unsigned-Shift/Shift-and-Round: bcds. bcdus. bcdsr.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdso v1i128:$vA, v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB)); (set v1i128:$vD, (int_ppc_altivec_bcdsro v1i128:$vA, v1i128:$vB, i1:$PS)). . Note! Their VA is accessed only 1 byte, i.e. VA.byte[7]. - Decimal (Unsigned) Truncate: bcdtrunc. bcdutrunc.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdso v1i128:$vA, v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB)). . Note! Their VA is accessed only 2 byte, i.e. VA.hword[3] (VA.bit[48:63]). VSX:; - QP Copy Sign: xscpsgnqp; . Similar to xscpsgndp; . (set f128:$vT, (fcopysign f128:$vB, f128:$vA). - QP Absolute/Negative-Absolute/Negate: xsabsqp xsnabsqp xsnegqp; . Similar to xsabsdp/xsnabsdp/xsnegdp; . (set f128:$vT, (fabs f128:$vB)) // xsabsqp; (set f128:$vT, (fneg (fabs f128:$vB))) // xsnabsqp; (set f128:$vT, (fneg f128:$vB)) // xsnegqp. - QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqp xsdivqp xsmulqp xssubqp xssqrtqp; . Similar to xsadddp; . isCommutable = 1; (set f128:$vT, (fadd f128:$vA, f128:$vB)) // xsaddqp; (set f128:$vT, (fmul f128:$vA, f128:$vB)) // xsmulqp. . isCommutable = 0; (set f128:$vT, (fdiv f128:$vA, f128:$vB)) // xsdivqp; (set f",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:6017,Security,access,accessed,6017,"; (set v1i128:$vD, (int_ppc_altivec_bcdctzo v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcdcfsqo v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcdctsqo v1i128:$vB)). - Decimal Copy-Sign/Set-Sign: bcdcpsgn. bcdsetsgn.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdcpsgno v1i128:$vA, v1i128:$vB)); (set v1i128:$vD, (int_ppc_altivec_bcdsetsgno v1i128:$vB, i1:$PS)). - Decimal Shift/Unsigned-Shift/Shift-and-Round: bcds. bcdus. bcdsr.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdso v1i128:$vA, v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB)); (set v1i128:$vD, (int_ppc_altivec_bcdsro v1i128:$vA, v1i128:$vB, i1:$PS)). . Note! Their VA is accessed only 1 byte, i.e. VA.byte[7]. - Decimal (Unsigned) Truncate: bcdtrunc. bcdutrunc.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdso v1i128:$vA, v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB)). . Note! Their VA is accessed only 2 byte, i.e. VA.hword[3] (VA.bit[48:63]). VSX:; - QP Copy Sign: xscpsgnqp; . Similar to xscpsgndp; . (set f128:$vT, (fcopysign f128:$vB, f128:$vA). - QP Absolute/Negative-Absolute/Negate: xsabsqp xsnabsqp xsnegqp; . Similar to xsabsdp/xsnabsdp/xsnegdp; . (set f128:$vT, (fabs f128:$vB)) // xsabsqp; (set f128:$vT, (fneg (fabs f128:$vB))) // xsnabsqp; (set f128:$vT, (fneg f128:$vB)) // xsnegqp. - QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqp xsdivqp xsmulqp xssubqp xssqrtqp; . Similar to xsadddp; . isCommutable = 1; (set f128:$vT, (fadd f128:$vA, f128:$vB)) // xsaddqp; (set f128:$vT, (fmul f128:$vA, f128:$vB)) // xsmulqp. . isCommutable = 0; (set f128:$vT, (fdiv f128:$vA, f128:$vB)) // xsdivqp; (set f128:$vT, (fsub f128:$vA, f128:$vB)) // xssubqp; (set f128:$vT, (fsqrt f128:$vB))) // xssqrtqp. - Round to Odd of QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqpo xsdivqpo xsmulqpo xssubqpo xssqrtqpo; . Similar to xsrsqrtedp??; def XSRSQRTEDP : XX2Form<60, 74,; (outs v",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:6498,Security,xss,xssubqp,6498,"bcdus. bcdsr.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdso v1i128:$vA, v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB)); (set v1i128:$vD, (int_ppc_altivec_bcdsro v1i128:$vA, v1i128:$vB, i1:$PS)). . Note! Their VA is accessed only 1 byte, i.e. VA.byte[7]. - Decimal (Unsigned) Truncate: bcdtrunc. bcdutrunc.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdso v1i128:$vA, v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB)). . Note! Their VA is accessed only 2 byte, i.e. VA.hword[3] (VA.bit[48:63]). VSX:; - QP Copy Sign: xscpsgnqp; . Similar to xscpsgndp; . (set f128:$vT, (fcopysign f128:$vB, f128:$vA). - QP Absolute/Negative-Absolute/Negate: xsabsqp xsnabsqp xsnegqp; . Similar to xsabsdp/xsnabsdp/xsnegdp; . (set f128:$vT, (fabs f128:$vB)) // xsabsqp; (set f128:$vT, (fneg (fabs f128:$vB))) // xsnabsqp; (set f128:$vT, (fneg f128:$vB)) // xsnegqp. - QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqp xsdivqp xsmulqp xssubqp xssqrtqp; . Similar to xsadddp; . isCommutable = 1; (set f128:$vT, (fadd f128:$vA, f128:$vB)) // xsaddqp; (set f128:$vT, (fmul f128:$vA, f128:$vB)) // xsmulqp. . isCommutable = 0; (set f128:$vT, (fdiv f128:$vA, f128:$vB)) // xsdivqp; (set f128:$vT, (fsub f128:$vA, f128:$vB)) // xssubqp; (set f128:$vT, (fsqrt f128:$vB))) // xssqrtqp. - Round to Odd of QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqpo xsdivqpo xsmulqpo xssubqpo xssqrtqpo; . Similar to xsrsqrtedp??; def XSRSQRTEDP : XX2Form<60, 74,; (outs vsfrc:$XT), (ins vsfrc:$XB),; ""xsrsqrtedp $XT, $XB"", IIC_VecFP,; [(set f64:$XT, (PPCfrsqrte f64:$XB))]>;. . Define DAG Node in PPCInstrInfo.td:; def PPCfaddrto: SDNode<""PPCISD::FADDRTO"", SDTFPBinOp, []>;; def PPCfdivrto: SDNode<""PPCISD::FDIVRTO"", SDTFPBinOp, []>;; def PPCfmulrto: SDNode<""PPCISD::FMULRTO"", SDTFPBinOp, []>;; def PPCfsubrto: SDNode<""PPCISD::FSUBRTO"", SDTFPBinOp, []>;; def PPCfsqrtrto: SDNode<""PPCISD::FSQRTRTO"", SDTFPUnaryOp, []>;. DAG",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:6506,Security,xss,xssqrtqp,6506,"bcdus. bcdsr.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdso v1i128:$vA, v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB)); (set v1i128:$vD, (int_ppc_altivec_bcdsro v1i128:$vA, v1i128:$vB, i1:$PS)). . Note! Their VA is accessed only 1 byte, i.e. VA.byte[7]. - Decimal (Unsigned) Truncate: bcdtrunc. bcdutrunc.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdso v1i128:$vA, v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB)). . Note! Their VA is accessed only 2 byte, i.e. VA.hword[3] (VA.bit[48:63]). VSX:; - QP Copy Sign: xscpsgnqp; . Similar to xscpsgndp; . (set f128:$vT, (fcopysign f128:$vB, f128:$vA). - QP Absolute/Negative-Absolute/Negate: xsabsqp xsnabsqp xsnegqp; . Similar to xsabsdp/xsnabsdp/xsnegdp; . (set f128:$vT, (fabs f128:$vB)) // xsabsqp; (set f128:$vT, (fneg (fabs f128:$vB))) // xsnabsqp; (set f128:$vT, (fneg f128:$vB)) // xsnegqp. - QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqp xsdivqp xsmulqp xssubqp xssqrtqp; . Similar to xsadddp; . isCommutable = 1; (set f128:$vT, (fadd f128:$vA, f128:$vB)) // xsaddqp; (set f128:$vT, (fmul f128:$vA, f128:$vB)) // xsmulqp. . isCommutable = 0; (set f128:$vT, (fdiv f128:$vA, f128:$vB)) // xsdivqp; (set f128:$vT, (fsub f128:$vA, f128:$vB)) // xssubqp; (set f128:$vT, (fsqrt f128:$vB))) // xssqrtqp. - Round to Odd of QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqpo xsdivqpo xsmulqpo xssubqpo xssqrtqpo; . Similar to xsrsqrtedp??; def XSRSQRTEDP : XX2Form<60, 74,; (outs vsfrc:$XT), (ins vsfrc:$XB),; ""xsrsqrtedp $XT, $XB"", IIC_VecFP,; [(set f64:$XT, (PPCfrsqrte f64:$XB))]>;. . Define DAG Node in PPCInstrInfo.td:; def PPCfaddrto: SDNode<""PPCISD::FADDRTO"", SDTFPBinOp, []>;; def PPCfdivrto: SDNode<""PPCISD::FDIVRTO"", SDTFPBinOp, []>;; def PPCfmulrto: SDNode<""PPCISD::FMULRTO"", SDTFPBinOp, []>;; def PPCfsubrto: SDNode<""PPCISD::FSUBRTO"", SDTFPBinOp, []>;; def PPCfsqrtrto: SDNode<""PPCISD::FSQRTRTO"", SDTFPUnaryOp, []>;. DAG",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:6785,Security,xss,xssubqp,6785,"nly 1 byte, i.e. VA.byte[7]. - Decimal (Unsigned) Truncate: bcdtrunc. bcdutrunc.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdso v1i128:$vA, v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB)). . Note! Their VA is accessed only 2 byte, i.e. VA.hword[3] (VA.bit[48:63]). VSX:; - QP Copy Sign: xscpsgnqp; . Similar to xscpsgndp; . (set f128:$vT, (fcopysign f128:$vB, f128:$vA). - QP Absolute/Negative-Absolute/Negate: xsabsqp xsnabsqp xsnegqp; . Similar to xsabsdp/xsnabsdp/xsnegdp; . (set f128:$vT, (fabs f128:$vB)) // xsabsqp; (set f128:$vT, (fneg (fabs f128:$vB))) // xsnabsqp; (set f128:$vT, (fneg f128:$vB)) // xsnegqp. - QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqp xsdivqp xsmulqp xssubqp xssqrtqp; . Similar to xsadddp; . isCommutable = 1; (set f128:$vT, (fadd f128:$vA, f128:$vB)) // xsaddqp; (set f128:$vT, (fmul f128:$vA, f128:$vB)) // xsmulqp. . isCommutable = 0; (set f128:$vT, (fdiv f128:$vA, f128:$vB)) // xsdivqp; (set f128:$vT, (fsub f128:$vA, f128:$vB)) // xssubqp; (set f128:$vT, (fsqrt f128:$vB))) // xssqrtqp. - Round to Odd of QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqpo xsdivqpo xsmulqpo xssubqpo xssqrtqpo; . Similar to xsrsqrtedp??; def XSRSQRTEDP : XX2Form<60, 74,; (outs vsfrc:$XT), (ins vsfrc:$XB),; ""xsrsqrtedp $XT, $XB"", IIC_VecFP,; [(set f64:$XT, (PPCfrsqrte f64:$XB))]>;. . Define DAG Node in PPCInstrInfo.td:; def PPCfaddrto: SDNode<""PPCISD::FADDRTO"", SDTFPBinOp, []>;; def PPCfdivrto: SDNode<""PPCISD::FDIVRTO"", SDTFPBinOp, []>;; def PPCfmulrto: SDNode<""PPCISD::FMULRTO"", SDTFPBinOp, []>;; def PPCfsubrto: SDNode<""PPCISD::FSUBRTO"", SDTFPBinOp, []>;; def PPCfsqrtrto: SDNode<""PPCISD::FSQRTRTO"", SDTFPUnaryOp, []>;. DAG patterns of each instruction (PPCInstrVSX.td):; . isCommutable = 1; (set f128:$vT, (PPCfaddrto f128:$vA, f128:$vB)) // xsaddqpo; (set f128:$vT, (PPCfmulrto f128:$vA, f128:$vB)) // xsmulqpo. . isCommutable = 0; (set f128:$vT, (PPCfdivrto f128:$vA, f128:$vB)) // xsdivqpo; (set f128:$",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:6831,Security,xss,xssqrtqp,6831,"nly 1 byte, i.e. VA.byte[7]. - Decimal (Unsigned) Truncate: bcdtrunc. bcdutrunc.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdso v1i128:$vA, v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB)). . Note! Their VA is accessed only 2 byte, i.e. VA.hword[3] (VA.bit[48:63]). VSX:; - QP Copy Sign: xscpsgnqp; . Similar to xscpsgndp; . (set f128:$vT, (fcopysign f128:$vB, f128:$vA). - QP Absolute/Negative-Absolute/Negate: xsabsqp xsnabsqp xsnegqp; . Similar to xsabsdp/xsnabsdp/xsnegdp; . (set f128:$vT, (fabs f128:$vB)) // xsabsqp; (set f128:$vT, (fneg (fabs f128:$vB))) // xsnabsqp; (set f128:$vT, (fneg f128:$vB)) // xsnegqp. - QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqp xsdivqp xsmulqp xssubqp xssqrtqp; . Similar to xsadddp; . isCommutable = 1; (set f128:$vT, (fadd f128:$vA, f128:$vB)) // xsaddqp; (set f128:$vT, (fmul f128:$vA, f128:$vB)) // xsmulqp. . isCommutable = 0; (set f128:$vT, (fdiv f128:$vA, f128:$vB)) // xsdivqp; (set f128:$vT, (fsub f128:$vA, f128:$vB)) // xssubqp; (set f128:$vT, (fsqrt f128:$vB))) // xssqrtqp. - Round to Odd of QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqpo xsdivqpo xsmulqpo xssubqpo xssqrtqpo; . Similar to xsrsqrtedp??; def XSRSQRTEDP : XX2Form<60, 74,; (outs vsfrc:$XT), (ins vsfrc:$XB),; ""xsrsqrtedp $XT, $XB"", IIC_VecFP,; [(set f64:$XT, (PPCfrsqrte f64:$XB))]>;. . Define DAG Node in PPCInstrInfo.td:; def PPCfaddrto: SDNode<""PPCISD::FADDRTO"", SDTFPBinOp, []>;; def PPCfdivrto: SDNode<""PPCISD::FDIVRTO"", SDTFPBinOp, []>;; def PPCfmulrto: SDNode<""PPCISD::FMULRTO"", SDTFPBinOp, []>;; def PPCfsubrto: SDNode<""PPCISD::FSUBRTO"", SDTFPBinOp, []>;; def PPCfsqrtrto: SDNode<""PPCISD::FSQRTRTO"", SDTFPUnaryOp, []>;. DAG patterns of each instruction (PPCInstrVSX.td):; . isCommutable = 1; (set f128:$vT, (PPCfaddrto f128:$vA, f128:$vB)) // xsaddqpo; (set f128:$vT, (PPCfmulrto f128:$vA, f128:$vB)) // xsmulqpo. . isCommutable = 0; (set f128:$vT, (PPCfdivrto f128:$vA, f128:$vB)) // xsdivqpo; (set f128:$",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:6932,Security,xss,xssubqpo,6932,"v1i128:$vA, v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB)). . Note! Their VA is accessed only 2 byte, i.e. VA.hword[3] (VA.bit[48:63]). VSX:; - QP Copy Sign: xscpsgnqp; . Similar to xscpsgndp; . (set f128:$vT, (fcopysign f128:$vB, f128:$vA). - QP Absolute/Negative-Absolute/Negate: xsabsqp xsnabsqp xsnegqp; . Similar to xsabsdp/xsnabsdp/xsnegdp; . (set f128:$vT, (fabs f128:$vB)) // xsabsqp; (set f128:$vT, (fneg (fabs f128:$vB))) // xsnabsqp; (set f128:$vT, (fneg f128:$vB)) // xsnegqp. - QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqp xsdivqp xsmulqp xssubqp xssqrtqp; . Similar to xsadddp; . isCommutable = 1; (set f128:$vT, (fadd f128:$vA, f128:$vB)) // xsaddqp; (set f128:$vT, (fmul f128:$vA, f128:$vB)) // xsmulqp. . isCommutable = 0; (set f128:$vT, (fdiv f128:$vA, f128:$vB)) // xsdivqp; (set f128:$vT, (fsub f128:$vA, f128:$vB)) // xssubqp; (set f128:$vT, (fsqrt f128:$vB))) // xssqrtqp. - Round to Odd of QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqpo xsdivqpo xsmulqpo xssubqpo xssqrtqpo; . Similar to xsrsqrtedp??; def XSRSQRTEDP : XX2Form<60, 74,; (outs vsfrc:$XT), (ins vsfrc:$XB),; ""xsrsqrtedp $XT, $XB"", IIC_VecFP,; [(set f64:$XT, (PPCfrsqrte f64:$XB))]>;. . Define DAG Node in PPCInstrInfo.td:; def PPCfaddrto: SDNode<""PPCISD::FADDRTO"", SDTFPBinOp, []>;; def PPCfdivrto: SDNode<""PPCISD::FDIVRTO"", SDTFPBinOp, []>;; def PPCfmulrto: SDNode<""PPCISD::FMULRTO"", SDTFPBinOp, []>;; def PPCfsubrto: SDNode<""PPCISD::FSUBRTO"", SDTFPBinOp, []>;; def PPCfsqrtrto: SDNode<""PPCISD::FSQRTRTO"", SDTFPUnaryOp, []>;. DAG patterns of each instruction (PPCInstrVSX.td):; . isCommutable = 1; (set f128:$vT, (PPCfaddrto f128:$vA, f128:$vB)) // xsaddqpo; (set f128:$vT, (PPCfmulrto f128:$vA, f128:$vB)) // xsmulqpo. . isCommutable = 0; (set f128:$vT, (PPCfdivrto f128:$vA, f128:$vB)) // xsdivqpo; (set f128:$vT, (PPCfsubrto f128:$vA, f128:$vB)) // xssubqpo; (set f128:$vT, (PPCfsqrtrto f128:$vB)) // xssqrtqpo. - QP (Negative) Multiply-{Add/Subtract}:",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:6941,Security,xss,xssqrtqpo,6941,"v1i128:$vA, v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB)). . Note! Their VA is accessed only 2 byte, i.e. VA.hword[3] (VA.bit[48:63]). VSX:; - QP Copy Sign: xscpsgnqp; . Similar to xscpsgndp; . (set f128:$vT, (fcopysign f128:$vB, f128:$vA). - QP Absolute/Negative-Absolute/Negate: xsabsqp xsnabsqp xsnegqp; . Similar to xsabsdp/xsnabsdp/xsnegdp; . (set f128:$vT, (fabs f128:$vB)) // xsabsqp; (set f128:$vT, (fneg (fabs f128:$vB))) // xsnabsqp; (set f128:$vT, (fneg f128:$vB)) // xsnegqp. - QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqp xsdivqp xsmulqp xssubqp xssqrtqp; . Similar to xsadddp; . isCommutable = 1; (set f128:$vT, (fadd f128:$vA, f128:$vB)) // xsaddqp; (set f128:$vT, (fmul f128:$vA, f128:$vB)) // xsmulqp. . isCommutable = 0; (set f128:$vT, (fdiv f128:$vA, f128:$vB)) // xsdivqp; (set f128:$vT, (fsub f128:$vA, f128:$vB)) // xssubqp; (set f128:$vT, (fsqrt f128:$vB))) // xssqrtqp. - Round to Odd of QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqpo xsdivqpo xsmulqpo xssubqpo xssqrtqpo; . Similar to xsrsqrtedp??; def XSRSQRTEDP : XX2Form<60, 74,; (outs vsfrc:$XT), (ins vsfrc:$XB),; ""xsrsqrtedp $XT, $XB"", IIC_VecFP,; [(set f64:$XT, (PPCfrsqrte f64:$XB))]>;. . Define DAG Node in PPCInstrInfo.td:; def PPCfaddrto: SDNode<""PPCISD::FADDRTO"", SDTFPBinOp, []>;; def PPCfdivrto: SDNode<""PPCISD::FDIVRTO"", SDTFPBinOp, []>;; def PPCfmulrto: SDNode<""PPCISD::FMULRTO"", SDTFPBinOp, []>;; def PPCfsubrto: SDNode<""PPCISD::FSUBRTO"", SDTFPBinOp, []>;; def PPCfsqrtrto: SDNode<""PPCISD::FSQRTRTO"", SDTFPUnaryOp, []>;. DAG patterns of each instruction (PPCInstrVSX.td):; . isCommutable = 1; (set f128:$vT, (PPCfaddrto f128:$vA, f128:$vB)) // xsaddqpo; (set f128:$vT, (PPCfmulrto f128:$vA, f128:$vB)) // xsmulqpo. . isCommutable = 0; (set f128:$vT, (PPCfdivrto f128:$vA, f128:$vB)) // xsdivqpo; (set f128:$vT, (PPCfsubrto f128:$vA, f128:$vB)) // xssubqpo; (set f128:$vT, (PPCfsqrtrto f128:$vB)) // xssqrtqpo. - QP (Negative) Multiply-{Add/Subtract}:",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:7794,Security,xss,xssubqpo,7794,"b f128:$vA, f128:$vB)) // xssubqp; (set f128:$vT, (fsqrt f128:$vB))) // xssqrtqp. - Round to Odd of QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqpo xsdivqpo xsmulqpo xssubqpo xssqrtqpo; . Similar to xsrsqrtedp??; def XSRSQRTEDP : XX2Form<60, 74,; (outs vsfrc:$XT), (ins vsfrc:$XB),; ""xsrsqrtedp $XT, $XB"", IIC_VecFP,; [(set f64:$XT, (PPCfrsqrte f64:$XB))]>;. . Define DAG Node in PPCInstrInfo.td:; def PPCfaddrto: SDNode<""PPCISD::FADDRTO"", SDTFPBinOp, []>;; def PPCfdivrto: SDNode<""PPCISD::FDIVRTO"", SDTFPBinOp, []>;; def PPCfmulrto: SDNode<""PPCISD::FMULRTO"", SDTFPBinOp, []>;; def PPCfsubrto: SDNode<""PPCISD::FSUBRTO"", SDTFPBinOp, []>;; def PPCfsqrtrto: SDNode<""PPCISD::FSQRTRTO"", SDTFPUnaryOp, []>;. DAG patterns of each instruction (PPCInstrVSX.td):; . isCommutable = 1; (set f128:$vT, (PPCfaddrto f128:$vA, f128:$vB)) // xsaddqpo; (set f128:$vT, (PPCfmulrto f128:$vA, f128:$vB)) // xsmulqpo. . isCommutable = 0; (set f128:$vT, (PPCfdivrto f128:$vA, f128:$vB)) // xsdivqpo; (set f128:$vT, (PPCfsubrto f128:$vA, f128:$vB)) // xssubqpo; (set f128:$vT, (PPCfsqrtrto f128:$vB)) // xssqrtqpo. - QP (Negative) Multiply-{Add/Subtract}: xsmaddqp xsmsubqp xsnmaddqp xsnmsubqp; . Ref: xsmaddadp/xsmsubadp/xsnmaddadp/xsnmsubadp. . isCommutable = 1; // xsmaddqp; [(set f128:$vT, (fma f128:$vA, f128:$vB, f128:$vTi))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsmsubqp; [(set f128:$vT, (fma f128:$vA, f128:$vB, (fneg f128:$vTi)))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsnmaddqp; [(set f128:$vT, (fneg (fma f128:$vA, f128:$vB, f128:$vTi)))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsnmsubqp; [(set f128:$vT, (fneg (fma f128:$vA, f128:$vB, (fneg f128:$vTi))))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. - Round to Odd of QP (Negative) Multiply-{Add/Subtract}:; xsmaddqpo xsmsubqpo xsnmaddqpo xsnmsubqpo; . Similar to xsrsqrtedp??. . Define DAG Node in PPCInstrInfo.td:; def PPCfmarto: SDNode<""P",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:7846,Security,xss,xssqrtqpo,7846,"b f128:$vA, f128:$vB)) // xssubqp; (set f128:$vT, (fsqrt f128:$vB))) // xssqrtqp. - Round to Odd of QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqpo xsdivqpo xsmulqpo xssubqpo xssqrtqpo; . Similar to xsrsqrtedp??; def XSRSQRTEDP : XX2Form<60, 74,; (outs vsfrc:$XT), (ins vsfrc:$XB),; ""xsrsqrtedp $XT, $XB"", IIC_VecFP,; [(set f64:$XT, (PPCfrsqrte f64:$XB))]>;. . Define DAG Node in PPCInstrInfo.td:; def PPCfaddrto: SDNode<""PPCISD::FADDRTO"", SDTFPBinOp, []>;; def PPCfdivrto: SDNode<""PPCISD::FDIVRTO"", SDTFPBinOp, []>;; def PPCfmulrto: SDNode<""PPCISD::FMULRTO"", SDTFPBinOp, []>;; def PPCfsubrto: SDNode<""PPCISD::FSUBRTO"", SDTFPBinOp, []>;; def PPCfsqrtrto: SDNode<""PPCISD::FSQRTRTO"", SDTFPUnaryOp, []>;. DAG patterns of each instruction (PPCInstrVSX.td):; . isCommutable = 1; (set f128:$vT, (PPCfaddrto f128:$vA, f128:$vB)) // xsaddqpo; (set f128:$vT, (PPCfmulrto f128:$vA, f128:$vB)) // xsmulqpo. . isCommutable = 0; (set f128:$vT, (PPCfdivrto f128:$vA, f128:$vB)) // xsdivqpo; (set f128:$vT, (PPCfsubrto f128:$vA, f128:$vB)) // xssubqpo; (set f128:$vT, (PPCfsqrtrto f128:$vB)) // xssqrtqpo. - QP (Negative) Multiply-{Add/Subtract}: xsmaddqp xsmsubqp xsnmaddqp xsnmsubqp; . Ref: xsmaddadp/xsmsubadp/xsnmaddadp/xsnmsubadp. . isCommutable = 1; // xsmaddqp; [(set f128:$vT, (fma f128:$vA, f128:$vB, f128:$vTi))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsmsubqp; [(set f128:$vT, (fma f128:$vA, f128:$vB, (fneg f128:$vTi)))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsnmaddqp; [(set f128:$vT, (fneg (fma f128:$vA, f128:$vB, f128:$vTi)))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsnmsubqp; [(set f128:$vT, (fneg (fma f128:$vA, f128:$vB, (fneg f128:$vTi))))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. - Round to Odd of QP (Negative) Multiply-{Add/Subtract}:; xsmaddqpo xsmsubqpo xsnmaddqpo xsnmsubqpo; . Similar to xsrsqrtedp??. . Define DAG Node in PPCInstrInfo.td:; def PPCfmarto: SDNode<""P",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:13253,Testability,test,testing,13253,"64:$XB)) // xscvudqp. - (Round &) Convert DP <-> HP: xscvdphp xscvhpdp; . Similar to XSCVDPSP; . No SDAG, intrinsic, builtin are required??. - Vector HP -> SP: xvcvhpsp xvcvsphp; . Similar to XVCVDPSP:; def XVCVDPSP : XX2Form<60, 393,; (outs vsrc:$XT), (ins vsrc:$XB),; ""xvcvdpsp $XT, $XB"", IIC_VecFP, []>;; . No SDAG, intrinsic, builtin are required??. - Round to Quad-Precision Integer: xsrqpi xsrqpix; . These are combination of ""XSRDPI"", ""XSRDPIC"", ""XSRDPIM"", .., because you; need to assign rounding mode in instruction; . Provide builtin?; (set f128:$vT, (int_ppc_vsx_xsrqpi f128:$vB)); (set f128:$vT, (int_ppc_vsx_xsrqpix f128:$vB)). - Round Quad-Precision to Double-Extended Precision (fp80): xsrqpxp; . Provide builtin?; (set f128:$vT, (int_ppc_vsx_xsrqpxp f128:$vB)). Fixed Point Facility:. - Exploit cmprb and cmpeqb (perhaps for something like; isalpha/isdigit/isupper/islower and isspace respectivelly). This can; perhaps be done through a builtin. - Provide testing for cnttz[dw]; - Insert Exponent DP/QP: xsiexpdp xsiexpqp; . Use intrinsic?; . xsiexpdp:; // Note: rA and rB are the unsigned integer value.; (set f128:$XT, (int_ppc_vsx_xsiexpdp i64:$rA, i64:$rB)). . xsiexpqp:; (set f128:$vT, (int_ppc_vsx_xsiexpqp f128:$vA, f64:$vB)). - Extract Exponent/Significand DP/QP: xsxexpdp xsxsigdp xsxexpqp xsxsigqp; . Use intrinsic?; . (set i64:$rT, (int_ppc_vsx_xsxexpdp f64$XB)) // xsxexpdp; (set i64:$rT, (int_ppc_vsx_xsxsigdp f64$XB)) // xsxsigdp; (set f128:$vT, (int_ppc_vsx_xsxexpqp f128$vB)) // xsxexpqp; (set f128:$vT, (int_ppc_vsx_xsxsigqp f128$vB)) // xsxsigqp. - Vector Insert Word: xxinsertw; - Useful for inserting f32/i32 elements into vectors (the element to be; inserted needs to be prepared); . Note: llvm has insertelem in ""Vector Operations""; ; yields <n x <ty>>; <result> = insertelement <n x <ty>> <val>, <ty> <elt>, <ty2> <idx>. But how to map to it??; [(set v1f128:$XT, (insertelement v1f128:$XTi, f128:$XB, i4:$UIMM))]>,; RegConstraint<""$XTi = $XT"">, NoEncode<""$XTi"">,",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:15446,Testability,Test,Test,15446,"he current pattern is better -; shift->convert); - It is useful for (uint_to_fp (vector_extract v4i32, N)); - Unfortunately, it can't be used for (sint_to_fp (vector_extract v4i32, N)); . Note: llvm has extractelement in ""Vector Operations""; ; yields <ty>; <result> = extractelement <n x <ty>> <val>, <ty2> <idx>. How to map to it??; [(set f128:$XT, (extractelement v1f128:$XB, i4:$UIMM))]. . Or use intrinsic?; (set f128:$XT, (int_ppc_vsx_xxextractuw v1f128:$XB, i4:$UIMM)). - Vector Insert Exponent DP/SP: xviexpdp xviexpsp; . Use intrinsic; (set v2f64:$XT, (int_ppc_vsx_xviexpdp v2f64:$XA, v2f64:$XB)); (set v4f32:$XT, (int_ppc_vsx_xviexpsp v4f32:$XA, v4f32:$XB)). - Vector Extract Exponent/Significand DP/SP: xvxexpdp xvxexpsp xvxsigdp xvxsigsp; . Use intrinsic; (set v2f64:$XT, (int_ppc_vsx_xvxexpdp v2f64:$XB)); (set v4f32:$XT, (int_ppc_vsx_xvxexpsp v4f32:$XB)); (set v2f64:$XT, (int_ppc_vsx_xvxsigdp v2f64:$XB)); (set v4f32:$XT, (int_ppc_vsx_xvxsigsp v4f32:$XB)). - Test Data Class SP/DP/QP: xststdcsp xststdcdp xststdcqp; . No SDAG, intrinsic, builtin are required?; Because it seems that we have no way to map BF field?. Instruction Form: [PO T XO B XO BX TX]; Asm: xststd* BF,XB,DCMX. BF is an index to CR register field. - Vector Test Data Class SP/DP: xvtstdcsp xvtstdcdp; . Use intrinsic; (set v4f32:$XT, (int_ppc_vsx_xvtstdcsp v4f32:$XB, i7:$DCMX)); (set v2f64:$XT, (int_ppc_vsx_xvtstdcdp v2f64:$XB, i7:$DCMX)). - Maximum/Minimum Type-C/Type-J DP: xsmaxcdp xsmaxjdp xsmincdp xsminjdp; . PowerISA_V3.0:; ""xsmaxcdp can be used to implement the C/C++/Java conditional operation; (x>y)?x:y for single-precision and double-precision arguments."". Note! c type and j type have different behavior when:; 1. Either input is NaN; 2. Both input are +-Infinity, +-Zero. . dtype map to llvm fmaxnum/fminnum; jtype use intrinsic. . xsmaxcdp xsmincdp; (set f64:$XT, (fmaxnum f64:$XA, f64:$XB)); (set f64:$XT, (fminnum f64:$XA, f64:$XB)). . xsmaxjdp xsminjdp; (set f64:$XT, (int_ppc_vsx_xsmaxjdp f64:$XA",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:15714,Testability,Test,Test,15714,"result> = extractelement <n x <ty>> <val>, <ty2> <idx>. How to map to it??; [(set f128:$XT, (extractelement v1f128:$XB, i4:$UIMM))]. . Or use intrinsic?; (set f128:$XT, (int_ppc_vsx_xxextractuw v1f128:$XB, i4:$UIMM)). - Vector Insert Exponent DP/SP: xviexpdp xviexpsp; . Use intrinsic; (set v2f64:$XT, (int_ppc_vsx_xviexpdp v2f64:$XA, v2f64:$XB)); (set v4f32:$XT, (int_ppc_vsx_xviexpsp v4f32:$XA, v4f32:$XB)). - Vector Extract Exponent/Significand DP/SP: xvxexpdp xvxexpsp xvxsigdp xvxsigsp; . Use intrinsic; (set v2f64:$XT, (int_ppc_vsx_xvxexpdp v2f64:$XB)); (set v4f32:$XT, (int_ppc_vsx_xvxexpsp v4f32:$XB)); (set v2f64:$XT, (int_ppc_vsx_xvxsigdp v2f64:$XB)); (set v4f32:$XT, (int_ppc_vsx_xvxsigsp v4f32:$XB)). - Test Data Class SP/DP/QP: xststdcsp xststdcdp xststdcqp; . No SDAG, intrinsic, builtin are required?; Because it seems that we have no way to map BF field?. Instruction Form: [PO T XO B XO BX TX]; Asm: xststd* BF,XB,DCMX. BF is an index to CR register field. - Vector Test Data Class SP/DP: xvtstdcsp xvtstdcdp; . Use intrinsic; (set v4f32:$XT, (int_ppc_vsx_xvtstdcsp v4f32:$XB, i7:$DCMX)); (set v2f64:$XT, (int_ppc_vsx_xvtstdcdp v2f64:$XB, i7:$DCMX)). - Maximum/Minimum Type-C/Type-J DP: xsmaxcdp xsmaxjdp xsmincdp xsminjdp; . PowerISA_V3.0:; ""xsmaxcdp can be used to implement the C/C++/Java conditional operation; (x>y)?x:y for single-precision and double-precision arguments."". Note! c type and j type have different behavior when:; 1. Either input is NaN; 2. Both input are +-Infinity, +-Zero. . dtype map to llvm fmaxnum/fminnum; jtype use intrinsic. . xsmaxcdp xsmincdp; (set f64:$XT, (fmaxnum f64:$XA, f64:$XB)); (set f64:$XT, (fminnum f64:$XA, f64:$XB)). . xsmaxjdp xsminjdp; (set f64:$XT, (int_ppc_vsx_xsmaxjdp f64:$XA, f64:$XB)); (set f64:$XT, (int_ppc_vsx_xsminjdp f64:$XA, f64:$XB)). - Vector Byte-Reverse H/W/D/Q Word: xxbrh xxbrw xxbrd xxbrq; . Use intrinsic; (set v8i16:$XT, (int_ppc_vsx_xxbrh v8i16:$XB)); (set v4i32:$XT, (int_ppc_vsx_xxbrw v4i32:$XB)); (set v2i64:$XT,",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt:249,Energy Efficiency,allocate,allocate,249,"To-do; -----. * Keep the address of the constant pool in a register instead of forming its; address all of the time.; * We can fold small constant offsets into the %hi/%lo references to constant; pool addresses as well.; * When in V9 mode, register allocate %icc[0-3].; * Add support for isel'ing UMUL_LOHI instead of marking it as Expand.; * Emit the 'Branch on Integer Register with Prediction' instructions. It's; not clear how to write a pattern for this though:. float %t1(int %a, int* %p) {; %C = seteq int %a, 0; br bool %C, label %T, label %F; T:; store int 123, int* %p; br label %F; F:; ret float undef; }. codegens to this:. t1:; save -96, %o6, %o6; 1) subcc %i0, 0, %l0; 1) bne .LBBt1_2 ! F; nop; .LBBt1_1: ! T; or %g0, 123, %l0; st %l0, [%i1]; .LBBt1_2: ! F; restore %g0, %g0, %g0; retl; nop. 1) should be replaced with a brz in V9 mode. * Same as above, but emit conditional move on register zero (p192) in V9; mode. Testcase:. int %t1(int %a, int %b) {; %C = seteq int %a, 0; %D = select bool %C, int %a, int %b; ret int %D; }. * Emit MULX/[SU]DIVX instructions in V9 mode instead of fiddling; with the Y register, if they are faster. * Codegen bswap(load)/store(bswap) -> load/store ASI. * Implement frame pointer elimination, e.g. eliminate save/restore for; leaf fns.; * Fill delay slots. * Use %g0 directly to materialize 0. No instruction is required.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt:1166,Performance,load,load,1166,"To-do; -----. * Keep the address of the constant pool in a register instead of forming its; address all of the time.; * We can fold small constant offsets into the %hi/%lo references to constant; pool addresses as well.; * When in V9 mode, register allocate %icc[0-3].; * Add support for isel'ing UMUL_LOHI instead of marking it as Expand.; * Emit the 'Branch on Integer Register with Prediction' instructions. It's; not clear how to write a pattern for this though:. float %t1(int %a, int* %p) {; %C = seteq int %a, 0; br bool %C, label %T, label %F; T:; store int 123, int* %p; br label %F; F:; ret float undef; }. codegens to this:. t1:; save -96, %o6, %o6; 1) subcc %i0, 0, %l0; 1) bne .LBBt1_2 ! F; nop; .LBBt1_1: ! T; or %g0, 123, %l0; st %l0, [%i1]; .LBBt1_2: ! F; restore %g0, %g0, %g0; retl; nop. 1) should be replaced with a brz in V9 mode. * Same as above, but emit conditional move on register zero (p192) in V9; mode. Testcase:. int %t1(int %a, int %b) {; %C = seteq int %a, 0; %D = select bool %C, int %a, int %b; ret int %D; }. * Emit MULX/[SU]DIVX instructions in V9 mode instead of fiddling; with the Y register, if they are faster. * Codegen bswap(load)/store(bswap) -> load/store ASI. * Implement frame pointer elimination, e.g. eliminate save/restore for; leaf fns.; * Fill delay slots. * Use %g0 directly to materialize 0. No instruction is required.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt:1188,Performance,load,load,1188,"To-do; -----. * Keep the address of the constant pool in a register instead of forming its; address all of the time.; * We can fold small constant offsets into the %hi/%lo references to constant; pool addresses as well.; * When in V9 mode, register allocate %icc[0-3].; * Add support for isel'ing UMUL_LOHI instead of marking it as Expand.; * Emit the 'Branch on Integer Register with Prediction' instructions. It's; not clear how to write a pattern for this though:. float %t1(int %a, int* %p) {; %C = seteq int %a, 0; br bool %C, label %T, label %F; T:; store int 123, int* %p; br label %F; F:; ret float undef; }. codegens to this:. t1:; save -96, %o6, %o6; 1) subcc %i0, 0, %l0; 1) bne .LBBt1_2 ! F; nop; .LBBt1_1: ! T; or %g0, 123, %l0; st %l0, [%i1]; .LBBt1_2: ! F; restore %g0, %g0, %g0; retl; nop. 1) should be replaced with a brz in V9 mode. * Same as above, but emit conditional move on register zero (p192) in V9; mode. Testcase:. int %t1(int %a, int %b) {; %C = seteq int %a, 0; %D = select bool %C, int %a, int %b; ret int %D; }. * Emit MULX/[SU]DIVX instructions in V9 mode instead of fiddling; with the Y register, if they are faster. * Codegen bswap(load)/store(bswap) -> load/store ASI. * Implement frame pointer elimination, e.g. eliminate save/restore for; leaf fns.; * Fill delay slots. * Use %g0 directly to materialize 0. No instruction is required.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt:385,Safety,Predict,Prediction,385,"To-do; -----. * Keep the address of the constant pool in a register instead of forming its; address all of the time.; * We can fold small constant offsets into the %hi/%lo references to constant; pool addresses as well.; * When in V9 mode, register allocate %icc[0-3].; * Add support for isel'ing UMUL_LOHI instead of marking it as Expand.; * Emit the 'Branch on Integer Register with Prediction' instructions. It's; not clear how to write a pattern for this though:. float %t1(int %a, int* %p) {; %C = seteq int %a, 0; br bool %C, label %T, label %F; T:; store int 123, int* %p; br label %F; F:; ret float undef; }. codegens to this:. t1:; save -96, %o6, %o6; 1) subcc %i0, 0, %l0; 1) bne .LBBt1_2 ! F; nop; .LBBt1_1: ! T; or %g0, 123, %l0; st %l0, [%i1]; .LBBt1_2: ! F; restore %g0, %g0, %g0; retl; nop. 1) should be replaced with a brz in V9 mode. * Same as above, but emit conditional move on register zero (p192) in V9; mode. Testcase:. int %t1(int %a, int %b) {; %C = seteq int %a, 0; %D = select bool %C, int %a, int %b; ret int %D; }. * Emit MULX/[SU]DIVX instructions in V9 mode instead of fiddling; with the Y register, if they are faster. * Codegen bswap(load)/store(bswap) -> load/store ASI. * Implement frame pointer elimination, e.g. eliminate save/restore for; leaf fns.; * Fill delay slots. * Use %g0 directly to materialize 0. No instruction is required.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt:931,Testability,Test,Testcase,931,"To-do; -----. * Keep the address of the constant pool in a register instead of forming its; address all of the time.; * We can fold small constant offsets into the %hi/%lo references to constant; pool addresses as well.; * When in V9 mode, register allocate %icc[0-3].; * Add support for isel'ing UMUL_LOHI instead of marking it as Expand.; * Emit the 'Branch on Integer Register with Prediction' instructions. It's; not clear how to write a pattern for this though:. float %t1(int %a, int* %p) {; %C = seteq int %a, 0; br bool %C, label %T, label %F; T:; store int 123, int* %p; br label %F; F:; ret float undef; }. codegens to this:. t1:; save -96, %o6, %o6; 1) subcc %i0, 0, %l0; 1) bne .LBBt1_2 ! F; nop; .LBBt1_1: ! T; or %g0, 123, %l0; st %l0, [%i1]; .LBBt1_2: ! F; restore %g0, %g0, %g0; retl; nop. 1) should be replaced with a brz in V9 mode. * Same as above, but emit conditional move on register zero (p192) in V9; mode. Testcase:. int %t1(int %a, int %b) {; %C = seteq int %a, 0; %D = select bool %C, int %a, int %b; ret int %D; }. * Emit MULX/[SU]DIVX instructions in V9 mode instead of fiddling; with the Y register, if they are faster. * Codegen bswap(load)/store(bswap) -> load/store ASI. * Implement frame pointer elimination, e.g. eliminate save/restore for; leaf fns.; * Fill delay slots. * Use %g0 directly to materialize 0. No instruction is required.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt:421,Usability,clear,clear,421,"To-do; -----. * Keep the address of the constant pool in a register instead of forming its; address all of the time.; * We can fold small constant offsets into the %hi/%lo references to constant; pool addresses as well.; * When in V9 mode, register allocate %icc[0-3].; * Add support for isel'ing UMUL_LOHI instead of marking it as Expand.; * Emit the 'Branch on Integer Register with Prediction' instructions. It's; not clear how to write a pattern for this though:. float %t1(int %a, int* %p) {; %C = seteq int %a, 0; br bool %C, label %T, label %F; T:; store int 123, int* %p; br label %F; F:; ret float undef; }. codegens to this:. t1:; save -96, %o6, %o6; 1) subcc %i0, 0, %l0; 1) bne .LBBt1_2 ! F; nop; .LBBt1_1: ! T; or %g0, 123, %l0; st %l0, [%i1]; .LBBt1_2: ! F; restore %g0, %g0, %g0; retl; nop. 1) should be replaced with a brz in V9 mode. * Same as above, but emit conditional move on register zero (p192) in V9; mode. Testcase:. int %t1(int %a, int %b) {; %C = seteq int %a, 0; %D = select bool %C, int %a, int %b; ret int %D; }. * Emit MULX/[SU]DIVX instructions in V9 mode instead of fiddling; with the Y register, if they are faster. * Codegen bswap(load)/store(bswap) -> load/store ASI. * Implement frame pointer elimination, e.g. eliminate save/restore for; leaf fns.; * Fill delay slots. * Use %g0 directly to materialize 0. No instruction is required.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:920,Energy Efficiency,schedul,scheduling,920,"//===---------------------------------------------------------------------===//; // Random notes about and ideas for the SystemZ backend.; //===---------------------------------------------------------------------===//. The initial backend is deliberately restricted to z10. We should add support; for later architectures at some point. --. If an inline asm ties an i32 ""r"" result to an i64 input, the input; will be treated as an i32, leaving the upper bits uninitialised.; For example:. define void @f4(i32 *%dst) {; %val = call i32 asm ""blah $0"", ""=r,0"" (i64 103); store i32 %val, i32 *%dst; ret void; }. from CodeGen/SystemZ/asm-09.ll will use LHI rather than LGHI.; to load 103. This seems to be a general target-independent problem. --. The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet f",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:3254,Energy Efficiency,allocate,allocate,3254,"use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG. --. We might want to model all access registers and use them to spill; 32-bit values. --. We might want to use the 'overflow' condition of eg. AR to support; llvm.sadd.with.overflow.i32 and related instructions - the generated code; for signed overflow check is currently quite bad. This would improve; the results of using -ftrapv.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:1071,Modifiability,extend,extend,1071,"and ideas for the SystemZ backend.; //===---------------------------------------------------------------------===//. The initial backend is deliberately restricted to z10. We should add support; for later architectures at some point. --. If an inline asm ties an i32 ""r"" result to an i64 input, the input; will be treated as an i32, leaving the upper bits uninitialised.; For example:. define void @f4(i32 *%dst) {; %val = call i32 asm ""blah $0"", ""=r,0"" (i64 103); store i32 %val, i32 *%dst; ret void; }. from CodeGen/SystemZ/asm-09.ll will use LHI rather than LGHI.; to load 103. This seems to be a general target-independent problem. --. The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:1086,Modifiability,variab,variable-length,1086,"and ideas for the SystemZ backend.; //===---------------------------------------------------------------------===//. The initial backend is deliberately restricted to z10. We should add support; for later architectures at some point. --. If an inline asm ties an i32 ""r"" result to an i64 input, the input; will be treated as an i32, leaving the upper bits uninitialised.; For example:. define void @f4(i32 *%dst) {; %val = call i32 asm ""blah $0"", ""=r,0"" (i64 103); store i32 %val, i32 *%dst; ret void; }. from CodeGen/SystemZ/asm-09.ll will use LHI rather than LGHI.; to load 103. This seems to be a general target-independent problem. --. The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2020,Modifiability,extend,extended,2020,"e MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:674,Performance,load,load,674,"//===---------------------------------------------------------------------===//; // Random notes about and ideas for the SystemZ backend.; //===---------------------------------------------------------------------===//. The initial backend is deliberately restricted to z10. We should add support; for later architectures at some point. --. If an inline asm ties an i32 ""r"" result to an i64 input, the input; will be treated as an i32, leaving the upper bits uninitialised.; For example:. define void @f4(i32 *%dst) {; %val = call i32 asm ""blah $0"", ""=r,0"" (i64 103); store i32 %val, i32 *%dst; ret void; }. from CodeGen/SystemZ/asm-09.ll will use LHI rather than LGHI.; to load 103. This seems to be a general target-independent problem. --. The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet f",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:776,Performance,LOAD,LOAD,776,"//===---------------------------------------------------------------------===//; // Random notes about and ideas for the SystemZ backend.; //===---------------------------------------------------------------------===//. The initial backend is deliberately restricted to z10. We should add support; for later architectures at some point. --. If an inline asm ties an i32 ""r"" result to an i64 input, the input; will be treated as an i32, leaving the upper bits uninitialised.; For example:. define void @f4(i32 *%dst) {; %val = call i32 asm ""blah $0"", ""=r,0"" (i64 103); store i32 %val, i32 *%dst; ret void; }. from CodeGen/SystemZ/asm-09.ll will use LHI rather than LGHI.; to load 103. This seems to be a general target-independent problem. --. The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet f",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:878,Performance,perform,performance,878,"//===---------------------------------------------------------------------===//; // Random notes about and ideas for the SystemZ backend.; //===---------------------------------------------------------------------===//. The initial backend is deliberately restricted to z10. We should add support; for later architectures at some point. --. If an inline asm ties an i32 ""r"" result to an i64 input, the input; will be treated as an i32, leaving the upper bits uninitialised.; For example:. define void @f4(i32 *%dst) {; %val = call i32 asm ""blah $0"", ""=r,0"" (i64 103); store i32 %val, i32 *%dst; ret void; }. from CodeGen/SystemZ/asm-09.ll will use LHI rather than LGHI.; to load 103. This seems to be a general target-independent problem. --. The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet f",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2029,Performance,load,loads,2029,"e MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2237,Performance,load,load,2237,"mily of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2465,Performance,optimiz,optimizations,2465,". ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-by",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2679,Performance,load,loading,2679,"ory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG. --. We might want to model all access registers and use them to spill; 32-bit values. --. We might want to use the 'overflow' condition of eg. AR to suppo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2920,Performance,load,load,2920,"use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG. --. We might want to model all access registers and use them to spill; 32-bit values. --. We might want to use the 'overflow' condition of eg. AR to support; llvm.sadd.with.overflow.i32 and related instructions - the generated code; for signed overflow check is currently quite bad. This would improve; the results of using -ftrapv.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2952,Performance,load,load,2952,"use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG. --. We might want to model all access registers and use them to spill; 32-bit values. --. We might want to use the 'overflow' condition of eg. AR to support; llvm.sadd.with.overflow.i32 and related instructions - the generated code; for signed overflow check is currently quite bad. This would improve; the results of using -ftrapv.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:3077,Performance,load,load,3077,"use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG. --. We might want to model all access registers and use them to spill; 32-bit values. --. We might want to use the 'overflow' condition of eg. AR to support; llvm.sadd.with.overflow.i32 and related instructions - the generated code; for signed overflow check is currently quite bad. This would improve; the results of using -ftrapv.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:3552,Security,access,access,3552,"use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG. --. We might want to model all access registers and use them to spill; 32-bit values. --. We might want to use the 'overflow' condition of eg. AR to support; llvm.sadd.with.overflow.i32 and related instructions - the generated code; for signed overflow check is currently quite bad. This would improve; the results of using -ftrapv.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:1489,Testability,LOG,LOGICAL,1489,"call i32 asm ""blah $0"", ""=r,0"" (i64 103); store i32 %val, i32 *%dst; ret void; }. from CodeGen/SystemZ/asm-09.ll will use LHI rather than LGHI.; to load 103. This seems to be a general target-independent problem. --. The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:1578,Testability,LOG,LOGICAL,1578," from CodeGen/SystemZ/asm-09.ll will use LHI rather than LGHI.; to load 103. This seems to be a general target-independent problem. --. The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:1692,Testability,LOG,LOGICAL,1692," The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it woul",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:1724,Testability,LOG,LOGICAL,1724," The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it woul",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:1900,Testability,LOG,LOGICAL,1900,"ling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load v",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:1925,Testability,LOG,LOGICAL,1925,"ling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load v",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:1953,Testability,LOG,LOGICAL,1953,"ling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load v",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:3297,Usability,simpl,simpler,3297,"use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG. --. We might want to model all access registers and use them to spill; 32-bit values. --. We might want to use the 'overflow' condition of eg. AR to support; llvm.sadd.with.overflow.i32 and related instructions - the generated code; for signed overflow check is currently quite bad. This would improve; the results of using -ftrapv.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/CMakeLists.txt:2088,Integrability,DEPEND,DEPENDS,2088,emitter); tablegen(LLVM WebAssemblyGenRegisterInfo.inc -gen-register-info); tablegen(LLVM WebAssemblyGenSubtargetInfo.inc -gen-subtarget). add_public_tablegen_target(WebAssemblyCommonTableGen). add_llvm_target(WebAssemblyCodeGen; WebAssemblyAddMissingPrototypes.cpp; WebAssemblyArgumentMove.cpp; WebAssemblyAsmPrinter.cpp; WebAssemblyCFGStackify.cpp; WebAssemblyCFGSort.cpp; WebAssemblyDebugFixup.cpp; WebAssemblyDebugValueManager.cpp; WebAssemblyLateEHPrepare.cpp; WebAssemblyExceptionInfo.cpp; WebAssemblyExplicitLocals.cpp; WebAssemblyFastISel.cpp; WebAssemblyFixBrTableDefaults.cpp; WebAssemblyFixIrreducibleControlFlow.cpp; WebAssemblyFixFunctionBitcasts.cpp; WebAssemblyFrameLowering.cpp; WebAssemblyISelDAGToDAG.cpp; WebAssemblyISelLowering.cpp; WebAssemblyInstrInfo.cpp; WebAssemblyLowerBrUnless.cpp; WebAssemblyLowerEmscriptenEHSjLj.cpp; WebAssemblyLowerRefTypesIntPtrConv.cpp; WebAssemblyMachineFunctionInfo.cpp; WebAssemblyMCInstLower.cpp; WebAssemblyMCLowerPrePass.cpp; WebAssemblyNullifyDebugValueLists.cpp; WebAssemblyOptimizeLiveIntervals.cpp; WebAssemblyOptimizeReturned.cpp; WebAssemblyPeephole.cpp; WebAssemblyRegisterInfo.cpp; WebAssemblyRegColoring.cpp; WebAssemblyRegNumbering.cpp; WebAssemblyRegStackify.cpp; WebAssemblyReplacePhysRegs.cpp; WebAssemblyRuntimeLibcallSignatures.cpp; WebAssemblySelectionDAGInfo.cpp; WebAssemblySetP2AlignOperands.cpp; WebAssemblySortRegion.cpp; WebAssemblyMemIntrinsicResults.cpp; WebAssemblySubtarget.cpp; WebAssemblyTargetMachine.cpp; WebAssemblyTargetObjectFile.cpp; WebAssemblyTargetTransformInfo.cpp; WebAssemblyUtilities.cpp. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; AsmPrinter; BinaryFormat; CodeGen; CodeGenTypes; Core; MC; Scalar; SelectionDAG; Support; Target; TargetParser; TransformUtils; WebAssemblyDesc; WebAssemblyInfo; WebAssemblyUtils. ADD_TO_COMPONENT; WebAssembly; ). add_subdirectory(AsmParser); add_subdirectory(Disassembler); add_subdirectory(MCTargetDesc); add_subdirectory(TargetInfo); add_subdirectory(Utils); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:4315,Availability,redundant,redundant,4315,"le pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebAssemblyRegRenumbering should sort registers; according to their usage frequency to maximize the usage of smaller encodings. //===---------------------------------------------------------------------===//. Many cases of irreducible control flow could be transformed more optimally; than via the transform in WebAssemblyFixIrreducibleControlFlow.cpp. It may also be worthwhile to do transforms before register coloring,; particularly when duplicating code, to allow register coloring to be aware of; the duplication. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify could use AliasAnalysis to reorder loads and stores more; aggressively. //===---------------------------------------------------------------------===//. WebAssemblyRe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:825,Deployability,integrat,integrated,825,"//===-- README.txt - Notes for WebAssembly code gen -----------------------===//. The object format emitted by the WebAssembly backed is documented in:. * https://github.com/WebAssembly/tool-conventions/blob/main/Linking.md. The C ABI is described in:. * https://github.com/WebAssembly/tool-conventions/blob/main/BasicCABI.md. For more information on WebAssembly itself, see the home page:. * https://webassembly.github.io/. Emscripten provides a C/C++ compilation environment based on clang which; includes standard libraries, tools, and packaging for producing WebAssembly; applications that can run in browsers and other environments. wasi-sdk provides a more minimal C/C++ SDK based on clang, llvm and a libc based; on musl, for producing WebAssembly applications that use the WASI ABI. Rust provides WebAssembly support integrated into Cargo. There are two; main options:; - wasm32-unknown-unknown, which provides a relatively minimal environment; that has an emphasis on being ""native""; - wasm32-unknown-emscripten, which uses Emscripten internally and; provides standard C/C++ libraries, filesystem emulation, GL and SDL; bindings; For more information, see:; * https://www.hellorust.com/. The following documents contain some information on the semantics and binary; encoding of WebAssembly itself:; * https://github.com/WebAssembly/design/blob/main/Semantics.md; * https://github.com/WebAssembly/design/blob/main/BinaryEncoding.md. Some notes on ways that the generated code could be improved follow:. //===---------------------------------------------------------------------===//. Br, br_if, and br_table instructions can support having a value on the value; stack across the jump (sometimes). We should (a) model this, and (b) extend; the stackifier to utilize it. //===---------------------------------------------------------------------===//. The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero; behavior. The ARM target has the same kind of min/max instruc",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:6092,Energy Efficiency,schedul,schedule,6092,"----------------------------------------------------------------===//. WebAssemblyRegStackify could use AliasAnalysis to reorder loads and stores more; aggressively. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify is currently a greedy algorithm. This means that, for; example, a binary operator will stackify with its user before its operands.; However, if moving the binary operator to its user moves it to a place where; its operands can't be moved to, it would be better to leave it in place, or; perhaps move it up, so that it can stackify its operands. A binary operator; has two operands and one result, so in such cases there could be a net win by; preferring the operands. //===---------------------------------------------------------------------===//. Instruction ordering has a significant influence on register stackification and; coloring. Consider experimenting with the MachineScheduler (enable via; enableMachineScheduler) and determine if it can be configured to schedule; instructions advantageously for this purpose. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify currently assumes that the stack must be empty after; an instruction with no return values, however wasm doesn't actually require; this. WebAssemblyRegStackify could be extended, or possibly rewritten, to take; full advantage of what WebAssembly permits. //===---------------------------------------------------------------------===//. Add support for mergeable sections in the Wasm writer, such as for strings and; floating-point constants. //===---------------------------------------------------------------------===//. The function @dynamic_alloca_redzone in test/CodeGen/WebAssembly/userstack.ll; ends up with a local.tee in its prolog which has an unused result, requiring; an extra drop:. global.get $push8=, 0; local.tee $push9=, 1, $pop8; drop $pop9; [...]. The prologue code initially thinks it",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:825,Integrability,integrat,integrated,825,"//===-- README.txt - Notes for WebAssembly code gen -----------------------===//. The object format emitted by the WebAssembly backed is documented in:. * https://github.com/WebAssembly/tool-conventions/blob/main/Linking.md. The C ABI is described in:. * https://github.com/WebAssembly/tool-conventions/blob/main/BasicCABI.md. For more information on WebAssembly itself, see the home page:. * https://webassembly.github.io/. Emscripten provides a C/C++ compilation environment based on clang which; includes standard libraries, tools, and packaging for producing WebAssembly; applications that can run in browsers and other environments. wasi-sdk provides a more minimal C/C++ SDK based on clang, llvm and a libc based; on musl, for producing WebAssembly applications that use the WASI ABI. Rust provides WebAssembly support integrated into Cargo. There are two; main options:; - wasm32-unknown-unknown, which provides a relatively minimal environment; that has an emphasis on being ""native""; - wasm32-unknown-emscripten, which uses Emscripten internally and; provides standard C/C++ libraries, filesystem emulation, GL and SDL; bindings; For more information, see:; * https://www.hellorust.com/. The following documents contain some information on the semantics and binary; encoding of WebAssembly itself:; * https://github.com/WebAssembly/design/blob/main/Semantics.md; * https://github.com/WebAssembly/design/blob/main/BinaryEncoding.md. Some notes on ways that the generated code could be improved follow:. //===---------------------------------------------------------------------===//. Br, br_if, and br_table instructions can support having a value on the value; stack across the jump (sometimes). We should (a) model this, and (b) extend; the stackifier to utilize it. //===---------------------------------------------------------------------===//. The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero; behavior. The ARM target has the same kind of min/max instruc",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:2592,Integrability,depend,dependencies,2592,"-----------===//. Br, br_if, and br_table instructions can support having a value on the value; stack across the jump (sometimes). We should (a) model this, and (b) extend; the stackifier to utilize it. //===---------------------------------------------------------------------===//. The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero; behavior. The ARM target has the same kind of min/max instructions and has; implemented optimizations for them; we should do similar optimizations for; WebAssembly. //===---------------------------------------------------------------------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMa",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:2713,Integrability,depend,dependencies,2713,"the jump (sometimes). We should (a) model this, and (b) extend; the stackifier to utilize it. //===---------------------------------------------------------------------===//. The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero; behavior. The ARM target has the same kind of min/max instructions and has; implemented optimizations for them; we should do similar optimizations for; WebAssembly. //===---------------------------------------------------------------------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3741,Integrability,Wrap,Wrapping,3741,"----------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebAssemblyRegRenumbering should sort registers; according to their usage frequency to maximize the usage of smaller encodings. //===---------------------------------------------------------------------===//. Man",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:1739,Modifiability,extend,extend,1739,"ng WebAssembly applications that use the WASI ABI. Rust provides WebAssembly support integrated into Cargo. There are two; main options:; - wasm32-unknown-unknown, which provides a relatively minimal environment; that has an emphasis on being ""native""; - wasm32-unknown-emscripten, which uses Emscripten internally and; provides standard C/C++ libraries, filesystem emulation, GL and SDL; bindings; For more information, see:; * https://www.hellorust.com/. The following documents contain some information on the semantics and binary; encoding of WebAssembly itself:; * https://github.com/WebAssembly/design/blob/main/Semantics.md; * https://github.com/WebAssembly/design/blob/main/BinaryEncoding.md. Some notes on ways that the generated code could be improved follow:. //===---------------------------------------------------------------------===//. Br, br_if, and br_table instructions can support having a value on the value; stack across the jump (sometimes). We should (a) model this, and (b) extend; the stackifier to utilize it. //===---------------------------------------------------------------------===//. The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero; behavior. The ARM target has the same kind of min/max instructions and has; implemented optimizations for them; we should do similar optimizations for; WebAssembly. //===---------------------------------------------------------------------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===--------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3195,Modifiability,extend,extending,3195,"---------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===--------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3907,Modifiability,variab,variables,3907,"ed hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebAssemblyRegRenumbering should sort registers; according to their usage frequency to maximize the usage of smaller encodings. //===---------------------------------------------------------------------===//. Many cases of irreducible control flow could be transformed more optimally; than via the transform in WebAssemblyFixIrreducibleControlFlow.cpp. It may also be worthwhile to do transforms befo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:6078,Modifiability,config,configured,6078,"----------------------------------------------------------------===//. WebAssemblyRegStackify could use AliasAnalysis to reorder loads and stores more; aggressively. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify is currently a greedy algorithm. This means that, for; example, a binary operator will stackify with its user before its operands.; However, if moving the binary operator to its user moves it to a place where; its operands can't be moved to, it would be better to leave it in place, or; perhaps move it up, so that it can stackify its operands. A binary operator; has two operands and one result, so in such cases there could be a net win by; preferring the operands. //===---------------------------------------------------------------------===//. Instruction ordering has a significant influence on register stackification and; coloring. Consider experimenting with the MachineScheduler (enable via; enableMachineScheduler) and determine if it can be configured to schedule; instructions advantageously for this purpose. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify currently assumes that the stack must be empty after; an instruction with no return values, however wasm doesn't actually require; this. WebAssemblyRegStackify could be extended, or possibly rewritten, to take; full advantage of what WebAssembly permits. //===---------------------------------------------------------------------===//. Add support for mergeable sections in the Wasm writer, such as for strings and; floating-point constants. //===---------------------------------------------------------------------===//. The function @dynamic_alloca_redzone in test/CodeGen/WebAssembly/userstack.ll; ends up with a local.tee in its prolog which has an unused result, requiring; an extra drop:. global.get $push8=, 0; local.tee $push9=, 1, $pop8; drop $pop9; [...]. The prologue code initially thinks it",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:6421,Modifiability,extend,extended,6421,"algorithm. This means that, for; example, a binary operator will stackify with its user before its operands.; However, if moving the binary operator to its user moves it to a place where; its operands can't be moved to, it would be better to leave it in place, or; perhaps move it up, so that it can stackify its operands. A binary operator; has two operands and one result, so in such cases there could be a net win by; preferring the operands. //===---------------------------------------------------------------------===//. Instruction ordering has a significant influence on register stackification and; coloring. Consider experimenting with the MachineScheduler (enable via; enableMachineScheduler) and determine if it can be configured to schedule; instructions advantageously for this purpose. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify currently assumes that the stack must be empty after; an instruction with no return values, however wasm doesn't actually require; this. WebAssemblyRegStackify could be extended, or possibly rewritten, to take; full advantage of what WebAssembly permits. //===---------------------------------------------------------------------===//. Add support for mergeable sections in the Wasm writer, such as for strings and; floating-point constants. //===---------------------------------------------------------------------===//. The function @dynamic_alloca_redzone in test/CodeGen/WebAssembly/userstack.ll; ends up with a local.tee in its prolog which has an unused result, requiring; an extra drop:. global.get $push8=, 0; local.tee $push9=, 1, $pop8; drop $pop9; [...]. The prologue code initially thinks it needs an FP register, but later it; turns out to be unneeded, so one could either approach this by being more; clever about not inserting code for an FP in the first place, or optimizing; away the copy later. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:2028,Performance,optimiz,optimizations,2028,"h uses Emscripten internally and; provides standard C/C++ libraries, filesystem emulation, GL and SDL; bindings; For more information, see:; * https://www.hellorust.com/. The following documents contain some information on the semantics and binary; encoding of WebAssembly itself:; * https://github.com/WebAssembly/design/blob/main/Semantics.md; * https://github.com/WebAssembly/design/blob/main/BinaryEncoding.md. Some notes on ways that the generated code could be improved follow:. //===---------------------------------------------------------------------===//. Br, br_if, and br_table instructions can support having a value on the value; stack across the jump (sometimes). We should (a) model this, and (b) extend; the stackifier to utilize it. //===---------------------------------------------------------------------===//. The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero; behavior. The ARM target has the same kind of min/max instructions and has; implemented optimizations for them; we should do similar optimizations for; WebAssembly. //===---------------------------------------------------------------------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===----------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:2073,Performance,optimiz,optimizations,2073,"h uses Emscripten internally and; provides standard C/C++ libraries, filesystem emulation, GL and SDL; bindings; For more information, see:; * https://www.hellorust.com/. The following documents contain some information on the semantics and binary; encoding of WebAssembly itself:; * https://github.com/WebAssembly/design/blob/main/Semantics.md; * https://github.com/WebAssembly/design/blob/main/BinaryEncoding.md. Some notes on ways that the generated code could be improved follow:. //===---------------------------------------------------------------------===//. Br, br_if, and br_table instructions can support having a value on the value; stack across the jump (sometimes). We should (a) model this, and (b) extend; the stackifier to utilize it. //===---------------------------------------------------------------------===//. The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero; behavior. The ARM target has the same kind of min/max instructions and has; implemented optimizations for them; we should do similar optimizations for; WebAssembly. //===---------------------------------------------------------------------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===----------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:2907,Performance,optimiz,optimization-related,2907,"ve zero; behavior. The ARM target has the same kind of min/max instructions and has; implemented optimizations for them; we should do similar optimizations for; WebAssembly. //===---------------------------------------------------------------------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same co",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3084,Performance,Optimiz,OptimizeReturned,3084,"---------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===--------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3246,Performance,optimiz,optimization,3246,"---------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===--------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3485,Performance,optimiz,optimizeSelect,3485,"ion uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebA",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3501,Performance,optimiz,optimizeCompareInstr,3501,"ion uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebA",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3523,Performance,optimiz,optimizeCondBranch,3523,"ion uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebA",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3544,Performance,optimiz,optimizeLoadInstr,3544,"ion uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebA",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:4338,Performance,optimiz,optimized,4338,"le pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebAssemblyRegRenumbering should sort registers; according to their usage frequency to maximize the usage of smaller encodings. //===---------------------------------------------------------------------===//. Many cases of irreducible control flow could be transformed more optimally; than via the transform in WebAssemblyFixIrreducibleControlFlow.cpp. It may also be worthwhile to do transforms before register coloring,; particularly when duplicating code, to allow register coloring to be aware of; the duplication. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify could use AliasAnalysis to reorder loads and stores more; aggressively. //===---------------------------------------------------------------------===//. WebAssemblyRe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:5184,Performance,load,loads,5184,"-----------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebAssemblyRegRenumbering should sort registers; according to their usage frequency to maximize the usage of smaller encodings. //===---------------------------------------------------------------------===//. Many cases of irreducible control flow could be transformed more optimally; than via the transform in WebAssemblyFixIrreducibleControlFlow.cpp. It may also be worthwhile to do transforms before register coloring,; particularly when duplicating code, to allow register coloring to be aware of; the duplication. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify could use AliasAnalysis to reorder loads and stores more; aggressively. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify is currently a greedy algorithm. This means that, for; example, a binary operator will stackify with its user before its operands.; However, if moving the binary operator to its user moves it to a place where; its operands can't be moved to, it would be better to leave it in place, or; perhaps move it up, so that it can stackify its operands. A binary operator; has two operands and one result, so in such cases there could be a net win by; preferring the operands. //===---------------------------------------------------------------------===//. Instruction ordering has a significant influence on register stackification and; coloring. Consider experimenting with the MachineScheduler (enable via; enableMachineScheduler) and determine if it can be configured to schedule; instructions advantageously for this purpose. //===--------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:7233,Performance,optimiz,optimizing,7233,"algorithm. This means that, for; example, a binary operator will stackify with its user before its operands.; However, if moving the binary operator to its user moves it to a place where; its operands can't be moved to, it would be better to leave it in place, or; perhaps move it up, so that it can stackify its operands. A binary operator; has two operands and one result, so in such cases there could be a net win by; preferring the operands. //===---------------------------------------------------------------------===//. Instruction ordering has a significant influence on register stackification and; coloring. Consider experimenting with the MachineScheduler (enable via; enableMachineScheduler) and determine if it can be configured to schedule; instructions advantageously for this purpose. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify currently assumes that the stack must be empty after; an instruction with no return values, however wasm doesn't actually require; this. WebAssemblyRegStackify could be extended, or possibly rewritten, to take; full advantage of what WebAssembly permits. //===---------------------------------------------------------------------===//. Add support for mergeable sections in the Wasm writer, such as for strings and; floating-point constants. //===---------------------------------------------------------------------===//. The function @dynamic_alloca_redzone in test/CodeGen/WebAssembly/userstack.ll; ends up with a local.tee in its prolog which has an unused result, requiring; an extra drop:. global.get $push8=, 0; local.tee $push9=, 1, $pop8; drop $pop9; [...]. The prologue code initially thinks it needs an FP register, but later it; turns out to be unneeded, so one could either approach this by being more; clever about not inserting code for an FP in the first place, or optimizing; away the copy later. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:4315,Safety,redund,redundant,4315,"le pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebAssemblyRegRenumbering should sort registers; according to their usage frequency to maximize the usage of smaller encodings. //===---------------------------------------------------------------------===//. Many cases of irreducible control flow could be transformed more optimally; than via the transform in WebAssemblyFixIrreducibleControlFlow.cpp. It may also be worthwhile to do transforms before register coloring,; particularly when duplicating code, to allow register coloring to be aware of; the duplication. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify could use AliasAnalysis to reorder loads and stores more; aggressively. //===---------------------------------------------------------------------===//. WebAssemblyRe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:6815,Testability,test,test,6815,"algorithm. This means that, for; example, a binary operator will stackify with its user before its operands.; However, if moving the binary operator to its user moves it to a place where; its operands can't be moved to, it would be better to leave it in place, or; perhaps move it up, so that it can stackify its operands. A binary operator; has two operands and one result, so in such cases there could be a net win by; preferring the operands. //===---------------------------------------------------------------------===//. Instruction ordering has a significant influence on register stackification and; coloring. Consider experimenting with the MachineScheduler (enable via; enableMachineScheduler) and determine if it can be configured to schedule; instructions advantageously for this purpose. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify currently assumes that the stack must be empty after; an instruction with no return values, however wasm doesn't actually require; this. WebAssemblyRegStackify could be extended, or possibly rewritten, to take; full advantage of what WebAssembly permits. //===---------------------------------------------------------------------===//. Add support for mergeable sections in the Wasm writer, such as for strings and; floating-point constants. //===---------------------------------------------------------------------===//. The function @dynamic_alloca_redzone in test/CodeGen/WebAssembly/userstack.ll; ends up with a local.tee in its prolog which has an unused result, requiring; an extra drop:. global.get $push8=, 0; local.tee $push9=, 1, $pop8; drop $pop9; [...]. The prologue code initially thinks it needs an FP register, but later it; turns out to be unneeded, so one could either approach this by being more; clever about not inserting code for an FP in the first place, or optimizing; away the copy later. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:2344,Usability,Simpl,SimplifyCFG,2344,"om/WebAssembly/design/blob/main/Semantics.md; * https://github.com/WebAssembly/design/blob/main/BinaryEncoding.md. Some notes on ways that the generated code could be improved follow:. //===---------------------------------------------------------------------===//. Br, br_if, and br_table instructions can support having a value on the value; stack across the jump (sometimes). We should (a) model this, and (b) extend; the stackifier to utilize it. //===---------------------------------------------------------------------===//. The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero; behavior. The ARM target has the same kind of min/max instructions and has; implemented optimizations for them; we should do similar optimizations for; WebAssembly. //===---------------------------------------------------------------------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clea",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt:387,Deployability,patch,patches,387,"//===---------------------------------------------------------------------===//; // Random ideas for the X86 backend: FP stack related stuff; //===---------------------------------------------------------------------===//. //===---------------------------------------------------------------------===//. Some targets (e.g. athlons) prefer freep to fstp ST(0):; http://gcc.gnu.org/ml/gcc-patches/2004-04/msg00659.html. //===---------------------------------------------------------------------===//. This should use fiadd on chips where it is profitable:; double foo(double P, int *I) { return P+*I; }. We have fiadd patterns now but the followings have the same cost and; complexity. We need a way to specify the later is more profitable. def FpADD32m : FpI<(ops RFP:$dst, RFP:$src1, f32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (extloadf64f32 addr:$src2)))]>;; // ST(0) = ST(0) + [mem32]. def FpIADD32m : FpI<(ops RFP:$dst, RFP:$src1, i32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (X86fild addr:$src2, i32)))]>;; // ST(0) = ST(0) + [mem32int]. //===---------------------------------------------------------------------===//. The FP stackifier should handle simple permutates to reduce number of shuffle; instructions, e.g. turning:. fld P	->		fld Q; fld Q			fld P; fxch. or:. fxch	->		fucomi; fucomi			jl X; jg X. Ideas:; http://gcc.gnu.org/ml/gcc-patches/2004-11/msg02410.html. //===---------------------------------------------------------------------===//. Add a target specific hook to DAG combiner to handle SINT_TO_FP and; FP_TO_SINT when the source operand is already in memory. //===---------------------------------------------------------------------===//. Open code rint,floor,ceil,trunc:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02006.html; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02011.html. Opencode the sincos[f] libcall. //===---------------------------------------------------------------------===//. None of the FPStack instructions are ha",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt:1382,Deployability,patch,patches,1382,"tches/2004-04/msg00659.html. //===---------------------------------------------------------------------===//. This should use fiadd on chips where it is profitable:; double foo(double P, int *I) { return P+*I; }. We have fiadd patterns now but the followings have the same cost and; complexity. We need a way to specify the later is more profitable. def FpADD32m : FpI<(ops RFP:$dst, RFP:$src1, f32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (extloadf64f32 addr:$src2)))]>;; // ST(0) = ST(0) + [mem32]. def FpIADD32m : FpI<(ops RFP:$dst, RFP:$src1, i32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (X86fild addr:$src2, i32)))]>;; // ST(0) = ST(0) + [mem32int]. //===---------------------------------------------------------------------===//. The FP stackifier should handle simple permutates to reduce number of shuffle; instructions, e.g. turning:. fld P	->		fld Q; fld Q			fld P; fxch. or:. fxch	->		fucomi; fucomi			jl X; jg X. Ideas:; http://gcc.gnu.org/ml/gcc-patches/2004-11/msg02410.html. //===---------------------------------------------------------------------===//. Add a target specific hook to DAG combiner to handle SINT_TO_FP and; FP_TO_SINT when the source operand is already in memory. //===---------------------------------------------------------------------===//. Open code rint,floor,ceil,trunc:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02006.html; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02011.html. Opencode the sincos[f] libcall. //===---------------------------------------------------------------------===//. None of the FPStack instructions are handled in; X86RegisterInfo::foldMemoryOperand, which prevents the spiller from; folding spill code into the instructions. //===---------------------------------------------------------------------===//. Currently the x86 codegen isn't very good at mixing SSE and FPStack; code:. unsigned int foo(double x) { return x; }. foo:; 	subl $20, %esp; 	movsd 24(%esp), %xmm0; 	movsd %xmm0, 8(%esp)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt:1761,Deployability,patch,patches,1761,"iadd patterns now but the followings have the same cost and; complexity. We need a way to specify the later is more profitable. def FpADD32m : FpI<(ops RFP:$dst, RFP:$src1, f32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (extloadf64f32 addr:$src2)))]>;; // ST(0) = ST(0) + [mem32]. def FpIADD32m : FpI<(ops RFP:$dst, RFP:$src1, i32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (X86fild addr:$src2, i32)))]>;; // ST(0) = ST(0) + [mem32int]. //===---------------------------------------------------------------------===//. The FP stackifier should handle simple permutates to reduce number of shuffle; instructions, e.g. turning:. fld P	->		fld Q; fld Q			fld P; fxch. or:. fxch	->		fucomi; fucomi			jl X; jg X. Ideas:; http://gcc.gnu.org/ml/gcc-patches/2004-11/msg02410.html. //===---------------------------------------------------------------------===//. Add a target specific hook to DAG combiner to handle SINT_TO_FP and; FP_TO_SINT when the source operand is already in memory. //===---------------------------------------------------------------------===//. Open code rint,floor,ceil,trunc:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02006.html; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02011.html. Opencode the sincos[f] libcall. //===---------------------------------------------------------------------===//. None of the FPStack instructions are handled in; X86RegisterInfo::foldMemoryOperand, which prevents the spiller from; folding spill code into the instructions. //===---------------------------------------------------------------------===//. Currently the x86 codegen isn't very good at mixing SSE and FPStack; code:. unsigned int foo(double x) { return x; }. foo:; 	subl $20, %esp; 	movsd 24(%esp), %xmm0; 	movsd %xmm0, 8(%esp); 	fldl 8(%esp); 	fisttpll (%esp); 	movl (%esp), %eax; 	addl $20, %esp; 	ret. This just requires being smarter when custom expanding fptoui. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt:1818,Deployability,patch,patches,1818,"iadd patterns now but the followings have the same cost and; complexity. We need a way to specify the later is more profitable. def FpADD32m : FpI<(ops RFP:$dst, RFP:$src1, f32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (extloadf64f32 addr:$src2)))]>;; // ST(0) = ST(0) + [mem32]. def FpIADD32m : FpI<(ops RFP:$dst, RFP:$src1, i32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (X86fild addr:$src2, i32)))]>;; // ST(0) = ST(0) + [mem32int]. //===---------------------------------------------------------------------===//. The FP stackifier should handle simple permutates to reduce number of shuffle; instructions, e.g. turning:. fld P	->		fld Q; fld Q			fld P; fxch. or:. fxch	->		fucomi; fucomi			jl X; jg X. Ideas:; http://gcc.gnu.org/ml/gcc-patches/2004-11/msg02410.html. //===---------------------------------------------------------------------===//. Add a target specific hook to DAG combiner to handle SINT_TO_FP and; FP_TO_SINT when the source operand is already in memory. //===---------------------------------------------------------------------===//. Open code rint,floor,ceil,trunc:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02006.html; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02011.html. Opencode the sincos[f] libcall. //===---------------------------------------------------------------------===//. None of the FPStack instructions are handled in; X86RegisterInfo::foldMemoryOperand, which prevents the spiller from; folding spill code into the instructions. //===---------------------------------------------------------------------===//. Currently the x86 codegen isn't very good at mixing SSE and FPStack; code:. unsigned int foo(double x) { return x; }. foo:; 	subl $20, %esp; 	movsd 24(%esp), %xmm0; 	movsd %xmm0, 8(%esp); 	fldl 8(%esp); 	fisttpll (%esp); 	movl (%esp), %eax; 	addl $20, %esp; 	ret. This just requires being smarter when custom expanding fptoui. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt:1212,Energy Efficiency,reduce,reduce,1212,"----------===//. //===---------------------------------------------------------------------===//. Some targets (e.g. athlons) prefer freep to fstp ST(0):; http://gcc.gnu.org/ml/gcc-patches/2004-04/msg00659.html. //===---------------------------------------------------------------------===//. This should use fiadd on chips where it is profitable:; double foo(double P, int *I) { return P+*I; }. We have fiadd patterns now but the followings have the same cost and; complexity. We need a way to specify the later is more profitable. def FpADD32m : FpI<(ops RFP:$dst, RFP:$src1, f32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (extloadf64f32 addr:$src2)))]>;; // ST(0) = ST(0) + [mem32]. def FpIADD32m : FpI<(ops RFP:$dst, RFP:$src1, i32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (X86fild addr:$src2, i32)))]>;; // ST(0) = ST(0) + [mem32int]. //===---------------------------------------------------------------------===//. The FP stackifier should handle simple permutates to reduce number of shuffle; instructions, e.g. turning:. fld P	->		fld Q; fld Q			fld P; fxch. or:. fxch	->		fucomi; fucomi			jl X; jg X. Ideas:; http://gcc.gnu.org/ml/gcc-patches/2004-11/msg02410.html. //===---------------------------------------------------------------------===//. Add a target specific hook to DAG combiner to handle SINT_TO_FP and; FP_TO_SINT when the source operand is already in memory. //===---------------------------------------------------------------------===//. Open code rint,floor,ceil,trunc:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02006.html; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02011.html. Opencode the sincos[f] libcall. //===---------------------------------------------------------------------===//. None of the FPStack instructions are handled in; X86RegisterInfo::foldMemoryOperand, which prevents the spiller from; folding spill code into the instructions. //===---------------------------------------------------------------------===//. Cur",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt:1191,Usability,simpl,simple,1191,"----------===//. //===---------------------------------------------------------------------===//. Some targets (e.g. athlons) prefer freep to fstp ST(0):; http://gcc.gnu.org/ml/gcc-patches/2004-04/msg00659.html. //===---------------------------------------------------------------------===//. This should use fiadd on chips where it is profitable:; double foo(double P, int *I) { return P+*I; }. We have fiadd patterns now but the followings have the same cost and; complexity. We need a way to specify the later is more profitable. def FpADD32m : FpI<(ops RFP:$dst, RFP:$src1, f32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (extloadf64f32 addr:$src2)))]>;; // ST(0) = ST(0) + [mem32]. def FpIADD32m : FpI<(ops RFP:$dst, RFP:$src1, i32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (X86fild addr:$src2, i32)))]>;; // ST(0) = ST(0) + [mem32int]. //===---------------------------------------------------------------------===//. The FP stackifier should handle simple permutates to reduce number of shuffle; instructions, e.g. turning:. fld P	->		fld Q; fld Q			fld P; fxch. or:. fxch	->		fucomi; fucomi			jl X; jg X. Ideas:; http://gcc.gnu.org/ml/gcc-patches/2004-11/msg02410.html. //===---------------------------------------------------------------------===//. Add a target specific hook to DAG combiner to handle SINT_TO_FP and; FP_TO_SINT when the source operand is already in memory. //===---------------------------------------------------------------------===//. Open code rint,floor,ceil,trunc:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02006.html; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02011.html. Opencode the sincos[f] libcall. //===---------------------------------------------------------------------===//. None of the FPStack instructions are handled in; X86RegisterInfo::foldMemoryOperand, which prevents the spiller from; folding spill code into the instructions. //===---------------------------------------------------------------------===//. Cur",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:3200,Availability,mask,mask,3200,"ith unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_Cubic17TessellateSurfaceE, bb cond_next140. This is what icc; emits:. movaps (%edx), %xmm2 #59.21; movaps (%edx), %xmm5 #60.21; movaps (%edx), %xmm4 #61.21; movaps (%edx), %xmm3 #62.21; movl 40(%ecx), %ebp #69.49; shufps $0, %xmm2, %xmm5 #60.21; movl 100(%esp), %ebx #69.20; movl (%ebx), %edi #69.20; imull %ebp, %edi #69.49; addl (%eax), %edi #70.33; shufps $85, %xmm2, %xmm4 #61.21; shufps $170, %xmm2, %xmm3 #62.21; shufps $255, ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:2071,Deployability,patch,patches,2071," A[0]+A[1]+A[2]+A[3];; }. Instead we get this:. _f32: ## @f32; 	pshufd	$1, %xmm0, %xmm1 ## xmm1 = xmm0[1,0,0,0]; 	addss	%xmm0, %xmm1; 	pshufd	$3, %xmm0, %xmm2 ## xmm2 = xmm0[3,0,0,0]; 	movhlps	%xmm0, %xmm0 ## xmm0 = xmm0[1,1]; 	movaps	%xmm0, %xmm3; 	addss	%xmm1, %xmm3; 	movdqa	%xmm2, %xmm0; 	addss	%xmm3, %xmm0; 	ret. Also, there are cases where some simple local SLP would improve codegen a bit.; compiling this:. _Complex float f32(_Complex float A, _Complex float B) {; return A+B;; }. into:. _f32: ## @f32; 	movdqa	%xmm0, %xmm2; 	addss	%xmm1, %xmm2; 	pshufd	$1, %xmm1, %xmm1 ## xmm1 = xmm1[1,0,0,0]; 	pshufd	$1, %xmm0, %xmm3 ## xmm3 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm3; 	movaps	%xmm2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:3318,Deployability,update,update,3318,"----------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_Cubic17TessellateSurfaceE, bb cond_next140. This is what icc; emits:. movaps (%edx), %xmm2 #59.21; movaps (%edx), %xmm5 #60.21; movaps (%edx), %xmm4 #61.21; movaps (%edx), %xmm3 #62.21; movl 40(%ecx), %ebp #69.49; shufps $0, %xmm2, %xmm5 #60.21; movl 100(%esp), %ebx #69.20; movl (%ebx), %edi #69.20; imull %ebp, %edi #69.49; addl (%eax), %edi #70.33; shufps $85, %xmm2, %xmm4 #61.21; shufps $170, %xmm2, %xmm3 #62.21; shufps $255, %xmm2, %xmm2 #63.21; lea (%ebp,%ebp,2), %ebx #69.49; negl %ebx #69.49; lea -3(%edi,%ebx), %ebx #70.33; shll $4, %ebx #68.37; addl ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:22263,Deployability,pipeline,pipelined,22263,"; %d = call float @fabsf(float %x); ret float %d; }. //===---------------------------------------------------------------------===//. This IR (from PR6194):. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128""; target triple = ""x86_64-apple-darwin10.0.0"". %0 = type { double, double }; %struct.float3 = type { float, float, float }. define void @test(%0, %struct.float3* nocapture %res) nounwind noinline ssp {; entry:; %tmp18 = extractvalue %0 %0, 0 ; <double> [#uses=1]; %tmp19 = bitcast double %tmp18 to i64 ; <i64> [#uses=1]; %tmp20 = zext i64 %tmp19 to i128 ; <i128> [#uses=1]; %tmp10 = lshr i128 %tmp20, 32 ; <i128> [#uses=1]; %tmp11 = trunc i128 %tmp10 to i32 ; <i32> [#uses=1]; %tmp12 = bitcast i32 %tmp11 to float ; <float> [#uses=1]; %tmp5 = getelementptr inbounds %struct.float3* %res, i64 0, i32 1 ; <float*> [#uses=1]; store float %tmp12, float* %tmp5; ret void; }. Compiles to:. _test: ## @test; 	movd	%xmm0, %rax; 	shrq	$32, %rax; 	movl	%eax, 4(%rdi); 	ret. This would be better kept in the SSE unit by treating XMM0 as a 4xfloat and; doing a shuffle from v[1] to v[0] then a float store. //===---------------------------------------------------------------------===//. [UNSAFE FP]. void foo(double, double, double);; void norm(double x, double y, double z) {; double scale = __builtin_sqrt(x*x + y*y + z*z);; foo(x/scale, y/scale, z/scale);; }. We currently generate an sqrtsd and 3 divsd instructions. This is bad, fp div is; slow and not pipelined. In -ffast-math mode we could compute ""1.0/scale"" first; and emit 3 mulsd in place of the divs. This can be done as a target-independent; transform. If we're dealing with floats instead of doubles we could even replace the sqrtss; and inversion with an rsqrtss instruction, which computes 1/sqrt faster at the; cost of reduced accuracy. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:4445,Energy Efficiency,schedul,scheduling,4445,"--------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_Cubic17TessellateSurfaceE, bb cond_next140. This is what icc; emits:. movaps (%edx), %xmm2 #59.21; movaps (%edx), %xmm5 #60.21; movaps (%edx), %xmm4 #61.21; movaps (%edx), %xmm3 #62.21; movl 40(%ecx), %ebp #69.49; shufps $0, %xmm2, %xmm5 #60.21; movl 100(%esp), %ebx #69.20; movl (%ebx), %edi #69.20; imull %ebp, %edi #69.49; addl (%eax), %edi #70.33; shufps $85, %xmm2, %xmm4 #61.21; shufps $170, %xmm2, %xmm3 #62.21; shufps $255, %xmm2, %xmm2 #63.21; lea (%ebp,%ebp,2), %ebx #69.49; negl %ebx #69.49; lea -3(%edi,%ebx), %ebx #70.33; shll $4, %ebx #68.37; addl 32(%ecx), %ebx #68.37; testb $15, %bl #91.13; jne L_B1.24 # Prob 5% #91.13. This is the llvm code after instruction scheduling:. cond_next140 (0xa910740, LLVM BB @0xa90beb0):; 	%reg1078 = MOV32ri -3; 	%reg1079 = ADD32rm %reg1078, %reg1068, 1, %noreg, 0; 	%reg1037 = MOV32rm %reg1024, 1, %noreg, 40; 	%reg1080 = IMUL32rr %reg1079, %reg1037; 	%reg1081 = MOV32rm %reg1058, 1, %noreg, 0; 	%reg1038 = LEA32r %reg1081, 1, %reg1080, -3; 	%reg1036 = MOV32rm %reg1024, 1, %noreg, 32; 	%reg1082 = SHL32ri %reg1038, 4; 	%reg1039 = ADD32rr %reg1036, %reg1082; 	%reg1083 = MOVAPSrm %reg1059, 1, %noreg, 0; 	%reg1034 = SHUFPSrr %reg1083, %reg1083, 170; 	%reg1032 = SHUFPSrr %reg1083, %reg1083, 0; 	%reg1035 = SHUFPSrr %reg1083, %reg1083, 255; 	%reg1033 = SHUFPSrr %reg1083, %reg1083, 85; 	%reg1040 = MOV32rr %reg1039; 	%reg1084 = AND32ri8 %reg1039, 15; 	CMP32ri8 %reg1084, 0; 	JE mbb<cond_next204,0xa914d30>. Still ok. After register allocation:. cond_next140 (0xa910740, LLVM BB @0xa90beb0):; 	%eax = MOV32ri -3; 	%edx = MOV32rm %stack.3, 1, %noreg, 0; 	ADD32rm %eax<def&use>, %edx, 1, %noreg, 0; 	%edx = MOV32rm %s",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:17549,Energy Efficiency,power,powers,17549,"---------------===//. We compile vector multiply-by-constant into poor code:. define <4 x i32> @f(<4 x i32> %i) nounwind {; 	%A = mul <4 x i32> %i, < i32 10, i32 10, i32 10, i32 10 >; 	ret <4 x i32> %A; }. On targets without SSE4.1, this compiles into:. LCPI1_0:					## <4 x i32>; 	.long	10; 	.long	10; 	.long	10; 	.long	10; 	.text; 	.align	4,0x90; 	.globl	_f; _f:; 	pshufd	$3, %xmm0, %xmm1; 	movd	%xmm1, %eax; 	imull	LCPI1_0+12, %eax; 	movd	%eax, %xmm1; 	pshufd	$1, %xmm0, %xmm2; 	movd	%xmm2, %eax; 	imull	LCPI1_0+4, %eax; 	movd	%eax, %xmm2; 	punpckldq	%xmm1, %xmm2; 	movd	%xmm0, %eax; 	imull	LCPI1_0, %eax; 	movd	%eax, %xmm1; 	movhlps	%xmm0, %xmm0; 	movd	%xmm0, %eax; 	imull	LCPI1_0+8, %eax; 	movd	%eax, %xmm0; 	punpckldq	%xmm0, %xmm1; 	movaps	%xmm1, %xmm0; 	punpckldq	%xmm2, %xmm0; 	ret. It would be better to synthesize integer vector multiplication by constants; using shifts and adds, pslld and paddd here. And even on targets with SSE4.1,; simple cases such as multiplication by powers of two would be better as; vector shifts than as multiplications. //===---------------------------------------------------------------------===//. We compile this:. __m128i; foo2 (char x); {; return _mm_set_epi8 (1, 0, 0, 0, 0, 0, 0, 0, 0, x, 0, 1, 0, 0, 0, 0);; }. into:; 	movl	$1, %eax; 	xorps	%xmm0, %xmm0; 	pinsrw	$2, %eax, %xmm0; 	movzbl	4(%esp), %eax; 	pinsrw	$3, %eax, %xmm0; 	movl	$256, %eax; 	pinsrw	$7, %eax, %xmm0; 	ret. gcc-4.2:; 	subl	$12, %esp; 	movzbl	16(%esp), %eax; 	movdqa	LC0, %xmm0; 	pinsrw	$3, %eax, %xmm0; 	addl	$12, %esp; 	ret; 	.const; 	.align 4; LC0:; 	.word	0; 	.word	0; 	.word	1; 	.word	0; 	.word	0; 	.word	0; 	.word	0; 	.word	256. With SSE4, it should be; movdqa .LC0(%rip), %xmm0; pinsrb $6, %edi, %xmm0. //===---------------------------------------------------------------------===//. We should transform a shuffle of two vectors of constants into a single vector; of constants. Also, insertelement of a constant into a vector of constants; should also result in a vector of con",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:22592,Energy Efficiency,reduce,reduced,22592,"; %d = call float @fabsf(float %x); ret float %d; }. //===---------------------------------------------------------------------===//. This IR (from PR6194):. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128""; target triple = ""x86_64-apple-darwin10.0.0"". %0 = type { double, double }; %struct.float3 = type { float, float, float }. define void @test(%0, %struct.float3* nocapture %res) nounwind noinline ssp {; entry:; %tmp18 = extractvalue %0 %0, 0 ; <double> [#uses=1]; %tmp19 = bitcast double %tmp18 to i64 ; <i64> [#uses=1]; %tmp20 = zext i64 %tmp19 to i128 ; <i128> [#uses=1]; %tmp10 = lshr i128 %tmp20, 32 ; <i128> [#uses=1]; %tmp11 = trunc i128 %tmp10 to i32 ; <i32> [#uses=1]; %tmp12 = bitcast i32 %tmp11 to float ; <float> [#uses=1]; %tmp5 = getelementptr inbounds %struct.float3* %res, i64 0, i32 1 ; <float*> [#uses=1]; store float %tmp12, float* %tmp5; ret void; }. Compiles to:. _test: ## @test; 	movd	%xmm0, %rax; 	shrq	$32, %rax; 	movl	%eax, 4(%rdi); 	ret. This would be better kept in the SSE unit by treating XMM0 as a 4xfloat and; doing a shuffle from v[1] to v[0] then a float store. //===---------------------------------------------------------------------===//. [UNSAFE FP]. void foo(double, double, double);; void norm(double x, double y, double z) {; double scale = __builtin_sqrt(x*x + y*y + z*z);; foo(x/scale, y/scale, z/scale);; }. We currently generate an sqrtsd and 3 divsd instructions. This is bad, fp div is; slow and not pipelined. In -ffast-math mode we could compute ""1.0/scale"" first; and emit 3 mulsd in place of the divs. This can be done as a target-independent; transform. If we're dealing with floats instead of doubles we could even replace the sqrtss; and inversion with an rsqrtss instruction, which computes 1/sqrt faster at the; cost of reduced accuracy. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11704,Integrability,Wrap,Wrapper,11704,"s good, but ""x"" does silly movzwl stuff around into a GPR. It seems; like movd would be sufficient in both cases as the value is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:305,Modifiability,Variab,Variable,305,"//===---------------------------------------------------------------------===//; // Random ideas for the X86 backend: SSE-specific stuff.; //===---------------------------------------------------------------------===//. //===---------------------------------------------------------------------===//. SSE Variable shift can be custom lowered to something like this, which uses a; small table + unaligned load + shuffle instead of going through memory. __m128i_shift_right:; 	.byte	 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15; 	.byte	 -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1. ...; __m128i shift_right(__m128i value, unsigned long offset) {; return _mm_shuffle_epi8(value,; _mm_loadu_si128((__m128 *) (___m128i_shift_right + offset)));; }. //===---------------------------------------------------------------------===//. SSE has instructions for doing operations on complex numbers, we should pattern; match them. For example, this should turn into a horizontal add:. typedef float __attribute__((vector_size(16))) v4f32;; float f32(v4f32 A) {; return A[0]+A[1]+A[2]+A[3];; }. Instead we get this:. _f32: ## @f32; 	pshufd	$1, %xmm0, %xmm1 ## xmm1 = xmm0[1,0,0,0]; 	addss	%xmm0, %xmm1; 	pshufd	$3, %xmm0, %xmm2 ## xmm2 = xmm0[3,0,0,0]; 	movhlps	%xmm0, %xmm0 ## xmm0 = xmm0[1,1]; 	movaps	%xmm0, %xmm3; 	addss	%xmm1, %xmm3; 	movdqa	%xmm2, %xmm0; 	addss	%xmm3, %xmm0; 	ret. Also, there are cases where some simple local SLP would improve codegen a bit.; compiling this:. _Complex float f32(_Complex float A, _Complex float B) {; return A+B;; }. into:. _f32: ## @f32; 	movdqa	%xmm0, %xmm2; 	addss	%xmm1, %xmm2; 	pshufd	$1, %xmm1, %xmm1 ## xmm1 = xmm1[1,0,0,0]; 	pshufd	$1, %xmm0, %xmm3 ## xmm3 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm3; 	movaps	%xmm2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:10834,Modifiability,extend,extended,10834,"----------------------------------------------------------------===//. We should materialize vector constants like ""all ones"" and ""signbit"" with ; code like:. cmpeqps xmm1, xmm1 ; xmm1 = all-ones. and:; cmpeqps xmm1, xmm1 ; xmm1 = all-ones; psrlq xmm1, 31 ; xmm1 = all 100000000000... instead of using a load from the constant pool. The later is important for; ABS/NEG/copysign etc. //===---------------------------------------------------------------------===//. These functions:. #include <xmmintrin.h>; __m128i a;; void x(unsigned short n) {; a = _mm_slli_epi32 (a, n);; }; void y(unsigned n) {; a = _mm_slli_epi32 (a, n);; }. compile to ( -O3 -static -fomit-frame-pointer):; _x:; movzwl 4(%esp), %eax; movd %eax, %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret; _y:; movd 4(%esp), %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret. ""y"" looks good, but ""x"" does silly movzwl stuff around into a GPR. It seems; like movd would be sufficient in both cases as the value is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:404,Performance,load,load,404,"//===---------------------------------------------------------------------===//; // Random ideas for the X86 backend: SSE-specific stuff.; //===---------------------------------------------------------------------===//. //===---------------------------------------------------------------------===//. SSE Variable shift can be custom lowered to something like this, which uses a; small table + unaligned load + shuffle instead of going through memory. __m128i_shift_right:; 	.byte	 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15; 	.byte	 -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1. ...; __m128i shift_right(__m128i value, unsigned long offset) {; return _mm_shuffle_epi8(value,; _mm_loadu_si128((__m128 *) (___m128i_shift_right + offset)));; }. //===---------------------------------------------------------------------===//. SSE has instructions for doing operations on complex numbers, we should pattern; match them. For example, this should turn into a horizontal add:. typedef float __attribute__((vector_size(16))) v4f32;; float f32(v4f32 A) {; return A[0]+A[1]+A[2]+A[3];; }. Instead we get this:. _f32: ## @f32; 	pshufd	$1, %xmm0, %xmm1 ## xmm1 = xmm0[1,0,0,0]; 	addss	%xmm0, %xmm1; 	pshufd	$3, %xmm0, %xmm2 ## xmm2 = xmm0[3,0,0,0]; 	movhlps	%xmm0, %xmm0 ## xmm0 = xmm0[1,1]; 	movaps	%xmm0, %xmm3; 	addss	%xmm1, %xmm3; 	movdqa	%xmm2, %xmm0; 	addss	%xmm3, %xmm0; 	ret. Also, there are cases where some simple local SLP would improve codegen a bit.; compiling this:. _Complex float f32(_Complex float A, _Complex float B) {; return A+B;; }. into:. _f32: ## @f32; 	movdqa	%xmm0, %xmm2; 	addss	%xmm1, %xmm2; 	pshufd	$1, %xmm1, %xmm1 ## xmm1 = xmm1[1,0,0,0]; 	pshufd	$1, %xmm0, %xmm3 ## xmm3 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm3; 	movaps	%xmm2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:2561,Performance,load,load,2561,"loat B) {; return A+B;; }. into:. _f32: ## @f32; 	movdqa	%xmm0, %xmm2; 	addss	%xmm1, %xmm2; 	pshufd	$1, %xmm1, %xmm1 ## xmm1 = xmm1[1,0,0,0]; 	pshufd	$1, %xmm0, %xmm3 ## xmm3 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm3; 	movaps	%xmm2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } o",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:2787,Performance,load,load,2787,"2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:2800,Performance,load,load,2800,"2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:2816,Performance,load,load,2816,"2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:8304,Performance,load,load,8304,"), %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Now consider if the ... code caused xmm1 to get spilled. This might produce; this code:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... xorps %xmm0, %xmm0; movaps c2(%esp), %xmm1; movss %xmm1, %xmm0; ret. However, since the reload is only used by these instructions, we could ; ""fold"" it into the uses, producing something like this:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... movss c2(%esp), %xmm0; ret. ... saving two instructions. The basic idea is that a reload from a spill slot, can, if only one 4-byte ; chunk is used, bring in 3 zeros the one element instead of 4 elements.; This can be used to simplify a variety of shuffle operations, where the; elements are fixed zeros. //===---------------------------------------------------------------------===//. This code generates ugly code, probably due to costs being off or something:. define void @test(float* %P, <4 x float>* %P2 ) {; %xFloat0.688 = load float* %P; %tmp = load <4 x float>* %P2; %inFloat3.713 = insertelement <4 x float> %tmp, float 0.0, i32 3; store <4 x float> %inFloat3.713, <4 x float>* %P2; ret void; }. Generates:. _test:; 	movl	8(%esp), %eax; 	movaps	(%eax), %xmm0; 	pxor	%xmm1, %xmm1; 	movaps	%xmm0, %xmm2; 	shufps	$50, %xmm1, %xmm2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/documentation/Performance/Conceptual/; Accelerate_sse_migration/index.html. e.g. SSE select using and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to commute some CMPP operations. //===--------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:8327,Performance,load,load,8327,"), %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Now consider if the ... code caused xmm1 to get spilled. This might produce; this code:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... xorps %xmm0, %xmm0; movaps c2(%esp), %xmm1; movss %xmm1, %xmm0; ret. However, since the reload is only used by these instructions, we could ; ""fold"" it into the uses, producing something like this:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... movss c2(%esp), %xmm0; ret. ... saving two instructions. The basic idea is that a reload from a spill slot, can, if only one 4-byte ; chunk is used, bring in 3 zeros the one element instead of 4 elements.; This can be used to simplify a variety of shuffle operations, where the; elements are fixed zeros. //===---------------------------------------------------------------------===//. This code generates ugly code, probably due to costs being off or something:. define void @test(float* %P, <4 x float>* %P2 ) {; %xFloat0.688 = load float* %P; %tmp = load <4 x float>* %P2; %inFloat3.713 = insertelement <4 x float> %tmp, float 0.0, i32 3; store <4 x float> %inFloat3.713, <4 x float>* %P2; ret void; }. Generates:. _test:; 	movl	8(%esp), %eax; 	movaps	(%eax), %xmm0; 	pxor	%xmm1, %xmm1; 	movaps	%xmm0, %xmm2; 	shufps	$50, %xmm1, %xmm2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/documentation/Performance/Conceptual/; Accelerate_sse_migration/index.html. e.g. SSE select using and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to commute some CMPP operations. //===--------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:9039,Performance,Perform,Performance,9039,"nts are fixed zeros. //===---------------------------------------------------------------------===//. This code generates ugly code, probably due to costs being off or something:. define void @test(float* %P, <4 x float>* %P2 ) {; %xFloat0.688 = load float* %P; %tmp = load <4 x float>* %P2; %inFloat3.713 = insertelement <4 x float> %tmp, float 0.0, i32 3; store <4 x float> %inFloat3.713, <4 x float>* %P2; ret void; }. Generates:. _test:; 	movl	8(%esp), %eax; 	movaps	(%eax), %xmm0; 	pxor	%xmm1, %xmm1; 	movaps	%xmm0, %xmm2; 	shufps	$50, %xmm1, %xmm2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/documentation/Performance/Conceptual/; Accelerate_sse_migration/index.html. e.g. SSE select using and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to commute some CMPP operations. //===---------------------------------------------------------------------===//. Apply the same transformation that merged four float into a single 128-bit load; to loads from constant pool. //===---------------------------------------------------------------------===//. Floating point max / min are commutable when -enable-unsafe-fp-path is; specified. We should turn int_x86_sse_max_ss and X86ISD::FMIN etc. into other; nodes which are selected to max / min instructions that are marked commutable. //===---------------------------------------------------------------------===//. We should materialize vector constants like ""all ones"" and ""signbit"" with ; code like:. cmpeqps xmm1, xmm1 ; xmm1 = all-ones. and:; cmpeqps xmm1, xmm1 ; xmm1 = all-ones; psrlq x",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:9454,Performance,load,load,9454," %inFloat3.713, <4 x float>* %P2; ret void; }. Generates:. _test:; 	movl	8(%esp), %eax; 	movaps	(%eax), %xmm0; 	pxor	%xmm1, %xmm1; 	movaps	%xmm0, %xmm2; 	shufps	$50, %xmm1, %xmm2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/documentation/Performance/Conceptual/; Accelerate_sse_migration/index.html. e.g. SSE select using and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to commute some CMPP operations. //===---------------------------------------------------------------------===//. Apply the same transformation that merged four float into a single 128-bit load; to loads from constant pool. //===---------------------------------------------------------------------===//. Floating point max / min are commutable when -enable-unsafe-fp-path is; specified. We should turn int_x86_sse_max_ss and X86ISD::FMIN etc. into other; nodes which are selected to max / min instructions that are marked commutable. //===---------------------------------------------------------------------===//. We should materialize vector constants like ""all ones"" and ""signbit"" with ; code like:. cmpeqps xmm1, xmm1 ; xmm1 = all-ones. and:; cmpeqps xmm1, xmm1 ; xmm1 = all-ones; psrlq xmm1, 31 ; xmm1 = all 100000000000... instead of using a load from the constant pool. The later is important for; ABS/NEG/copysign etc. //===---------------------------------------------------------------------===//. These functions:. #include <xmmintrin.h>; __m128i a;; void x(unsigned short n) {; a = _mm_slli_epi32 (a, n);; }; void y(unsigned n) {; a = _mm_slli_epi32 (a, n",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:9463,Performance,load,loads,9463," %inFloat3.713, <4 x float>* %P2; ret void; }. Generates:. _test:; 	movl	8(%esp), %eax; 	movaps	(%eax), %xmm0; 	pxor	%xmm1, %xmm1; 	movaps	%xmm0, %xmm2; 	shufps	$50, %xmm1, %xmm2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/documentation/Performance/Conceptual/; Accelerate_sse_migration/index.html. e.g. SSE select using and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to commute some CMPP operations. //===---------------------------------------------------------------------===//. Apply the same transformation that merged four float into a single 128-bit load; to loads from constant pool. //===---------------------------------------------------------------------===//. Floating point max / min are commutable when -enable-unsafe-fp-path is; specified. We should turn int_x86_sse_max_ss and X86ISD::FMIN etc. into other; nodes which are selected to max / min instructions that are marked commutable. //===---------------------------------------------------------------------===//. We should materialize vector constants like ""all ones"" and ""signbit"" with ; code like:. cmpeqps xmm1, xmm1 ; xmm1 = all-ones. and:; cmpeqps xmm1, xmm1 ; xmm1 = all-ones; psrlq xmm1, 31 ; xmm1 = all 100000000000... instead of using a load from the constant pool. The later is important for; ABS/NEG/copysign etc. //===---------------------------------------------------------------------===//. These functions:. #include <xmmintrin.h>; __m128i a;; void x(unsigned short n) {; a = _mm_slli_epi32 (a, n);; }; void y(unsigned n) {; a = _mm_slli_epi32 (a, n",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:10114,Performance,load,load,10114,"sing and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to commute some CMPP operations. //===---------------------------------------------------------------------===//. Apply the same transformation that merged four float into a single 128-bit load; to loads from constant pool. //===---------------------------------------------------------------------===//. Floating point max / min are commutable when -enable-unsafe-fp-path is; specified. We should turn int_x86_sse_max_ss and X86ISD::FMIN etc. into other; nodes which are selected to max / min instructions that are marked commutable. //===---------------------------------------------------------------------===//. We should materialize vector constants like ""all ones"" and ""signbit"" with ; code like:. cmpeqps xmm1, xmm1 ; xmm1 = all-ones. and:; cmpeqps xmm1, xmm1 ; xmm1 = all-ones; psrlq xmm1, 31 ; xmm1 = all 100000000000... instead of using a load from the constant pool. The later is important for; ABS/NEG/copysign etc. //===---------------------------------------------------------------------===//. These functions:. #include <xmmintrin.h>; __m128i a;; void x(unsigned short n) {; a = _mm_slli_epi32 (a, n);; }; void y(unsigned n) {; a = _mm_slli_epi32 (a, n);; }. compile to ( -O3 -static -fomit-frame-pointer):; _x:; movzwl 4(%esp), %eax; movd %eax, %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret; _y:; movd 4(%esp), %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret. ""y"" looks good, but ""x"" does silly movzwl stuff around into a GPR. It seems; like movd would be sufficient in both cases as the value is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11443,Performance,load,loads,11443,"le to ( -O3 -static -fomit-frame-pointer):; _x:; movzwl 4(%esp), %eax; movd %eax, %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret; _y:; movd 4(%esp), %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret. ""y"" looks good, but ""x"" does silly movzwl stuff around into a GPR. It seems; like movd would be sufficient in both cases as the value is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p),",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11481,Performance,load,load,11481,"le to ( -O3 -static -fomit-frame-pointer):; _x:; movzwl 4(%esp), %eax; movd %eax, %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret; _y:; movd 4(%esp), %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret. ""y"" looks good, but ""x"" does silly movzwl stuff around into a GPR. It seems; like movd would be sufficient in both cases as the value is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p),",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11568,Performance,load,loads,11568,"m1, _a; ret; _y:; movd 4(%esp), %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret. ""y"" looks good, but ""x"" does silly movzwl stuff around into a GPR. It seems; like movd would be sufficient in both cases as the value is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11781,Performance,load,load,11781," is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11807,Performance,load,load,11807," is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11906,Performance,load,load,11906,"uld be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11925,Performance,load,load,11925,"uld be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11957,Performance,load,load,11957,"uld be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11988,Performance,load,load,11988,"uld be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:12049,Performance,load,load,12049,"nclude <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:12150,Performance,load,loads,12150,"%xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:12241,Performance,load,load,12241,"%xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:12438,Performance,load,load,12438,"aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't expo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:12466,Performance,load,load,12466,"aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't expo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:12516,Performance,load,load,12516,"aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't expo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:13209,Performance,load,load,13209,"ra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't exposed; until post-legalize, and at that point, the fneg has been custom expanded into ; an X86 fxor. This means that we need to handle this case in the x86 backend; instead of in target independent code. //===---------------------------------------------------------------------===//. Non-SSE4 insert into 16 x i8 is atrociously bad. //===---------------------------------------------------------------------===//. <2 x i64> extract is substantially worse than <2 x f64>, even if the destination; is memory. //===---------------------------------------------------------------------===//. INSERTPS can match any insert (extract, imm1), imm2 for 4 x float, and insert; any number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments i",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:13318,Performance,load,loads,13318,"mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't exposed; until post-legalize, and at that point, the fneg has been custom expanded into ; an X86 fxor. This means that we need to handle this case in the x86 backend; instead of in target independent code. //===---------------------------------------------------------------------===//. Non-SSE4 insert into 16 x i8 is atrociously bad. //===---------------------------------------------------------------------===//. <2 x i64> extract is substantially worse than <2 x f64>, even if the destination; is memory. //===---------------------------------------------------------------------===//. INSERTPS can match any insert (extract, imm1), imm2 for 4 x float, and insert; any number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments in LowerINSERT_VECTOR_ELT_SSE4. //===-----------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:13350,Performance,load,load,13350,"mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't exposed; until post-legalize, and at that point, the fneg has been custom expanded into ; an X86 fxor. This means that we need to handle this case in the x86 backend; instead of in target independent code. //===---------------------------------------------------------------------===//. Non-SSE4 insert into 16 x i8 is atrociously bad. //===---------------------------------------------------------------------===//. <2 x i64> extract is substantially worse than <2 x f64>, even if the destination; is memory. //===---------------------------------------------------------------------===//. INSERTPS can match any insert (extract, imm1), imm2 for 4 x float, and insert; any number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments in LowerINSERT_VECTOR_ELT_SSE4. //===-----------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:13445,Performance,load,load,13445,"ore, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't exposed; until post-legalize, and at that point, the fneg has been custom expanded into ; an X86 fxor. This means that we need to handle this case in the x86 backend; instead of in target independent code. //===---------------------------------------------------------------------===//. Non-SSE4 insert into 16 x i8 is atrociously bad. //===---------------------------------------------------------------------===//. <2 x i64> extract is substantially worse than <2 x f64>, even if the destination; is memory. //===---------------------------------------------------------------------===//. INSERTPS can match any insert (extract, imm1), imm2 for 4 x float, and insert; any number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments in LowerINSERT_VECTOR_ELT_SSE4. //===---------------------------------------------------------------------===//. On a random note, SSE2 should declare insert/extract of 2 x f64 as legal, not; Custom. All combinations of insert/extract reg-reg, reg-m",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:15175,Performance,load,loads,15175,"y number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments in LowerINSERT_VECTOR_ELT_SSE4. //===---------------------------------------------------------------------===//. On a random note, SSE2 should declare insert/extract of 2 x f64 as legal, not; Custom. All combinations of insert/extract reg-reg, reg-mem, and mem-reg are; legal, it'll just take a few extra patterns written in the .td file. Note: this is not a code quality issue; the custom lowered code happens to be; right, but we shouldn't have to custom lower anything. This is probably related; to <2 x i64> ops being so bad. //===---------------------------------------------------------------------===//. LLVM currently generates stack realignment code, when it is not necessary; needed. The problem is that we need to know about stack alignment too early,; before RA runs. At that point we don't know, whether there will be vector spill, or not.; Stack realignment logic is overly conservative here, but otherwise we can; produce unaligned loads/stores. Fixing this will require some huge RA changes. Testcase:; #include <emmintrin.h>. typedef short vSInt16 __attribute__ ((__vector_size__ (16)));. static const vSInt16 a = {- 22725, - 12873, - 22725, - 12873, - 22725, - 12873,; - 22725, - 12873};;. vSInt16 madd(vSInt16 b); {; return _mm_madd_epi16(a, b);; }. Generated code (x86-32, linux):; madd:; pushl %ebp; movl %esp, %ebp; andl $-16, %esp; movaps .LCPI1_0, %xmm1; pmaddwd %xmm1, %xmm0; movl %ebp, %esp; popl %ebp; ret. //===---------------------------------------------------------------------===//. Consider:; #include <emmintrin.h> ; __m128 foo2 (float x) {; return _mm_set_ps (0, 0, x, 0);; }. In x86-32 mode, we generate this spiffy code:. _foo2:; 	movss	4(%esp), %xmm0; 	pshufd	$81, %xmm0, %xmm0; 	ret. in x86-64 mode, we generate this code, which could be better:. _foo2:; 	xorps	%xmm1, %xmm1; 	movss	%xmm0, %xmm1; 	pshufd	$81, %xmm1, %xmm0; 	ret. In sse4 mode, we could use insertps t",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:20172,Performance,load,load,20172,"d of movsd to implement (scalar_to_vector (loadf64)); when code size is critical. movlps is slower than movsd on core2 but it's one; byte shorter. //===---------------------------------------------------------------------===//. We should use a dynamic programming based approach to tell when using FPStack; operations is cheaper than SSE. SciMark montecarlo contains code like this; for example:. double MonteCarlo_num_flops(int Num_samples) {; return ((double) Num_samples)* 4.0;; }. In fpstack mode, this compiles into:. LCPI1_0:					; 	.long	1082130432	## float 4.000000e+00; _MonteCarlo_num_flops:; 	subl	$4, %esp; 	movl	8(%esp), %eax; 	movl	%eax, (%esp); 	fildl	(%esp); 	fmuls	LCPI1_0; 	addl	$4, %esp; 	ret; ; in SSE mode, it compiles into significantly slower code:. _MonteCarlo_num_flops:; 	subl	$12, %esp; 	cvtsi2sd	16(%esp), %xmm0; 	mulsd	LCPI1_0, %xmm0; 	movsd	%xmm0, (%esp); 	fldl	(%esp); 	addl	$12, %esp; 	ret. There are also other cases in scimark where using fpstack is better, it is; cheaper to do fld1 than load from a constant pool for example, so; ""load, add 1.0, store"" is better done in the fp stack, etc. //===---------------------------------------------------------------------===//. These should compile into the same code (PR6214): Perhaps instcombine should; canonicalize the former into the later?. define float @foo(float %x) nounwind {; %t = bitcast float %x to i32; %s = and i32 %t, 2147483647; %d = bitcast i32 %s to float; ret float %d; }. declare float @fabsf(float %n); define float @bar(float %x) nounwind {; %d = call float @fabsf(float %x); ret float %d; }. //===---------------------------------------------------------------------===//. This IR (from PR6194):. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128""; target triple = ""x86_64-apple-darwin10.0.0"". %0 = type { double, double }; %struct.float3 = type { float, float, float }. define voi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:20216,Performance,load,load,20216,"d of movsd to implement (scalar_to_vector (loadf64)); when code size is critical. movlps is slower than movsd on core2 but it's one; byte shorter. //===---------------------------------------------------------------------===//. We should use a dynamic programming based approach to tell when using FPStack; operations is cheaper than SSE. SciMark montecarlo contains code like this; for example:. double MonteCarlo_num_flops(int Num_samples) {; return ((double) Num_samples)* 4.0;; }. In fpstack mode, this compiles into:. LCPI1_0:					; 	.long	1082130432	## float 4.000000e+00; _MonteCarlo_num_flops:; 	subl	$4, %esp; 	movl	8(%esp), %eax; 	movl	%eax, (%esp); 	fildl	(%esp); 	fmuls	LCPI1_0; 	addl	$4, %esp; 	ret; ; in SSE mode, it compiles into significantly slower code:. _MonteCarlo_num_flops:; 	subl	$12, %esp; 	cvtsi2sd	16(%esp), %xmm0; 	mulsd	LCPI1_0, %xmm0; 	movsd	%xmm0, (%esp); 	fldl	(%esp); 	addl	$12, %esp; 	ret. There are also other cases in scimark where using fpstack is better, it is; cheaper to do fld1 than load from a constant pool for example, so; ""load, add 1.0, store"" is better done in the fp stack, etc. //===---------------------------------------------------------------------===//. These should compile into the same code (PR6214): Perhaps instcombine should; canonicalize the former into the later?. define float @foo(float %x) nounwind {; %t = bitcast float %x to i32; %s = and i32 %t, 2147483647; %d = bitcast i32 %s to float; ret float %d; }. declare float @fabsf(float %n); define float @bar(float %x) nounwind {; %d = call float @fabsf(float %x); ret float %d; }. //===---------------------------------------------------------------------===//. This IR (from PR6194):. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128""; target triple = ""x86_64-apple-darwin10.0.0"". %0 = type { double, double }; %struct.float3 = type { float, float, float }. define voi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:2202,Safety,unsafe,unsafemath,2202,"xmm2 ## xmm2 = xmm0[3,0,0,0]; 	movhlps	%xmm0, %xmm0 ## xmm0 = xmm0[1,1]; 	movaps	%xmm0, %xmm3; 	addss	%xmm1, %xmm3; 	movdqa	%xmm2, %xmm0; 	addss	%xmm3, %xmm0; 	ret. Also, there are cases where some simple local SLP would improve codegen a bit.; compiling this:. _Complex float f32(_Complex float A, _Complex float B) {; return A+B;; }. into:. _f32: ## @f32; 	movdqa	%xmm0, %xmm2; 	addss	%xmm1, %xmm2; 	pshufd	$1, %xmm1, %xmm1 ## xmm1 = xmm1[1,0,0,0]; 	pshufd	$1, %xmm0, %xmm3 ## xmm3 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm3; 	movaps	%xmm2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===----",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:9623,Safety,unsafe,unsafe-fp-path,9623,"2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/documentation/Performance/Conceptual/; Accelerate_sse_migration/index.html. e.g. SSE select using and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to commute some CMPP operations. //===---------------------------------------------------------------------===//. Apply the same transformation that merged four float into a single 128-bit load; to loads from constant pool. //===---------------------------------------------------------------------===//. Floating point max / min are commutable when -enable-unsafe-fp-path is; specified. We should turn int_x86_sse_max_ss and X86ISD::FMIN etc. into other; nodes which are selected to max / min instructions that are marked commutable. //===---------------------------------------------------------------------===//. We should materialize vector constants like ""all ones"" and ""signbit"" with ; code like:. cmpeqps xmm1, xmm1 ; xmm1 = all-ones. and:; cmpeqps xmm1, xmm1 ; xmm1 = all-ones; psrlq xmm1, 31 ; xmm1 = all 100000000000... instead of using a load from the constant pool. The later is important for; ABS/NEG/copysign etc. //===---------------------------------------------------------------------===//. These functions:. #include <xmmintrin.h>; __m128i a;; void x(unsigned short n) {; a = _mm_slli_epi32 (a, n);; }; void y(unsigned n) {; a = _mm_slli_epi32 (a, n);; }. compile to ( -O3 -static -fomit-frame-pointer):; _x:; movzwl 4(%esp), %eax; movd %eax, %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret; _y:; movd 4(%esp",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:21993,Safety,UNSAFE,UNSAFE,21993,"; %d = call float @fabsf(float %x); ret float %d; }. //===---------------------------------------------------------------------===//. This IR (from PR6194):. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128""; target triple = ""x86_64-apple-darwin10.0.0"". %0 = type { double, double }; %struct.float3 = type { float, float, float }. define void @test(%0, %struct.float3* nocapture %res) nounwind noinline ssp {; entry:; %tmp18 = extractvalue %0 %0, 0 ; <double> [#uses=1]; %tmp19 = bitcast double %tmp18 to i64 ; <i64> [#uses=1]; %tmp20 = zext i64 %tmp19 to i128 ; <i128> [#uses=1]; %tmp10 = lshr i128 %tmp20, 32 ; <i128> [#uses=1]; %tmp11 = trunc i128 %tmp10 to i32 ; <i32> [#uses=1]; %tmp12 = bitcast i32 %tmp11 to float ; <float> [#uses=1]; %tmp5 = getelementptr inbounds %struct.float3* %res, i64 0, i32 1 ; <float*> [#uses=1]; store float %tmp12, float* %tmp5; ret void; }. Compiles to:. _test: ## @test; 	movd	%xmm0, %rax; 	shrq	$32, %rax; 	movl	%eax, 4(%rdi); 	ret. This would be better kept in the SSE unit by treating XMM0 as a 4xfloat and; doing a shuffle from v[1] to v[0] then a float store. //===---------------------------------------------------------------------===//. [UNSAFE FP]. void foo(double, double, double);; void norm(double x, double y, double z) {; double scale = __builtin_sqrt(x*x + y*y + z*z);; foo(x/scale, y/scale, z/scale);; }. We currently generate an sqrtsd and 3 divsd instructions. This is bad, fp div is; slow and not pipelined. In -ffast-math mode we could compute ""1.0/scale"" first; and emit 3 mulsd in place of the divs. This can be done as a target-independent; transform. If we're dealing with floats instead of doubles we could even replace the sqrtss; and inversion with an rsqrtss instruction, which computes 1/sqrt faster at the; cost of reduced accuracy. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:3718,Security,expose,exposed,3718,"eing lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_Cubic17TessellateSurfaceE, bb cond_next140. This is what icc; emits:. movaps (%edx), %xmm2 #59.21; movaps (%edx), %xmm5 #60.21; movaps (%edx), %xmm4 #61.21; movaps (%edx), %xmm3 #62.21; movl 40(%ecx), %ebp #69.49; shufps $0, %xmm2, %xmm5 #60.21; movl 100(%esp), %ebx #69.20; movl (%ebx), %edi #69.20; imull %ebp, %edi #69.49; addl (%eax), %edi #70.33; shufps $85, %xmm2, %xmm4 #61.21; shufps $170, %xmm2, %xmm3 #62.21; shufps $255, %xmm2, %xmm2 #63.21; lea (%ebp,%ebp,2), %ebx #69.49; negl %ebx #69.49; lea -3(%edi,%ebx), %ebx #70.33; shll $4, %ebx #68.37; addl 32(%ecx), %ebx #68.37; testb $15, %bl #91.13; jne L_B1.24 # Prob 5% #91.13. This is the llvm code after instruction scheduling:. cond_next140 (0xa910740, LLVM BB @0xa90beb0):; 	%reg1078 = MOV32ri -3; 	%reg1079 = ADD32rm %reg1078, %reg1068, 1, %noreg, 0; 	%reg1037 = MOV32rm %reg1024, 1, %noreg, 40; 	%reg1080 = IMUL32rr %reg1079, %reg1037; 	%reg1081 = MOV32rm %reg1058, 1, %noreg, 0; 	%reg1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:6551,Security,expose,exposes,6551," 	%esi = MOV32rm %esi, 1, %noreg, 0; 	MOV32mr %stack.4, 1, %noreg, 0, %esi; 	%eax = LEA32r %esi, 1, %eax, -3; 	%esi = MOV32rm %stack.7, 1, %noreg, 0; 	%esi = MOV32rm %esi, 1, %noreg, 32; 	%edi = MOV32rr %eax; 	SHL32ri %edi<def&use>, 4; 	ADD32rr %edi<def&use>, %esi; 	%xmm0 = MOVAPSrm %ecx, 1, %noreg, 0; 	%xmm1 = MOVAPSrr %xmm0; 	SHUFPSrr %xmm1<def&use>, %xmm1, 170; 	%xmm2 = MOVAPSrr %xmm0; 	SHUFPSrr %xmm2<def&use>, %xmm2, 0; 	%xmm3 = MOVAPSrr %xmm0; 	SHUFPSrr %xmm3<def&use>, %xmm3, 255; 	SHUFPSrr %xmm0<def&use>, %xmm0, 85; 	%ebx = MOV32rr %edi; 	AND32ri8 %ebx<def&use>, 15; 	CMP32ri8 %ebx, 0; 	JE mbb<cond_next204,0xa914d30>. This looks really bad. The problem is shufps is a destructive opcode. Since it; appears as operand two in more than one shufps ops. It resulted in a number of; copies. Note icc also suffers from the same problem. Either the instruction; selector should select pshufd or The register allocator can made the two-address; to three-address transformation. It also exposes some other problems. See MOV32ri -3 and the spills. //===---------------------------------------------------------------------===//. Consider:. __m128 test(float a) {; return _mm_set_ps(0.0, 0.0, 0.0, a*a);; }. This compiles into:. movss 4(%esp), %xmm1; mulss %xmm1, %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Because mulss doesn't modify the top 3 elements, the top elements of ; xmm1 are already zero'd. We could compile this to:. movss 4(%esp), %xmm0; mulss %xmm0, %xmm0; ret. //===---------------------------------------------------------------------===//. Here's a sick and twisted idea. Consider code like this:. __m128 test(__m128 a) {; float b = *(float*)&A;; ...; return _mm_set_ps(0.0, 0.0, 0.0, b);; }. This might compile to this code:. movaps c(%esp), %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Now consider if the ... code caused xmm1 to get spilled. This might produce; this code:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... xorps %xmm0, %xmm0; movaps c2(%esp), ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:13462,Security,expose,exposed,13462,"ore, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't exposed; until post-legalize, and at that point, the fneg has been custom expanded into ; an X86 fxor. This means that we need to handle this case in the x86 backend; instead of in target independent code. //===---------------------------------------------------------------------===//. Non-SSE4 insert into 16 x i8 is atrociously bad. //===---------------------------------------------------------------------===//. <2 x i64> extract is substantially worse than <2 x f64>, even if the destination; is memory. //===---------------------------------------------------------------------===//. INSERTPS can match any insert (extract, imm1), imm2 for 4 x float, and insert; any number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments in LowerINSERT_VECTOR_ELT_SSE4. //===---------------------------------------------------------------------===//. On a random note, SSE2 should declare insert/extract of 2 x f64 as legal, not; Custom. All combinations of insert/extract reg-reg, reg-m",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:2500,Testability,test,testcase,2500,"loat B) {; return A+B;; }. into:. _f32: ## @f32; 	movdqa	%xmm0, %xmm2; 	addss	%xmm1, %xmm2; 	pshufd	$1, %xmm1, %xmm1 ## xmm1 = xmm1[1,0,0,0]; 	pshufd	$1, %xmm0, %xmm3 ## xmm3 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm3; 	movaps	%xmm2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } o",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:3707,Testability,test,test,3707,"eing lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_Cubic17TessellateSurfaceE, bb cond_next140. This is what icc; emits:. movaps (%edx), %xmm2 #59.21; movaps (%edx), %xmm5 #60.21; movaps (%edx), %xmm4 #61.21; movaps (%edx), %xmm3 #62.21; movl 40(%ecx), %ebp #69.49; shufps $0, %xmm2, %xmm5 #60.21; movl 100(%esp), %ebx #69.20; movl (%ebx), %edi #69.20; imull %ebp, %edi #69.49; addl (%eax), %edi #70.33; shufps $85, %xmm2, %xmm4 #61.21; shufps $170, %xmm2, %xmm3 #62.21; shufps $255, %xmm2, %xmm2 #63.21; lea (%ebp,%ebp,2), %ebx #69.49; negl %ebx #69.49; lea -3(%edi,%ebx), %ebx #70.33; shll $4, %ebx #68.37; addl 32(%ecx), %ebx #68.37; testb $15, %bl #91.13; jne L_B1.24 # Prob 5% #91.13. This is the llvm code after instruction scheduling:. cond_next140 (0xa910740, LLVM BB @0xa90beb0):; 	%reg1078 = MOV32ri -3; 	%reg1079 = ADD32rm %reg1078, %reg1068, 1, %noreg, 0; 	%reg1037 = MOV32rm %reg1024, 1, %noreg, 40; 	%reg1080 = IMUL32rr %reg1079, %reg1037; 	%reg1081 = MOV32rm %reg1058, 1, %noreg, 0; 	%reg1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:4352,Testability,test,testb,4352,"o movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_Cubic17TessellateSurfaceE, bb cond_next140. This is what icc; emits:. movaps (%edx), %xmm2 #59.21; movaps (%edx), %xmm5 #60.21; movaps (%edx), %xmm4 #61.21; movaps (%edx), %xmm3 #62.21; movl 40(%ecx), %ebp #69.49; shufps $0, %xmm2, %xmm5 #60.21; movl 100(%esp), %ebx #69.20; movl (%ebx), %edi #69.20; imull %ebp, %edi #69.49; addl (%eax), %edi #70.33; shufps $85, %xmm2, %xmm4 #61.21; shufps $170, %xmm2, %xmm3 #62.21; shufps $255, %xmm2, %xmm2 #63.21; lea (%ebp,%ebp,2), %ebx #69.49; negl %ebx #69.49; lea -3(%edi,%ebx), %ebx #70.33; shll $4, %ebx #68.37; addl 32(%ecx), %ebx #68.37; testb $15, %bl #91.13; jne L_B1.24 # Prob 5% #91.13. This is the llvm code after instruction scheduling:. cond_next140 (0xa910740, LLVM BB @0xa90beb0):; 	%reg1078 = MOV32ri -3; 	%reg1079 = ADD32rm %reg1078, %reg1068, 1, %noreg, 0; 	%reg1037 = MOV32rm %reg1024, 1, %noreg, 40; 	%reg1080 = IMUL32rr %reg1079, %reg1037; 	%reg1081 = MOV32rm %reg1058, 1, %noreg, 0; 	%reg1038 = LEA32r %reg1081, 1, %reg1080, -3; 	%reg1036 = MOV32rm %reg1024, 1, %noreg, 32; 	%reg1082 = SHL32ri %reg1038, 4; 	%reg1039 = ADD32rr %reg1036, %reg1082; 	%reg1083 = MOVAPSrm %reg1059, 1, %noreg, 0; 	%reg1034 = SHUFPSrr %reg1083, %reg1083, 170; 	%reg1032 = SHUFPSrr %reg1083, %reg1083, 0; 	%reg1035 = SHUFPSrr %reg1083, %reg1083, 255; 	%reg1033 = SHUFPSrr %reg1083, %reg1083, 85; 	%reg1040 = MOV32rr %reg1039; 	%reg1084 = AND32ri8 %reg1039, 15; 	CMP32ri8 %reg1084, 0; 	JE mbb<cond_next204,0xa914d30>. Still ok. After register allocation:. cond_next140 (0xa910740, LLVM BB @0xa90beb0):; 	%eax = MOV32ri -3; 	%edx = MOV32rm %stack.3, 1, %n",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:6710,Testability,test,test,6710,"m %esi, 1, %noreg, 32; 	%edi = MOV32rr %eax; 	SHL32ri %edi<def&use>, 4; 	ADD32rr %edi<def&use>, %esi; 	%xmm0 = MOVAPSrm %ecx, 1, %noreg, 0; 	%xmm1 = MOVAPSrr %xmm0; 	SHUFPSrr %xmm1<def&use>, %xmm1, 170; 	%xmm2 = MOVAPSrr %xmm0; 	SHUFPSrr %xmm2<def&use>, %xmm2, 0; 	%xmm3 = MOVAPSrr %xmm0; 	SHUFPSrr %xmm3<def&use>, %xmm3, 255; 	SHUFPSrr %xmm0<def&use>, %xmm0, 85; 	%ebx = MOV32rr %edi; 	AND32ri8 %ebx<def&use>, 15; 	CMP32ri8 %ebx, 0; 	JE mbb<cond_next204,0xa914d30>. This looks really bad. The problem is shufps is a destructive opcode. Since it; appears as operand two in more than one shufps ops. It resulted in a number of; copies. Note icc also suffers from the same problem. Either the instruction; selector should select pshufd or The register allocator can made the two-address; to three-address transformation. It also exposes some other problems. See MOV32ri -3 and the spills. //===---------------------------------------------------------------------===//. Consider:. __m128 test(float a) {; return _mm_set_ps(0.0, 0.0, 0.0, a*a);; }. This compiles into:. movss 4(%esp), %xmm1; mulss %xmm1, %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Because mulss doesn't modify the top 3 elements, the top elements of ; xmm1 are already zero'd. We could compile this to:. movss 4(%esp), %xmm0; mulss %xmm0, %xmm0; ret. //===---------------------------------------------------------------------===//. Here's a sick and twisted idea. Consider code like this:. __m128 test(__m128 a) {; float b = *(float*)&A;; ...; return _mm_set_ps(0.0, 0.0, 0.0, b);; }. This might compile to this code:. movaps c(%esp), %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Now consider if the ... code caused xmm1 to get spilled. This might produce; this code:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... xorps %xmm0, %xmm0; movaps c2(%esp), %xmm1; movss %xmm1, %xmm0; ret. However, since the reload is only used by these instructions, we could ; ""fold"" it into the uses, producing something like this:. mo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:7194,Testability,test,test,7194,"d. The problem is shufps is a destructive opcode. Since it; appears as operand two in more than one shufps ops. It resulted in a number of; copies. Note icc also suffers from the same problem. Either the instruction; selector should select pshufd or The register allocator can made the two-address; to three-address transformation. It also exposes some other problems. See MOV32ri -3 and the spills. //===---------------------------------------------------------------------===//. Consider:. __m128 test(float a) {; return _mm_set_ps(0.0, 0.0, 0.0, a*a);; }. This compiles into:. movss 4(%esp), %xmm1; mulss %xmm1, %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Because mulss doesn't modify the top 3 elements, the top elements of ; xmm1 are already zero'd. We could compile this to:. movss 4(%esp), %xmm0; mulss %xmm0, %xmm0; ret. //===---------------------------------------------------------------------===//. Here's a sick and twisted idea. Consider code like this:. __m128 test(__m128 a) {; float b = *(float*)&A;; ...; return _mm_set_ps(0.0, 0.0, 0.0, b);; }. This might compile to this code:. movaps c(%esp), %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Now consider if the ... code caused xmm1 to get spilled. This might produce; this code:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... xorps %xmm0, %xmm0; movaps c2(%esp), %xmm1; movss %xmm1, %xmm0; ret. However, since the reload is only used by these instructions, we could ; ""fold"" it into the uses, producing something like this:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... movss c2(%esp), %xmm0; ret. ... saving two instructions. The basic idea is that a reload from a spill slot, can, if only one 4-byte ; chunk is used, bring in 3 zeros the one element instead of 4 elements.; This can be used to simplify a variety of shuffle operations, where the; elements are fixed zeros. //===---------------------------------------------------------------------===//. This code generates ugly code, probably due to costs",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:8251,Testability,test,test,8251,", 0.0, b);; }. This might compile to this code:. movaps c(%esp), %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Now consider if the ... code caused xmm1 to get spilled. This might produce; this code:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... xorps %xmm0, %xmm0; movaps c2(%esp), %xmm1; movss %xmm1, %xmm0; ret. However, since the reload is only used by these instructions, we could ; ""fold"" it into the uses, producing something like this:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... movss c2(%esp), %xmm0; ret. ... saving two instructions. The basic idea is that a reload from a spill slot, can, if only one 4-byte ; chunk is used, bring in 3 zeros the one element instead of 4 elements.; This can be used to simplify a variety of shuffle operations, where the; elements are fixed zeros. //===---------------------------------------------------------------------===//. This code generates ugly code, probably due to costs being off or something:. define void @test(float* %P, <4 x float>* %P2 ) {; %xFloat0.688 = load float* %P; %tmp = load <4 x float>* %P2; %inFloat3.713 = insertelement <4 x float> %tmp, float 0.0, i32 3; store <4 x float> %inFloat3.713, <4 x float>* %P2; ret void; }. Generates:. _test:; 	movl	8(%esp), %eax; 	movaps	(%eax), %xmm0; 	pxor	%xmm1, %xmm1; 	movaps	%xmm0, %xmm2; 	shufps	$50, %xmm1, %xmm2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/documentation/Performance/Conceptual/; Accelerate_sse_migration/index.html. e.g. SSE select using and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11408,Testability,test,tests,11408,"le to ( -O3 -static -fomit-frame-pointer):; _x:; movzwl 4(%esp), %eax; movd %eax, %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret; _y:; movd 4(%esp), %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret. ""y"" looks good, but ""x"" does silly movzwl stuff around into a GPR. It seems; like movd would be sufficient in both cases as the value is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p),",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:12236,Testability,stub,stub,12236,"%xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:13171,Testability,stub,stub,13171," anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't exposed; until post-legalize, and at that point, the fneg has been custom expanded into ; an X86 fxor. This means that we need to handle this case in the x86 backend; instead of in target independent code. //===---------------------------------------------------------------------===//. Non-SSE4 insert into 16 x i8 is atrociously bad. //===---------------------------------------------------------------------===//. <2 x i64> extract is substantially worse than <2 x f64>, even if the destination; is memory. //===---------------------------------------------------------------------===//. INSERTPS can match any insert (extract, ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:15100,Testability,log,logic,15100,"y number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments in LowerINSERT_VECTOR_ELT_SSE4. //===---------------------------------------------------------------------===//. On a random note, SSE2 should declare insert/extract of 2 x f64 as legal, not; Custom. All combinations of insert/extract reg-reg, reg-mem, and mem-reg are; legal, it'll just take a few extra patterns written in the .td file. Note: this is not a code quality issue; the custom lowered code happens to be; right, but we shouldn't have to custom lower anything. This is probably related; to <2 x i64> ops being so bad. //===---------------------------------------------------------------------===//. LLVM currently generates stack realignment code, when it is not necessary; needed. The problem is that we need to know about stack alignment too early,; before RA runs. At that point we don't know, whether there will be vector spill, or not.; Stack realignment logic is overly conservative here, but otherwise we can; produce unaligned loads/stores. Fixing this will require some huge RA changes. Testcase:; #include <emmintrin.h>. typedef short vSInt16 __attribute__ ((__vector_size__ (16)));. static const vSInt16 a = {- 22725, - 12873, - 22725, - 12873, - 22725, - 12873,; - 22725, - 12873};;. vSInt16 madd(vSInt16 b); {; return _mm_madd_epi16(a, b);; }. Generated code (x86-32, linux):; madd:; pushl %ebp; movl %esp, %ebp; andl $-16, %esp; movaps .LCPI1_0, %xmm1; pmaddwd %xmm1, %xmm0; movl %ebp, %esp; popl %ebp; ret. //===---------------------------------------------------------------------===//. Consider:; #include <emmintrin.h> ; __m128 foo2 (float x) {; return _mm_set_ps (0, 0, x, 0);; }. In x86-32 mode, we generate this spiffy code:. _foo2:; 	movss	4(%esp), %xmm0; 	pshufd	$81, %xmm0, %xmm0; 	ret. in x86-64 mode, we generate this code, which could be better:. _foo2:; 	xorps	%xmm1, %xmm1; 	movss	%xmm0, %xmm1; 	pshufd	$81, %xmm1, %xmm0; 	ret. In sse4 mode, we could use insertps t",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:15236,Testability,Test,Testcase,15236,"LT_SSE4. //===---------------------------------------------------------------------===//. On a random note, SSE2 should declare insert/extract of 2 x f64 as legal, not; Custom. All combinations of insert/extract reg-reg, reg-mem, and mem-reg are; legal, it'll just take a few extra patterns written in the .td file. Note: this is not a code quality issue; the custom lowered code happens to be; right, but we shouldn't have to custom lower anything. This is probably related; to <2 x i64> ops being so bad. //===---------------------------------------------------------------------===//. LLVM currently generates stack realignment code, when it is not necessary; needed. The problem is that we need to know about stack alignment too early,; before RA runs. At that point we don't know, whether there will be vector spill, or not.; Stack realignment logic is overly conservative here, but otherwise we can; produce unaligned loads/stores. Fixing this will require some huge RA changes. Testcase:; #include <emmintrin.h>. typedef short vSInt16 __attribute__ ((__vector_size__ (16)));. static const vSInt16 a = {- 22725, - 12873, - 22725, - 12873, - 22725, - 12873,; - 22725, - 12873};;. vSInt16 madd(vSInt16 b); {; return _mm_madd_epi16(a, b);; }. Generated code (x86-32, linux):; madd:; pushl %ebp; movl %esp, %ebp; andl $-16, %esp; movaps .LCPI1_0, %xmm1; pmaddwd %xmm1, %xmm0; movl %ebp, %esp; popl %ebp; ret. //===---------------------------------------------------------------------===//. Consider:; #include <emmintrin.h> ; __m128 foo2 (float x) {; return _mm_set_ps (0, 0, x, 0);; }. In x86-32 mode, we generate this spiffy code:. _foo2:; 	movss	4(%esp), %xmm0; 	pshufd	$81, %xmm0, %xmm0; 	ret. in x86-64 mode, we generate this code, which could be better:. _foo2:; 	xorps	%xmm1, %xmm1; 	movss	%xmm0, %xmm1; 	pshufd	$81, %xmm1, %xmm0; 	ret. In sse4 mode, we could use insertps to make both better. Here's another testcase that could use insertps [mem]:. #include <xmmintrin.h>; extern float x2, ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:16169,Testability,test,testcase,16169,"stores. Fixing this will require some huge RA changes. Testcase:; #include <emmintrin.h>. typedef short vSInt16 __attribute__ ((__vector_size__ (16)));. static const vSInt16 a = {- 22725, - 12873, - 22725, - 12873, - 22725, - 12873,; - 22725, - 12873};;. vSInt16 madd(vSInt16 b); {; return _mm_madd_epi16(a, b);; }. Generated code (x86-32, linux):; madd:; pushl %ebp; movl %esp, %ebp; andl $-16, %esp; movaps .LCPI1_0, %xmm1; pmaddwd %xmm1, %xmm0; movl %ebp, %esp; popl %ebp; ret. //===---------------------------------------------------------------------===//. Consider:; #include <emmintrin.h> ; __m128 foo2 (float x) {; return _mm_set_ps (0, 0, x, 0);; }. In x86-32 mode, we generate this spiffy code:. _foo2:; 	movss	4(%esp), %xmm0; 	pshufd	$81, %xmm0, %xmm0; 	ret. in x86-64 mode, we generate this code, which could be better:. _foo2:; 	xorps	%xmm1, %xmm1; 	movss	%xmm0, %xmm1; 	pshufd	$81, %xmm1, %xmm0; 	ret. In sse4 mode, we could use insertps to make both better. Here's another testcase that could use insertps [mem]:. #include <xmmintrin.h>; extern float x2, x3;; __m128 foo1 (float x1, float x4) {; return _mm_set_ps (x2, x1, x3, x4);; }. gcc mainline compiles it to:. foo1:; insertps $0x10, x2(%rip), %xmm0; insertps $0x10, x3(%rip), %xmm1; movaps %xmm1, %xmm2; movlhps %xmm0, %xmm2; movaps %xmm2, %xmm0; ret. //===---------------------------------------------------------------------===//. We compile vector multiply-by-constant into poor code:. define <4 x i32> @f(<4 x i32> %i) nounwind {; 	%A = mul <4 x i32> %i, < i32 10, i32 10, i32 10, i32 10 >; 	ret <4 x i32> %A; }. On targets without SSE4.1, this compiles into:. LCPI1_0:					## <4 x i32>; 	.long	10; 	.long	10; 	.long	10; 	.long	10; 	.text; 	.align	4,0x90; 	.globl	_f; _f:; 	pshufd	$3, %xmm0, %xmm1; 	movd	%xmm1, %eax; 	imull	LCPI1_0+12, %eax; 	movd	%eax, %xmm1; 	pshufd	$1, %xmm0, %xmm2; 	movd	%xmm2, %eax; 	imull	LCPI1_0+4, %eax; 	movd	%eax, %xmm2; 	punpckldq	%xmm1, %xmm2; 	movd	%xmm0, %eax; 	imull	LCPI1_0, %eax; 	movd	%ea",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:21153,Testability,test,test,21153,"r to do fld1 than load from a constant pool for example, so; ""load, add 1.0, store"" is better done in the fp stack, etc. //===---------------------------------------------------------------------===//. These should compile into the same code (PR6214): Perhaps instcombine should; canonicalize the former into the later?. define float @foo(float %x) nounwind {; %t = bitcast float %x to i32; %s = and i32 %t, 2147483647; %d = bitcast i32 %s to float; ret float %d; }. declare float @fabsf(float %n); define float @bar(float %x) nounwind {; %d = call float @fabsf(float %x); ret float %d; }. //===---------------------------------------------------------------------===//. This IR (from PR6194):. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128""; target triple = ""x86_64-apple-darwin10.0.0"". %0 = type { double, double }; %struct.float3 = type { float, float, float }. define void @test(%0, %struct.float3* nocapture %res) nounwind noinline ssp {; entry:; %tmp18 = extractvalue %0 %0, 0 ; <double> [#uses=1]; %tmp19 = bitcast double %tmp18 to i64 ; <i64> [#uses=1]; %tmp20 = zext i64 %tmp19 to i128 ; <i128> [#uses=1]; %tmp10 = lshr i128 %tmp20, 32 ; <i128> [#uses=1]; %tmp11 = trunc i128 %tmp10 to i32 ; <i32> [#uses=1]; %tmp12 = bitcast i32 %tmp11 to float ; <float> [#uses=1]; %tmp5 = getelementptr inbounds %struct.float3* %res, i64 0, i32 1 ; <float*> [#uses=1]; store float %tmp12, float* %tmp5; ret void; }. Compiles to:. _test: ## @test; 	movd	%xmm0, %rax; 	shrq	$32, %rax; 	movl	%eax, 4(%rdi); 	ret. This would be better kept in the SSE unit by treating XMM0 as a 4xfloat and; doing a shuffle from v[1] to v[0] then a float store. //===---------------------------------------------------------------------===//. [UNSAFE FP]. void foo(double, double, double);; void norm(double x, double y, double z) {; double scale = __builtin_sqrt(x*x + y*y + z*z);; foo(x/scale, y/scale, z",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:21711,Testability,test,test,21711,"; %d = call float @fabsf(float %x); ret float %d; }. //===---------------------------------------------------------------------===//. This IR (from PR6194):. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128""; target triple = ""x86_64-apple-darwin10.0.0"". %0 = type { double, double }; %struct.float3 = type { float, float, float }. define void @test(%0, %struct.float3* nocapture %res) nounwind noinline ssp {; entry:; %tmp18 = extractvalue %0 %0, 0 ; <double> [#uses=1]; %tmp19 = bitcast double %tmp18 to i64 ; <i64> [#uses=1]; %tmp20 = zext i64 %tmp19 to i128 ; <i128> [#uses=1]; %tmp10 = lshr i128 %tmp20, 32 ; <i128> [#uses=1]; %tmp11 = trunc i128 %tmp10 to i32 ; <i32> [#uses=1]; %tmp12 = bitcast i32 %tmp11 to float ; <float> [#uses=1]; %tmp5 = getelementptr inbounds %struct.float3* %res, i64 0, i32 1 ; <float*> [#uses=1]; store float %tmp12, float* %tmp5; ret void; }. Compiles to:. _test: ## @test; 	movd	%xmm0, %rax; 	shrq	$32, %rax; 	movl	%eax, 4(%rdi); 	ret. This would be better kept in the SSE unit by treating XMM0 as a 4xfloat and; doing a shuffle from v[1] to v[0] then a float store. //===---------------------------------------------------------------------===//. [UNSAFE FP]. void foo(double, double, double);; void norm(double x, double y, double z) {; double scale = __builtin_sqrt(x*x + y*y + z*z);; foo(x/scale, y/scale, z/scale);; }. We currently generate an sqrtsd and 3 divsd instructions. This is bad, fp div is; slow and not pipelined. In -ffast-math mode we could compute ""1.0/scale"" first; and emit 3 mulsd in place of the divs. This can be done as a target-independent; transform. If we're dealing with floats instead of doubles we could even replace the sqrtss; and inversion with an rsqrtss instruction, which computes 1/sqrt faster at the; cost of reduced accuracy. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:1430,Usability,simpl,simple,1430," through memory. __m128i_shift_right:; 	.byte	 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15; 	.byte	 -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1. ...; __m128i shift_right(__m128i value, unsigned long offset) {; return _mm_shuffle_epi8(value,; _mm_loadu_si128((__m128 *) (___m128i_shift_right + offset)));; }. //===---------------------------------------------------------------------===//. SSE has instructions for doing operations on complex numbers, we should pattern; match them. For example, this should turn into a horizontal add:. typedef float __attribute__((vector_size(16))) v4f32;; float f32(v4f32 A) {; return A[0]+A[1]+A[2]+A[3];; }. Instead we get this:. _f32: ## @f32; 	pshufd	$1, %xmm0, %xmm1 ## xmm1 = xmm0[1,0,0,0]; 	addss	%xmm0, %xmm1; 	pshufd	$3, %xmm0, %xmm2 ## xmm2 = xmm0[3,0,0,0]; 	movhlps	%xmm0, %xmm0 ## xmm0 = xmm0[1,1]; 	movaps	%xmm0, %xmm3; 	addss	%xmm1, %xmm3; 	movdqa	%xmm2, %xmm0; 	addss	%xmm3, %xmm0; 	ret. Also, there are cases where some simple local SLP would improve codegen a bit.; compiling this:. _Complex float f32(_Complex float A, _Complex float B) {; return A+B;; }. into:. _f32: ## @f32; 	movdqa	%xmm0, %xmm2; 	addss	%xmm1, %xmm2; 	pshufd	$1, %xmm1, %xmm1 ## xmm1 = xmm1[1,0,0,0]; 	pshufd	$1, %xmm0, %xmm3 ## xmm3 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm3; 	movaps	%xmm2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===-----------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:3588,Usability,clear,clear,3588,":. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_Cubic17TessellateSurfaceE, bb cond_next140. This is what icc; emits:. movaps (%edx), %xmm2 #59.21; movaps (%edx), %xmm5 #60.21; movaps (%edx), %xmm4 #61.21; movaps (%edx), %xmm3 #62.21; movl 40(%ecx), %ebp #69.49; shufps $0, %xmm2, %xmm5 #60.21; movl 100(%esp), %ebx #69.20; movl (%ebx), %edi #69.20; imull %ebp, %edi #69.49; addl (%eax), %edi #70.33; shufps $85, %xmm2, %xmm4 #61.21; shufps $170, %xmm2, %xmm3 #62.21; shufps $255, %xmm2, %xmm2 #63.21; lea (%ebp,%ebp,2), %ebx #69.49; negl %ebx #69.49; lea -3(%edi,%ebx), %ebx #70.33; shll $4, %ebx #68.37; addl 32(%ecx), %ebx #68.37; testb $15, %bl #91.13; jne L_B1.24 # Prob 5% #91.13. This is the llvm code after instruction scheduling:. cond_next140 (0xa910740, LLVM BB @0xa90beb0):; 	%reg1078 = MOV32ri -3; 	%reg1079 = ADD32rm %reg1078, %reg1068, 1, %noreg, 0; 	%re",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:8000,Usability,simpl,simplify,8000," %xmm0, %xmm0; ret. //===---------------------------------------------------------------------===//. Here's a sick and twisted idea. Consider code like this:. __m128 test(__m128 a) {; float b = *(float*)&A;; ...; return _mm_set_ps(0.0, 0.0, 0.0, b);; }. This might compile to this code:. movaps c(%esp), %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Now consider if the ... code caused xmm1 to get spilled. This might produce; this code:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... xorps %xmm0, %xmm0; movaps c2(%esp), %xmm1; movss %xmm1, %xmm0; ret. However, since the reload is only used by these instructions, we could ; ""fold"" it into the uses, producing something like this:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... movss c2(%esp), %xmm0; ret. ... saving two instructions. The basic idea is that a reload from a spill slot, can, if only one 4-byte ; chunk is used, bring in 3 zeros the one element instead of 4 elements.; This can be used to simplify a variety of shuffle operations, where the; elements are fixed zeros. //===---------------------------------------------------------------------===//. This code generates ugly code, probably due to costs being off or something:. define void @test(float* %P, <4 x float>* %P2 ) {; %xFloat0.688 = load float* %P; %tmp = load <4 x float>* %P2; %inFloat3.713 = insertelement <4 x float> %tmp, float 0.0, i32 3; store <4 x float> %inFloat3.713, <4 x float>* %P2; ret void; }. Generates:. _test:; 	movl	8(%esp), %eax; 	movaps	(%eax), %xmm0; 	pxor	%xmm1, %xmm1; 	movaps	%xmm0, %xmm2; 	shufps	$50, %xmm1, %xmm2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/doc",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:8990,Usability,Guid,Guide,8990,"d of 4 elements.; This can be used to simplify a variety of shuffle operations, where the; elements are fixed zeros. //===---------------------------------------------------------------------===//. This code generates ugly code, probably due to costs being off or something:. define void @test(float* %P, <4 x float>* %P2 ) {; %xFloat0.688 = load float* %P; %tmp = load <4 x float>* %P2; %inFloat3.713 = insertelement <4 x float> %tmp, float 0.0, i32 3; store <4 x float> %inFloat3.713, <4 x float>* %P2; ret void; }. Generates:. _test:; 	movl	8(%esp), %eax; 	movaps	(%eax), %xmm0; 	pxor	%xmm1, %xmm1; 	movaps	%xmm0, %xmm2; 	shufps	$50, %xmm1, %xmm2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/documentation/Performance/Conceptual/; Accelerate_sse_migration/index.html. e.g. SSE select using and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to commute some CMPP operations. //===---------------------------------------------------------------------===//. Apply the same transformation that merged four float into a single 128-bit load; to loads from constant pool. //===---------------------------------------------------------------------===//. Floating point max / min are commutable when -enable-unsafe-fp-path is; specified. We should turn int_x86_sse_max_ss and X86ISD::FMIN etc. into other; nodes which are selected to max / min instructions that are marked commutable. //===---------------------------------------------------------------------===//. We should materialize vector constants like ""all ones"" and ""signbit"" with ; code l",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:14195,Usability,simpl,simple,14195,"esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't exposed; until post-legalize, and at that point, the fneg has been custom expanded into ; an X86 fxor. This means that we need to handle this case in the x86 backend; instead of in target independent code. //===---------------------------------------------------------------------===//. Non-SSE4 insert into 16 x i8 is atrociously bad. //===---------------------------------------------------------------------===//. <2 x i64> extract is substantially worse than <2 x f64>, even if the destination; is memory. //===---------------------------------------------------------------------===//. INSERTPS can match any insert (extract, imm1), imm2 for 4 x float, and insert; any number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments in LowerINSERT_VECTOR_ELT_SSE4. //===---------------------------------------------------------------------===//. On a random note, SSE2 should declare insert/extract of 2 x f64 as legal, not; Custom. All combinations of insert/extract reg-reg, reg-mem, and mem-reg are; legal, it'll just take a few extra patterns written in the .td file. Note: this is not a code quality issue; the custom lowered code happens to be; right, but we shouldn't have to custom lower anything. This is probably related; to <2 x i64> ops being so bad. //===---------------------------------------------------------------------===//. LLVM currently generates stack realignment code, when it is not necessary; needed. The problem is that we need to know about stack alignment too early,; before RA runs. At that point we don't know, whether there will be vector spill, or not.; Stack realignment logic is overly conservative here, but otherwise we can; produce unaligned loads/stores. F",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:17510,Usability,simpl,simple,17510,"---------------===//. We compile vector multiply-by-constant into poor code:. define <4 x i32> @f(<4 x i32> %i) nounwind {; 	%A = mul <4 x i32> %i, < i32 10, i32 10, i32 10, i32 10 >; 	ret <4 x i32> %A; }. On targets without SSE4.1, this compiles into:. LCPI1_0:					## <4 x i32>; 	.long	10; 	.long	10; 	.long	10; 	.long	10; 	.text; 	.align	4,0x90; 	.globl	_f; _f:; 	pshufd	$3, %xmm0, %xmm1; 	movd	%xmm1, %eax; 	imull	LCPI1_0+12, %eax; 	movd	%eax, %xmm1; 	pshufd	$1, %xmm0, %xmm2; 	movd	%xmm2, %eax; 	imull	LCPI1_0+4, %eax; 	movd	%eax, %xmm2; 	punpckldq	%xmm1, %xmm2; 	movd	%xmm0, %eax; 	imull	LCPI1_0, %eax; 	movd	%eax, %xmm1; 	movhlps	%xmm0, %xmm0; 	movd	%xmm0, %eax; 	imull	LCPI1_0+8, %eax; 	movd	%eax, %xmm0; 	punpckldq	%xmm0, %xmm1; 	movaps	%xmm1, %xmm0; 	punpckldq	%xmm2, %xmm0; 	ret. It would be better to synthesize integer vector multiplication by constants; using shifts and adds, pslld and paddd here. And even on targets with SSE4.1,; simple cases such as multiplication by powers of two would be better as; vector shifts than as multiplications. //===---------------------------------------------------------------------===//. We compile this:. __m128i; foo2 (char x); {; return _mm_set_epi8 (1, 0, 0, 0, 0, 0, 0, 0, 0, x, 0, 1, 0, 0, 0, 0);; }. into:; 	movl	$1, %eax; 	xorps	%xmm0, %xmm0; 	pinsrw	$2, %eax, %xmm0; 	movzbl	4(%esp), %eax; 	pinsrw	$3, %eax, %xmm0; 	movl	$256, %eax; 	pinsrw	$7, %eax, %xmm0; 	ret. gcc-4.2:; 	subl	$12, %esp; 	movzbl	16(%esp), %eax; 	movdqa	LC0, %xmm0; 	pinsrw	$3, %eax, %xmm0; 	addl	$12, %esp; 	ret; 	.const; 	.align 4; LC0:; 	.word	0; 	.word	0; 	.word	1; 	.word	0; 	.word	0; 	.word	0; 	.word	0; 	.word	256. With SSE4, it should be; movdqa .LC0(%rip), %xmm0; pinsrb $6, %edi, %xmm0. //===---------------------------------------------------------------------===//. We should transform a shuffle of two vectors of constants into a single vector; of constants. Also, insertelement of a constant into a vector of constants; should also result in a vector of con",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:1469,Availability,down,down,1469,"ing branches instead of cmove to implement FP to; unsigned i64?. _conv:; 	ucomiss	LC0(%rip), %xmm0; 	cvttss2siq	%xmm0, %rdx; 	jb	L3; 	subss	LC0(%rip), %xmm0; 	movabsq	$-9223372036854775808, %rax; 	cvttss2siq	%xmm0, %rdx; 	xorq	%rax, %rdx; L3:; 	movq	%rdx, %rax; 	ret. instead of. _conv:; 	movss LCPI1_0(%rip), %xmm1; 	cvttss2siq %xmm0, %rcx; 	movaps %xmm0, %xmm2; 	subss %xmm1, %xmm2; 	cvttss2siq %xmm2, %rax; 	movabsq $-9223372036854775808, %rdx; 	xorq %rdx, %rax; 	ucomiss %xmm1, %xmm0; 	cmovb %rcx, %rax; 	ret. Seems like the jb branch has high likelihood of being taken. It would have; saved a few instructions. //===---------------------------------------------------------------------===//. It's not possible to reference AH, BH, CH, and DH registers in an instruction; requiring REX prefix. However, divb and mulb both produce results in AH. If isel; emits a CopyFromReg which gets turned into a movb and that can be allocated a; r8b - r15b. To get around this, isel emits a CopyFromReg from AX and then right shift it; down by 8 and truncate it. It's not pretty but it works. We need some register; allocation magic to make the hack go away (e.g. putting additional constraints; on the result of the movb). //===---------------------------------------------------------------------===//. The x86-64 ABI for hidden-argument struct returns requires that the; incoming value of %rdi be copied into %rax by the callee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Callees usually; need to keep this value live for most of their body anyway, so it; doesn't add a significant burden on them. We currently implement this in codegen, however this is suboptimal; because it means that it would be quite awkward to implement the; optimization for callers. A better implementation would be to relax the LLVM IR rules for sret; arguments to allow a function with an sret argument to have a non-void; return type, and",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:2843,Availability,redundant,redundant,2843,"lee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Callees usually; need to keep this value live for most of their body anyway, so it; doesn't add a significant burden on them. We currently implement this in codegen, however this is suboptimal; because it means that it would be quite awkward to implement the; optimization for callers. A better implementation would be to relax the LLVM IR rules for sret; arguments to allow a function with an sret argument to have a non-void; return type, and to have the front-end to set up the sret argument value; as the return value of the function. The front-end could more easily; emit uses of the returned struct value to be in terms of the function's; lowered return value, and it would free non-C frontends from a; complication only required by a C-based ABI. //===---------------------------------------------------------------------===//. We get a redundant zero extension for code like this:. int mask[1000];; int foo(unsigned x) {; if (x < 10); x = x * 45;; else; x = x * 78;; return mask[x];; }. _foo:; LBB1_0:	## entry; 	cmpl	$9, %edi; 	jbe	LBB1_3	## bb; LBB1_1:	## bb1; 	imull	$78, %edi, %eax; LBB1_2:	## bb2; 	movl	%eax, %eax <----; 	movq	_mask@GOTPCREL(%rip), %rcx; 	movl	(%rcx,%rax,4), %eax; 	ret; LBB1_3:	## bb; 	imull	$45, %edi, %eax; 	jmp	LBB1_2	## bb2; ; Before regalloc, we have:. %reg1025 = IMUL32rri8 %reg1024, 45, implicit-def %eflags; JMP mbb<bb2,0x203afb0>; Successors according to CFG: 0x203afb0 (#3). bb1: 0x203af60, LLVM BB @0x1e02310, ID#2:; Predecessors according to CFG: 0x203aec0 (#0); %reg1026 = IMUL32rri8 %reg1024, 78, implicit-def %eflags; Successors according to CFG: 0x203afb0 (#3). bb2: 0x203afb0, LLVM BB @0x1e02340, ID#3:; Predecessors according to CFG: 0x203af10 (#1) 0x203af60 (#2); %reg1027 = PHI %reg1025, mbb<bb,0x203af10>,; %reg1026, mbb<bb1,0x203af60>; %reg1029 = MOVZX64rr32 %reg1027. so we'd have to know that IMUL32rri8 le",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:2893,Availability,mask,mask,2893,"value,; which would often require a callee-saved register. Callees usually; need to keep this value live for most of their body anyway, so it; doesn't add a significant burden on them. We currently implement this in codegen, however this is suboptimal; because it means that it would be quite awkward to implement the; optimization for callers. A better implementation would be to relax the LLVM IR rules for sret; arguments to allow a function with an sret argument to have a non-void; return type, and to have the front-end to set up the sret argument value; as the return value of the function. The front-end could more easily; emit uses of the returned struct value to be in terms of the function's; lowered return value, and it would free non-C frontends from a; complication only required by a C-based ABI. //===---------------------------------------------------------------------===//. We get a redundant zero extension for code like this:. int mask[1000];; int foo(unsigned x) {; if (x < 10); x = x * 45;; else; x = x * 78;; return mask[x];; }. _foo:; LBB1_0:	## entry; 	cmpl	$9, %edi; 	jbe	LBB1_3	## bb; LBB1_1:	## bb1; 	imull	$78, %edi, %eax; LBB1_2:	## bb2; 	movl	%eax, %eax <----; 	movq	_mask@GOTPCREL(%rip), %rcx; 	movl	(%rcx,%rax,4), %eax; 	ret; LBB1_3:	## bb; 	imull	$45, %edi, %eax; 	jmp	LBB1_2	## bb2; ; Before regalloc, we have:. %reg1025 = IMUL32rri8 %reg1024, 45, implicit-def %eflags; JMP mbb<bb2,0x203afb0>; Successors according to CFG: 0x203afb0 (#3). bb1: 0x203af60, LLVM BB @0x1e02310, ID#2:; Predecessors according to CFG: 0x203aec0 (#0); %reg1026 = IMUL32rri8 %reg1024, 78, implicit-def %eflags; Successors according to CFG: 0x203afb0 (#3). bb2: 0x203afb0, LLVM BB @0x1e02340, ID#3:; Predecessors according to CFG: 0x203af10 (#1) 0x203af60 (#2); %reg1027 = PHI %reg1025, mbb<bb,0x203af10>,; %reg1026, mbb<bb1,0x203af60>; %reg1029 = MOVZX64rr32 %reg1027. so we'd have to know that IMUL32rri8 leaves the high word zero extended and to; be able to recognize the zero extend. T",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:2981,Availability,mask,mask,2981,"value,; which would often require a callee-saved register. Callees usually; need to keep this value live for most of their body anyway, so it; doesn't add a significant burden on them. We currently implement this in codegen, however this is suboptimal; because it means that it would be quite awkward to implement the; optimization for callers. A better implementation would be to relax the LLVM IR rules for sret; arguments to allow a function with an sret argument to have a non-void; return type, and to have the front-end to set up the sret argument value; as the return value of the function. The front-end could more easily; emit uses of the returned struct value to be in terms of the function's; lowered return value, and it would free non-C frontends from a; complication only required by a C-based ABI. //===---------------------------------------------------------------------===//. We get a redundant zero extension for code like this:. int mask[1000];; int foo(unsigned x) {; if (x < 10); x = x * 45;; else; x = x * 78;; return mask[x];; }. _foo:; LBB1_0:	## entry; 	cmpl	$9, %edi; 	jbe	LBB1_3	## bb; LBB1_1:	## bb1; 	imull	$78, %edi, %eax; LBB1_2:	## bb2; 	movl	%eax, %eax <----; 	movq	_mask@GOTPCREL(%rip), %rcx; 	movl	(%rcx,%rax,4), %eax; 	ret; LBB1_3:	## bb; 	imull	$45, %edi, %eax; 	jmp	LBB1_2	## bb2; ; Before regalloc, we have:. %reg1025 = IMUL32rri8 %reg1024, 45, implicit-def %eflags; JMP mbb<bb2,0x203afb0>; Successors according to CFG: 0x203afb0 (#3). bb1: 0x203af60, LLVM BB @0x1e02310, ID#2:; Predecessors according to CFG: 0x203aec0 (#0); %reg1026 = IMUL32rri8 %reg1024, 78, implicit-def %eflags; Successors according to CFG: 0x203afb0 (#3). bb2: 0x203afb0, LLVM BB @0x1e02340, ID#3:; Predecessors according to CFG: 0x203af10 (#1) 0x203af60 (#2); %reg1027 = PHI %reg1025, mbb<bb,0x203af10>,; %reg1026, mbb<bb1,0x203af60>; %reg1029 = MOVZX64rr32 %reg1027. so we'd have to know that IMUL32rri8 leaves the high word zero extended and to; be able to recognize the zero extend. T",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:1366,Energy Efficiency,allocate,allocated,1366," //===---------------------------------------------------------------------===//. Are we better off using branches instead of cmove to implement FP to; unsigned i64?. _conv:; 	ucomiss	LC0(%rip), %xmm0; 	cvttss2siq	%xmm0, %rdx; 	jb	L3; 	subss	LC0(%rip), %xmm0; 	movabsq	$-9223372036854775808, %rax; 	cvttss2siq	%xmm0, %rdx; 	xorq	%rax, %rdx; L3:; 	movq	%rdx, %rax; 	ret. instead of. _conv:; 	movss LCPI1_0(%rip), %xmm1; 	cvttss2siq %xmm0, %rcx; 	movaps %xmm0, %xmm2; 	subss %xmm1, %xmm2; 	cvttss2siq %xmm2, %rax; 	movabsq $-9223372036854775808, %rdx; 	xorq %rdx, %rax; 	ucomiss %xmm1, %xmm0; 	cmovb %rcx, %rax; 	ret. Seems like the jb branch has high likelihood of being taken. It would have; saved a few instructions. //===---------------------------------------------------------------------===//. It's not possible to reference AH, BH, CH, and DH registers in an instruction; requiring REX prefix. However, divb and mulb both produce results in AH. If isel; emits a CopyFromReg which gets turned into a movb and that can be allocated a; r8b - r15b. To get around this, isel emits a CopyFromReg from AX and then right shift it; down by 8 and truncate it. It's not pretty but it works. We need some register; allocation magic to make the hack go away (e.g. putting additional constraints; on the result of the movb). //===---------------------------------------------------------------------===//. The x86-64 ABI for hidden-argument struct returns requires that the; incoming value of %rdi be copied into %rax by the callee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Callees usually; need to keep this value live for most of their body anyway, so it; doesn't add a significant burden on them. We currently implement this in codegen, however this is suboptimal; because it means that it would be quite awkward to implement the; optimization for callers. A better implementation would be to relax the LLVM IR r",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:5988,Energy Efficiency,reduce,reduce,5988,"the following code; (from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):; extern unsigned long table[];; unsigned long foo(unsigned char *p) {; unsigned long tag = *p;; return table[tag >> 4] + table[tag & 0xf];; }. Current code generated:; 	movzbl	(%rdi), %eax; 	movq	%rax, %rcx; 	andq	$240, %rcx; 	shrq	%rcx; 	andq	$15, %rax; 	movq	table(,%rax,8), %rax; 	addq	table(%rcx), %rax; 	ret. Issues:; 1. First movq should be movl; saves a byte.; 2. Both andq's should be andl; saves another two bytes. I think this was; implemented at one point, but subsequently regressed.; 3. shrq should be shrl; saves another byte.; 4. The first andq can be completely eliminated by using a slightly more; expensive addressing mode. //===---------------------------------------------------------------------===//. Consider the following (contrived testcase, but contains common factors):. #include <stdarg.h>; int test(int x, ...) {; int sum, i;; va_list l;; va_start(l, x);; for (i = 0; i < x; i++); sum += va_arg(l, int);; va_end(l);; return sum;; }. Testcase given in C because fixing it will likely involve changing the IR; generated for it. The primary issue with the result is that it doesn't do any; of the optimizations which are possible if we know the address of a va_list; in the current function is never taken:; 1. We shouldn't spill the XMM registers because we only call va_arg with ""int"".; 2. It would be nice if we could sroa the va_list.; 3. Probably overkill, but it'd be cool if we could peel off the first five; iterations of the loop. Other optimizations involving functions which use va_arg on floats which don't; have the address of a va_list taken:; 1. Conversely to the above, we shouldn't spill general registers if we only; call va_arg on ""double"".; 2. If we know nothing more than 64 bits wide is read from the XMM registers,; we can change the spilling code to reduce the amount of stack used by half. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:3885,Modifiability,extend,extended,3885,"e this:. int mask[1000];; int foo(unsigned x) {; if (x < 10); x = x * 45;; else; x = x * 78;; return mask[x];; }. _foo:; LBB1_0:	## entry; 	cmpl	$9, %edi; 	jbe	LBB1_3	## bb; LBB1_1:	## bb1; 	imull	$78, %edi, %eax; LBB1_2:	## bb2; 	movl	%eax, %eax <----; 	movq	_mask@GOTPCREL(%rip), %rcx; 	movl	(%rcx,%rax,4), %eax; 	ret; LBB1_3:	## bb; 	imull	$45, %edi, %eax; 	jmp	LBB1_2	## bb2; ; Before regalloc, we have:. %reg1025 = IMUL32rri8 %reg1024, 45, implicit-def %eflags; JMP mbb<bb2,0x203afb0>; Successors according to CFG: 0x203afb0 (#3). bb1: 0x203af60, LLVM BB @0x1e02310, ID#2:; Predecessors according to CFG: 0x203aec0 (#0); %reg1026 = IMUL32rri8 %reg1024, 78, implicit-def %eflags; Successors according to CFG: 0x203afb0 (#3). bb2: 0x203afb0, LLVM BB @0x1e02340, ID#3:; Predecessors according to CFG: 0x203af10 (#1) 0x203af60 (#2); %reg1027 = PHI %reg1025, mbb<bb,0x203af10>,; %reg1026, mbb<bb1,0x203af60>; %reg1029 = MOVZX64rr32 %reg1027. so we'd have to know that IMUL32rri8 leaves the high word zero extended and to; be able to recognize the zero extend. This could also presumably be implemented; if we have whole-function selectiondags. //===---------------------------------------------------------------------===//. Take the following code; (from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):; extern unsigned long table[];; unsigned long foo(unsigned char *p) {; unsigned long tag = *p;; return table[tag >> 4] + table[tag & 0xf];; }. Current code generated:; 	movzbl	(%rdi), %eax; 	movq	%rax, %rcx; 	andq	$240, %rcx; 	shrq	%rcx; 	andq	$15, %rax; 	movq	table(,%rax,8), %rax; 	addq	table(%rcx), %rax; 	ret. Issues:; 1. First movq should be movl; saves a byte.; 2. Both andq's should be andl; saves another two bytes. I think this was; implemented at one point, but subsequently regressed.; 3. shrq should be shrl; saves another byte.; 4. The first andq can be completely eliminated by using a slightly more; expensive addressing mode. //===---------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:3932,Modifiability,extend,extend,3932,"e this:. int mask[1000];; int foo(unsigned x) {; if (x < 10); x = x * 45;; else; x = x * 78;; return mask[x];; }. _foo:; LBB1_0:	## entry; 	cmpl	$9, %edi; 	jbe	LBB1_3	## bb; LBB1_1:	## bb1; 	imull	$78, %edi, %eax; LBB1_2:	## bb2; 	movl	%eax, %eax <----; 	movq	_mask@GOTPCREL(%rip), %rcx; 	movl	(%rcx,%rax,4), %eax; 	ret; LBB1_3:	## bb; 	imull	$45, %edi, %eax; 	jmp	LBB1_2	## bb2; ; Before regalloc, we have:. %reg1025 = IMUL32rri8 %reg1024, 45, implicit-def %eflags; JMP mbb<bb2,0x203afb0>; Successors according to CFG: 0x203afb0 (#3). bb1: 0x203af60, LLVM BB @0x1e02310, ID#2:; Predecessors according to CFG: 0x203aec0 (#0); %reg1026 = IMUL32rri8 %reg1024, 78, implicit-def %eflags; Successors according to CFG: 0x203afb0 (#3). bb2: 0x203afb0, LLVM BB @0x1e02340, ID#3:; Predecessors according to CFG: 0x203af10 (#1) 0x203af60 (#2); %reg1027 = PHI %reg1025, mbb<bb,0x203af10>,; %reg1026, mbb<bb1,0x203af60>; %reg1029 = MOVZX64rr32 %reg1027. so we'd have to know that IMUL32rri8 leaves the high word zero extended and to; be able to recognize the zero extend. This could also presumably be implemented; if we have whole-function selectiondags. //===---------------------------------------------------------------------===//. Take the following code; (from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):; extern unsigned long table[];; unsigned long foo(unsigned char *p) {; unsigned long tag = *p;; return table[tag >> 4] + table[tag & 0xf];; }. Current code generated:; 	movzbl	(%rdi), %eax; 	movq	%rax, %rcx; 	andq	$240, %rcx; 	shrq	%rcx; 	andq	$15, %rax; 	movq	table(,%rax,8), %rax; 	addq	table(%rcx), %rax; 	ret. Issues:; 1. First movq should be movl; saves a byte.; 2. Both andq's should be andl; saves another two bytes. I think this was; implemented at one point, but subsequently regressed.; 3. shrq should be shrl; saves another byte.; 4. The first andq can be completely eliminated by using a slightly more; expensive addressing mode. //===---------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:88,Performance,Optimiz,Optimization,88,"//===- README_X86_64.txt - Notes for X86-64 code gen ----------------------===//. AMD64 Optimization Manual 8.2 has some nice information about optimizing integer; multiplication by a constant. How much of it applies to Intel's X86-64; implementation? There are definite trade-offs to consider: latency vs. register; pressure vs. code size. //===---------------------------------------------------------------------===//. Are we better off using branches instead of cmove to implement FP to; unsigned i64?. _conv:; 	ucomiss	LC0(%rip), %xmm0; 	cvttss2siq	%xmm0, %rdx; 	jb	L3; 	subss	LC0(%rip), %xmm0; 	movabsq	$-9223372036854775808, %rax; 	cvttss2siq	%xmm0, %rdx; 	xorq	%rax, %rdx; L3:; 	movq	%rdx, %rax; 	ret. instead of. _conv:; 	movss LCPI1_0(%rip), %xmm1; 	cvttss2siq %xmm0, %rcx; 	movaps %xmm0, %xmm2; 	subss %xmm1, %xmm2; 	cvttss2siq %xmm2, %rax; 	movabsq $-9223372036854775808, %rdx; 	xorq %rdx, %rax; 	ucomiss %xmm1, %xmm0; 	cmovb %rcx, %rax; 	ret. Seems like the jb branch has high likelihood of being taken. It would have; saved a few instructions. //===---------------------------------------------------------------------===//. It's not possible to reference AH, BH, CH, and DH registers in an instruction; requiring REX prefix. However, divb and mulb both produce results in AH. If isel; emits a CopyFromReg which gets turned into a movb and that can be allocated a; r8b - r15b. To get around this, isel emits a CopyFromReg from AX and then right shift it; down by 8 and truncate it. It's not pretty but it works. We need some register; allocation magic to make the hack go away (e.g. putting additional constraints; on the result of the movb). //===---------------------------------------------------------------------===//. The x86-64 ABI for hidden-argument struct returns requires that the; incoming value of %rdi be copied into %rax by the callee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Ca",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:144,Performance,optimiz,optimizing,144,"//===- README_X86_64.txt - Notes for X86-64 code gen ----------------------===//. AMD64 Optimization Manual 8.2 has some nice information about optimizing integer; multiplication by a constant. How much of it applies to Intel's X86-64; implementation? There are definite trade-offs to consider: latency vs. register; pressure vs. code size. //===---------------------------------------------------------------------===//. Are we better off using branches instead of cmove to implement FP to; unsigned i64?. _conv:; 	ucomiss	LC0(%rip), %xmm0; 	cvttss2siq	%xmm0, %rdx; 	jb	L3; 	subss	LC0(%rip), %xmm0; 	movabsq	$-9223372036854775808, %rax; 	cvttss2siq	%xmm0, %rdx; 	xorq	%rax, %rdx; L3:; 	movq	%rdx, %rax; 	ret. instead of. _conv:; 	movss LCPI1_0(%rip), %xmm1; 	cvttss2siq %xmm0, %rcx; 	movaps %xmm0, %xmm2; 	subss %xmm1, %xmm2; 	cvttss2siq %xmm2, %rax; 	movabsq $-9223372036854775808, %rdx; 	xorq %rdx, %rax; 	ucomiss %xmm1, %xmm0; 	cmovb %rcx, %rax; 	ret. Seems like the jb branch has high likelihood of being taken. It would have; saved a few instructions. //===---------------------------------------------------------------------===//. It's not possible to reference AH, BH, CH, and DH registers in an instruction; requiring REX prefix. However, divb and mulb both produce results in AH. If isel; emits a CopyFromReg which gets turned into a movb and that can be allocated a; r8b - r15b. To get around this, isel emits a CopyFromReg from AX and then right shift it; down by 8 and truncate it. It's not pretty but it works. We need some register; allocation magic to make the hack go away (e.g. putting additional constraints; on the result of the movb). //===---------------------------------------------------------------------===//. The x86-64 ABI for hidden-argument struct returns requires that the; incoming value of %rdi be copied into %rax by the callee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Ca",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:295,Performance,latency,latency,295,"//===- README_X86_64.txt - Notes for X86-64 code gen ----------------------===//. AMD64 Optimization Manual 8.2 has some nice information about optimizing integer; multiplication by a constant. How much of it applies to Intel's X86-64; implementation? There are definite trade-offs to consider: latency vs. register; pressure vs. code size. //===---------------------------------------------------------------------===//. Are we better off using branches instead of cmove to implement FP to; unsigned i64?. _conv:; 	ucomiss	LC0(%rip), %xmm0; 	cvttss2siq	%xmm0, %rdx; 	jb	L3; 	subss	LC0(%rip), %xmm0; 	movabsq	$-9223372036854775808, %rax; 	cvttss2siq	%xmm0, %rdx; 	xorq	%rax, %rdx; L3:; 	movq	%rdx, %rax; 	ret. instead of. _conv:; 	movss LCPI1_0(%rip), %xmm1; 	cvttss2siq %xmm0, %rcx; 	movaps %xmm0, %xmm2; 	subss %xmm1, %xmm2; 	cvttss2siq %xmm2, %rax; 	movabsq $-9223372036854775808, %rdx; 	xorq %rdx, %rax; 	ucomiss %xmm1, %xmm0; 	cmovb %rcx, %rax; 	ret. Seems like the jb branch has high likelihood of being taken. It would have; saved a few instructions. //===---------------------------------------------------------------------===//. It's not possible to reference AH, BH, CH, and DH registers in an instruction; requiring REX prefix. However, divb and mulb both produce results in AH. If isel; emits a CopyFromReg which gets turned into a movb and that can be allocated a; r8b - r15b. To get around this, isel emits a CopyFromReg from AX and then right shift it; down by 8 and truncate it. It's not pretty but it works. We need some register; allocation magic to make the hack go away (e.g. putting additional constraints; on the result of the movb). //===---------------------------------------------------------------------===//. The x86-64 ABI for hidden-argument struct returns requires that the; incoming value of %rdi be copied into %rax by the callee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Ca",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:2259,Performance,optimiz,optimization,2259," instruction; requiring REX prefix. However, divb and mulb both produce results in AH. If isel; emits a CopyFromReg which gets turned into a movb and that can be allocated a; r8b - r15b. To get around this, isel emits a CopyFromReg from AX and then right shift it; down by 8 and truncate it. It's not pretty but it works. We need some register; allocation magic to make the hack go away (e.g. putting additional constraints; on the result of the movb). //===---------------------------------------------------------------------===//. The x86-64 ABI for hidden-argument struct returns requires that the; incoming value of %rdi be copied into %rax by the callee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Callees usually; need to keep this value live for most of their body anyway, so it; doesn't add a significant burden on them. We currently implement this in codegen, however this is suboptimal; because it means that it would be quite awkward to implement the; optimization for callers. A better implementation would be to relax the LLVM IR rules for sret; arguments to allow a function with an sret argument to have a non-void; return type, and to have the front-end to set up the sret argument value; as the return value of the function. The front-end could more easily; emit uses of the returned struct value to be in terms of the function's; lowered return value, and it would free non-C frontends from a; complication only required by a C-based ABI. //===---------------------------------------------------------------------===//. We get a redundant zero extension for code like this:. int mask[1000];; int foo(unsigned x) {; if (x < 10); x = x * 45;; else; x = x * 78;; return mask[x];; }. _foo:; LBB1_0:	## entry; 	cmpl	$9, %edi; 	jbe	LBB1_3	## bb; LBB1_1:	## bb1; 	imull	$78, %edi, %eax; LBB1_2:	## bb2; 	movl	%eax, %eax <----; 	movq	_mask@GOTPCREL(%rip), %rcx; 	movl	(%rcx,%rax,4), %eax; 	ret; LBB",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:5311,Performance,optimiz,optimizations,5311,"the following code; (from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):; extern unsigned long table[];; unsigned long foo(unsigned char *p) {; unsigned long tag = *p;; return table[tag >> 4] + table[tag & 0xf];; }. Current code generated:; 	movzbl	(%rdi), %eax; 	movq	%rax, %rcx; 	andq	$240, %rcx; 	shrq	%rcx; 	andq	$15, %rax; 	movq	table(,%rax,8), %rax; 	addq	table(%rcx), %rax; 	ret. Issues:; 1. First movq should be movl; saves a byte.; 2. Both andq's should be andl; saves another two bytes. I think this was; implemented at one point, but subsequently regressed.; 3. shrq should be shrl; saves another byte.; 4. The first andq can be completely eliminated by using a slightly more; expensive addressing mode. //===---------------------------------------------------------------------===//. Consider the following (contrived testcase, but contains common factors):. #include <stdarg.h>; int test(int x, ...) {; int sum, i;; va_list l;; va_start(l, x);; for (i = 0; i < x; i++); sum += va_arg(l, int);; va_end(l);; return sum;; }. Testcase given in C because fixing it will likely involve changing the IR; generated for it. The primary issue with the result is that it doesn't do any; of the optimizations which are possible if we know the address of a va_list; in the current function is never taken:; 1. We shouldn't spill the XMM registers because we only call va_arg with ""int"".; 2. It would be nice if we could sroa the va_list.; 3. Probably overkill, but it'd be cool if we could peel off the first five; iterations of the loop. Other optimizations involving functions which use va_arg on floats which don't; have the address of a va_list taken:; 1. Conversely to the above, we shouldn't spill general registers if we only; call va_arg on ""double"".; 2. If we know nothing more than 64 bits wide is read from the XMM registers,; we can change the spilling code to reduce the amount of stack used by half. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:5660,Performance,optimiz,optimizations,5660,"the following code; (from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):; extern unsigned long table[];; unsigned long foo(unsigned char *p) {; unsigned long tag = *p;; return table[tag >> 4] + table[tag & 0xf];; }. Current code generated:; 	movzbl	(%rdi), %eax; 	movq	%rax, %rcx; 	andq	$240, %rcx; 	shrq	%rcx; 	andq	$15, %rax; 	movq	table(,%rax,8), %rax; 	addq	table(%rcx), %rax; 	ret. Issues:; 1. First movq should be movl; saves a byte.; 2. Both andq's should be andl; saves another two bytes. I think this was; implemented at one point, but subsequently regressed.; 3. shrq should be shrl; saves another byte.; 4. The first andq can be completely eliminated by using a slightly more; expensive addressing mode. //===---------------------------------------------------------------------===//. Consider the following (contrived testcase, but contains common factors):. #include <stdarg.h>; int test(int x, ...) {; int sum, i;; va_list l;; va_start(l, x);; for (i = 0; i < x; i++); sum += va_arg(l, int);; va_end(l);; return sum;; }. Testcase given in C because fixing it will likely involve changing the IR; generated for it. The primary issue with the result is that it doesn't do any; of the optimizations which are possible if we know the address of a va_list; in the current function is never taken:; 1. We shouldn't spill the XMM registers because we only call va_arg with ""int"".; 2. It would be nice if we could sroa the va_list.; 3. Probably overkill, but it'd be cool if we could peel off the first five; iterations of the loop. Other optimizations involving functions which use va_arg on floats which don't; have the address of a va_list taken:; 1. Conversely to the above, we shouldn't spill general registers if we only; call va_arg on ""double"".; 2. If we know nothing more than 64 bits wide is read from the XMM registers,; we can change the spilling code to reduce the amount of stack used by half. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:2843,Safety,redund,redundant,2843,"lee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Callees usually; need to keep this value live for most of their body anyway, so it; doesn't add a significant burden on them. We currently implement this in codegen, however this is suboptimal; because it means that it would be quite awkward to implement the; optimization for callers. A better implementation would be to relax the LLVM IR rules for sret; arguments to allow a function with an sret argument to have a non-void; return type, and to have the front-end to set up the sret argument value; as the return value of the function. The front-end could more easily; emit uses of the returned struct value to be in terms of the function's; lowered return value, and it would free non-C frontends from a; complication only required by a C-based ABI. //===---------------------------------------------------------------------===//. We get a redundant zero extension for code like this:. int mask[1000];; int foo(unsigned x) {; if (x < 10); x = x * 45;; else; x = x * 78;; return mask[x];; }. _foo:; LBB1_0:	## entry; 	cmpl	$9, %edi; 	jbe	LBB1_3	## bb; LBB1_1:	## bb1; 	imull	$78, %edi, %eax; LBB1_2:	## bb2; 	movl	%eax, %eax <----; 	movq	_mask@GOTPCREL(%rip), %rcx; 	movl	(%rcx,%rax,4), %eax; 	ret; LBB1_3:	## bb; 	imull	$45, %edi, %eax; 	jmp	LBB1_2	## bb2; ; Before regalloc, we have:. %reg1025 = IMUL32rri8 %reg1024, 45, implicit-def %eflags; JMP mbb<bb2,0x203afb0>; Successors according to CFG: 0x203afb0 (#3). bb1: 0x203af60, LLVM BB @0x1e02310, ID#2:; Predecessors according to CFG: 0x203aec0 (#0); %reg1026 = IMUL32rri8 %reg1024, 78, implicit-def %eflags; Successors according to CFG: 0x203afb0 (#3). bb2: 0x203afb0, LLVM BB @0x1e02340, ID#3:; Predecessors according to CFG: 0x203af10 (#1) 0x203af60 (#2); %reg1027 = PHI %reg1025, mbb<bb,0x203af10>,; %reg1026, mbb<bb1,0x203af60>; %reg1029 = MOVZX64rr32 %reg1027. so we'd have to know that IMUL32rri8 le",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:4945,Testability,test,testcase,4945,"uld also presumably be implemented; if we have whole-function selectiondags. //===---------------------------------------------------------------------===//. Take the following code; (from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):; extern unsigned long table[];; unsigned long foo(unsigned char *p) {; unsigned long tag = *p;; return table[tag >> 4] + table[tag & 0xf];; }. Current code generated:; 	movzbl	(%rdi), %eax; 	movq	%rax, %rcx; 	andq	$240, %rcx; 	shrq	%rcx; 	andq	$15, %rax; 	movq	table(,%rax,8), %rax; 	addq	table(%rcx), %rax; 	ret. Issues:; 1. First movq should be movl; saves a byte.; 2. Both andq's should be andl; saves another two bytes. I think this was; implemented at one point, but subsequently regressed.; 3. shrq should be shrl; saves another byte.; 4. The first andq can be completely eliminated by using a slightly more; expensive addressing mode. //===---------------------------------------------------------------------===//. Consider the following (contrived testcase, but contains common factors):. #include <stdarg.h>; int test(int x, ...) {; int sum, i;; va_list l;; va_start(l, x);; for (i = 0; i < x; i++); sum += va_arg(l, int);; va_end(l);; return sum;; }. Testcase given in C because fixing it will likely involve changing the IR; generated for it. The primary issue with the result is that it doesn't do any; of the optimizations which are possible if we know the address of a va_list; in the current function is never taken:; 1. We shouldn't spill the XMM registers because we only call va_arg with ""int"".; 2. It would be nice if we could sroa the va_list.; 3. Probably overkill, but it'd be cool if we could peel off the first five; iterations of the loop. Other optimizations involving functions which use va_arg on floats which don't; have the address of a va_list taken:; 1. Conversely to the above, we shouldn't spill general registers if we only; call va_arg on ""double"".; 2. If we know nothing more than 64 bits wide is read from the XMM registe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:5011,Testability,test,test,5011,"ctiondags. //===---------------------------------------------------------------------===//. Take the following code; (from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):; extern unsigned long table[];; unsigned long foo(unsigned char *p) {; unsigned long tag = *p;; return table[tag >> 4] + table[tag & 0xf];; }. Current code generated:; 	movzbl	(%rdi), %eax; 	movq	%rax, %rcx; 	andq	$240, %rcx; 	shrq	%rcx; 	andq	$15, %rax; 	movq	table(,%rax,8), %rax; 	addq	table(%rcx), %rax; 	ret. Issues:; 1. First movq should be movl; saves a byte.; 2. Both andq's should be andl; saves another two bytes. I think this was; implemented at one point, but subsequently regressed.; 3. shrq should be shrl; saves another byte.; 4. The first andq can be completely eliminated by using a slightly more; expensive addressing mode. //===---------------------------------------------------------------------===//. Consider the following (contrived testcase, but contains common factors):. #include <stdarg.h>; int test(int x, ...) {; int sum, i;; va_list l;; va_start(l, x);; for (i = 0; i < x; i++); sum += va_arg(l, int);; va_end(l);; return sum;; }. Testcase given in C because fixing it will likely involve changing the IR; generated for it. The primary issue with the result is that it doesn't do any; of the optimizations which are possible if we know the address of a va_list; in the current function is never taken:; 1. We shouldn't spill the XMM registers because we only call va_arg with ""int"".; 2. It would be nice if we could sroa the va_list.; 3. Probably overkill, but it'd be cool if we could peel off the first five; iterations of the loop. Other optimizations involving functions which use va_arg on floats which don't; have the address of a va_list taken:; 1. Conversely to the above, we shouldn't spill general registers if we only; call va_arg on ""double"".; 2. If we know nothing more than 64 bits wide is read from the XMM registers,; we can change the spilling code to reduce the amount of stack",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:5150,Testability,Test,Testcase,5150,"the following code; (from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):; extern unsigned long table[];; unsigned long foo(unsigned char *p) {; unsigned long tag = *p;; return table[tag >> 4] + table[tag & 0xf];; }. Current code generated:; 	movzbl	(%rdi), %eax; 	movq	%rax, %rcx; 	andq	$240, %rcx; 	shrq	%rcx; 	andq	$15, %rax; 	movq	table(,%rax,8), %rax; 	addq	table(%rcx), %rax; 	ret. Issues:; 1. First movq should be movl; saves a byte.; 2. Both andq's should be andl; saves another two bytes. I think this was; implemented at one point, but subsequently regressed.; 3. shrq should be shrl; saves another byte.; 4. The first andq can be completely eliminated by using a slightly more; expensive addressing mode. //===---------------------------------------------------------------------===//. Consider the following (contrived testcase, but contains common factors):. #include <stdarg.h>; int test(int x, ...) {; int sum, i;; va_list l;; va_start(l, x);; for (i = 0; i < x; i++); sum += va_arg(l, int);; va_end(l);; return sum;; }. Testcase given in C because fixing it will likely involve changing the IR; generated for it. The primary issue with the result is that it doesn't do any; of the optimizations which are possible if we know the address of a va_list; in the current function is never taken:; 1. We shouldn't spill the XMM registers because we only call va_arg with ""int"".; 2. It would be nice if we could sroa the va_list.; 3. Probably overkill, but it'd be cool if we could peel off the first five; iterations of the loop. Other optimizations involving functions which use va_arg on floats which don't; have the address of a va_list taken:; 1. Conversely to the above, we shouldn't spill general registers if we only; call va_arg on ""double"".; 2. If we know nothing more than 64 bits wide is read from the XMM registers,; we can change the spilling code to reduce the amount of stack used by half. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:16522,Availability,redundant,redundant,16522,"6 %tmp98 to i32		; <i32> [#uses=1]; 	%tmp585 = sub i32 32, %tmp583584		; <i32> [#uses=1]; 	%tmp614615 = sext i16 %tmp101 to i32		; <i32> [#uses=1]; 	%tmp621622 = sext i16 %tmp104 to i32		; <i32> [#uses=1]; 	%tmp623 = sub i32 32, %tmp621622		; <i32> [#uses=1]; 	br label %bb114. produces:. LBB3_5:	# bb114.preheader; 	movswl	-68(%ebp), %eax; 	movl	$32, %ecx; 	movl	%ecx, -80(%ebp); 	subl	%eax, -80(%ebp); 	movswl	-52(%ebp), %eax; 	movl	%ecx, -84(%ebp); 	subl	%eax, -84(%ebp); 	movswl	-70(%ebp), %eax; 	movl	%ecx, -88(%ebp); 	subl	%eax, -88(%ebp); 	movswl	-50(%ebp), %eax; 	subl	%eax, %ecx; 	movl	%ecx, -76(%ebp); 	movswl	-42(%ebp), %eax; 	movl	%eax, -92(%ebp); 	movswl	-66(%ebp), %eax; 	movl	%eax, -96(%ebp); 	movw	$0, -98(%ebp). This appears to be bad because the RA is not folding the store to the stack ; slot into the movl. The above instructions could be:; 	movl $32, -80(%ebp); ...; 	movl $32, -84(%ebp); ...; This seems like a cross between remat and spill folding. This has redundant subtractions of %eax from a stack slot. However, %ecx doesn't; change, so we could simply subtract %eax from %ecx first and then use %ecx (or; vice-versa). //===---------------------------------------------------------------------===//. This code:. 	%tmp659 = icmp slt i16 %tmp654, 0		; <i1> [#uses=1]; 	br i1 %tmp659, label %cond_true662, label %cond_next715. produces this:. 	testw	%cx, %cx; 	movswl	%cx, %esi; 	jns	LBB4_109	# cond_next715. Shark tells us that using %cx in the testw instruction is sub-optimal. It; suggests using the 32-bit register (which is what ICC uses). //===---------------------------------------------------------------------===//. We compile this:. void compare (long long foo) {; if (foo < 4294967297LL); abort();; }. to:. compare:; subl $4, %esp; cmpl $0, 8(%esp); setne %al; movzbw %al, %ax; cmpl $1, 12(%esp); setg %cl; movzbw %cl, %cx; cmove %ax, %cx; testb $1, %cl; jne .LBB1_2 # UnifiedReturnBlock; .LBB1_1: # ifthen; call abort; .LBB1_2: # UnifiedReturnBlock; addl $4, %esp",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:23588,Availability,Down,Downside,23588,"abs instruction, turning it into an *integer* operation, like this:. 	xorl 2147483648, [mem+4] ## 2147483648 = (1 << 31). you could also use xorb, but xorl is less likely to lead to a partial register; stall. Here is a contrived testcase:. double a, b, c;; void test(double *P) {; double X = *P;; a = X;; bar();; X = -X;; b = X;; bar();; c = X;; }. //===---------------------------------------------------------------------===//. The generated code on x86 for checking for signed overflow on a multiply the; obvious way is much longer than it needs to be. int x(int a, int b) {; long long prod = (long long)a*b;; return prod > 0x7FFFFFFF || prod < (-0x7FFFFFFF-1);; }. See PR2053 for more details. //===---------------------------------------------------------------------===//. We should investigate using cdq/ctld (effect: edx = sar eax, 31); more aggressively; it should cost the same as a move+shift on any modern; processor, but it's a lot shorter. Downside is that it puts more; pressure on register allocation because it has fixed operands. Example:; int abs(int x) {return x < 0 ? -x : x;}. gcc compiles this to the following when using march/mtune=pentium2/3/4/m/etc.:; abs:; movl 4(%esp), %eax; cltd; xorl %edx, %eax; subl %edx, %eax; ret. //===---------------------------------------------------------------------===//. Take the following code (from ; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=16541):. extern unsigned char first_one[65536];; int FirstOnet(unsigned long long arg1); {; if (arg1 >> 48); return (first_one[arg1 >> 48]);; return 0;; }. The following code is currently generated:; FirstOnet:; movl 8(%esp), %eax; cmpl $65536, %eax; movl 4(%esp), %ecx; jb .LBB1_2 # UnifiedReturnBlock; .LBB1_1: # ifthen; shrl $16, %eax; movzbl first_one(%eax), %eax; ret; .LBB1_2: # UnifiedReturnBlock; xorl %eax, %eax; ret. We could change the ""movl 8(%esp), %eax"" into ""movzwl 10(%esp), %eax""; this; lets us change the cmpl into a testl, which is shorter, and eliminate the shift. //===---",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:40575,Availability,redundant,redundant,40575,"add32carry:; 	leal	(%rsi,%rdi), %eax; 	cmpl	%esi, %eax; 	adcl	$0, %eax; 	ret. //===---------------------------------------------------------------------===//. The hot loop of 256.bzip2 contains code that looks a bit like this:. int foo(char *P, char *Q, int x, int y) {; if (P[0] != Q[0]); return P[0] < Q[0];; if (P[1] != Q[1]); return P[1] < Q[1];; if (P[2] != Q[2]); return P[2] < Q[2];; return P[3] < Q[3];; }. In the real code, we get a lot more wrong than this. However, even in this; code we generate:. _foo: ## @foo; ## %bb.0: ## %entry; 	movb	(%rsi), %al; 	movb	(%rdi), %cl; 	cmpb	%al, %cl; 	je	LBB0_2; LBB0_1: ## %if.then; 	cmpb	%al, %cl; 	jmp	LBB0_5; LBB0_2: ## %if.end; 	movb	1(%rsi), %al; 	movb	1(%rdi), %cl; 	cmpb	%al, %cl; 	jne	LBB0_1; ## %bb.3: ## %if.end38; 	movb	2(%rsi), %al; 	movb	2(%rdi), %cl; 	cmpb	%al, %cl; 	jne	LBB0_1; ## %bb.4: ## %if.end60; 	movb	3(%rdi), %al; 	cmpb	3(%rsi), %al; LBB0_5: ## %if.end60; 	setl	%al; 	movzbl	%al, %eax; 	ret. Note that we generate jumps to LBB0_1 which does a redundant compare. The; redundant compare also forces the register values to be live, which prevents; folding one of the loads into the compare. In contrast, GCC 4.2 produces:. _foo:; 	movzbl	(%rsi), %eax; 	cmpb	%al, (%rdi); 	jne	L10; L12:; 	movzbl	1(%rsi), %eax; 	cmpb	%al, 1(%rdi); 	jne	L10; 	movzbl	2(%rsi), %eax; 	cmpb	%al, 2(%rdi); 	jne	L10; 	movzbl	3(%rdi), %eax; 	cmpb	3(%rsi), %al; L10:; 	setl	%al; 	movzbl	%al, %eax; 	ret. which is ""perfect"". //===---------------------------------------------------------------------===//. For the branch in the following code:; int a();; int b(int x, int y) {; if (x & (1<<(y&7))); return a();; return y;; }. We currently generate:; 	movb	%sil, %al; 	andb	$7, %al; 	movzbl	%al, %eax; 	btl	%eax, %edi; 	jae	.LBB0_2. movl+andl would be shorter than the movb+andb+movzbl sequence. //===---------------------------------------------------------------------===//. For the following:; struct u1 {; float x, y;; };; float foo(struct u1 u) {; retu",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:40599,Availability,redundant,redundant,40599,"------------------------------------------------------===//. The hot loop of 256.bzip2 contains code that looks a bit like this:. int foo(char *P, char *Q, int x, int y) {; if (P[0] != Q[0]); return P[0] < Q[0];; if (P[1] != Q[1]); return P[1] < Q[1];; if (P[2] != Q[2]); return P[2] < Q[2];; return P[3] < Q[3];; }. In the real code, we get a lot more wrong than this. However, even in this; code we generate:. _foo: ## @foo; ## %bb.0: ## %entry; 	movb	(%rsi), %al; 	movb	(%rdi), %cl; 	cmpb	%al, %cl; 	je	LBB0_2; LBB0_1: ## %if.then; 	cmpb	%al, %cl; 	jmp	LBB0_5; LBB0_2: ## %if.end; 	movb	1(%rsi), %al; 	movb	1(%rdi), %cl; 	cmpb	%al, %cl; 	jne	LBB0_1; ## %bb.3: ## %if.end38; 	movb	2(%rsi), %al; 	movb	2(%rdi), %cl; 	cmpb	%al, %cl; 	jne	LBB0_1; ## %bb.4: ## %if.end60; 	movb	3(%rdi), %al; 	cmpb	3(%rsi), %al; LBB0_5: ## %if.end60; 	setl	%al; 	movzbl	%al, %eax; 	ret. Note that we generate jumps to LBB0_1 which does a redundant compare. The; redundant compare also forces the register values to be live, which prevents; folding one of the loads into the compare. In contrast, GCC 4.2 produces:. _foo:; 	movzbl	(%rsi), %eax; 	cmpb	%al, (%rdi); 	jne	L10; L12:; 	movzbl	1(%rsi), %eax; 	cmpb	%al, 1(%rdi); 	jne	L10; 	movzbl	2(%rsi), %eax; 	cmpb	%al, 2(%rdi); 	jne	L10; 	movzbl	3(%rdi), %eax; 	cmpb	3(%rsi), %al; L10:; 	setl	%al; 	movzbl	%al, %eax; 	ret. which is ""perfect"". //===---------------------------------------------------------------------===//. For the branch in the following code:; int a();; int b(int x, int y) {; if (x & (1<<(y&7))); return a();; return y;; }. We currently generate:; 	movb	%sil, %al; 	andb	$7, %al; 	movzbl	%al, %eax; 	btl	%eax, %edi; 	jae	.LBB0_2. movl+andl would be shorter than the movb+andb+movzbl sequence. //===---------------------------------------------------------------------===//. For the following:; struct u1 {; float x, y;; };; float foo(struct u1 u) {; return u.x + u.y;; }. We currently generate:; 	movdqa	%xmm0, %xmm1; 	pshufd	$1, %xmm0, %xmm0 # xmm0 = ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:280,Deployability,patch,patches,280,"//===---------------------------------------------------------------------===//; // Random ideas for the X86 backend.; //===---------------------------------------------------------------------===//. Improvements to the multiply -> shift/add algorithm:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg01590.html. //===---------------------------------------------------------------------===//. Improve code like this (occurs fairly frequently, e.g. in LLVM):; long long foo(int x) { return 1LL << x; }. http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01109.html; http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01128.html; http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01136.html. Another useful one would be ~0ULL >> X and ~0ULL << X. One better solution for 1LL << x is:; xorl %eax, %eax; xorl %edx, %edx; testb $32, %cl; sete %al; setne %dl; sall %cl, %eax; sall %cl, %edx. But that requires good 8-bit subreg support. Also, this might be better. It's an extra shift, but it's one instruction; shorter, and doesn't stress 8-bit subreg support.; (From http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01148.html,; but without the unnecessary and.); movl %ecx, %eax; shrl $5, %eax; movl %eax, %edx; xorl $1, %edx; sall %cl, %eax; sall %cl. %edx. 64-bit shifts (in general) expand to really bad code. Instead of using; cmovs, we should expand to a conditional branch like GCC produces. //===---------------------------------------------------------------------===//. Some isel ideas:. 1. Dynamic programming based approach when compile time is not an; issue.; 2. Code duplication (addressing mode) during isel.; 3. Other ideas from ""Register-Sensitive Selection, Duplication, and; Sequencing of Instructions"".; 4. Scheduling for reduced register pressure. E.g. ""Minimum Register; Instruction Sequence Problem: Revisiting Optimal Code Generation for DAGs""; and other related papers.; http://citeseer.ist.psu.edu/govindarajan01minimum.html. //===---------------------------------------------------------------------=",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:527,Deployability,patch,patches,527,"//===---------------------------------------------------------------------===//; // Random ideas for the X86 backend.; //===---------------------------------------------------------------------===//. Improvements to the multiply -> shift/add algorithm:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg01590.html. //===---------------------------------------------------------------------===//. Improve code like this (occurs fairly frequently, e.g. in LLVM):; long long foo(int x) { return 1LL << x; }. http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01109.html; http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01128.html; http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01136.html. Another useful one would be ~0ULL >> X and ~0ULL << X. One better solution for 1LL << x is:; xorl %eax, %eax; xorl %edx, %edx; testb $32, %cl; sete %al; setne %dl; sall %cl, %eax; sall %cl, %edx. But that requires good 8-bit subreg support. Also, this might be better. It's an extra shift, but it's one instruction; shorter, and doesn't stress 8-bit subreg support.; (From http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01148.html,; but without the unnecessary and.); movl %ecx, %eax; shrl $5, %eax; movl %eax, %edx; xorl $1, %edx; sall %cl, %eax; sall %cl. %edx. 64-bit shifts (in general) expand to really bad code. Instead of using; cmovs, we should expand to a conditional branch like GCC produces. //===---------------------------------------------------------------------===//. Some isel ideas:. 1. Dynamic programming based approach when compile time is not an; issue.; 2. Code duplication (addressing mode) during isel.; 3. Other ideas from ""Register-Sensitive Selection, Duplication, and; Sequencing of Instructions"".; 4. Scheduling for reduced register pressure. E.g. ""Minimum Register; Instruction Sequence Problem: Revisiting Optimal Code Generation for DAGs""; and other related papers.; http://citeseer.ist.psu.edu/govindarajan01minimum.html. //===---------------------------------------------------------------------=",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:584,Deployability,patch,patches,584,"//===---------------------------------------------------------------------===//; // Random ideas for the X86 backend.; //===---------------------------------------------------------------------===//. Improvements to the multiply -> shift/add algorithm:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg01590.html. //===---------------------------------------------------------------------===//. Improve code like this (occurs fairly frequently, e.g. in LLVM):; long long foo(int x) { return 1LL << x; }. http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01109.html; http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01128.html; http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01136.html. Another useful one would be ~0ULL >> X and ~0ULL << X. One better solution for 1LL << x is:; xorl %eax, %eax; xorl %edx, %edx; testb $32, %cl; sete %al; setne %dl; sall %cl, %eax; sall %cl, %edx. But that requires good 8-bit subreg support. Also, this might be better. It's an extra shift, but it's one instruction; shorter, and doesn't stress 8-bit subreg support.; (From http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01148.html,; but without the unnecessary and.); movl %ecx, %eax; shrl $5, %eax; movl %eax, %edx; xorl $1, %edx; sall %cl, %eax; sall %cl. %edx. 64-bit shifts (in general) expand to really bad code. Instead of using; cmovs, we should expand to a conditional branch like GCC produces. //===---------------------------------------------------------------------===//. Some isel ideas:. 1. Dynamic programming based approach when compile time is not an; issue.; 2. Code duplication (addressing mode) during isel.; 3. Other ideas from ""Register-Sensitive Selection, Duplication, and; Sequencing of Instructions"".; 4. Scheduling for reduced register pressure. E.g. ""Minimum Register; Instruction Sequence Problem: Revisiting Optimal Code Generation for DAGs""; and other related papers.; http://citeseer.ist.psu.edu/govindarajan01minimum.html. //===---------------------------------------------------------------------=",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:641,Deployability,patch,patches,641,"//===---------------------------------------------------------------------===//; // Random ideas for the X86 backend.; //===---------------------------------------------------------------------===//. Improvements to the multiply -> shift/add algorithm:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg01590.html. //===---------------------------------------------------------------------===//. Improve code like this (occurs fairly frequently, e.g. in LLVM):; long long foo(int x) { return 1LL << x; }. http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01109.html; http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01128.html; http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01136.html. Another useful one would be ~0ULL >> X and ~0ULL << X. One better solution for 1LL << x is:; xorl %eax, %eax; xorl %edx, %edx; testb $32, %cl; sete %al; setne %dl; sall %cl, %eax; sall %cl, %edx. But that requires good 8-bit subreg support. Also, this might be better. It's an extra shift, but it's one instruction; shorter, and doesn't stress 8-bit subreg support.; (From http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01148.html,; but without the unnecessary and.); movl %ecx, %eax; shrl $5, %eax; movl %eax, %edx; xorl $1, %edx; sall %cl, %eax; sall %cl. %edx. 64-bit shifts (in general) expand to really bad code. Instead of using; cmovs, we should expand to a conditional branch like GCC produces. //===---------------------------------------------------------------------===//. Some isel ideas:. 1. Dynamic programming based approach when compile time is not an; issue.; 2. Code duplication (addressing mode) during isel.; 3. Other ideas from ""Register-Sensitive Selection, Duplication, and; Sequencing of Instructions"".; 4. Scheduling for reduced register pressure. E.g. ""Minimum Register; Instruction Sequence Problem: Revisiting Optimal Code Generation for DAGs""; and other related papers.; http://citeseer.ist.psu.edu/govindarajan01minimum.html. //===---------------------------------------------------------------------=",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:1071,Deployability,patch,patches,1071,"/; // Random ideas for the X86 backend.; //===---------------------------------------------------------------------===//. Improvements to the multiply -> shift/add algorithm:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg01590.html. //===---------------------------------------------------------------------===//. Improve code like this (occurs fairly frequently, e.g. in LLVM):; long long foo(int x) { return 1LL << x; }. http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01109.html; http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01128.html; http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01136.html. Another useful one would be ~0ULL >> X and ~0ULL << X. One better solution for 1LL << x is:; xorl %eax, %eax; xorl %edx, %edx; testb $32, %cl; sete %al; setne %dl; sall %cl, %eax; sall %cl, %edx. But that requires good 8-bit subreg support. Also, this might be better. It's an extra shift, but it's one instruction; shorter, and doesn't stress 8-bit subreg support.; (From http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01148.html,; but without the unnecessary and.); movl %ecx, %eax; shrl $5, %eax; movl %eax, %edx; xorl $1, %edx; sall %cl, %eax; sall %cl. %edx. 64-bit shifts (in general) expand to really bad code. Instead of using; cmovs, we should expand to a conditional branch like GCC produces. //===---------------------------------------------------------------------===//. Some isel ideas:. 1. Dynamic programming based approach when compile time is not an; issue.; 2. Code duplication (addressing mode) during isel.; 3. Other ideas from ""Register-Sensitive Selection, Duplication, and; Sequencing of Instructions"".; 4. Scheduling for reduced register pressure. E.g. ""Minimum Register; Instruction Sequence Problem: Revisiting Optimal Code Generation for DAGs""; and other related papers.; http://citeseer.ist.psu.edu/govindarajan01minimum.html. //===---------------------------------------------------------------------===//. Should we promote i16 to i32 to avoid partial register update stalls?. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:2062,Deployability,update,update,2062,"rom http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01148.html,; but without the unnecessary and.); movl %ecx, %eax; shrl $5, %eax; movl %eax, %edx; xorl $1, %edx; sall %cl, %eax; sall %cl. %edx. 64-bit shifts (in general) expand to really bad code. Instead of using; cmovs, we should expand to a conditional branch like GCC produces. //===---------------------------------------------------------------------===//. Some isel ideas:. 1. Dynamic programming based approach when compile time is not an; issue.; 2. Code duplication (addressing mode) during isel.; 3. Other ideas from ""Register-Sensitive Selection, Duplication, and; Sequencing of Instructions"".; 4. Scheduling for reduced register pressure. E.g. ""Minimum Register; Instruction Sequence Problem: Revisiting Optimal Code Generation for DAGs""; and other related papers.; http://citeseer.ist.psu.edu/govindarajan01minimum.html. //===---------------------------------------------------------------------===//. Should we promote i16 to i32 to avoid partial register update stalls?. //===---------------------------------------------------------------------===//. Leave any_extend as pseudo instruction and hint to register; allocator. Delay codegen until post register allocation.; Note. any_extend is now turned into an INSERT_SUBREG. We still need to teach; the coalescer how to deal with it though. //===---------------------------------------------------------------------===//. It appears icc use push for parameter passing. Need to investigate. //===---------------------------------------------------------------------===//. The instruction selector sometimes misses folding a load into a compare. The; pattern is written as (cmp reg, (load p)). Because the compare isn't; commutative, it is not matched with the load on both sides. The dag combiner; should be made smart enough to canonicalize the load into the RHS of a compare; when it can invert the result of the compare for free. //===------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:1701,Energy Efficiency,Schedul,Scheduling,1701,"<< X. One better solution for 1LL << x is:; xorl %eax, %eax; xorl %edx, %edx; testb $32, %cl; sete %al; setne %dl; sall %cl, %eax; sall %cl, %edx. But that requires good 8-bit subreg support. Also, this might be better. It's an extra shift, but it's one instruction; shorter, and doesn't stress 8-bit subreg support.; (From http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01148.html,; but without the unnecessary and.); movl %ecx, %eax; shrl $5, %eax; movl %eax, %edx; xorl $1, %edx; sall %cl, %eax; sall %cl. %edx. 64-bit shifts (in general) expand to really bad code. Instead of using; cmovs, we should expand to a conditional branch like GCC produces. //===---------------------------------------------------------------------===//. Some isel ideas:. 1. Dynamic programming based approach when compile time is not an; issue.; 2. Code duplication (addressing mode) during isel.; 3. Other ideas from ""Register-Sensitive Selection, Duplication, and; Sequencing of Instructions"".; 4. Scheduling for reduced register pressure. E.g. ""Minimum Register; Instruction Sequence Problem: Revisiting Optimal Code Generation for DAGs""; and other related papers.; http://citeseer.ist.psu.edu/govindarajan01minimum.html. //===---------------------------------------------------------------------===//. Should we promote i16 to i32 to avoid partial register update stalls?. //===---------------------------------------------------------------------===//. Leave any_extend as pseudo instruction and hint to register; allocator. Delay codegen until post register allocation.; Note. any_extend is now turned into an INSERT_SUBREG. We still need to teach; the coalescer how to deal with it though. //===---------------------------------------------------------------------===//. It appears icc use push for parameter passing. Need to investigate. //===---------------------------------------------------------------------===//. The instruction selector sometimes misses folding a load into a compare. The; pattern is writ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:1716,Energy Efficiency,reduce,reduced,1716,"<< X. One better solution for 1LL << x is:; xorl %eax, %eax; xorl %edx, %edx; testb $32, %cl; sete %al; setne %dl; sall %cl, %eax; sall %cl, %edx. But that requires good 8-bit subreg support. Also, this might be better. It's an extra shift, but it's one instruction; shorter, and doesn't stress 8-bit subreg support.; (From http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01148.html,; but without the unnecessary and.); movl %ecx, %eax; shrl $5, %eax; movl %eax, %edx; xorl $1, %edx; sall %cl, %eax; sall %cl. %edx. 64-bit shifts (in general) expand to really bad code. Instead of using; cmovs, we should expand to a conditional branch like GCC produces. //===---------------------------------------------------------------------===//. Some isel ideas:. 1. Dynamic programming based approach when compile time is not an; issue.; 2. Code duplication (addressing mode) during isel.; 3. Other ideas from ""Register-Sensitive Selection, Duplication, and; Sequencing of Instructions"".; 4. Scheduling for reduced register pressure. E.g. ""Minimum Register; Instruction Sequence Problem: Revisiting Optimal Code Generation for DAGs""; and other related papers.; http://citeseer.ist.psu.edu/govindarajan01minimum.html. //===---------------------------------------------------------------------===//. Should we promote i16 to i32 to avoid partial register update stalls?. //===---------------------------------------------------------------------===//. Leave any_extend as pseudo instruction and hint to register; allocator. Delay codegen until post register allocation.; Note. any_extend is now turned into an INSERT_SUBREG. We still need to teach; the coalescer how to deal with it though. //===---------------------------------------------------------------------===//. It appears icc use push for parameter passing. Need to investigate. //===---------------------------------------------------------------------===//. The instruction selector sometimes misses folding a load into a compare. The; pattern is writ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:3243,Energy Efficiency,efficient,efficient,3243,"Delay codegen until post register allocation.; Note. any_extend is now turned into an INSERT_SUBREG. We still need to teach; the coalescer how to deal with it though. //===---------------------------------------------------------------------===//. It appears icc use push for parameter passing. Need to investigate. //===---------------------------------------------------------------------===//. The instruction selector sometimes misses folding a load into a compare. The; pattern is written as (cmp reg, (load p)). Because the compare isn't; commutative, it is not matched with the load on both sides. The dag combiner; should be made smart enough to canonicalize the load into the RHS of a compare; when it can invert the result of the compare for free. //===---------------------------------------------------------------------===//. In many cases, LLVM generates code like this:. _test:; movl 8(%esp), %eax; cmpl %eax, 4(%esp); setl %al; movzbl %al, %eax; ret. on some processors (which ones?), it is more efficient to do this:. _test:; movl 8(%esp), %ebx; xor %eax, %eax; cmpl %ebx, 4(%esp); setl %al; ret. Doing this correctly is tricky though, as the xor clobbers the flags. //===---------------------------------------------------------------------===//. We should generate bts/btr/etc instructions on targets where they are cheap or; when codesize is important. e.g., for:. void setbit(int *target, int bit) {; *target |= (1 << bit);; }; void clearbit(int *target, int bit) {; *target &= ~(1 << bit);; }. //===---------------------------------------------------------------------===//. Instead of the following for memset char*, 1, 10:. 	movl $16843009, 4(%edx); 	movl $16843009, (%edx); 	movw $257, 8(%edx). It might be better to generate. 	movl $16843009, %eax; 	movl %eax, 4(%edx); 	movl %eax, (%edx); 	movw al, 8(%edx); 	; when we can spare a register. It reduces code size. //===---------------------------------------------------------------------===//. Evaluate what the best way to",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:4102,Energy Efficiency,reduce,reduces,4102," this:. _test:; movl 8(%esp), %eax; cmpl %eax, 4(%esp); setl %al; movzbl %al, %eax; ret. on some processors (which ones?), it is more efficient to do this:. _test:; movl 8(%esp), %ebx; xor %eax, %eax; cmpl %ebx, 4(%esp); setl %al; ret. Doing this correctly is tricky though, as the xor clobbers the flags. //===---------------------------------------------------------------------===//. We should generate bts/btr/etc instructions on targets where they are cheap or; when codesize is important. e.g., for:. void setbit(int *target, int bit) {; *target |= (1 << bit);; }; void clearbit(int *target, int bit) {; *target &= ~(1 << bit);; }. //===---------------------------------------------------------------------===//. Instead of the following for memset char*, 1, 10:. 	movl $16843009, 4(%edx); 	movl $16843009, (%edx); 	movw $257, 8(%edx). It might be better to generate. 	movl $16843009, %eax; 	movl %eax, 4(%edx); 	movl %eax, (%edx); 	movw al, 8(%edx); 	; when we can spare a register. It reduces code size. //===---------------------------------------------------------------------===//. Evaluate what the best way to codegen sdiv X, (2^C) is. For X/8, we currently; get this:. define i32 @test1(i32 %X) {; %Y = sdiv i32 %X, 8; ret i32 %Y; }. _test1:; movl 4(%esp), %eax; movl %eax, %ecx; sarl $31, %ecx; shrl $29, %ecx; addl %ecx, %eax; sarl $3, %eax; ret. GCC knows several different ways to codegen it, one of which is this:. _test1:; movl 4(%esp), %eax; cmpl $-1, %eax; leal 7(%eax), %ecx; cmovle %ecx, %eax; sarl $3, %eax; ret. which is probably slower, but it's interesting at least :). //===---------------------------------------------------------------------===//. We are currently lowering large (1MB+) memmove/memcpy to rep/stosl and rep/movsl; We should leave these as libcalls for everything over a much lower threshold,; since libc is hand tuned for medium and large mem ops (avoiding RFO for large; stores, TLB preheating, etc). //===----------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:8767,Energy Efficiency,reduce,reduce,8767,"//. It appears gcc place string data with linkonce linkage in; .section __TEXT,__const_coal,coalesced instead of; .section __DATA,__const_coal,coalesced.; Take a look at darwin.h, there are other Darwin assembler directives that we; do not make use of. //===---------------------------------------------------------------------===//. define i32 @foo(i32* %a, i32 %t) {; entry:; 	br label %cond_true. cond_true:		; preds = %cond_true, %entry; 	%x.0.0 = phi i32 [ 0, %entry ], [ %tmp9, %cond_true ]		; <i32> [#uses=3]; 	%t_addr.0.0 = phi i32 [ %t, %entry ], [ %tmp7, %cond_true ]		; <i32> [#uses=1]; 	%tmp2 = getelementptr i32* %a, i32 %x.0.0		; <i32*> [#uses=1]; 	%tmp3 = load i32* %tmp2		; <i32> [#uses=1]; 	%tmp5 = add i32 %t_addr.0.0, %x.0.0		; <i32> [#uses=1]; 	%tmp7 = add i32 %tmp5, %tmp3		; <i32> [#uses=2]; 	%tmp9 = add i32 %x.0.0, 1		; <i32> [#uses=2]; 	%tmp = icmp sgt i32 %tmp9, 39		; <i1> [#uses=1]; 	br i1 %tmp, label %bb12, label %cond_true. bb12:		; preds = %cond_true; 	ret i32 %tmp7; }; is pessimized by -loop-reduce and -indvars. //===---------------------------------------------------------------------===//. u32 to float conversion improvement:. float uint32_2_float( unsigned u ) {; float fl = (int) (u & 0xffff);; float fh = (int) (u >> 16);; fh *= 0x1.0p16f;; return fh + fl;; }. 00000000 subl $0x04,%esp; 00000003 movl 0x08(%esp,1),%eax; 00000007 movl %eax,%ecx; 00000009 shrl $0x10,%ecx; 0000000c cvtsi2ss %ecx,%xmm0; 00000010 andl $0x0000ffff,%eax; 00000015 cvtsi2ss %eax,%xmm1; 00000019 mulss 0x00000078,%xmm0; 00000021 addss %xmm1,%xmm0; 00000025 movss %xmm0,(%esp,1); 0000002a flds (%esp,1); 0000002d addl $0x04,%esp; 00000030 ret. //===---------------------------------------------------------------------===//. When using fastcc abi, align stack slot of argument of type double on 8 byte; boundary to improve performance. //===---------------------------------------------------------------------===//. GCC's ix86_expand_int_movcc function (in i386.c) has a ton of inte",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:21452,Energy Efficiency,schedul,scheduling,21452," 	fldl	8(%esp); 	fisttpll	(%esp); 	movl	4(%esp), %edx; 	movl	(%esp), %eax; 	addl	$20, %esp; 	#FP_REG_KILL; 	ret. This should just fldl directly from the input stack slot. //===---------------------------------------------------------------------===//. This code:; int foo (int x) { return (x & 65535) | 255; }. Should compile into:. _foo:; movzwl 4(%esp), %eax; orl $255, %eax; ret. instead of:; _foo:; 	movl	$65280, %eax; 	andl	4(%esp), %eax; 	orl	$255, %eax; 	ret. //===---------------------------------------------------------------------===//. We're codegen'ing multiply of long longs inefficiently:. unsigned long long LLM(unsigned long long arg1, unsigned long long arg2) {; return arg1 * arg2;; }. We compile to (fomit-frame-pointer):. _LLM:; 	pushl	%esi; 	movl	8(%esp), %ecx; 	movl	16(%esp), %esi; 	movl	%esi, %eax; 	mull	%ecx; 	imull	12(%esp), %esi; 	addl	%edx, %esi; 	imull	20(%esp), %ecx; 	movl	%esi, %edx; 	addl	%ecx, %edx; 	popl	%esi; 	ret. This looks like a scheduling deficiency and lack of remat of the load from; the argument area. ICC apparently produces:. movl 8(%esp), %ecx; imull 12(%esp), %ecx; movl 16(%esp), %eax; imull 4(%esp), %eax ; addl %eax, %ecx ; movl 4(%esp), %eax; mull 12(%esp) ; addl %ecx, %edx; ret. Note that it remat'd loads from 4(esp) and 12(esp). See this GCC PR:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=17236. //===---------------------------------------------------------------------===//. We can fold a store into ""zeroing a reg"". Instead of:. xorl %eax, %eax; movl %eax, 124(%esp). we should get:. movl $0, 124(%esp). if the flags of the xor are dead. Likewise, we isel ""x<<1"" into ""add reg,reg"". If reg is spilled, this should; be folded into: shl [mem], 1. //===---------------------------------------------------------------------===//. In SSE mode, we turn abs and neg into a load from the constant pool plus a xor; or and instruction, for example:. 	xorpd	LCPI1_0, %xmm2. However, if xmm2 gets spilled, we end up with really ugly code like this:.",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:42526,Energy Efficiency,schedul,scheduled,42526,"the movb+andb+movzbl sequence. //===---------------------------------------------------------------------===//. For the following:; struct u1 {; float x, y;; };; float foo(struct u1 u) {; return u.x + u.y;; }. We currently generate:; 	movdqa	%xmm0, %xmm1; 	pshufd	$1, %xmm0, %xmm0 # xmm0 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm0; 	ret. We could save an instruction here by commuting the addss. //===---------------------------------------------------------------------===//. This (from PR9661):. float clamp_float(float a) {; if (a > 1.0f); return 1.0f;; else if (a < 0.0f); return 0.0f;; else; return a;; }. Could compile to:. clamp_float: # @clamp_float; movss .LCPI0_0(%rip), %xmm1; minss %xmm1, %xmm0; pxor %xmm1, %xmm1; maxss %xmm1, %xmm0; ret. with -ffast-math. //===---------------------------------------------------------------------===//. This function (from PR9803):. int clamp2(int a) {; if (a > 5); a = 5;; if (a < 0) ; return 0;; return a;; }. Compiles to:. _clamp2: ## @clamp2; pushq %rbp; movq %rsp, %rbp; cmpl $5, %edi; movl $5, %ecx; cmovlel %edi, %ecx; testl %ecx, %ecx; movl $0, %eax; cmovnsl %ecx, %eax; popq %rbp; ret. The move of 0 could be scheduled above the test to make it is xor reg,reg. //===---------------------------------------------------------------------===//. GCC PR48986. We currently compile this:. void bar(void);; void yyy(int* p) {; if (__sync_fetch_and_add(p, -1) == 1); bar();; }. into:; 	movl	$-1, %eax; 	lock; 	xaddl	%eax, (%rdi); 	cmpl	$1, %eax; 	je	LBB0_2. Instead we could generate:. 	lock; 	dec %rdi; 	je LBB0_2. The trick is to match ""fetch_and_add(X, -C) == C"". //===---------------------------------------------------------------------===//. unsigned t(unsigned a, unsigned b) {; return a <= b ? 5 : -5;; }. We generate:; 	movl	$5, %ecx; 	cmpl	%esi, %edi; 	movl	$-5, %eax; 	cmovbel	%ecx, %eax. GCC:; 	cmpl	%edi, %esi; 	sbbl	%eax, %eax; 	andl	$-10, %eax; 	addl	$5, %eax. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:12174,Integrability,wrap,wrapped,12174,">. //===---------------------------------------------------------------------===//. In c99 mode, the preprocessor doesn't like assembly comments like #TRUNCATE. //===---------------------------------------------------------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===---------------------------------------------------------------------===//. This code:. void test(int X) {; if (X) abort();; }. is currently compiled to:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne LBB1_1; addl $12, %esp; ret; LBB1_1:; call L_abort$stub. It would be better to produce:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne L_abort$stub; addl $12, %esp; ret. This can be applied to any no-return function call that takes no arguments etc.; Alternatively, the stack save/restore logic could be shrink-wrapped, producing; something like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; subl $12, %esp; call L_abort$stub. Both are useful in different situations. Finally, it could be shrink-wrapped; and tail called, like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; pop %eax # realign stack.; call L_abort$stub. Though this probably isn't worth it. //===---------------------------------------------------------------------===//. Sometimes it is better to codegen subtractions from a constant (e.g. 7-x) with; a neg instead of a sub instruction. Consider:. int test(char X) { return 7-X; }. we currently produce:; _test:; movl $7, %eax; movsbl 4(%esp), %ecx; subl %ecx, %eax; ret. We would use one fewer register if codegen'd as:. movsbl 4(%esp), %eax; 	neg %eax; add $7, %eax; ret. Note that this isn't beneficial if the load can be folded into the sub. In; this case, we want a sub:. int test(int X) { return 7-X; }; _test:; movl $7, %eax; subl 4(%esp), %eax; ret. /",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:12372,Integrability,wrap,wrapped,12372,"----------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===---------------------------------------------------------------------===//. This code:. void test(int X) {; if (X) abort();; }. is currently compiled to:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne LBB1_1; addl $12, %esp; ret; LBB1_1:; call L_abort$stub. It would be better to produce:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne L_abort$stub; addl $12, %esp; ret. This can be applied to any no-return function call that takes no arguments etc.; Alternatively, the stack save/restore logic could be shrink-wrapped, producing; something like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; subl $12, %esp; call L_abort$stub. Both are useful in different situations. Finally, it could be shrink-wrapped; and tail called, like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; pop %eax # realign stack.; call L_abort$stub. Though this probably isn't worth it. //===---------------------------------------------------------------------===//. Sometimes it is better to codegen subtractions from a constant (e.g. 7-x) with; a neg instead of a sub instruction. Consider:. int test(char X) { return 7-X; }. we currently produce:; _test:; movl $7, %eax; movsbl 4(%esp), %ecx; subl %ecx, %eax; ret. We would use one fewer register if codegen'd as:. movsbl 4(%esp), %eax; 	neg %eax; add $7, %eax; ret. Note that this isn't beneficial if the load can be folded into the sub. In; this case, we want a sub:. int test(int X) { return 7-X; }; _test:; movl $7, %eax; subl 4(%esp), %eax; ret. //===---------------------------------------------------------------------===//. Leaf functions that require one 4-byte spill slot have a prolog like this:. _foo:; pushl %esi; subl $4, %esp; ...; and an epilog lik",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:6267,Modifiability,extend,extend,6267,"--------------------------------------===//. Optimize copysign(x, *y) to use an integer load from y. //===---------------------------------------------------------------------===//. The following tests perform worse with LSR:. lambda, siod, optimizer-eval, ackermann, hash2, nestedloop, strcat, and Treesor. //===---------------------------------------------------------------------===//. Adding to the list of cmp / test poor codegen issues:. int test(__m128 *A, __m128 *B) {; if (_mm_comige_ss(*A, *B)); return 3;; else; return 4;; }. _test:; 	movl 8(%esp), %eax; 	movaps (%eax), %xmm0; 	movl 4(%esp), %eax; 	movaps (%eax), %xmm1; 	comiss %xmm0, %xmm1; 	setae %al; 	movzbl %al, %ecx; 	movl $3, %eax; 	movl $4, %edx; 	cmpl $0, %ecx; 	cmove %edx, %eax; 	ret. Note the setae, movzbl, cmpl, cmove can be replaced with a single cmovae. There; are a number of issues. 1) We are introducing a setcc between the result of the; intrisic call and select. 2) The intrinsic is expected to produce a i32 value; so a any extend (which becomes a zero extend) is added. We probably need some kind of target DAG combine hook to fix this. //===---------------------------------------------------------------------===//. We generate significantly worse code for this than GCC:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=21150; http://gcc.gnu.org/bugzilla/attachment.cgi?id=8701. There is also one case we do worse on PPC. //===---------------------------------------------------------------------===//. For this:. int test(int a); {; return a * 3;; }. We currently emits; 	imull $3, 4(%esp), %eax. Perhaps this is what we really should generate is? Is imull three or four; cycles? Note: ICC generates this:; 	movl	4(%esp), %eax; 	leal	(%eax,%eax,2), %eax. The current instruction priority is based on pattern complexity. The former is; more ""complex"" because it folds a load so the latter will not be emitted. Perhaps we should use AddedComplexity to give LEA32r a higher priority? We; should always try to match LEA",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:6296,Modifiability,extend,extend,6296,"--------------------------------------===//. Optimize copysign(x, *y) to use an integer load from y. //===---------------------------------------------------------------------===//. The following tests perform worse with LSR:. lambda, siod, optimizer-eval, ackermann, hash2, nestedloop, strcat, and Treesor. //===---------------------------------------------------------------------===//. Adding to the list of cmp / test poor codegen issues:. int test(__m128 *A, __m128 *B) {; if (_mm_comige_ss(*A, *B)); return 3;; else; return 4;; }. _test:; 	movl 8(%esp), %eax; 	movaps (%eax), %xmm0; 	movl 4(%esp), %eax; 	movaps (%eax), %xmm1; 	comiss %xmm0, %xmm1; 	setae %al; 	movzbl %al, %ecx; 	movl $3, %eax; 	movl $4, %edx; 	cmpl $0, %ecx; 	cmove %edx, %eax; 	ret. Note the setae, movzbl, cmpl, cmove can be replaced with a single cmovae. There; are a number of issues. 1) We are introducing a setcc between the result of the; intrisic call and select. 2) The intrinsic is expected to produce a i32 value; so a any extend (which becomes a zero extend) is added. We probably need some kind of target DAG combine hook to fix this. //===---------------------------------------------------------------------===//. We generate significantly worse code for this than GCC:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=21150; http://gcc.gnu.org/bugzilla/attachment.cgi?id=8701. There is also one case we do worse on PPC. //===---------------------------------------------------------------------===//. For this:. int test(int a); {; return a * 3;; }. We currently emits; 	imull $3, 4(%esp), %eax. Perhaps this is what we really should generate is? Is imull three or four; cycles? Note: ICC generates this:; 	movl	4(%esp), %eax; 	leal	(%eax,%eax,2), %eax. The current instruction priority is based on pattern complexity. The former is; more ""complex"" because it folds a load so the latter will not be emitted. Perhaps we should use AddedComplexity to give LEA32r a higher priority? We; should always try to match LEA",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:19053,Modifiability,variab,variables,19053,"uments on the top of the stack (their normal place for; non-tail call optimized calls) that source from the callers arguments; or that source from a virtual register (also possibly sourcing from; callers arguments).; This is done to prevent overwriting of parameters (see example; below) that might be used later. example: . int callee(int32, int64); ; int caller(int32 arg1, int32 arg2) { ; int64 local = arg2 * 2; ; return callee(arg2, (int64)local); ; }. [arg1] [!arg2 no longer valid since we moved local onto it]; [arg2] -> [(int64); [RETADDR] local ]. Moving arg1 onto the stack slot of callee function would overwrite; arg2 of the caller. Possible optimizations:. - Analyse the actual parameters of the callee to see which would; overwrite a caller parameter which is used by the callee and only; push them onto the top of the stack. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg1,arg2);; }. Here we don't need to write any variables to the top of the stack; since they don't overwrite each other. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg2,arg1);; }. Here we need to push the arguments because they overwrite each; other. //===---------------------------------------------------------------------===//. main (); {; int i = 0;; unsigned long int z = 0;. do {; z -= 0x00004000;; i++;; if (i > 0x00040000); abort ();; } while (z > 0);; exit (0);; }. gcc compiles this to:. _main:; 	subl	$28, %esp; 	xorl	%eax, %eax; 	jmp	L2; L3:; 	cmpl	$262144, %eax; 	je	L10; L2:; 	addl	$1, %eax; 	cmpl	$262145, %eax; 	jne	L3; 	call	L_abort$stub; L10:; 	movl	$0, (%esp); 	call	L_exit$stub. llvm:. _main:; 	subl	$12, %esp; 	movl	$1, %eax; 	movl	$16384, %ecx; LBB1_1:	# bb; 	cmpl	$262145, %eax; 	jge	LBB1_4	# cond_true; LBB1_2:	# cond_next; 	incl	%eax; 	addl	$4294950912, %ecx; 	cmpl	$16384, %ecx; 	jne	LBB1_1	# bb; LBB1_3:	# bb11; 	xorl	%eax, %eax; 	addl	$12, %esp; 	ret; LBB1_4:	# cond_true; 	call	L_abort$stu",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:20090,Modifiability,rewrite,rewrite,20090,"write each other. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg2,arg1);; }. Here we need to push the arguments because they overwrite each; other. //===---------------------------------------------------------------------===//. main (); {; int i = 0;; unsigned long int z = 0;. do {; z -= 0x00004000;; i++;; if (i > 0x00040000); abort ();; } while (z > 0);; exit (0);; }. gcc compiles this to:. _main:; 	subl	$28, %esp; 	xorl	%eax, %eax; 	jmp	L2; L3:; 	cmpl	$262144, %eax; 	je	L10; L2:; 	addl	$1, %eax; 	cmpl	$262145, %eax; 	jne	L3; 	call	L_abort$stub; L10:; 	movl	$0, (%esp); 	call	L_exit$stub. llvm:. _main:; 	subl	$12, %esp; 	movl	$1, %eax; 	movl	$16384, %ecx; LBB1_1:	# bb; 	cmpl	$262145, %eax; 	jge	LBB1_4	# cond_true; LBB1_2:	# cond_next; 	incl	%eax; 	addl	$4294950912, %ecx; 	cmpl	$16384, %ecx; 	jne	LBB1_1	# bb; LBB1_3:	# bb11; 	xorl	%eax, %eax; 	addl	$12, %esp; 	ret; LBB1_4:	# cond_true; 	call	L_abort$stub. 1. LSR should rewrite the first cmp with induction variable %ecx.; 2. DAG combiner should fold; leal 1(%eax), %edx; cmpl $262145, %edx; =>; cmpl $262144, %eax. //===---------------------------------------------------------------------===//. define i64 @test(double %X) {; 	%Y = fptosi double %X to i64; 	ret i64 %Y; }. compiles to:. _test:; 	subl	$20, %esp; 	movsd	24(%esp), %xmm0; 	movsd	%xmm0, 8(%esp); 	fldl	8(%esp); 	fisttpll	(%esp); 	movl	4(%esp), %edx; 	movl	(%esp), %eax; 	addl	$20, %esp; 	#FP_REG_KILL; 	ret. This should just fldl directly from the input stack slot. //===---------------------------------------------------------------------===//. This code:; int foo (int x) { return (x & 65535) | 255; }. Should compile into:. _foo:; movzwl 4(%esp), %eax; orl $255, %eax; ret. instead of:; _foo:; 	movl	$65280, %eax; 	andl	4(%esp), %eax; 	orl	$255, %eax; 	ret. //===---------------------------------------------------------------------===//. We're codegen'ing multiply of long longs inefficiently:. unsigned long long LLM(un",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:20127,Modifiability,variab,variable,20127,"write each other. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg2,arg1);; }. Here we need to push the arguments because they overwrite each; other. //===---------------------------------------------------------------------===//. main (); {; int i = 0;; unsigned long int z = 0;. do {; z -= 0x00004000;; i++;; if (i > 0x00040000); abort ();; } while (z > 0);; exit (0);; }. gcc compiles this to:. _main:; 	subl	$28, %esp; 	xorl	%eax, %eax; 	jmp	L2; L3:; 	cmpl	$262144, %eax; 	je	L10; L2:; 	addl	$1, %eax; 	cmpl	$262145, %eax; 	jne	L3; 	call	L_abort$stub; L10:; 	movl	$0, (%esp); 	call	L_exit$stub. llvm:. _main:; 	subl	$12, %esp; 	movl	$1, %eax; 	movl	$16384, %ecx; LBB1_1:	# bb; 	cmpl	$262145, %eax; 	jge	LBB1_4	# cond_true; LBB1_2:	# cond_next; 	incl	%eax; 	addl	$4294950912, %ecx; 	cmpl	$16384, %ecx; 	jne	LBB1_1	# bb; LBB1_3:	# bb11; 	xorl	%eax, %eax; 	addl	$12, %esp; 	ret; LBB1_4:	# cond_true; 	call	L_abort$stub. 1. LSR should rewrite the first cmp with induction variable %ecx.; 2. DAG combiner should fold; leal 1(%eax), %edx; cmpl $262145, %edx; =>; cmpl $262144, %eax. //===---------------------------------------------------------------------===//. define i64 @test(double %X) {; 	%Y = fptosi double %X to i64; 	ret i64 %Y; }. compiles to:. _test:; 	subl	$20, %esp; 	movsd	24(%esp), %xmm0; 	movsd	%xmm0, 8(%esp); 	fldl	8(%esp); 	fisttpll	(%esp); 	movl	4(%esp), %edx; 	movl	(%esp), %eax; 	addl	$20, %esp; 	#FP_REG_KILL; 	ret. This should just fldl directly from the input stack slot. //===---------------------------------------------------------------------===//. This code:; int foo (int x) { return (x & 65535) | 255; }. Should compile into:. _foo:; movzwl 4(%esp), %eax; orl $255, %eax; ret. instead of:; _foo:; 	movl	$65280, %eax; 	andl	4(%esp), %eax; 	orl	$255, %eax; 	ret. //===---------------------------------------------------------------------===//. We're codegen'ing multiply of long longs inefficiently:. unsigned long long LLM(un",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:2680,Performance,load,load,2680,"and; Sequencing of Instructions"".; 4. Scheduling for reduced register pressure. E.g. ""Minimum Register; Instruction Sequence Problem: Revisiting Optimal Code Generation for DAGs""; and other related papers.; http://citeseer.ist.psu.edu/govindarajan01minimum.html. //===---------------------------------------------------------------------===//. Should we promote i16 to i32 to avoid partial register update stalls?. //===---------------------------------------------------------------------===//. Leave any_extend as pseudo instruction and hint to register; allocator. Delay codegen until post register allocation.; Note. any_extend is now turned into an INSERT_SUBREG. We still need to teach; the coalescer how to deal with it though. //===---------------------------------------------------------------------===//. It appears icc use push for parameter passing. Need to investigate. //===---------------------------------------------------------------------===//. The instruction selector sometimes misses folding a load into a compare. The; pattern is written as (cmp reg, (load p)). Because the compare isn't; commutative, it is not matched with the load on both sides. The dag combiner; should be made smart enough to canonicalize the load into the RHS of a compare; when it can invert the result of the compare for free. //===---------------------------------------------------------------------===//. In many cases, LLVM generates code like this:. _test:; movl 8(%esp), %eax; cmpl %eax, 4(%esp); setl %al; movzbl %al, %eax; ret. on some processors (which ones?), it is more efficient to do this:. _test:; movl 8(%esp), %ebx; xor %eax, %eax; cmpl %ebx, 4(%esp); setl %al; ret. Doing this correctly is tricky though, as the xor clobbers the flags. //===---------------------------------------------------------------------===//. We should generate bts/btr/etc instructions on targets where they are cheap or; when codesize is important. e.g., for:. void setbit(int *target, int bit) {; *target |= ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:2739,Performance,load,load,2739,"register pressure. E.g. ""Minimum Register; Instruction Sequence Problem: Revisiting Optimal Code Generation for DAGs""; and other related papers.; http://citeseer.ist.psu.edu/govindarajan01minimum.html. //===---------------------------------------------------------------------===//. Should we promote i16 to i32 to avoid partial register update stalls?. //===---------------------------------------------------------------------===//. Leave any_extend as pseudo instruction and hint to register; allocator. Delay codegen until post register allocation.; Note. any_extend is now turned into an INSERT_SUBREG. We still need to teach; the coalescer how to deal with it though. //===---------------------------------------------------------------------===//. It appears icc use push for parameter passing. Need to investigate. //===---------------------------------------------------------------------===//. The instruction selector sometimes misses folding a load into a compare. The; pattern is written as (cmp reg, (load p)). Because the compare isn't; commutative, it is not matched with the load on both sides. The dag combiner; should be made smart enough to canonicalize the load into the RHS of a compare; when it can invert the result of the compare for free. //===---------------------------------------------------------------------===//. In many cases, LLVM generates code like this:. _test:; movl 8(%esp), %eax; cmpl %eax, 4(%esp); setl %al; movzbl %al, %eax; ret. on some processors (which ones?), it is more efficient to do this:. _test:; movl 8(%esp), %ebx; xor %eax, %eax; cmpl %ebx, 4(%esp); setl %al; ret. Doing this correctly is tricky though, as the xor clobbers the flags. //===---------------------------------------------------------------------===//. We should generate bts/btr/etc instructions on targets where they are cheap or; when codesize is important. e.g., for:. void setbit(int *target, int bit) {; *target |= (1 << bit);; }; void clearbit(int *target, int bit) {; *targ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:2816,Performance,load,load,2816,"blem: Revisiting Optimal Code Generation for DAGs""; and other related papers.; http://citeseer.ist.psu.edu/govindarajan01minimum.html. //===---------------------------------------------------------------------===//. Should we promote i16 to i32 to avoid partial register update stalls?. //===---------------------------------------------------------------------===//. Leave any_extend as pseudo instruction and hint to register; allocator. Delay codegen until post register allocation.; Note. any_extend is now turned into an INSERT_SUBREG. We still need to teach; the coalescer how to deal with it though. //===---------------------------------------------------------------------===//. It appears icc use push for parameter passing. Need to investigate. //===---------------------------------------------------------------------===//. The instruction selector sometimes misses folding a load into a compare. The; pattern is written as (cmp reg, (load p)). Because the compare isn't; commutative, it is not matched with the load on both sides. The dag combiner; should be made smart enough to canonicalize the load into the RHS of a compare; when it can invert the result of the compare for free. //===---------------------------------------------------------------------===//. In many cases, LLVM generates code like this:. _test:; movl 8(%esp), %eax; cmpl %eax, 4(%esp); setl %al; movzbl %al, %eax; ret. on some processors (which ones?), it is more efficient to do this:. _test:; movl 8(%esp), %ebx; xor %eax, %eax; cmpl %ebx, 4(%esp); setl %al; ret. Doing this correctly is tricky though, as the xor clobbers the flags. //===---------------------------------------------------------------------===//. We should generate bts/btr/etc instructions on targets where they are cheap or; when codesize is important. e.g., for:. void setbit(int *target, int bit) {; *target |= (1 << bit);; }; void clearbit(int *target, int bit) {; *target &= ~(1 << bit);; }. //===----------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:2902,Performance,load,load,2902,"1minimum.html. //===---------------------------------------------------------------------===//. Should we promote i16 to i32 to avoid partial register update stalls?. //===---------------------------------------------------------------------===//. Leave any_extend as pseudo instruction and hint to register; allocator. Delay codegen until post register allocation.; Note. any_extend is now turned into an INSERT_SUBREG. We still need to teach; the coalescer how to deal with it though. //===---------------------------------------------------------------------===//. It appears icc use push for parameter passing. Need to investigate. //===---------------------------------------------------------------------===//. The instruction selector sometimes misses folding a load into a compare. The; pattern is written as (cmp reg, (load p)). Because the compare isn't; commutative, it is not matched with the load on both sides. The dag combiner; should be made smart enough to canonicalize the load into the RHS of a compare; when it can invert the result of the compare for free. //===---------------------------------------------------------------------===//. In many cases, LLVM generates code like this:. _test:; movl 8(%esp), %eax; cmpl %eax, 4(%esp); setl %al; movzbl %al, %eax; ret. on some processors (which ones?), it is more efficient to do this:. _test:; movl 8(%esp), %ebx; xor %eax, %eax; cmpl %ebx, 4(%esp); setl %al; ret. Doing this correctly is tricky though, as the xor clobbers the flags. //===---------------------------------------------------------------------===//. We should generate bts/btr/etc instructions on targets where they are cheap or; when codesize is important. e.g., for:. void setbit(int *target, int bit) {; *target |= (1 << bit);; }; void clearbit(int *target, int bit) {; *target &= ~(1 << bit);; }. //===---------------------------------------------------------------------===//. Instead of the following for memset char*, 1, 10:. 	movl $16843009, 4(%edx); 	movl $",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:4968,Performance,tune,tuned,4968,"(%edx); 	movw $257, 8(%edx). It might be better to generate. 	movl $16843009, %eax; 	movl %eax, 4(%edx); 	movl %eax, (%edx); 	movw al, 8(%edx); 	; when we can spare a register. It reduces code size. //===---------------------------------------------------------------------===//. Evaluate what the best way to codegen sdiv X, (2^C) is. For X/8, we currently; get this:. define i32 @test1(i32 %X) {; %Y = sdiv i32 %X, 8; ret i32 %Y; }. _test1:; movl 4(%esp), %eax; movl %eax, %ecx; sarl $31, %ecx; shrl $29, %ecx; addl %ecx, %eax; sarl $3, %eax; ret. GCC knows several different ways to codegen it, one of which is this:. _test1:; movl 4(%esp), %eax; cmpl $-1, %eax; leal 7(%eax), %ecx; cmovle %ecx, %eax; sarl $3, %eax; ret. which is probably slower, but it's interesting at least :). //===---------------------------------------------------------------------===//. We are currently lowering large (1MB+) memmove/memcpy to rep/stosl and rep/movsl; We should leave these as libcalls for everything over a much lower threshold,; since libc is hand tuned for medium and large mem ops (avoiding RFO for large; stores, TLB preheating, etc). //===---------------------------------------------------------------------===//. Optimize this into something reasonable:; x * copysign(1.0, y) * copysign(1.0, z). //===---------------------------------------------------------------------===//. Optimize copysign(x, *y) to use an integer load from y. //===---------------------------------------------------------------------===//. The following tests perform worse with LSR:. lambda, siod, optimizer-eval, ackermann, hash2, nestedloop, strcat, and Treesor. //===---------------------------------------------------------------------===//. Adding to the list of cmp / test poor codegen issues:. int test(__m128 *A, __m128 *B) {; if (_mm_comige_ss(*A, *B)); return 3;; else; return 4;; }. _test:; 	movl 8(%esp), %eax; 	movaps (%eax), %xmm0; 	movl 4(%esp), %eax; 	movaps (%eax), %xmm1; 	comiss %xmm0, %xmm1; 	setae %a",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:5139,Performance,Optimiz,Optimize,5139,"----------------------------===//. Evaluate what the best way to codegen sdiv X, (2^C) is. For X/8, we currently; get this:. define i32 @test1(i32 %X) {; %Y = sdiv i32 %X, 8; ret i32 %Y; }. _test1:; movl 4(%esp), %eax; movl %eax, %ecx; sarl $31, %ecx; shrl $29, %ecx; addl %ecx, %eax; sarl $3, %eax; ret. GCC knows several different ways to codegen it, one of which is this:. _test1:; movl 4(%esp), %eax; cmpl $-1, %eax; leal 7(%eax), %ecx; cmovle %ecx, %eax; sarl $3, %eax; ret. which is probably slower, but it's interesting at least :). //===---------------------------------------------------------------------===//. We are currently lowering large (1MB+) memmove/memcpy to rep/stosl and rep/movsl; We should leave these as libcalls for everything over a much lower threshold,; since libc is hand tuned for medium and large mem ops (avoiding RFO for large; stores, TLB preheating, etc). //===---------------------------------------------------------------------===//. Optimize this into something reasonable:; x * copysign(1.0, y) * copysign(1.0, z). //===---------------------------------------------------------------------===//. Optimize copysign(x, *y) to use an integer load from y. //===---------------------------------------------------------------------===//. The following tests perform worse with LSR:. lambda, siod, optimizer-eval, ackermann, hash2, nestedloop, strcat, and Treesor. //===---------------------------------------------------------------------===//. Adding to the list of cmp / test poor codegen issues:. int test(__m128 *A, __m128 *B) {; if (_mm_comige_ss(*A, *B)); return 3;; else; return 4;; }. _test:; 	movl 8(%esp), %eax; 	movaps (%eax), %xmm0; 	movl 4(%esp), %eax; 	movaps (%eax), %xmm1; 	comiss %xmm0, %xmm1; 	setae %al; 	movzbl %al, %ecx; 	movl $3, %eax; 	movl $4, %edx; 	cmpl $0, %ecx; 	cmove %edx, %eax; 	ret. Note the setae, movzbl, cmpl, cmove can be replaced with a single cmovae. There; are a number of issues. 1) We are introducing a setcc between the res",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:5303,Performance,Optimiz,Optimize,5303," i32 %X, 8; ret i32 %Y; }. _test1:; movl 4(%esp), %eax; movl %eax, %ecx; sarl $31, %ecx; shrl $29, %ecx; addl %ecx, %eax; sarl $3, %eax; ret. GCC knows several different ways to codegen it, one of which is this:. _test1:; movl 4(%esp), %eax; cmpl $-1, %eax; leal 7(%eax), %ecx; cmovle %ecx, %eax; sarl $3, %eax; ret. which is probably slower, but it's interesting at least :). //===---------------------------------------------------------------------===//. We are currently lowering large (1MB+) memmove/memcpy to rep/stosl and rep/movsl; We should leave these as libcalls for everything over a much lower threshold,; since libc is hand tuned for medium and large mem ops (avoiding RFO for large; stores, TLB preheating, etc). //===---------------------------------------------------------------------===//. Optimize this into something reasonable:; x * copysign(1.0, y) * copysign(1.0, z). //===---------------------------------------------------------------------===//. Optimize copysign(x, *y) to use an integer load from y. //===---------------------------------------------------------------------===//. The following tests perform worse with LSR:. lambda, siod, optimizer-eval, ackermann, hash2, nestedloop, strcat, and Treesor. //===---------------------------------------------------------------------===//. Adding to the list of cmp / test poor codegen issues:. int test(__m128 *A, __m128 *B) {; if (_mm_comige_ss(*A, *B)); return 3;; else; return 4;; }. _test:; 	movl 8(%esp), %eax; 	movaps (%eax), %xmm0; 	movl 4(%esp), %eax; 	movaps (%eax), %xmm1; 	comiss %xmm0, %xmm1; 	setae %al; 	movzbl %al, %ecx; 	movl $3, %eax; 	movl $4, %edx; 	cmpl $0, %ecx; 	cmove %edx, %eax; 	ret. Note the setae, movzbl, cmpl, cmove can be replaced with a single cmovae. There; are a number of issues. 1) We are introducing a setcc between the result of the; intrisic call and select. 2) The intrinsic is expected to produce a i32 value; so a any extend (which becomes a zero extend) is added. We probably need",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:5346,Performance,load,load,5346," i32 %X, 8; ret i32 %Y; }. _test1:; movl 4(%esp), %eax; movl %eax, %ecx; sarl $31, %ecx; shrl $29, %ecx; addl %ecx, %eax; sarl $3, %eax; ret. GCC knows several different ways to codegen it, one of which is this:. _test1:; movl 4(%esp), %eax; cmpl $-1, %eax; leal 7(%eax), %ecx; cmovle %ecx, %eax; sarl $3, %eax; ret. which is probably slower, but it's interesting at least :). //===---------------------------------------------------------------------===//. We are currently lowering large (1MB+) memmove/memcpy to rep/stosl and rep/movsl; We should leave these as libcalls for everything over a much lower threshold,; since libc is hand tuned for medium and large mem ops (avoiding RFO for large; stores, TLB preheating, etc). //===---------------------------------------------------------------------===//. Optimize this into something reasonable:; x * copysign(1.0, y) * copysign(1.0, z). //===---------------------------------------------------------------------===//. Optimize copysign(x, *y) to use an integer load from y. //===---------------------------------------------------------------------===//. The following tests perform worse with LSR:. lambda, siod, optimizer-eval, ackermann, hash2, nestedloop, strcat, and Treesor. //===---------------------------------------------------------------------===//. Adding to the list of cmp / test poor codegen issues:. int test(__m128 *A, __m128 *B) {; if (_mm_comige_ss(*A, *B)); return 3;; else; return 4;; }. _test:; 	movl 8(%esp), %eax; 	movaps (%eax), %xmm0; 	movl 4(%esp), %eax; 	movaps (%eax), %xmm1; 	comiss %xmm0, %xmm1; 	setae %al; 	movzbl %al, %ecx; 	movl $3, %eax; 	movl $4, %edx; 	cmpl $0, %ecx; 	cmove %edx, %eax; 	ret. Note the setae, movzbl, cmpl, cmove can be replaced with a single cmovae. There; are a number of issues. 1) We are introducing a setcc between the result of the; intrisic call and select. 2) The intrinsic is expected to produce a i32 value; so a any extend (which becomes a zero extend) is added. We probably need",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:5460,Performance,perform,perform,5460,"%eax; ret. GCC knows several different ways to codegen it, one of which is this:. _test1:; movl 4(%esp), %eax; cmpl $-1, %eax; leal 7(%eax), %ecx; cmovle %ecx, %eax; sarl $3, %eax; ret. which is probably slower, but it's interesting at least :). //===---------------------------------------------------------------------===//. We are currently lowering large (1MB+) memmove/memcpy to rep/stosl and rep/movsl; We should leave these as libcalls for everything over a much lower threshold,; since libc is hand tuned for medium and large mem ops (avoiding RFO for large; stores, TLB preheating, etc). //===---------------------------------------------------------------------===//. Optimize this into something reasonable:; x * copysign(1.0, y) * copysign(1.0, z). //===---------------------------------------------------------------------===//. Optimize copysign(x, *y) to use an integer load from y. //===---------------------------------------------------------------------===//. The following tests perform worse with LSR:. lambda, siod, optimizer-eval, ackermann, hash2, nestedloop, strcat, and Treesor. //===---------------------------------------------------------------------===//. Adding to the list of cmp / test poor codegen issues:. int test(__m128 *A, __m128 *B) {; if (_mm_comige_ss(*A, *B)); return 3;; else; return 4;; }. _test:; 	movl 8(%esp), %eax; 	movaps (%eax), %xmm0; 	movl 4(%esp), %eax; 	movaps (%eax), %xmm1; 	comiss %xmm0, %xmm1; 	setae %al; 	movzbl %al, %ecx; 	movl $3, %eax; 	movl $4, %edx; 	cmpl $0, %ecx; 	cmove %edx, %eax; 	ret. Note the setae, movzbl, cmpl, cmove can be replaced with a single cmovae. There; are a number of issues. 1) We are introducing a setcc between the result of the; intrisic call and select. 2) The intrinsic is expected to produce a i32 value; so a any extend (which becomes a zero extend) is added. We probably need some kind of target DAG combine hook to fix this. //===---------------------------------------------------------------------===//. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:5499,Performance,optimiz,optimizer-eval,5499,"of which is this:. _test1:; movl 4(%esp), %eax; cmpl $-1, %eax; leal 7(%eax), %ecx; cmovle %ecx, %eax; sarl $3, %eax; ret. which is probably slower, but it's interesting at least :). //===---------------------------------------------------------------------===//. We are currently lowering large (1MB+) memmove/memcpy to rep/stosl and rep/movsl; We should leave these as libcalls for everything over a much lower threshold,; since libc is hand tuned for medium and large mem ops (avoiding RFO for large; stores, TLB preheating, etc). //===---------------------------------------------------------------------===//. Optimize this into something reasonable:; x * copysign(1.0, y) * copysign(1.0, z). //===---------------------------------------------------------------------===//. Optimize copysign(x, *y) to use an integer load from y. //===---------------------------------------------------------------------===//. The following tests perform worse with LSR:. lambda, siod, optimizer-eval, ackermann, hash2, nestedloop, strcat, and Treesor. //===---------------------------------------------------------------------===//. Adding to the list of cmp / test poor codegen issues:. int test(__m128 *A, __m128 *B) {; if (_mm_comige_ss(*A, *B)); return 3;; else; return 4;; }. _test:; 	movl 8(%esp), %eax; 	movaps (%eax), %xmm0; 	movl 4(%esp), %eax; 	movaps (%eax), %xmm1; 	comiss %xmm0, %xmm1; 	setae %al; 	movzbl %al, %ecx; 	movl $3, %eax; 	movl $4, %edx; 	cmpl $0, %ecx; 	cmove %edx, %eax; 	ret. Note the setae, movzbl, cmpl, cmove can be replaced with a single cmovae. There; are a number of issues. 1) We are introducing a setcc between the result of the; intrisic call and select. 2) The intrinsic is expected to produce a i32 value; so a any extend (which becomes a zero extend) is added. We probably need some kind of target DAG combine hook to fix this. //===---------------------------------------------------------------------===//. We generate significantly worse code for this than GCC:; http:/",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:7113,Performance,load,load,7113,"ber of issues. 1) We are introducing a setcc between the result of the; intrisic call and select. 2) The intrinsic is expected to produce a i32 value; so a any extend (which becomes a zero extend) is added. We probably need some kind of target DAG combine hook to fix this. //===---------------------------------------------------------------------===//. We generate significantly worse code for this than GCC:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=21150; http://gcc.gnu.org/bugzilla/attachment.cgi?id=8701. There is also one case we do worse on PPC. //===---------------------------------------------------------------------===//. For this:. int test(int a); {; return a * 3;; }. We currently emits; 	imull $3, 4(%esp), %eax. Perhaps this is what we really should generate is? Is imull three or four; cycles? Note: ICC generates this:; 	movl	4(%esp), %eax; 	leal	(%eax,%eax,2), %eax. The current instruction priority is based on pattern complexity. The former is; more ""complex"" because it folds a load so the latter will not be emitted. Perhaps we should use AddedComplexity to give LEA32r a higher priority? We; should always try to match LEA first since the LEA matching code does some; estimate to determine whether the match is profitable. However, if we care more about code size, then imull is better. It's two bytes; shorter than movl + leal. On a Pentium M, both variants have the same characteristics with regard; to throughput; however, the multiplication has a latency of four cycles, as; opposed to two cycles for the movl+lea variant. //===---------------------------------------------------------------------===//. It appears gcc place string data with linkonce linkage in; .section __TEXT,__const_coal,coalesced instead of; .section __DATA,__const_coal,coalesced.; Take a look at darwin.h, there are other Darwin assembler directives that we; do not make use of. //===---------------------------------------------------------------------===//. define i32 @foo(i32* %a, i32 %t) ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:7542,Performance,throughput,throughput,7542,"1150; http://gcc.gnu.org/bugzilla/attachment.cgi?id=8701. There is also one case we do worse on PPC. //===---------------------------------------------------------------------===//. For this:. int test(int a); {; return a * 3;; }. We currently emits; 	imull $3, 4(%esp), %eax. Perhaps this is what we really should generate is? Is imull three or four; cycles? Note: ICC generates this:; 	movl	4(%esp), %eax; 	leal	(%eax,%eax,2), %eax. The current instruction priority is based on pattern complexity. The former is; more ""complex"" because it folds a load so the latter will not be emitted. Perhaps we should use AddedComplexity to give LEA32r a higher priority? We; should always try to match LEA first since the LEA matching code does some; estimate to determine whether the match is profitable. However, if we care more about code size, then imull is better. It's two bytes; shorter than movl + leal. On a Pentium M, both variants have the same characteristics with regard; to throughput; however, the multiplication has a latency of four cycles, as; opposed to two cycles for the movl+lea variant. //===---------------------------------------------------------------------===//. It appears gcc place string data with linkonce linkage in; .section __TEXT,__const_coal,coalesced instead of; .section __DATA,__const_coal,coalesced.; Take a look at darwin.h, there are other Darwin assembler directives that we; do not make use of. //===---------------------------------------------------------------------===//. define i32 @foo(i32* %a, i32 %t) {; entry:; 	br label %cond_true. cond_true:		; preds = %cond_true, %entry; 	%x.0.0 = phi i32 [ 0, %entry ], [ %tmp9, %cond_true ]		; <i32> [#uses=3]; 	%t_addr.0.0 = phi i32 [ %t, %entry ], [ %tmp7, %cond_true ]		; <i32> [#uses=1]; 	%tmp2 = getelementptr i32* %a, i32 %x.0.0		; <i32*> [#uses=1]; 	%tmp3 = load i32* %tmp2		; <i32> [#uses=1]; 	%tmp5 = add i32 %t_addr.0.0, %x.0.0		; <i32> [#uses=1]; 	%tmp7 = add i32 %tmp5, %tmp3		; <i32> [#uses=2]; 	%tmp9 = ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:7588,Performance,latency,latency,7588,"1150; http://gcc.gnu.org/bugzilla/attachment.cgi?id=8701. There is also one case we do worse on PPC. //===---------------------------------------------------------------------===//. For this:. int test(int a); {; return a * 3;; }. We currently emits; 	imull $3, 4(%esp), %eax. Perhaps this is what we really should generate is? Is imull three or four; cycles? Note: ICC generates this:; 	movl	4(%esp), %eax; 	leal	(%eax,%eax,2), %eax. The current instruction priority is based on pattern complexity. The former is; more ""complex"" because it folds a load so the latter will not be emitted. Perhaps we should use AddedComplexity to give LEA32r a higher priority? We; should always try to match LEA first since the LEA matching code does some; estimate to determine whether the match is profitable. However, if we care more about code size, then imull is better. It's two bytes; shorter than movl + leal. On a Pentium M, both variants have the same characteristics with regard; to throughput; however, the multiplication has a latency of four cycles, as; opposed to two cycles for the movl+lea variant. //===---------------------------------------------------------------------===//. It appears gcc place string data with linkonce linkage in; .section __TEXT,__const_coal,coalesced instead of; .section __DATA,__const_coal,coalesced.; Take a look at darwin.h, there are other Darwin assembler directives that we; do not make use of. //===---------------------------------------------------------------------===//. define i32 @foo(i32* %a, i32 %t) {; entry:; 	br label %cond_true. cond_true:		; preds = %cond_true, %entry; 	%x.0.0 = phi i32 [ 0, %entry ], [ %tmp9, %cond_true ]		; <i32> [#uses=3]; 	%t_addr.0.0 = phi i32 [ %t, %entry ], [ %tmp7, %cond_true ]		; <i32> [#uses=1]; 	%tmp2 = getelementptr i32* %a, i32 %x.0.0		; <i32*> [#uses=1]; 	%tmp3 = load i32* %tmp2		; <i32> [#uses=1]; 	%tmp5 = add i32 %t_addr.0.0, %x.0.0		; <i32> [#uses=1]; 	%tmp7 = add i32 %tmp5, %tmp3		; <i32> [#uses=2]; 	%tmp9 = ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:8412,Performance,load,load,8412,"'s two bytes; shorter than movl + leal. On a Pentium M, both variants have the same characteristics with regard; to throughput; however, the multiplication has a latency of four cycles, as; opposed to two cycles for the movl+lea variant. //===---------------------------------------------------------------------===//. It appears gcc place string data with linkonce linkage in; .section __TEXT,__const_coal,coalesced instead of; .section __DATA,__const_coal,coalesced.; Take a look at darwin.h, there are other Darwin assembler directives that we; do not make use of. //===---------------------------------------------------------------------===//. define i32 @foo(i32* %a, i32 %t) {; entry:; 	br label %cond_true. cond_true:		; preds = %cond_true, %entry; 	%x.0.0 = phi i32 [ 0, %entry ], [ %tmp9, %cond_true ]		; <i32> [#uses=3]; 	%t_addr.0.0 = phi i32 [ %t, %entry ], [ %tmp7, %cond_true ]		; <i32> [#uses=1]; 	%tmp2 = getelementptr i32* %a, i32 %x.0.0		; <i32*> [#uses=1]; 	%tmp3 = load i32* %tmp2		; <i32> [#uses=1]; 	%tmp5 = add i32 %t_addr.0.0, %x.0.0		; <i32> [#uses=1]; 	%tmp7 = add i32 %tmp5, %tmp3		; <i32> [#uses=2]; 	%tmp9 = add i32 %x.0.0, 1		; <i32> [#uses=2]; 	%tmp = icmp sgt i32 %tmp9, 39		; <i1> [#uses=1]; 	br i1 %tmp, label %bb12, label %cond_true. bb12:		; preds = %cond_true; 	ret i32 %tmp7; }; is pessimized by -loop-reduce and -indvars. //===---------------------------------------------------------------------===//. u32 to float conversion improvement:. float uint32_2_float( unsigned u ) {; float fl = (int) (u & 0xffff);; float fh = (int) (u >> 16);; fh *= 0x1.0p16f;; return fh + fl;; }. 00000000 subl $0x04,%esp; 00000003 movl 0x08(%esp,1),%eax; 00000007 movl %eax,%ecx; 00000009 shrl $0x10,%ecx; 0000000c cvtsi2ss %ecx,%xmm0; 00000010 andl $0x0000ffff,%eax; 00000015 cvtsi2ss %eax,%xmm1; 00000019 mulss 0x00000078,%xmm0; 00000021 addss %xmm1,%xmm0; 00000025 movss %xmm0,(%esp,1); 0000002a flds (%esp,1); 0000002d addl $0x04,%esp; 00000030 ret. //===--------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:9581,Performance,perform,performance,9581," <i32> [#uses=2]; 	%tmp9 = add i32 %x.0.0, 1		; <i32> [#uses=2]; 	%tmp = icmp sgt i32 %tmp9, 39		; <i1> [#uses=1]; 	br i1 %tmp, label %bb12, label %cond_true. bb12:		; preds = %cond_true; 	ret i32 %tmp7; }; is pessimized by -loop-reduce and -indvars. //===---------------------------------------------------------------------===//. u32 to float conversion improvement:. float uint32_2_float( unsigned u ) {; float fl = (int) (u & 0xffff);; float fh = (int) (u >> 16);; fh *= 0x1.0p16f;; return fh + fl;; }. 00000000 subl $0x04,%esp; 00000003 movl 0x08(%esp,1),%eax; 00000007 movl %eax,%ecx; 00000009 shrl $0x10,%ecx; 0000000c cvtsi2ss %ecx,%xmm0; 00000010 andl $0x0000ffff,%eax; 00000015 cvtsi2ss %eax,%xmm1; 00000019 mulss 0x00000078,%xmm0; 00000021 addss %xmm1,%xmm0; 00000025 movss %xmm0,(%esp,1); 0000002a flds (%esp,1); 0000002d addl $0x04,%esp; 00000030 ret. //===---------------------------------------------------------------------===//. When using fastcc abi, align stack slot of argument of type double on 8 byte; boundary to improve performance. //===---------------------------------------------------------------------===//. GCC's ix86_expand_int_movcc function (in i386.c) has a ton of interesting; simplifications for integer ""x cmp y ? a : b"". //===---------------------------------------------------------------------===//. Consider the expansion of:. define i32 @test3(i32 %X) {; %tmp1 = urem i32 %X, 255; ret i32 %tmp1; }. Currently it compiles to:. ...; movl $2155905153, %ecx; movl 8(%esp), %esi; movl %esi, %eax; mull %ecx; ... This could be ""reassociated"" into:. movl $2155905153, %eax; movl 8(%esp), %ecx; mull %ecx. to avoid the copy. In fact, the existing two-address stuff would do this; except that mul isn't a commutative 2-addr instruction. I guess this has; to be done at isel time based on the #uses to mul?. //===---------------------------------------------------------------------===//. Make sure the instruction which starts a loop does not cross a cacheline; bound",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:10522,Performance,cache,cacheline,10522," abi, align stack slot of argument of type double on 8 byte; boundary to improve performance. //===---------------------------------------------------------------------===//. GCC's ix86_expand_int_movcc function (in i386.c) has a ton of interesting; simplifications for integer ""x cmp y ? a : b"". //===---------------------------------------------------------------------===//. Consider the expansion of:. define i32 @test3(i32 %X) {; %tmp1 = urem i32 %X, 255; ret i32 %tmp1; }. Currently it compiles to:. ...; movl $2155905153, %ecx; movl 8(%esp), %esi; movl %esi, %eax; mull %ecx; ... This could be ""reassociated"" into:. movl $2155905153, %eax; movl 8(%esp), %ecx; mull %ecx. to avoid the copy. In fact, the existing two-address stuff would do this; except that mul isn't a commutative 2-addr instruction. I guess this has; to be done at isel time based on the #uses to mul?. //===---------------------------------------------------------------------===//. Make sure the instruction which starts a loop does not cross a cacheline; boundary. This requires knowning the exact length of each machine instruction.; That is somewhat complicated, but doable. Example 256.bzip2:. In the new trace, the hot loop has an instruction which crosses a cacheline; boundary. In addition to potential cache misses, this can't help decoding as I; imagine there has to be some kind of complicated decoder reset and realignment; to grab the bytes from the next cacheline. 532 532 0x3cfc movb (1809(%esp, %esi), %bl <<<--- spans 2 64 byte lines; 942 942 0x3d03 movl %dh, (1809(%esp, %esi); 937 937 0x3d0a incl %esi; 3 3 0x3d0b cmpb %bl, %dl; 27 27 0x3d0d jnz 0x000062db <main+11707>. //===---------------------------------------------------------------------===//. In c99 mode, the preprocessor doesn't like assembly comments like #TRUNCATE. //===---------------------------------------------------------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:10741,Performance,cache,cacheline,10741,"386.c) has a ton of interesting; simplifications for integer ""x cmp y ? a : b"". //===---------------------------------------------------------------------===//. Consider the expansion of:. define i32 @test3(i32 %X) {; %tmp1 = urem i32 %X, 255; ret i32 %tmp1; }. Currently it compiles to:. ...; movl $2155905153, %ecx; movl 8(%esp), %esi; movl %esi, %eax; mull %ecx; ... This could be ""reassociated"" into:. movl $2155905153, %eax; movl 8(%esp), %ecx; mull %ecx. to avoid the copy. In fact, the existing two-address stuff would do this; except that mul isn't a commutative 2-addr instruction. I guess this has; to be done at isel time based on the #uses to mul?. //===---------------------------------------------------------------------===//. Make sure the instruction which starts a loop does not cross a cacheline; boundary. This requires knowning the exact length of each machine instruction.; That is somewhat complicated, but doable. Example 256.bzip2:. In the new trace, the hot loop has an instruction which crosses a cacheline; boundary. In addition to potential cache misses, this can't help decoding as I; imagine there has to be some kind of complicated decoder reset and realignment; to grab the bytes from the next cacheline. 532 532 0x3cfc movb (1809(%esp, %esi), %bl <<<--- spans 2 64 byte lines; 942 942 0x3d03 movl %dh, (1809(%esp, %esi); 937 937 0x3d0a incl %esi; 3 3 0x3d0b cmpb %bl, %dl; 27 27 0x3d0d jnz 0x000062db <main+11707>. //===---------------------------------------------------------------------===//. In c99 mode, the preprocessor doesn't like assembly comments like #TRUNCATE. //===---------------------------------------------------------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===----------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:10787,Performance,cache,cache,10787,"--------------===//. Consider the expansion of:. define i32 @test3(i32 %X) {; %tmp1 = urem i32 %X, 255; ret i32 %tmp1; }. Currently it compiles to:. ...; movl $2155905153, %ecx; movl 8(%esp), %esi; movl %esi, %eax; mull %ecx; ... This could be ""reassociated"" into:. movl $2155905153, %eax; movl 8(%esp), %ecx; mull %ecx. to avoid the copy. In fact, the existing two-address stuff would do this; except that mul isn't a commutative 2-addr instruction. I guess this has; to be done at isel time based on the #uses to mul?. //===---------------------------------------------------------------------===//. Make sure the instruction which starts a loop does not cross a cacheline; boundary. This requires knowning the exact length of each machine instruction.; That is somewhat complicated, but doable. Example 256.bzip2:. In the new trace, the hot loop has an instruction which crosses a cacheline; boundary. In addition to potential cache misses, this can't help decoding as I; imagine there has to be some kind of complicated decoder reset and realignment; to grab the bytes from the next cacheline. 532 532 0x3cfc movb (1809(%esp, %esi), %bl <<<--- spans 2 64 byte lines; 942 942 0x3d03 movl %dh, (1809(%esp, %esi); 937 937 0x3d0a incl %esi; 3 3 0x3d0b cmpb %bl, %dl; 27 27 0x3d0d jnz 0x000062db <main+11707>. //===---------------------------------------------------------------------===//. In c99 mode, the preprocessor doesn't like assembly comments like #TRUNCATE. //===---------------------------------------------------------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===---------------------------------------------------------------------===//. This code:. void test(int X) {; if (X) abort();; }. is currently compiled to:. _test:; subl $12, %esp; cmpl $0, 16(%esp); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:10944,Performance,cache,cacheline,10944,"--------------===//. Consider the expansion of:. define i32 @test3(i32 %X) {; %tmp1 = urem i32 %X, 255; ret i32 %tmp1; }. Currently it compiles to:. ...; movl $2155905153, %ecx; movl 8(%esp), %esi; movl %esi, %eax; mull %ecx; ... This could be ""reassociated"" into:. movl $2155905153, %eax; movl 8(%esp), %ecx; mull %ecx. to avoid the copy. In fact, the existing two-address stuff would do this; except that mul isn't a commutative 2-addr instruction. I guess this has; to be done at isel time based on the #uses to mul?. //===---------------------------------------------------------------------===//. Make sure the instruction which starts a loop does not cross a cacheline; boundary. This requires knowning the exact length of each machine instruction.; That is somewhat complicated, but doable. Example 256.bzip2:. In the new trace, the hot loop has an instruction which crosses a cacheline; boundary. In addition to potential cache misses, this can't help decoding as I; imagine there has to be some kind of complicated decoder reset and realignment; to grab the bytes from the next cacheline. 532 532 0x3cfc movb (1809(%esp, %esi), %bl <<<--- spans 2 64 byte lines; 942 942 0x3d03 movl %dh, (1809(%esp, %esi); 937 937 0x3d0a incl %esi; 3 3 0x3d0b cmpb %bl, %dl; 27 27 0x3d0d jnz 0x000062db <main+11707>. //===---------------------------------------------------------------------===//. In c99 mode, the preprocessor doesn't like assembly comments like #TRUNCATE. //===---------------------------------------------------------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===---------------------------------------------------------------------===//. This code:. void test(int X) {; if (X) abort();; }. is currently compiled to:. _test:; subl $12, %esp; cmpl $0, 16(%esp); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:11435,Performance,load,load,11435,"------------------------------===//. Make sure the instruction which starts a loop does not cross a cacheline; boundary. This requires knowning the exact length of each machine instruction.; That is somewhat complicated, but doable. Example 256.bzip2:. In the new trace, the hot loop has an instruction which crosses a cacheline; boundary. In addition to potential cache misses, this can't help decoding as I; imagine there has to be some kind of complicated decoder reset and realignment; to grab the bytes from the next cacheline. 532 532 0x3cfc movb (1809(%esp, %esi), %bl <<<--- spans 2 64 byte lines; 942 942 0x3d03 movl %dh, (1809(%esp, %esi); 937 937 0x3d0a incl %esi; 3 3 0x3d0b cmpb %bl, %dl; 27 27 0x3d0d jnz 0x000062db <main+11707>. //===---------------------------------------------------------------------===//. In c99 mode, the preprocessor doesn't like assembly comments like #TRUNCATE. //===---------------------------------------------------------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===---------------------------------------------------------------------===//. This code:. void test(int X) {; if (X) abort();; }. is currently compiled to:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne LBB1_1; addl $12, %esp; ret; LBB1_1:; call L_abort$stub. It would be better to produce:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne L_abort$stub; addl $12, %esp; ret. This can be applied to any no-return function call that takes no arguments etc.; Alternatively, the stack save/restore logic could be shrink-wrapped, producing; something like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; subl $12, %esp; call L_abort$stub. Both are useful in different situations. Finally, it could be shrink-wrapped; and tail called, like this:. _test:; cmpl",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:13018,Performance,load,load,13018,"dl $12, %esp; ret. This can be applied to any no-return function call that takes no arguments etc.; Alternatively, the stack save/restore logic could be shrink-wrapped, producing; something like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; subl $12, %esp; call L_abort$stub. Both are useful in different situations. Finally, it could be shrink-wrapped; and tail called, like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; pop %eax # realign stack.; call L_abort$stub. Though this probably isn't worth it. //===---------------------------------------------------------------------===//. Sometimes it is better to codegen subtractions from a constant (e.g. 7-x) with; a neg instead of a sub instruction. Consider:. int test(char X) { return 7-X; }. we currently produce:; _test:; movl $7, %eax; movsbl 4(%esp), %ecx; subl %ecx, %eax; ret. We would use one fewer register if codegen'd as:. movsbl 4(%esp), %eax; 	neg %eax; add $7, %eax; ret. Note that this isn't beneficial if the load can be folded into the sub. In; this case, we want a sub:. int test(int X) { return 7-X; }; _test:; movl $7, %eax; subl 4(%esp), %eax; ret. //===---------------------------------------------------------------------===//. Leaf functions that require one 4-byte spill slot have a prolog like this:. _foo:; pushl %esi; subl $4, %esp; ...; and an epilog like this:; addl $4, %esp; popl %esi; ret. It would be smaller, and potentially faster, to push eax on entry and to; pop into a dummy register instead of using addl/subl of esp. Just don't pop ; into any return registers :). //===---------------------------------------------------------------------===//. The X86 backend should fold (branch (or (setcc, setcc))) into multiple ; branches. We generate really poor code for:. double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. For example, the entry BB is:. _testf:; subl $20, %esp; pxor %xmm0, %xmm0; movsd 24(%esp), %xmm1; ucomisd %xmm0, %xmm1; setnp %al; sete %cl",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:14444,Performance,load,load,14444," faster, to push eax on entry and to; pop into a dummy register instead of using addl/subl of esp. Just don't pop ; into any return registers :). //===---------------------------------------------------------------------===//. The X86 backend should fold (branch (or (setcc, setcc))) into multiple ; branches. We generate really poor code for:. double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. For example, the entry BB is:. _testf:; subl $20, %esp; pxor %xmm0, %xmm0; movsd 24(%esp), %xmm1; ucomisd %xmm0, %xmm1; setnp %al; sete %cl; testb %cl, %al; jne LBB1_5 # UnifiedReturnBlock; LBB1_1: # cond_true. it would be better to replace the last four instructions with:. 	jp LBB1_1; 	je LBB1_5; LBB1_1:. We also codegen the inner ?: into a diamond:. cvtss2sd LCPI1_0(%rip), %xmm2; cvtss2sd LCPI1_1(%rip), %xmm3; ucomisd %xmm1, %xmm0; ja LBB1_3 # cond_true; LBB1_2: # cond_true; movapd %xmm3, %xmm2; LBB1_3: # cond_true; movapd %xmm2, %xmm0; ret. We should sink the load into xmm3 into the LBB1_2 block. This should; be pretty easy, and will nuke all the copies. //===---------------------------------------------------------------------===//. This:; #include <algorithm>; inline std::pair<unsigned, bool> full_add(unsigned a, unsigned b); { return std::make_pair(a + b, a + b < a); }; bool no_overflow(unsigned a, unsigned b); { return !full_add(a, b).second; }. Should compile to:; 	addl	%esi, %edi; 	setae	%al; 	movzbl	%al, %eax; 	ret. on x86-64, instead of the rather stupid-looking:; 	addl	%esi, %edi; 	setb	%al; 	xorb	$1, %al; 	movzbl	%al, %eax; 	ret. //===---------------------------------------------------------------------===//. The following code:. bb114.preheader:		; preds = %cond_next94; 	%tmp231232 = sext i16 %tmp62 to i32		; <i32> [#uses=1]; 	%tmp233 = sub i32 32, %tmp231232		; <i32> [#uses=1]; 	%tmp245246 = sext i16 %tmp65 to i32		; <i32> [#uses=1]; 	%tmp252253 = sext i16 %tmp68 to i32		; <i32> [#uses=1]; 	%tmp254 = sub i32 32, %tmp252253		; <i32> [#use",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:17998,Performance,optimiz,optimization,17998,"------------------------------------------------===//. We compile this:. void compare (long long foo) {; if (foo < 4294967297LL); abort();; }. to:. compare:; subl $4, %esp; cmpl $0, 8(%esp); setne %al; movzbw %al, %ax; cmpl $1, 12(%esp); setg %cl; movzbw %cl, %cx; cmove %ax, %cx; testb $1, %cl; jne .LBB1_2 # UnifiedReturnBlock; .LBB1_1: # ifthen; call abort; .LBB1_2: # UnifiedReturnBlock; addl $4, %esp; ret. (also really horrible code on ppc). This is due to the expand code for 64-bit; compares. GCC produces multiple branches, which is much nicer:. compare:; subl $12, %esp; movl 20(%esp), %edx; movl 16(%esp), %eax; decl %edx; jle .L7; .L5:; addl $12, %esp; ret; .p2align 4,,7; .L7:; jl .L4; cmpl $0, %eax; .p2align 4,,8; ja .L5; .L4:; .p2align 4,,9; call abort. //===---------------------------------------------------------------------===//. Tail call optimization improvements: Tail call optimization currently; pushes all arguments on the top of the stack (their normal place for; non-tail call optimized calls) that source from the callers arguments; or that source from a virtual register (also possibly sourcing from; callers arguments).; This is done to prevent overwriting of parameters (see example; below) that might be used later. example: . int callee(int32, int64); ; int caller(int32 arg1, int32 arg2) { ; int64 local = arg2 * 2; ; return callee(arg2, (int64)local); ; }. [arg1] [!arg2 no longer valid since we moved local onto it]; [arg2] -> [(int64); [RETADDR] local ]. Moving arg1 onto the stack slot of callee function would overwrite; arg2 of the caller. Possible optimizations:. - Analyse the actual parameters of the callee to see which would; overwrite a caller parameter which is used by the callee and only; push them onto the top of the stack. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg1,arg2);; }. Here we don't need to write any variables to the top of the stack; since they don't overwrite each other. int callee ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:18035,Performance,optimiz,optimization,18035,"------------------------------------------------===//. We compile this:. void compare (long long foo) {; if (foo < 4294967297LL); abort();; }. to:. compare:; subl $4, %esp; cmpl $0, 8(%esp); setne %al; movzbw %al, %ax; cmpl $1, 12(%esp); setg %cl; movzbw %cl, %cx; cmove %ax, %cx; testb $1, %cl; jne .LBB1_2 # UnifiedReturnBlock; .LBB1_1: # ifthen; call abort; .LBB1_2: # UnifiedReturnBlock; addl $4, %esp; ret. (also really horrible code on ppc). This is due to the expand code for 64-bit; compares. GCC produces multiple branches, which is much nicer:. compare:; subl $12, %esp; movl 20(%esp), %edx; movl 16(%esp), %eax; decl %edx; jle .L7; .L5:; addl $12, %esp; ret; .p2align 4,,7; .L7:; jl .L4; cmpl $0, %eax; .p2align 4,,8; ja .L5; .L4:; .p2align 4,,9; call abort. //===---------------------------------------------------------------------===//. Tail call optimization improvements: Tail call optimization currently; pushes all arguments on the top of the stack (their normal place for; non-tail call optimized calls) that source from the callers arguments; or that source from a virtual register (also possibly sourcing from; callers arguments).; This is done to prevent overwriting of parameters (see example; below) that might be used later. example: . int callee(int32, int64); ; int caller(int32 arg1, int32 arg2) { ; int64 local = arg2 * 2; ; return callee(arg2, (int64)local); ; }. [arg1] [!arg2 no longer valid since we moved local onto it]; [arg2] -> [(int64); [RETADDR] local ]. Moving arg1 onto the stack slot of callee function would overwrite; arg2 of the caller. Possible optimizations:. - Analyse the actual parameters of the callee to see which would; overwrite a caller parameter which is used by the callee and only; push them onto the top of the stack. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg1,arg2);; }. Here we don't need to write any variables to the top of the stack; since they don't overwrite each other. int callee ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:18143,Performance,optimiz,optimized,18143,"------------------------------------------------===//. We compile this:. void compare (long long foo) {; if (foo < 4294967297LL); abort();; }. to:. compare:; subl $4, %esp; cmpl $0, 8(%esp); setne %al; movzbw %al, %ax; cmpl $1, 12(%esp); setg %cl; movzbw %cl, %cx; cmove %ax, %cx; testb $1, %cl; jne .LBB1_2 # UnifiedReturnBlock; .LBB1_1: # ifthen; call abort; .LBB1_2: # UnifiedReturnBlock; addl $4, %esp; ret. (also really horrible code on ppc). This is due to the expand code for 64-bit; compares. GCC produces multiple branches, which is much nicer:. compare:; subl $12, %esp; movl 20(%esp), %edx; movl 16(%esp), %eax; decl %edx; jle .L7; .L5:; addl $12, %esp; ret; .p2align 4,,7; .L7:; jl .L4; cmpl $0, %eax; .p2align 4,,8; ja .L5; .L4:; .p2align 4,,9; call abort. //===---------------------------------------------------------------------===//. Tail call optimization improvements: Tail call optimization currently; pushes all arguments on the top of the stack (their normal place for; non-tail call optimized calls) that source from the callers arguments; or that source from a virtual register (also possibly sourcing from; callers arguments).; This is done to prevent overwriting of parameters (see example; below) that might be used later. example: . int callee(int32, int64); ; int caller(int32 arg1, int32 arg2) { ; int64 local = arg2 * 2; ; return callee(arg2, (int64)local); ; }. [arg1] [!arg2 no longer valid since we moved local onto it]; [arg2] -> [(int64); [RETADDR] local ]. Moving arg1 onto the stack slot of callee function would overwrite; arg2 of the caller. Possible optimizations:. - Analyse the actual parameters of the callee to see which would; overwrite a caller parameter which is used by the callee and only; push them onto the top of the stack. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg1,arg2);; }. Here we don't need to write any variables to the top of the stack; since they don't overwrite each other. int callee ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:18728,Performance,optimiz,optimizations,18728,"), %edx; movl 16(%esp), %eax; decl %edx; jle .L7; .L5:; addl $12, %esp; ret; .p2align 4,,7; .L7:; jl .L4; cmpl $0, %eax; .p2align 4,,8; ja .L5; .L4:; .p2align 4,,9; call abort. //===---------------------------------------------------------------------===//. Tail call optimization improvements: Tail call optimization currently; pushes all arguments on the top of the stack (their normal place for; non-tail call optimized calls) that source from the callers arguments; or that source from a virtual register (also possibly sourcing from; callers arguments).; This is done to prevent overwriting of parameters (see example; below) that might be used later. example: . int callee(int32, int64); ; int caller(int32 arg1, int32 arg2) { ; int64 local = arg2 * 2; ; return callee(arg2, (int64)local); ; }. [arg1] [!arg2 no longer valid since we moved local onto it]; [arg2] -> [(int64); [RETADDR] local ]. Moving arg1 onto the stack slot of callee function would overwrite; arg2 of the caller. Possible optimizations:. - Analyse the actual parameters of the callee to see which would; overwrite a caller parameter which is used by the callee and only; push them onto the top of the stack. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg1,arg2);; }. Here we don't need to write any variables to the top of the stack; since they don't overwrite each other. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg2,arg1);; }. Here we need to push the arguments because they overwrite each; other. //===---------------------------------------------------------------------===//. main (); {; int i = 0;; unsigned long int z = 0;. do {; z -= 0x00004000;; i++;; if (i > 0x00040000); abort ();; } while (z > 0);; exit (0);; }. gcc compiles this to:. _main:; 	subl	$28, %esp; 	xorl	%eax, %eax; 	jmp	L2; L3:; 	cmpl	$262144, %eax; 	je	L10; L2:; 	addl	$1, %eax; 	cmpl	$262145, %eax; 	jne	L3; 	call	L_abort$stub; L10:; 	movl	$0, (%es",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:21499,Performance,load,load,21499," 	fldl	8(%esp); 	fisttpll	(%esp); 	movl	4(%esp), %edx; 	movl	(%esp), %eax; 	addl	$20, %esp; 	#FP_REG_KILL; 	ret. This should just fldl directly from the input stack slot. //===---------------------------------------------------------------------===//. This code:; int foo (int x) { return (x & 65535) | 255; }. Should compile into:. _foo:; movzwl 4(%esp), %eax; orl $255, %eax; ret. instead of:; _foo:; 	movl	$65280, %eax; 	andl	4(%esp), %eax; 	orl	$255, %eax; 	ret. //===---------------------------------------------------------------------===//. We're codegen'ing multiply of long longs inefficiently:. unsigned long long LLM(unsigned long long arg1, unsigned long long arg2) {; return arg1 * arg2;; }. We compile to (fomit-frame-pointer):. _LLM:; 	pushl	%esi; 	movl	8(%esp), %ecx; 	movl	16(%esp), %esi; 	movl	%esi, %eax; 	mull	%ecx; 	imull	12(%esp), %esi; 	addl	%edx, %esi; 	imull	20(%esp), %ecx; 	movl	%esi, %edx; 	addl	%ecx, %edx; 	popl	%esi; 	ret. This looks like a scheduling deficiency and lack of remat of the load from; the argument area. ICC apparently produces:. movl 8(%esp), %ecx; imull 12(%esp), %ecx; movl 16(%esp), %eax; imull 4(%esp), %eax ; addl %eax, %ecx ; movl 4(%esp), %eax; mull 12(%esp) ; addl %ecx, %edx; ret. Note that it remat'd loads from 4(esp) and 12(esp). See this GCC PR:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=17236. //===---------------------------------------------------------------------===//. We can fold a store into ""zeroing a reg"". Instead of:. xorl %eax, %eax; movl %eax, 124(%esp). we should get:. movl $0, 124(%esp). if the flags of the xor are dead. Likewise, we isel ""x<<1"" into ""add reg,reg"". If reg is spilled, this should; be folded into: shl [mem], 1. //===---------------------------------------------------------------------===//. In SSE mode, we turn abs and neg into a load from the constant pool plus a xor; or and instruction, for example:. 	xorpd	LCPI1_0, %xmm2. However, if xmm2 gets spilled, we end up with really ugly code like this:.",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:21737,Performance,load,loads,21737,":; int foo (int x) { return (x & 65535) | 255; }. Should compile into:. _foo:; movzwl 4(%esp), %eax; orl $255, %eax; ret. instead of:; _foo:; 	movl	$65280, %eax; 	andl	4(%esp), %eax; 	orl	$255, %eax; 	ret. //===---------------------------------------------------------------------===//. We're codegen'ing multiply of long longs inefficiently:. unsigned long long LLM(unsigned long long arg1, unsigned long long arg2) {; return arg1 * arg2;; }. We compile to (fomit-frame-pointer):. _LLM:; 	pushl	%esi; 	movl	8(%esp), %ecx; 	movl	16(%esp), %esi; 	movl	%esi, %eax; 	mull	%ecx; 	imull	12(%esp), %esi; 	addl	%edx, %esi; 	imull	20(%esp), %ecx; 	movl	%esi, %edx; 	addl	%ecx, %edx; 	popl	%esi; 	ret. This looks like a scheduling deficiency and lack of remat of the load from; the argument area. ICC apparently produces:. movl 8(%esp), %ecx; imull 12(%esp), %ecx; movl 16(%esp), %eax; imull 4(%esp), %eax ; addl %eax, %ecx ; movl 4(%esp), %eax; mull 12(%esp) ; addl %ecx, %edx; ret. Note that it remat'd loads from 4(esp) and 12(esp). See this GCC PR:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=17236. //===---------------------------------------------------------------------===//. We can fold a store into ""zeroing a reg"". Instead of:. xorl %eax, %eax; movl %eax, 124(%esp). we should get:. movl $0, 124(%esp). if the flags of the xor are dead. Likewise, we isel ""x<<1"" into ""add reg,reg"". If reg is spilled, this should; be folded into: shl [mem], 1. //===---------------------------------------------------------------------===//. In SSE mode, we turn abs and neg into a load from the constant pool plus a xor; or and instruction, for example:. 	xorpd	LCPI1_0, %xmm2. However, if xmm2 gets spilled, we end up with really ugly code like this:. 	movsd	(%esp), %xmm0; 	xorpd	LCPI1_0, %xmm0; 	movsd	%xmm0, (%esp). Since we 'know' that this is a 'neg', we can actually ""fold"" the spill into; the neg/abs instruction, turning it into an *integer* operation, like this:. 	xorl 2147483648, [mem+4] ## 214748364",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:22310,Performance,load,load,22310,"%esp), %esi; 	addl	%edx, %esi; 	imull	20(%esp), %ecx; 	movl	%esi, %edx; 	addl	%ecx, %edx; 	popl	%esi; 	ret. This looks like a scheduling deficiency and lack of remat of the load from; the argument area. ICC apparently produces:. movl 8(%esp), %ecx; imull 12(%esp), %ecx; movl 16(%esp), %eax; imull 4(%esp), %eax ; addl %eax, %ecx ; movl 4(%esp), %eax; mull 12(%esp) ; addl %ecx, %edx; ret. Note that it remat'd loads from 4(esp) and 12(esp). See this GCC PR:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=17236. //===---------------------------------------------------------------------===//. We can fold a store into ""zeroing a reg"". Instead of:. xorl %eax, %eax; movl %eax, 124(%esp). we should get:. movl $0, 124(%esp). if the flags of the xor are dead. Likewise, we isel ""x<<1"" into ""add reg,reg"". If reg is spilled, this should; be folded into: shl [mem], 1. //===---------------------------------------------------------------------===//. In SSE mode, we turn abs and neg into a load from the constant pool plus a xor; or and instruction, for example:. 	xorpd	LCPI1_0, %xmm2. However, if xmm2 gets spilled, we end up with really ugly code like this:. 	movsd	(%esp), %xmm0; 	xorpd	LCPI1_0, %xmm0; 	movsd	%xmm0, (%esp). Since we 'know' that this is a 'neg', we can actually ""fold"" the spill into; the neg/abs instruction, turning it into an *integer* operation, like this:. 	xorl 2147483648, [mem+4] ## 2147483648 = (1 << 31). you could also use xorb, but xorl is less likely to lead to a partial register; stall. Here is a contrived testcase:. double a, b, c;; void test(double *P) {; double X = *P;; a = X;; bar();; X = -X;; b = X;; bar();; c = X;; }. //===---------------------------------------------------------------------===//. The generated code on x86 for checking for signed overflow on a multiply the; obvious way is much longer than it needs to be. int x(int a, int b) {; long long prod = (long long)a*b;; return prod > 0x7FFFFFFF || prod < (-0x7FFFFFFF-1);; }. See PR2053 for more det",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:25861,Performance,load,load,25861,"label %bb7, label %bb. bb:		; preds = %entry; 	%tmp6 = add i32 %b, %a		; <i32> [#uses=1]; 	ret i32 %tmp6. bb7:		; preds = %entry; 	%tmp10 = sub i32 %a, %c		; <i32> [#uses=1]; 	ret i32 %tmp10; }. to:. foo: # @foo; # %bb.0: # %entry; 	movl	4(%esp), %ecx; 	cmpb	$0, 16(%esp); 	je	.LBB0_2; # %bb.1: # %bb; 	movl	8(%esp), %eax; 	addl	%ecx, %eax; 	ret; .LBB0_2: # %bb7; 	movl	12(%esp), %edx; 	movl	%ecx, %eax; 	subl	%edx, %eax; 	ret. There's an obviously unnecessary movl in .LBB0_2, and we could eliminate a; couple more movls by putting 4(%esp) into %eax instead of %ecx. //===---------------------------------------------------------------------===//. Take the following:. target datalayout = ""e-p:32:32:32-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:32:64-f32:32:32-f64:32:64-v64:64:64-v128:128:128-a0:0:64-f80:128:128-S128""; target triple = ""i386-apple-darwin8""; @in_exit.4870.b = internal global i1 false		; <i1*> [#uses=2]; define fastcc void @abort_gzip() noreturn nounwind {; entry:; 	%tmp.b.i = load i1* @in_exit.4870.b		; <i1> [#uses=1]; 	br i1 %tmp.b.i, label %bb.i, label %bb4.i; bb.i:		; preds = %entry; 	tail call void @exit( i32 1 ) noreturn nounwind ; 	unreachable; bb4.i:		; preds = %entry; 	store i1 true, i1* @in_exit.4870.b; 	tail call void @exit( i32 1 ) noreturn nounwind ; 	unreachable; }; declare void @exit(i32) noreturn nounwind . This compiles into:; _abort_gzip: ## @abort_gzip; ## %bb.0: ## %entry; 	subl	$12, %esp; 	movb	_in_exit.4870.b, %al; 	cmpb	$1, %al; 	jne	LBB0_2. We somehow miss folding the movb into the cmpb. //===---------------------------------------------------------------------===//. We compile:. int test(int x, int y) {; return x-y-1;; }. into (-m64):. _test:; 	decl	%edi; 	movl	%edi, %eax; 	subl	%esi, %eax; 	ret. it would be better to codegen as: x+~y (notl+addl). //===---------------------------------------------------------------------===//. This code:. int foo(const char *str,...); {; __builtin_va_list a; int x;; __builtin_va_start(a,str); x = __builtin_",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:31992,Performance,load,load,31992,"modify-write instructions, the root node in the isel match is; the store, and isel has no way for the use of the EFLAGS result of the; arithmetic to be remapped to the new node. Add and subtract instructions set OF on signed overflow and CF on unsiged; overflow, while test instructions always clear OF and CF. In order to; replace a test with an add or subtract in a situation where OF or CF is; needed, codegen must be able to prove that the operation cannot see; signed or unsigned overflow, respectively. //===---------------------------------------------------------------------===//. memcpy/memmove do not lower to SSE copies when possible. A silly example is:; define <16 x float> @foo(<16 x float> %A) nounwind {; 	%tmp = alloca <16 x float>, align 16; 	%tmp2 = alloca <16 x float>, align 16; 	store <16 x float> %A, <16 x float>* %tmp; 	%s = bitcast <16 x float>* %tmp to i8*; 	%s2 = bitcast <16 x float>* %tmp2 to i8*; 	call void @llvm.memcpy.i64(i8* %s, i8* %s2, i64 64, i32 16); 	%R = load <16 x float>* %tmp2; 	ret <16 x float> %R; }. declare void @llvm.memcpy.i64(i8* nocapture, i8* nocapture, i64, i32) nounwind. which compiles to:. _foo:; 	subl	$140, %esp; 	movaps	%xmm3, 112(%esp); 	movaps	%xmm2, 96(%esp); 	movaps	%xmm1, 80(%esp); 	movaps	%xmm0, 64(%esp); 	movl	60(%esp), %eax; 	movl	%eax, 124(%esp); 	movl	56(%esp), %eax; 	movl	%eax, 120(%esp); 	movl	52(%esp), %eax; <many many more 32-bit copies>; 	movaps	(%esp), %xmm0; 	movaps	16(%esp), %xmm1; 	movaps	32(%esp), %xmm2; 	movaps	48(%esp), %xmm3; 	addl	$140, %esp; 	ret. On Nehalem, it may even be cheaper to just use movups when unaligned than to; fall back to lower-granularity chunks. //===---------------------------------------------------------------------===//. Implement processor-specific optimizations for parity with GCC on these; processors. GCC does two optimizations:. 1. ix86_pad_returns inserts a noop before ret instructions if immediately; preceded by a conditional branch or is the target of a jump.; 2. ix86_avo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:32762,Performance,optimiz,optimizations,32762,"6 x float>, align 16; 	store <16 x float> %A, <16 x float>* %tmp; 	%s = bitcast <16 x float>* %tmp to i8*; 	%s2 = bitcast <16 x float>* %tmp2 to i8*; 	call void @llvm.memcpy.i64(i8* %s, i8* %s2, i64 64, i32 16); 	%R = load <16 x float>* %tmp2; 	ret <16 x float> %R; }. declare void @llvm.memcpy.i64(i8* nocapture, i8* nocapture, i64, i32) nounwind. which compiles to:. _foo:; 	subl	$140, %esp; 	movaps	%xmm3, 112(%esp); 	movaps	%xmm2, 96(%esp); 	movaps	%xmm1, 80(%esp); 	movaps	%xmm0, 64(%esp); 	movl	60(%esp), %eax; 	movl	%eax, 124(%esp); 	movl	56(%esp), %eax; 	movl	%eax, 120(%esp); 	movl	52(%esp), %eax; <many many more 32-bit copies>; 	movaps	(%esp), %xmm0; 	movaps	16(%esp), %xmm1; 	movaps	32(%esp), %xmm2; 	movaps	48(%esp), %xmm3; 	addl	$140, %esp; 	ret. On Nehalem, it may even be cheaper to just use movups when unaligned than to; fall back to lower-granularity chunks. //===---------------------------------------------------------------------===//. Implement processor-specific optimizations for parity with GCC on these; processors. GCC does two optimizations:. 1. ix86_pad_returns inserts a noop before ret instructions if immediately; preceded by a conditional branch or is the target of a jump.; 2. ix86_avoid_jump_misspredicts inserts noops in cases where a 16-byte block of; code contains more than 3 branches.; ; The first one is done for all AMDs, Core2, and ""Generic""; The second one is done for: Atom, Pentium Pro, all AMDs, Pentium 4, Nocona,; Core 2, and ""Generic"". //===---------------------------------------------------------------------===//; Testcase:; int x(int a) { return (a&0xf0)>>4; }. Current output:; 	movl	4(%esp), %eax; 	shrl	$4, %eax; 	andl	$15, %eax; 	ret. Ideal output:; 	movzbl	4(%esp), %eax; 	shrl	$4, %eax; 	ret. //===---------------------------------------------------------------------===//. Re-implement atomic builtins __sync_add_and_fetch() and __sync_sub_and_fetch; properly. When the return value is not used (i.e. only care about the value in the; mem",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:32831,Performance,optimiz,optimizations,32831,">* %tmp; 	%s = bitcast <16 x float>* %tmp to i8*; 	%s2 = bitcast <16 x float>* %tmp2 to i8*; 	call void @llvm.memcpy.i64(i8* %s, i8* %s2, i64 64, i32 16); 	%R = load <16 x float>* %tmp2; 	ret <16 x float> %R; }. declare void @llvm.memcpy.i64(i8* nocapture, i8* nocapture, i64, i32) nounwind. which compiles to:. _foo:; 	subl	$140, %esp; 	movaps	%xmm3, 112(%esp); 	movaps	%xmm2, 96(%esp); 	movaps	%xmm1, 80(%esp); 	movaps	%xmm0, 64(%esp); 	movl	60(%esp), %eax; 	movl	%eax, 124(%esp); 	movl	56(%esp), %eax; 	movl	%eax, 120(%esp); 	movl	52(%esp), %eax; <many many more 32-bit copies>; 	movaps	(%esp), %xmm0; 	movaps	16(%esp), %xmm1; 	movaps	32(%esp), %xmm2; 	movaps	48(%esp), %xmm3; 	addl	$140, %esp; 	ret. On Nehalem, it may even be cheaper to just use movups when unaligned than to; fall back to lower-granularity chunks. //===---------------------------------------------------------------------===//. Implement processor-specific optimizations for parity with GCC on these; processors. GCC does two optimizations:. 1. ix86_pad_returns inserts a noop before ret instructions if immediately; preceded by a conditional branch or is the target of a jump.; 2. ix86_avoid_jump_misspredicts inserts noops in cases where a 16-byte block of; code contains more than 3 branches.; ; The first one is done for all AMDs, Core2, and ""Generic""; The second one is done for: Atom, Pentium Pro, all AMDs, Pentium 4, Nocona,; Core 2, and ""Generic"". //===---------------------------------------------------------------------===//; Testcase:; int x(int a) { return (a&0xf0)>>4; }. Current output:; 	movl	4(%esp), %eax; 	shrl	$4, %eax; 	andl	$15, %eax; 	ret. Ideal output:; 	movzbl	4(%esp), %eax; 	shrl	$4, %eax; 	ret. //===---------------------------------------------------------------------===//. Re-implement atomic builtins __sync_add_and_fetch() and __sync_sub_and_fetch; properly. When the return value is not used (i.e. only care about the value in the; memory), x86 does not have to use add to implement these. In",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:35015,Performance,load,load,35015,"tput and a chain and we; want to map it into one that just output a chain. The current trick is to select; it into a MERGE_VALUES with the first definition being an implicit_def. The; proper solution is to add new ISD opcodes for the no-output variant. DAG; combiner can then transform the node before it gets to target node selection. Problem #2 is we are adding a whole bunch of x86 atomic instructions when in; fact these instructions are identical to the non-lock versions. We need a way to; add target specific information to target nodes and have this information; carried over to machine instructions. Asm printer (or JIT) can use this; information to add the ""lock"" prefix. //===---------------------------------------------------------------------===//. struct B {; unsigned char y0 : 1;; };. int bar(struct B* a) { return a->y0; }. define i32 @bar(%struct.B* nocapture %a) nounwind readonly optsize {; %1 = getelementptr inbounds %struct.B* %a, i64 0, i32 0; %2 = load i8* %1, align 1; %3 = and i8 %2, 1; %4 = zext i8 %3 to i32; ret i32 %4; }. bar: # @bar; # %bb.0:; movb (%rdi), %al; andb $1, %al; movzbl %al, %eax; ret. Missed optimization: should be movl+andl. //===---------------------------------------------------------------------===//. The x86_64 abi says:. Booleans, when stored in a memory object, are stored as single byte objects the; value of which is always 0 (false) or 1 (true). We are not using this fact:. int bar(_Bool *a) { return *a; }. define i32 @bar(i8* nocapture %a) nounwind readonly optsize {; %1 = load i8* %a, align 1, !tbaa !0; %tmp = and i8 %1, 1; %2 = zext i8 %tmp to i32; ret i32 %2; }. bar:; movb (%rdi), %al; andb $1, %al; movzbl %al, %eax; ret. GCC produces. bar:; movzbl (%rdi), %eax; ret. //===---------------------------------------------------------------------===//. Take the following C code:; int f(int a, int b) { return (unsigned char)a == (unsigned char)b; }. We generate the following IR with clang:; define i32 @f(i32 %a, i32 %b) nounwind rea",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:35180,Performance,optimiz,optimization,35180,"ion being an implicit_def. The; proper solution is to add new ISD opcodes for the no-output variant. DAG; combiner can then transform the node before it gets to target node selection. Problem #2 is we are adding a whole bunch of x86 atomic instructions when in; fact these instructions are identical to the non-lock versions. We need a way to; add target specific information to target nodes and have this information; carried over to machine instructions. Asm printer (or JIT) can use this; information to add the ""lock"" prefix. //===---------------------------------------------------------------------===//. struct B {; unsigned char y0 : 1;; };. int bar(struct B* a) { return a->y0; }. define i32 @bar(%struct.B* nocapture %a) nounwind readonly optsize {; %1 = getelementptr inbounds %struct.B* %a, i64 0, i32 0; %2 = load i8* %1, align 1; %3 = and i8 %2, 1; %4 = zext i8 %3 to i32; ret i32 %4; }. bar: # @bar; # %bb.0:; movb (%rdi), %al; andb $1, %al; movzbl %al, %eax; ret. Missed optimization: should be movl+andl. //===---------------------------------------------------------------------===//. The x86_64 abi says:. Booleans, when stored in a memory object, are stored as single byte objects the; value of which is always 0 (false) or 1 (true). We are not using this fact:. int bar(_Bool *a) { return *a; }. define i32 @bar(i8* nocapture %a) nounwind readonly optsize {; %1 = load i8* %a, align 1, !tbaa !0; %tmp = and i8 %1, 1; %2 = zext i8 %tmp to i32; ret i32 %2; }. bar:; movb (%rdi), %al; andb $1, %al; movzbl %al, %eax; ret. GCC produces. bar:; movzbl (%rdi), %eax; ret. //===---------------------------------------------------------------------===//. Take the following C code:; int f(int a, int b) { return (unsigned char)a == (unsigned char)b; }. We generate the following IR with clang:; define i32 @f(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = xor i32 %b, %a ; <i32> [#uses=1]; %tmp6 = and i32 %tmp, 255 ; <i32> [#uses=1]; %cmp = icmp eq i32 %tmp6, 0 ; <i1> [#uses=1]; %c",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:35578,Performance,load,load,35578,"ave this information; carried over to machine instructions. Asm printer (or JIT) can use this; information to add the ""lock"" prefix. //===---------------------------------------------------------------------===//. struct B {; unsigned char y0 : 1;; };. int bar(struct B* a) { return a->y0; }. define i32 @bar(%struct.B* nocapture %a) nounwind readonly optsize {; %1 = getelementptr inbounds %struct.B* %a, i64 0, i32 0; %2 = load i8* %1, align 1; %3 = and i8 %2, 1; %4 = zext i8 %3 to i32; ret i32 %4; }. bar: # @bar; # %bb.0:; movb (%rdi), %al; andb $1, %al; movzbl %al, %eax; ret. Missed optimization: should be movl+andl. //===---------------------------------------------------------------------===//. The x86_64 abi says:. Booleans, when stored in a memory object, are stored as single byte objects the; value of which is always 0 (false) or 1 (true). We are not using this fact:. int bar(_Bool *a) { return *a; }. define i32 @bar(i8* nocapture %a) nounwind readonly optsize {; %1 = load i8* %a, align 1, !tbaa !0; %tmp = and i8 %1, 1; %2 = zext i8 %tmp to i32; ret i32 %2; }. bar:; movb (%rdi), %al; andb $1, %al; movzbl %al, %eax; ret. GCC produces. bar:; movzbl (%rdi), %eax; ret. //===---------------------------------------------------------------------===//. Take the following C code:; int f(int a, int b) { return (unsigned char)a == (unsigned char)b; }. We generate the following IR with clang:; define i32 @f(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = xor i32 %b, %a ; <i32> [#uses=1]; %tmp6 = and i32 %tmp, 255 ; <i32> [#uses=1]; %cmp = icmp eq i32 %tmp6, 0 ; <i1> [#uses=1]; %conv5 = zext i1 %cmp to i32 ; <i32> [#uses=1]; ret i32 %conv5; }. And the following x86 code:; 	xorl	%esi, %edi; 	testb	$-1, %dil; 	sete	%al; 	movzbl	%al, %eax; 	ret. A cmpb instead of the xorl+testb would be one instruction shorter. //===---------------------------------------------------------------------===//. Given the following C code:; int f(int a, int b) { return (signed char)a == (signed",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:37276,Performance,load,load,37276,"	%esi, %edi; 	testb	$-1, %dil; 	sete	%al; 	movzbl	%al, %eax; 	ret. A cmpb instead of the xorl+testb would be one instruction shorter. //===---------------------------------------------------------------------===//. Given the following C code:; int f(int a, int b) { return (signed char)a == (signed char)b; }. We generate the following IR with clang:; define i32 @f(i32 %a, i32 %b) nounwind readnone {; entry:; %sext = shl i32 %a, 24 ; <i32> [#uses=1]; %conv1 = ashr i32 %sext, 24 ; <i32> [#uses=1]; %sext6 = shl i32 %b, 24 ; <i32> [#uses=1]; %conv4 = ashr i32 %sext6, 24 ; <i32> [#uses=1]; %cmp = icmp eq i32 %conv1, %conv4 ; <i1> [#uses=1]; %conv5 = zext i1 %cmp to i32 ; <i32> [#uses=1]; ret i32 %conv5; }. And the following x86 code:; 	movsbl	%sil, %eax; 	movsbl	%dil, %ecx; 	cmpl	%eax, %ecx; 	sete	%al; 	movzbl	%al, %eax; 	ret. It should be possible to eliminate the sign extensions. //===---------------------------------------------------------------------===//. LLVM misses a load+store narrowing opportunity in this code:. %struct.bf = type { i64, i16, i16, i32 }. @bfi = external global %struct.bf* ; <%struct.bf**> [#uses=2]. define void @t1() nounwind ssp {; entry:; %0 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %1 = getelementptr %struct.bf* %0, i64 0, i32 1 ; <i16*> [#uses=1]; %2 = bitcast i16* %1 to i32* ; <i32*> [#uses=2]; %3 = load i32* %2, align 1 ; <i32> [#uses=1]; %4 = and i32 %3, -65537 ; <i32> [#uses=1]; store i32 %4, i32* %2, align 1; %5 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %6 = getelementptr %struct.bf* %5, i64 0, i32 1 ; <i16*> [#uses=1]; %7 = bitcast i16* %6 to i32* ; <i32*> [#uses=2]; %8 = load i32* %7, align 1 ; <i32> [#uses=1]; %9 = and i32 %8, -131073 ; <i32> [#uses=1]; store i32 %9, i32* %7, align 1; ret void; }. LLVM currently emits this:. movq bfi(%rip), %rax; andl $-65537, 8(%rax); movq bfi(%rip), %rax; andl $-131073, 8(%rax); ret. It could narrow the loads and stores to emit this:. movq bfi(%rip), %rax; andb ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:37476,Performance,load,load,37476,"------------------------------------------===//. Given the following C code:; int f(int a, int b) { return (signed char)a == (signed char)b; }. We generate the following IR with clang:; define i32 @f(i32 %a, i32 %b) nounwind readnone {; entry:; %sext = shl i32 %a, 24 ; <i32> [#uses=1]; %conv1 = ashr i32 %sext, 24 ; <i32> [#uses=1]; %sext6 = shl i32 %b, 24 ; <i32> [#uses=1]; %conv4 = ashr i32 %sext6, 24 ; <i32> [#uses=1]; %cmp = icmp eq i32 %conv1, %conv4 ; <i1> [#uses=1]; %conv5 = zext i1 %cmp to i32 ; <i32> [#uses=1]; ret i32 %conv5; }. And the following x86 code:; 	movsbl	%sil, %eax; 	movsbl	%dil, %ecx; 	cmpl	%eax, %ecx; 	sete	%al; 	movzbl	%al, %eax; 	ret. It should be possible to eliminate the sign extensions. //===---------------------------------------------------------------------===//. LLVM misses a load+store narrowing opportunity in this code:. %struct.bf = type { i64, i16, i16, i32 }. @bfi = external global %struct.bf* ; <%struct.bf**> [#uses=2]. define void @t1() nounwind ssp {; entry:; %0 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %1 = getelementptr %struct.bf* %0, i64 0, i32 1 ; <i16*> [#uses=1]; %2 = bitcast i16* %1 to i32* ; <i32*> [#uses=2]; %3 = load i32* %2, align 1 ; <i32> [#uses=1]; %4 = and i32 %3, -65537 ; <i32> [#uses=1]; store i32 %4, i32* %2, align 1; %5 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %6 = getelementptr %struct.bf* %5, i64 0, i32 1 ; <i16*> [#uses=1]; %7 = bitcast i16* %6 to i32* ; <i32*> [#uses=2]; %8 = load i32* %7, align 1 ; <i32> [#uses=1]; %9 = and i32 %8, -131073 ; <i32> [#uses=1]; store i32 %9, i32* %7, align 1; ret void; }. LLVM currently emits this:. movq bfi(%rip), %rax; andl $-65537, 8(%rax); movq bfi(%rip), %rax; andl $-131073, 8(%rax); ret. It could narrow the loads and stores to emit this:. movq bfi(%rip), %rax; andb $-2, 10(%rax); movq bfi(%rip), %rax; andb $-3, 10(%rax); ret. The trouble is that there is a TokenFactor between the store and the; load, making it non-trivial to dete",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:37657,Performance,load,load,37657,"unwind readnone {; entry:; %sext = shl i32 %a, 24 ; <i32> [#uses=1]; %conv1 = ashr i32 %sext, 24 ; <i32> [#uses=1]; %sext6 = shl i32 %b, 24 ; <i32> [#uses=1]; %conv4 = ashr i32 %sext6, 24 ; <i32> [#uses=1]; %cmp = icmp eq i32 %conv1, %conv4 ; <i1> [#uses=1]; %conv5 = zext i1 %cmp to i32 ; <i32> [#uses=1]; ret i32 %conv5; }. And the following x86 code:; 	movsbl	%sil, %eax; 	movsbl	%dil, %ecx; 	cmpl	%eax, %ecx; 	sete	%al; 	movzbl	%al, %eax; 	ret. It should be possible to eliminate the sign extensions. //===---------------------------------------------------------------------===//. LLVM misses a load+store narrowing opportunity in this code:. %struct.bf = type { i64, i16, i16, i32 }. @bfi = external global %struct.bf* ; <%struct.bf**> [#uses=2]. define void @t1() nounwind ssp {; entry:; %0 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %1 = getelementptr %struct.bf* %0, i64 0, i32 1 ; <i16*> [#uses=1]; %2 = bitcast i16* %1 to i32* ; <i32*> [#uses=2]; %3 = load i32* %2, align 1 ; <i32> [#uses=1]; %4 = and i32 %3, -65537 ; <i32> [#uses=1]; store i32 %4, i32* %2, align 1; %5 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %6 = getelementptr %struct.bf* %5, i64 0, i32 1 ; <i16*> [#uses=1]; %7 = bitcast i16* %6 to i32* ; <i32*> [#uses=2]; %8 = load i32* %7, align 1 ; <i32> [#uses=1]; %9 = and i32 %8, -131073 ; <i32> [#uses=1]; store i32 %9, i32* %7, align 1; ret void; }. LLVM currently emits this:. movq bfi(%rip), %rax; andl $-65537, 8(%rax); movq bfi(%rip), %rax; andl $-131073, 8(%rax); ret. It could narrow the loads and stores to emit this:. movq bfi(%rip), %rax; andb $-2, 10(%rax); movq bfi(%rip), %rax; andb $-3, 10(%rax); ret. The trouble is that there is a TokenFactor between the store and the; load, making it non-trivial to determine if there's anything between; the load and the store which would prohibit narrowing. //===---------------------------------------------------------------------===//. This code:; void foo(unsigned x) {; if (x == 0)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:37778,Performance,load,load,37778,"unwind readnone {; entry:; %sext = shl i32 %a, 24 ; <i32> [#uses=1]; %conv1 = ashr i32 %sext, 24 ; <i32> [#uses=1]; %sext6 = shl i32 %b, 24 ; <i32> [#uses=1]; %conv4 = ashr i32 %sext6, 24 ; <i32> [#uses=1]; %cmp = icmp eq i32 %conv1, %conv4 ; <i1> [#uses=1]; %conv5 = zext i1 %cmp to i32 ; <i32> [#uses=1]; ret i32 %conv5; }. And the following x86 code:; 	movsbl	%sil, %eax; 	movsbl	%dil, %ecx; 	cmpl	%eax, %ecx; 	sete	%al; 	movzbl	%al, %eax; 	ret. It should be possible to eliminate the sign extensions. //===---------------------------------------------------------------------===//. LLVM misses a load+store narrowing opportunity in this code:. %struct.bf = type { i64, i16, i16, i32 }. @bfi = external global %struct.bf* ; <%struct.bf**> [#uses=2]. define void @t1() nounwind ssp {; entry:; %0 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %1 = getelementptr %struct.bf* %0, i64 0, i32 1 ; <i16*> [#uses=1]; %2 = bitcast i16* %1 to i32* ; <i32*> [#uses=2]; %3 = load i32* %2, align 1 ; <i32> [#uses=1]; %4 = and i32 %3, -65537 ; <i32> [#uses=1]; store i32 %4, i32* %2, align 1; %5 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %6 = getelementptr %struct.bf* %5, i64 0, i32 1 ; <i16*> [#uses=1]; %7 = bitcast i16* %6 to i32* ; <i32*> [#uses=2]; %8 = load i32* %7, align 1 ; <i32> [#uses=1]; %9 = and i32 %8, -131073 ; <i32> [#uses=1]; store i32 %9, i32* %7, align 1; ret void; }. LLVM currently emits this:. movq bfi(%rip), %rax; andl $-65537, 8(%rax); movq bfi(%rip), %rax; andl $-131073, 8(%rax); ret. It could narrow the loads and stores to emit this:. movq bfi(%rip), %rax; andb $-2, 10(%rax); movq bfi(%rip), %rax; andb $-3, 10(%rax); ret. The trouble is that there is a TokenFactor between the store and the; load, making it non-trivial to determine if there's anything between; the load and the store which would prohibit narrowing. //===---------------------------------------------------------------------===//. This code:; void foo(unsigned x) {; if (x == 0)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:37959,Performance,load,load,37959,"es=1]; ret i32 %conv5; }. And the following x86 code:; 	movsbl	%sil, %eax; 	movsbl	%dil, %ecx; 	cmpl	%eax, %ecx; 	sete	%al; 	movzbl	%al, %eax; 	ret. It should be possible to eliminate the sign extensions. //===---------------------------------------------------------------------===//. LLVM misses a load+store narrowing opportunity in this code:. %struct.bf = type { i64, i16, i16, i32 }. @bfi = external global %struct.bf* ; <%struct.bf**> [#uses=2]. define void @t1() nounwind ssp {; entry:; %0 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %1 = getelementptr %struct.bf* %0, i64 0, i32 1 ; <i16*> [#uses=1]; %2 = bitcast i16* %1 to i32* ; <i32*> [#uses=2]; %3 = load i32* %2, align 1 ; <i32> [#uses=1]; %4 = and i32 %3, -65537 ; <i32> [#uses=1]; store i32 %4, i32* %2, align 1; %5 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %6 = getelementptr %struct.bf* %5, i64 0, i32 1 ; <i16*> [#uses=1]; %7 = bitcast i16* %6 to i32* ; <i32*> [#uses=2]; %8 = load i32* %7, align 1 ; <i32> [#uses=1]; %9 = and i32 %8, -131073 ; <i32> [#uses=1]; store i32 %9, i32* %7, align 1; ret void; }. LLVM currently emits this:. movq bfi(%rip), %rax; andl $-65537, 8(%rax); movq bfi(%rip), %rax; andl $-131073, 8(%rax); ret. It could narrow the loads and stores to emit this:. movq bfi(%rip), %rax; andb $-2, 10(%rax); movq bfi(%rip), %rax; andb $-3, 10(%rax); ret. The trouble is that there is a TokenFactor between the store and the; load, making it non-trivial to determine if there's anything between; the load and the store which would prohibit narrowing. //===---------------------------------------------------------------------===//. This code:; void foo(unsigned x) {; if (x == 0) bar();; else if (x == 1) qux();; }. currently compiles into:; _foo:; 	movl	4(%esp), %eax; 	cmpl	$1, %eax; 	je	LBB0_3; 	testl	%eax, %eax; 	jne	LBB0_4. the testl could be removed:; _foo:; 	movl	4(%esp), %eax; 	cmpl	$1, %eax; 	je	LBB0_3; 	jb	LBB0_4. 0 is the only unsigned number < 1. //===-----------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:38233,Performance,load,loads,38233,"-----------------===//. LLVM misses a load+store narrowing opportunity in this code:. %struct.bf = type { i64, i16, i16, i32 }. @bfi = external global %struct.bf* ; <%struct.bf**> [#uses=2]. define void @t1() nounwind ssp {; entry:; %0 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %1 = getelementptr %struct.bf* %0, i64 0, i32 1 ; <i16*> [#uses=1]; %2 = bitcast i16* %1 to i32* ; <i32*> [#uses=2]; %3 = load i32* %2, align 1 ; <i32> [#uses=1]; %4 = and i32 %3, -65537 ; <i32> [#uses=1]; store i32 %4, i32* %2, align 1; %5 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %6 = getelementptr %struct.bf* %5, i64 0, i32 1 ; <i16*> [#uses=1]; %7 = bitcast i16* %6 to i32* ; <i32*> [#uses=2]; %8 = load i32* %7, align 1 ; <i32> [#uses=1]; %9 = and i32 %8, -131073 ; <i32> [#uses=1]; store i32 %9, i32* %7, align 1; ret void; }. LLVM currently emits this:. movq bfi(%rip), %rax; andl $-65537, 8(%rax); movq bfi(%rip), %rax; andl $-131073, 8(%rax); ret. It could narrow the loads and stores to emit this:. movq bfi(%rip), %rax; andb $-2, 10(%rax); movq bfi(%rip), %rax; andb $-3, 10(%rax); ret. The trouble is that there is a TokenFactor between the store and the; load, making it non-trivial to determine if there's anything between; the load and the store which would prohibit narrowing. //===---------------------------------------------------------------------===//. This code:; void foo(unsigned x) {; if (x == 0) bar();; else if (x == 1) qux();; }. currently compiles into:; _foo:; 	movl	4(%esp), %eax; 	cmpl	$1, %eax; 	je	LBB0_3; 	testl	%eax, %eax; 	jne	LBB0_4. the testl could be removed:; _foo:; 	movl	4(%esp), %eax; 	cmpl	$1, %eax; 	je	LBB0_3; 	jb	LBB0_4. 0 is the only unsigned number < 1. //===---------------------------------------------------------------------===//. This code:. %0 = type { i32, i1 }. define i32 @add32carry(i32 %sum, i32 %x) nounwind readnone ssp {; entry:; %uadd = tail call %0 @llvm.uadd.with.overflow.i32(i32 %sum, i32 %x); %cmp = extractvalue",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:38424,Performance,load,load,38424,"nwind ssp {; entry:; %0 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %1 = getelementptr %struct.bf* %0, i64 0, i32 1 ; <i16*> [#uses=1]; %2 = bitcast i16* %1 to i32* ; <i32*> [#uses=2]; %3 = load i32* %2, align 1 ; <i32> [#uses=1]; %4 = and i32 %3, -65537 ; <i32> [#uses=1]; store i32 %4, i32* %2, align 1; %5 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %6 = getelementptr %struct.bf* %5, i64 0, i32 1 ; <i16*> [#uses=1]; %7 = bitcast i16* %6 to i32* ; <i32*> [#uses=2]; %8 = load i32* %7, align 1 ; <i32> [#uses=1]; %9 = and i32 %8, -131073 ; <i32> [#uses=1]; store i32 %9, i32* %7, align 1; ret void; }. LLVM currently emits this:. movq bfi(%rip), %rax; andl $-65537, 8(%rax); movq bfi(%rip), %rax; andl $-131073, 8(%rax); ret. It could narrow the loads and stores to emit this:. movq bfi(%rip), %rax; andb $-2, 10(%rax); movq bfi(%rip), %rax; andb $-3, 10(%rax); ret. The trouble is that there is a TokenFactor between the store and the; load, making it non-trivial to determine if there's anything between; the load and the store which would prohibit narrowing. //===---------------------------------------------------------------------===//. This code:; void foo(unsigned x) {; if (x == 0) bar();; else if (x == 1) qux();; }. currently compiles into:; _foo:; 	movl	4(%esp), %eax; 	cmpl	$1, %eax; 	je	LBB0_3; 	testl	%eax, %eax; 	jne	LBB0_4. the testl could be removed:; _foo:; 	movl	4(%esp), %eax; 	cmpl	$1, %eax; 	je	LBB0_3; 	jb	LBB0_4. 0 is the only unsigned number < 1. //===---------------------------------------------------------------------===//. This code:. %0 = type { i32, i1 }. define i32 @add32carry(i32 %sum, i32 %x) nounwind readnone ssp {; entry:; %uadd = tail call %0 @llvm.uadd.with.overflow.i32(i32 %sum, i32 %x); %cmp = extractvalue %0 %uadd, 1; %inc = zext i1 %cmp to i32; %add = add i32 %x, %sum; %z.0 = add i32 %add, %inc; ret i32 %z.0; }. declare %0 @llvm.uadd.with.overflow.i32(i32, i32) nounwind readnone. compiles to:. _add32carry: ## @ad",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:38498,Performance,load,load,38498,"nwind ssp {; entry:; %0 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %1 = getelementptr %struct.bf* %0, i64 0, i32 1 ; <i16*> [#uses=1]; %2 = bitcast i16* %1 to i32* ; <i32*> [#uses=2]; %3 = load i32* %2, align 1 ; <i32> [#uses=1]; %4 = and i32 %3, -65537 ; <i32> [#uses=1]; store i32 %4, i32* %2, align 1; %5 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %6 = getelementptr %struct.bf* %5, i64 0, i32 1 ; <i16*> [#uses=1]; %7 = bitcast i16* %6 to i32* ; <i32*> [#uses=2]; %8 = load i32* %7, align 1 ; <i32> [#uses=1]; %9 = and i32 %8, -131073 ; <i32> [#uses=1]; store i32 %9, i32* %7, align 1; ret void; }. LLVM currently emits this:. movq bfi(%rip), %rax; andl $-65537, 8(%rax); movq bfi(%rip), %rax; andl $-131073, 8(%rax); ret. It could narrow the loads and stores to emit this:. movq bfi(%rip), %rax; andb $-2, 10(%rax); movq bfi(%rip), %rax; andb $-3, 10(%rax); ret. The trouble is that there is a TokenFactor between the store and the; load, making it non-trivial to determine if there's anything between; the load and the store which would prohibit narrowing. //===---------------------------------------------------------------------===//. This code:; void foo(unsigned x) {; if (x == 0) bar();; else if (x == 1) qux();; }. currently compiles into:; _foo:; 	movl	4(%esp), %eax; 	cmpl	$1, %eax; 	je	LBB0_3; 	testl	%eax, %eax; 	jne	LBB0_4. the testl could be removed:; _foo:; 	movl	4(%esp), %eax; 	cmpl	$1, %eax; 	je	LBB0_3; 	jb	LBB0_4. 0 is the only unsigned number < 1. //===---------------------------------------------------------------------===//. This code:. %0 = type { i32, i1 }. define i32 @add32carry(i32 %sum, i32 %x) nounwind readnone ssp {; entry:; %uadd = tail call %0 @llvm.uadd.with.overflow.i32(i32 %sum, i32 %x); %cmp = extractvalue %0 %uadd, 1; %inc = zext i1 %cmp to i32; %add = add i32 %x, %sum; %z.0 = add i32 %add, %inc; ret i32 %z.0; }. declare %0 @llvm.uadd.with.overflow.i32(i32, i32) nounwind readnone. compiles to:. _add32carry: ## @ad",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:40696,Performance,load,loads,40696,"------------------------------------------------------===//. The hot loop of 256.bzip2 contains code that looks a bit like this:. int foo(char *P, char *Q, int x, int y) {; if (P[0] != Q[0]); return P[0] < Q[0];; if (P[1] != Q[1]); return P[1] < Q[1];; if (P[2] != Q[2]); return P[2] < Q[2];; return P[3] < Q[3];; }. In the real code, we get a lot more wrong than this. However, even in this; code we generate:. _foo: ## @foo; ## %bb.0: ## %entry; 	movb	(%rsi), %al; 	movb	(%rdi), %cl; 	cmpb	%al, %cl; 	je	LBB0_2; LBB0_1: ## %if.then; 	cmpb	%al, %cl; 	jmp	LBB0_5; LBB0_2: ## %if.end; 	movb	1(%rsi), %al; 	movb	1(%rdi), %cl; 	cmpb	%al, %cl; 	jne	LBB0_1; ## %bb.3: ## %if.end38; 	movb	2(%rsi), %al; 	movb	2(%rdi), %cl; 	cmpb	%al, %cl; 	jne	LBB0_1; ## %bb.4: ## %if.end60; 	movb	3(%rdi), %al; 	cmpb	3(%rsi), %al; LBB0_5: ## %if.end60; 	setl	%al; 	movzbl	%al, %eax; 	ret. Note that we generate jumps to LBB0_1 which does a redundant compare. The; redundant compare also forces the register values to be live, which prevents; folding one of the loads into the compare. In contrast, GCC 4.2 produces:. _foo:; 	movzbl	(%rsi), %eax; 	cmpb	%al, (%rdi); 	jne	L10; L12:; 	movzbl	1(%rsi), %eax; 	cmpb	%al, 1(%rdi); 	jne	L10; 	movzbl	2(%rsi), %eax; 	cmpb	%al, 2(%rdi); 	jne	L10; 	movzbl	3(%rdi), %eax; 	cmpb	3(%rsi), %al; L10:; 	setl	%al; 	movzbl	%al, %eax; 	ret. which is ""perfect"". //===---------------------------------------------------------------------===//. For the branch in the following code:; int a();; int b(int x, int y) {; if (x & (1<<(y&7))); return a();; return y;; }. We currently generate:; 	movb	%sil, %al; 	andb	$7, %al; 	movzbl	%al, %eax; 	btl	%eax, %edi; 	jae	.LBB0_2. movl+andl would be shorter than the movb+andb+movzbl sequence. //===---------------------------------------------------------------------===//. For the following:; struct u1 {; float x, y;; };; float foo(struct u1 u) {; return u.x + u.y;; }. We currently generate:; 	movdqa	%xmm0, %xmm1; 	pshufd	$1, %xmm0, %xmm0 # xmm0 = ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:2039,Safety,avoid,avoid,2039,"rom http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01148.html,; but without the unnecessary and.); movl %ecx, %eax; shrl $5, %eax; movl %eax, %edx; xorl $1, %edx; sall %cl, %eax; sall %cl. %edx. 64-bit shifts (in general) expand to really bad code. Instead of using; cmovs, we should expand to a conditional branch like GCC produces. //===---------------------------------------------------------------------===//. Some isel ideas:. 1. Dynamic programming based approach when compile time is not an; issue.; 2. Code duplication (addressing mode) during isel.; 3. Other ideas from ""Register-Sensitive Selection, Duplication, and; Sequencing of Instructions"".; 4. Scheduling for reduced register pressure. E.g. ""Minimum Register; Instruction Sequence Problem: Revisiting Optimal Code Generation for DAGs""; and other related papers.; http://citeseer.ist.psu.edu/govindarajan01minimum.html. //===---------------------------------------------------------------------===//. Should we promote i16 to i32 to avoid partial register update stalls?. //===---------------------------------------------------------------------===//. Leave any_extend as pseudo instruction and hint to register; allocator. Delay codegen until post register allocation.; Note. any_extend is now turned into an INSERT_SUBREG. We still need to teach; the coalescer how to deal with it though. //===---------------------------------------------------------------------===//. It appears icc use push for parameter passing. Need to investigate. //===---------------------------------------------------------------------===//. The instruction selector sometimes misses folding a load into a compare. The; pattern is written as (cmp reg, (load p)). Because the compare isn't; commutative, it is not matched with the load on both sides. The dag combiner; should be made smart enough to canonicalize the load into the RHS of a compare; when it can invert the result of the compare for free. //===------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:5004,Safety,avoid,avoiding,5004,"(%edx); 	movw $257, 8(%edx). It might be better to generate. 	movl $16843009, %eax; 	movl %eax, 4(%edx); 	movl %eax, (%edx); 	movw al, 8(%edx); 	; when we can spare a register. It reduces code size. //===---------------------------------------------------------------------===//. Evaluate what the best way to codegen sdiv X, (2^C) is. For X/8, we currently; get this:. define i32 @test1(i32 %X) {; %Y = sdiv i32 %X, 8; ret i32 %Y; }. _test1:; movl 4(%esp), %eax; movl %eax, %ecx; sarl $31, %ecx; shrl $29, %ecx; addl %ecx, %eax; sarl $3, %eax; ret. GCC knows several different ways to codegen it, one of which is this:. _test1:; movl 4(%esp), %eax; cmpl $-1, %eax; leal 7(%eax), %ecx; cmovle %ecx, %eax; sarl $3, %eax; ret. which is probably slower, but it's interesting at least :). //===---------------------------------------------------------------------===//. We are currently lowering large (1MB+) memmove/memcpy to rep/stosl and rep/movsl; We should leave these as libcalls for everything over a much lower threshold,; since libc is hand tuned for medium and large mem ops (avoiding RFO for large; stores, TLB preheating, etc). //===---------------------------------------------------------------------===//. Optimize this into something reasonable:; x * copysign(1.0, y) * copysign(1.0, z). //===---------------------------------------------------------------------===//. Optimize copysign(x, *y) to use an integer load from y. //===---------------------------------------------------------------------===//. The following tests perform worse with LSR:. lambda, siod, optimizer-eval, ackermann, hash2, nestedloop, strcat, and Treesor. //===---------------------------------------------------------------------===//. Adding to the list of cmp / test poor codegen issues:. int test(__m128 *A, __m128 *B) {; if (_mm_comige_ss(*A, *B)); return 3;; else; return 4;; }. _test:; 	movl 8(%esp), %eax; 	movaps (%eax), %xmm0; 	movl 4(%esp), %eax; 	movaps (%eax), %xmm1; 	comiss %xmm0, %xmm1; 	setae %a",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:10181,Safety,avoid,avoid,10181,"000010 andl $0x0000ffff,%eax; 00000015 cvtsi2ss %eax,%xmm1; 00000019 mulss 0x00000078,%xmm0; 00000021 addss %xmm1,%xmm0; 00000025 movss %xmm0,(%esp,1); 0000002a flds (%esp,1); 0000002d addl $0x04,%esp; 00000030 ret. //===---------------------------------------------------------------------===//. When using fastcc abi, align stack slot of argument of type double on 8 byte; boundary to improve performance. //===---------------------------------------------------------------------===//. GCC's ix86_expand_int_movcc function (in i386.c) has a ton of interesting; simplifications for integer ""x cmp y ? a : b"". //===---------------------------------------------------------------------===//. Consider the expansion of:. define i32 @test3(i32 %X) {; %tmp1 = urem i32 %X, 255; ret i32 %tmp1; }. Currently it compiles to:. ...; movl $2155905153, %ecx; movl 8(%esp), %esi; movl %esi, %eax; mull %ecx; ... This could be ""reassociated"" into:. movl $2155905153, %eax; movl 8(%esp), %ecx; mull %ecx. to avoid the copy. In fact, the existing two-address stuff would do this; except that mul isn't a commutative 2-addr instruction. I guess this has; to be done at isel time based on the #uses to mul?. //===---------------------------------------------------------------------===//. Make sure the instruction which starts a loop does not cross a cacheline; boundary. This requires knowning the exact length of each machine instruction.; That is somewhat complicated, but doable. Example 256.bzip2:. In the new trace, the hot loop has an instruction which crosses a cacheline; boundary. In addition to potential cache misses, this can't help decoding as I; imagine there has to be some kind of complicated decoder reset and realignment; to grab the bytes from the next cacheline. 532 532 0x3cfc movb (1809(%esp, %esi), %bl <<<--- spans 2 64 byte lines; 942 942 0x3d03 movl %dh, (1809(%esp, %esi); 937 937 0x3d0a incl %esi; 3 3 0x3d0b cmpb %bl, %dl; 27 27 0x3d0d jnz 0x000062db <main+11707>. //===----------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:11775,Safety,abort,abort,11775,"dition to potential cache misses, this can't help decoding as I; imagine there has to be some kind of complicated decoder reset and realignment; to grab the bytes from the next cacheline. 532 532 0x3cfc movb (1809(%esp, %esi), %bl <<<--- spans 2 64 byte lines; 942 942 0x3d03 movl %dh, (1809(%esp, %esi); 937 937 0x3d0a incl %esi; 3 3 0x3d0b cmpb %bl, %dl; 27 27 0x3d0d jnz 0x000062db <main+11707>. //===---------------------------------------------------------------------===//. In c99 mode, the preprocessor doesn't like assembly comments like #TRUNCATE. //===---------------------------------------------------------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===---------------------------------------------------------------------===//. This code:. void test(int X) {; if (X) abort();; }. is currently compiled to:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne LBB1_1; addl $12, %esp; ret; LBB1_1:; call L_abort$stub. It would be better to produce:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne L_abort$stub; addl $12, %esp; ret. This can be applied to any no-return function call that takes no arguments etc.; Alternatively, the stack save/restore logic could be shrink-wrapped, producing; something like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; subl $12, %esp; call L_abort$stub. Both are useful in different situations. Finally, it could be shrink-wrapped; and tail called, like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; pop %eax # realign stack.; call L_abort$stub. Though this probably isn't worth it. //===---------------------------------------------------------------------===//. Sometimes it is better to codegen subtractions from a constant (e.g. 7-x) with; a neg instead of a sub instruction. Consider:. int test(char ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:16522,Safety,redund,redundant,16522,"6 %tmp98 to i32		; <i32> [#uses=1]; 	%tmp585 = sub i32 32, %tmp583584		; <i32> [#uses=1]; 	%tmp614615 = sext i16 %tmp101 to i32		; <i32> [#uses=1]; 	%tmp621622 = sext i16 %tmp104 to i32		; <i32> [#uses=1]; 	%tmp623 = sub i32 32, %tmp621622		; <i32> [#uses=1]; 	br label %bb114. produces:. LBB3_5:	# bb114.preheader; 	movswl	-68(%ebp), %eax; 	movl	$32, %ecx; 	movl	%ecx, -80(%ebp); 	subl	%eax, -80(%ebp); 	movswl	-52(%ebp), %eax; 	movl	%ecx, -84(%ebp); 	subl	%eax, -84(%ebp); 	movswl	-70(%ebp), %eax; 	movl	%ecx, -88(%ebp); 	subl	%eax, -88(%ebp); 	movswl	-50(%ebp), %eax; 	subl	%eax, %ecx; 	movl	%ecx, -76(%ebp); 	movswl	-42(%ebp), %eax; 	movl	%eax, -92(%ebp); 	movswl	-66(%ebp), %eax; 	movl	%eax, -96(%ebp); 	movw	$0, -98(%ebp). This appears to be bad because the RA is not folding the store to the stack ; slot into the movl. The above instructions could be:; 	movl $32, -80(%ebp); ...; 	movl $32, -84(%ebp); ...; This seems like a cross between remat and spill folding. This has redundant subtractions of %eax from a stack slot. However, %ecx doesn't; change, so we could simply subtract %eax from %ecx first and then use %ecx (or; vice-versa). //===---------------------------------------------------------------------===//. This code:. 	%tmp659 = icmp slt i16 %tmp654, 0		; <i1> [#uses=1]; 	br i1 %tmp659, label %cond_true662, label %cond_next715. produces this:. 	testw	%cx, %cx; 	movswl	%cx, %esi; 	jns	LBB4_109	# cond_next715. Shark tells us that using %cx in the testw instruction is sub-optimal. It; suggests using the 32-bit register (which is what ICC uses). //===---------------------------------------------------------------------===//. We compile this:. void compare (long long foo) {; if (foo < 4294967297LL); abort();; }. to:. compare:; subl $4, %esp; cmpl $0, 8(%esp); setne %al; movzbw %al, %ax; cmpl $1, 12(%esp); setg %cl; movzbw %cl, %cx; cmove %ax, %cx; testb $1, %cl; jne .LBB1_2 # UnifiedReturnBlock; .LBB1_1: # ifthen; call abort; .LBB1_2: # UnifiedReturnBlock; addl $4, %esp",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:17267,Safety,abort,abort,17267,"bp); 	movw	$0, -98(%ebp). This appears to be bad because the RA is not folding the store to the stack ; slot into the movl. The above instructions could be:; 	movl $32, -80(%ebp); ...; 	movl $32, -84(%ebp); ...; This seems like a cross between remat and spill folding. This has redundant subtractions of %eax from a stack slot. However, %ecx doesn't; change, so we could simply subtract %eax from %ecx first and then use %ecx (or; vice-versa). //===---------------------------------------------------------------------===//. This code:. 	%tmp659 = icmp slt i16 %tmp654, 0		; <i1> [#uses=1]; 	br i1 %tmp659, label %cond_true662, label %cond_next715. produces this:. 	testw	%cx, %cx; 	movswl	%cx, %esi; 	jns	LBB4_109	# cond_next715. Shark tells us that using %cx in the testw instruction is sub-optimal. It; suggests using the 32-bit register (which is what ICC uses). //===---------------------------------------------------------------------===//. We compile this:. void compare (long long foo) {; if (foo < 4294967297LL); abort();; }. to:. compare:; subl $4, %esp; cmpl $0, 8(%esp); setne %al; movzbw %al, %ax; cmpl $1, 12(%esp); setg %cl; movzbw %cl, %cx; cmove %ax, %cx; testb $1, %cl; jne .LBB1_2 # UnifiedReturnBlock; .LBB1_1: # ifthen; call abort; .LBB1_2: # UnifiedReturnBlock; addl $4, %esp; ret. (also really horrible code on ppc). This is due to the expand code for 64-bit; compares. GCC produces multiple branches, which is much nicer:. compare:; subl $12, %esp; movl 20(%esp), %edx; movl 16(%esp), %eax; decl %edx; jle .L7; .L5:; addl $12, %esp; ret; .p2align 4,,7; .L7:; jl .L4; cmpl $0, %eax; .p2align 4,,8; ja .L5; .L4:; .p2align 4,,9; call abort. //===---------------------------------------------------------------------===//. Tail call optimization improvements: Tail call optimization currently; pushes all arguments on the top of the stack (their normal place for; non-tail call optimized calls) that source from the callers arguments; or that source from a virtual register (also",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:17491,Safety,abort,abort,17491,"ween remat and spill folding. This has redundant subtractions of %eax from a stack slot. However, %ecx doesn't; change, so we could simply subtract %eax from %ecx first and then use %ecx (or; vice-versa). //===---------------------------------------------------------------------===//. This code:. 	%tmp659 = icmp slt i16 %tmp654, 0		; <i1> [#uses=1]; 	br i1 %tmp659, label %cond_true662, label %cond_next715. produces this:. 	testw	%cx, %cx; 	movswl	%cx, %esi; 	jns	LBB4_109	# cond_next715. Shark tells us that using %cx in the testw instruction is sub-optimal. It; suggests using the 32-bit register (which is what ICC uses). //===---------------------------------------------------------------------===//. We compile this:. void compare (long long foo) {; if (foo < 4294967297LL); abort();; }. to:. compare:; subl $4, %esp; cmpl $0, 8(%esp); setne %al; movzbw %al, %ax; cmpl $1, 12(%esp); setg %cl; movzbw %cl, %cx; cmove %ax, %cx; testb $1, %cl; jne .LBB1_2 # UnifiedReturnBlock; .LBB1_1: # ifthen; call abort; .LBB1_2: # UnifiedReturnBlock; addl $4, %esp; ret. (also really horrible code on ppc). This is due to the expand code for 64-bit; compares. GCC produces multiple branches, which is much nicer:. compare:; subl $12, %esp; movl 20(%esp), %edx; movl 16(%esp), %eax; decl %edx; jle .L7; .L5:; addl $12, %esp; ret; .p2align 4,,7; .L7:; jl .L4; cmpl $0, %eax; .p2align 4,,8; ja .L5; .L4:; .p2align 4,,9; call abort. //===---------------------------------------------------------------------===//. Tail call optimization improvements: Tail call optimization currently; pushes all arguments on the top of the stack (their normal place for; non-tail call optimized calls) that source from the callers arguments; or that source from a virtual register (also possibly sourcing from; callers arguments).; This is done to prevent overwriting of parameters (see example; below) that might be used later. example: . int callee(int32, int64); ; int caller(int32 arg1, int32 arg2) { ; int64 local = arg2 ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:17900,Safety,abort,abort,17900,"produces this:. 	testw	%cx, %cx; 	movswl	%cx, %esi; 	jns	LBB4_109	# cond_next715. Shark tells us that using %cx in the testw instruction is sub-optimal. It; suggests using the 32-bit register (which is what ICC uses). //===---------------------------------------------------------------------===//. We compile this:. void compare (long long foo) {; if (foo < 4294967297LL); abort();; }. to:. compare:; subl $4, %esp; cmpl $0, 8(%esp); setne %al; movzbw %al, %ax; cmpl $1, 12(%esp); setg %cl; movzbw %cl, %cx; cmove %ax, %cx; testb $1, %cl; jne .LBB1_2 # UnifiedReturnBlock; .LBB1_1: # ifthen; call abort; .LBB1_2: # UnifiedReturnBlock; addl $4, %esp; ret. (also really horrible code on ppc). This is due to the expand code for 64-bit; compares. GCC produces multiple branches, which is much nicer:. compare:; subl $12, %esp; movl 20(%esp), %edx; movl 16(%esp), %eax; decl %edx; jle .L7; .L5:; addl $12, %esp; ret; .p2align 4,,7; .L7:; jl .L4; cmpl $0, %eax; .p2align 4,,8; ja .L5; .L4:; .p2align 4,,9; call abort. //===---------------------------------------------------------------------===//. Tail call optimization improvements: Tail call optimization currently; pushes all arguments on the top of the stack (their normal place for; non-tail call optimized calls) that source from the callers arguments; or that source from a virtual register (also possibly sourcing from; callers arguments).; This is done to prevent overwriting of parameters (see example; below) that might be used later. example: . int callee(int32, int64); ; int caller(int32 arg1, int32 arg2) { ; int64 local = arg2 * 2; ; return callee(arg2, (int64)local); ; }. [arg1] [!arg2 no longer valid since we moved local onto it]; [arg2] -> [(int64); [RETADDR] local ]. Moving arg1 onto the stack slot of callee function would overwrite; arg2 of the caller. Possible optimizations:. - Analyse the actual parameters of the callee to see which would; overwrite a caller parameter which is used by the callee and only; push them onto th",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:19487,Safety,abort,abort,19487,"2 * 2; ; return callee(arg2, (int64)local); ; }. [arg1] [!arg2 no longer valid since we moved local onto it]; [arg2] -> [(int64); [RETADDR] local ]. Moving arg1 onto the stack slot of callee function would overwrite; arg2 of the caller. Possible optimizations:. - Analyse the actual parameters of the callee to see which would; overwrite a caller parameter which is used by the callee and only; push them onto the top of the stack. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg1,arg2);; }. Here we don't need to write any variables to the top of the stack; since they don't overwrite each other. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg2,arg1);; }. Here we need to push the arguments because they overwrite each; other. //===---------------------------------------------------------------------===//. main (); {; int i = 0;; unsigned long int z = 0;. do {; z -= 0x00004000;; i++;; if (i > 0x00040000); abort ();; } while (z > 0);; exit (0);; }. gcc compiles this to:. _main:; 	subl	$28, %esp; 	xorl	%eax, %eax; 	jmp	L2; L3:; 	cmpl	$262144, %eax; 	je	L10; L2:; 	addl	$1, %eax; 	cmpl	$262145, %eax; 	jne	L3; 	call	L_abort$stub; L10:; 	movl	$0, (%esp); 	call	L_exit$stub. llvm:. _main:; 	subl	$12, %esp; 	movl	$1, %eax; 	movl	$16384, %ecx; LBB1_1:	# bb; 	cmpl	$262145, %eax; 	jge	LBB1_4	# cond_true; LBB1_2:	# cond_next; 	incl	%eax; 	addl	$4294950912, %ecx; 	cmpl	$16384, %ecx; 	jne	LBB1_1	# bb; LBB1_3:	# bb11; 	xorl	%eax, %eax; 	addl	$12, %esp; 	ret; LBB1_4:	# cond_true; 	call	L_abort$stub. 1. LSR should rewrite the first cmp with induction variable %ecx.; 2. DAG combiner should fold; leal 1(%eax), %edx; cmpl $262145, %edx; =>; cmpl $262144, %eax. //===---------------------------------------------------------------------===//. define i64 @test(double %X) {; 	%Y = fptosi double %X to i64; 	ret i64 %Y; }. compiles to:. _test:; 	subl	$20, %esp; 	movsd	24(%esp), %xmm0; 	movsd	%xmm0, 8(%esp); 	",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:40575,Safety,redund,redundant,40575,"add32carry:; 	leal	(%rsi,%rdi), %eax; 	cmpl	%esi, %eax; 	adcl	$0, %eax; 	ret. //===---------------------------------------------------------------------===//. The hot loop of 256.bzip2 contains code that looks a bit like this:. int foo(char *P, char *Q, int x, int y) {; if (P[0] != Q[0]); return P[0] < Q[0];; if (P[1] != Q[1]); return P[1] < Q[1];; if (P[2] != Q[2]); return P[2] < Q[2];; return P[3] < Q[3];; }. In the real code, we get a lot more wrong than this. However, even in this; code we generate:. _foo: ## @foo; ## %bb.0: ## %entry; 	movb	(%rsi), %al; 	movb	(%rdi), %cl; 	cmpb	%al, %cl; 	je	LBB0_2; LBB0_1: ## %if.then; 	cmpb	%al, %cl; 	jmp	LBB0_5; LBB0_2: ## %if.end; 	movb	1(%rsi), %al; 	movb	1(%rdi), %cl; 	cmpb	%al, %cl; 	jne	LBB0_1; ## %bb.3: ## %if.end38; 	movb	2(%rsi), %al; 	movb	2(%rdi), %cl; 	cmpb	%al, %cl; 	jne	LBB0_1; ## %bb.4: ## %if.end60; 	movb	3(%rdi), %al; 	cmpb	3(%rsi), %al; LBB0_5: ## %if.end60; 	setl	%al; 	movzbl	%al, %eax; 	ret. Note that we generate jumps to LBB0_1 which does a redundant compare. The; redundant compare also forces the register values to be live, which prevents; folding one of the loads into the compare. In contrast, GCC 4.2 produces:. _foo:; 	movzbl	(%rsi), %eax; 	cmpb	%al, (%rdi); 	jne	L10; L12:; 	movzbl	1(%rsi), %eax; 	cmpb	%al, 1(%rdi); 	jne	L10; 	movzbl	2(%rsi), %eax; 	cmpb	%al, 2(%rdi); 	jne	L10; 	movzbl	3(%rdi), %eax; 	cmpb	3(%rsi), %al; L10:; 	setl	%al; 	movzbl	%al, %eax; 	ret. which is ""perfect"". //===---------------------------------------------------------------------===//. For the branch in the following code:; int a();; int b(int x, int y) {; if (x & (1<<(y&7))); return a();; return y;; }. We currently generate:; 	movb	%sil, %al; 	andb	$7, %al; 	movzbl	%al, %eax; 	btl	%eax, %edi; 	jae	.LBB0_2. movl+andl would be shorter than the movb+andb+movzbl sequence. //===---------------------------------------------------------------------===//. For the following:; struct u1 {; float x, y;; };; float foo(struct u1 u) {; retu",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:40599,Safety,redund,redundant,40599,"------------------------------------------------------===//. The hot loop of 256.bzip2 contains code that looks a bit like this:. int foo(char *P, char *Q, int x, int y) {; if (P[0] != Q[0]); return P[0] < Q[0];; if (P[1] != Q[1]); return P[1] < Q[1];; if (P[2] != Q[2]); return P[2] < Q[2];; return P[3] < Q[3];; }. In the real code, we get a lot more wrong than this. However, even in this; code we generate:. _foo: ## @foo; ## %bb.0: ## %entry; 	movb	(%rsi), %al; 	movb	(%rdi), %cl; 	cmpb	%al, %cl; 	je	LBB0_2; LBB0_1: ## %if.then; 	cmpb	%al, %cl; 	jmp	LBB0_5; LBB0_2: ## %if.end; 	movb	1(%rsi), %al; 	movb	1(%rdi), %cl; 	cmpb	%al, %cl; 	jne	LBB0_1; ## %bb.3: ## %if.end38; 	movb	2(%rsi), %al; 	movb	2(%rdi), %cl; 	cmpb	%al, %cl; 	jne	LBB0_1; ## %bb.4: ## %if.end60; 	movb	3(%rdi), %al; 	cmpb	3(%rsi), %al; LBB0_5: ## %if.end60; 	setl	%al; 	movzbl	%al, %eax; 	ret. Note that we generate jumps to LBB0_1 which does a redundant compare. The; redundant compare also forces the register values to be live, which prevents; folding one of the loads into the compare. In contrast, GCC 4.2 produces:. _foo:; 	movzbl	(%rsi), %eax; 	cmpb	%al, (%rdi); 	jne	L10; L12:; 	movzbl	1(%rsi), %eax; 	cmpb	%al, 1(%rdi); 	jne	L10; 	movzbl	2(%rsi), %eax; 	cmpb	%al, 2(%rdi); 	jne	L10; 	movzbl	3(%rdi), %eax; 	cmpb	3(%rsi), %al; L10:; 	setl	%al; 	movzbl	%al, %eax; 	ret. which is ""perfect"". //===---------------------------------------------------------------------===//. For the branch in the following code:; int a();; int b(int x, int y) {; if (x & (1<<(y&7))); return a();; return y;; }. We currently generate:; 	movb	%sil, %al; 	andb	$7, %al; 	movzbl	%al, %eax; 	btl	%eax, %edi; 	jae	.LBB0_2. movl+andl would be shorter than the movb+andb+movzbl sequence. //===---------------------------------------------------------------------===//. For the following:; struct u1 {; float x, y;; };; float foo(struct u1 u) {; return u.x + u.y;; }. We currently generate:; 	movdqa	%xmm0, %xmm1; 	pshufd	$1, %xmm0, %xmm0 # xmm0 = ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:799,Testability,test,testb,799,"//===---------------------------------------------------------------------===//; // Random ideas for the X86 backend.; //===---------------------------------------------------------------------===//. Improvements to the multiply -> shift/add algorithm:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg01590.html. //===---------------------------------------------------------------------===//. Improve code like this (occurs fairly frequently, e.g. in LLVM):; long long foo(int x) { return 1LL << x; }. http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01109.html; http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01128.html; http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01136.html. Another useful one would be ~0ULL >> X and ~0ULL << X. One better solution for 1LL << x is:; xorl %eax, %eax; xorl %edx, %edx; testb $32, %cl; sete %al; setne %dl; sall %cl, %eax; sall %cl, %edx. But that requires good 8-bit subreg support. Also, this might be better. It's an extra shift, but it's one instruction; shorter, and doesn't stress 8-bit subreg support.; (From http://gcc.gnu.org/ml/gcc-patches/2004-09/msg01148.html,; but without the unnecessary and.); movl %ecx, %eax; shrl $5, %eax; movl %eax, %edx; xorl $1, %edx; sall %cl, %eax; sall %cl. %edx. 64-bit shifts (in general) expand to really bad code. Instead of using; cmovs, we should expand to a conditional branch like GCC produces. //===---------------------------------------------------------------------===//. Some isel ideas:. 1. Dynamic programming based approach when compile time is not an; issue.; 2. Code duplication (addressing mode) during isel.; 3. Other ideas from ""Register-Sensitive Selection, Duplication, and; Sequencing of Instructions"".; 4. Scheduling for reduced register pressure. E.g. ""Minimum Register; Instruction Sequence Problem: Revisiting Optimal Code Generation for DAGs""; and other related papers.; http://citeseer.ist.psu.edu/govindarajan01minimum.html. //===---------------------------------------------------------------------=",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:5454,Testability,test,tests,5454,"%eax; ret. GCC knows several different ways to codegen it, one of which is this:. _test1:; movl 4(%esp), %eax; cmpl $-1, %eax; leal 7(%eax), %ecx; cmovle %ecx, %eax; sarl $3, %eax; ret. which is probably slower, but it's interesting at least :). //===---------------------------------------------------------------------===//. We are currently lowering large (1MB+) memmove/memcpy to rep/stosl and rep/movsl; We should leave these as libcalls for everything over a much lower threshold,; since libc is hand tuned for medium and large mem ops (avoiding RFO for large; stores, TLB preheating, etc). //===---------------------------------------------------------------------===//. Optimize this into something reasonable:; x * copysign(1.0, y) * copysign(1.0, z). //===---------------------------------------------------------------------===//. Optimize copysign(x, *y) to use an integer load from y. //===---------------------------------------------------------------------===//. The following tests perform worse with LSR:. lambda, siod, optimizer-eval, ackermann, hash2, nestedloop, strcat, and Treesor. //===---------------------------------------------------------------------===//. Adding to the list of cmp / test poor codegen issues:. int test(__m128 *A, __m128 *B) {; if (_mm_comige_ss(*A, *B)); return 3;; else; return 4;; }. _test:; 	movl 8(%esp), %eax; 	movaps (%eax), %xmm0; 	movl 4(%esp), %eax; 	movaps (%eax), %xmm1; 	comiss %xmm0, %xmm1; 	setae %al; 	movzbl %al, %ecx; 	movl $3, %eax; 	movl $4, %edx; 	cmpl $0, %ecx; 	cmove %edx, %eax; 	ret. Note the setae, movzbl, cmpl, cmove can be replaced with a single cmovae. There; are a number of issues. 1) We are introducing a setcc between the result of the; intrisic call and select. 2) The intrinsic is expected to produce a i32 value; so a any extend (which becomes a zero extend) is added. We probably need some kind of target DAG combine hook to fix this. //===---------------------------------------------------------------------===//. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:5675,Testability,test,test,5675,"but it's interesting at least :). //===---------------------------------------------------------------------===//. We are currently lowering large (1MB+) memmove/memcpy to rep/stosl and rep/movsl; We should leave these as libcalls for everything over a much lower threshold,; since libc is hand tuned for medium and large mem ops (avoiding RFO for large; stores, TLB preheating, etc). //===---------------------------------------------------------------------===//. Optimize this into something reasonable:; x * copysign(1.0, y) * copysign(1.0, z). //===---------------------------------------------------------------------===//. Optimize copysign(x, *y) to use an integer load from y. //===---------------------------------------------------------------------===//. The following tests perform worse with LSR:. lambda, siod, optimizer-eval, ackermann, hash2, nestedloop, strcat, and Treesor. //===---------------------------------------------------------------------===//. Adding to the list of cmp / test poor codegen issues:. int test(__m128 *A, __m128 *B) {; if (_mm_comige_ss(*A, *B)); return 3;; else; return 4;; }. _test:; 	movl 8(%esp), %eax; 	movaps (%eax), %xmm0; 	movl 4(%esp), %eax; 	movaps (%eax), %xmm1; 	comiss %xmm0, %xmm1; 	setae %al; 	movzbl %al, %ecx; 	movl $3, %eax; 	movl $4, %edx; 	cmpl $0, %ecx; 	cmove %edx, %eax; 	ret. Note the setae, movzbl, cmpl, cmove can be replaced with a single cmovae. There; are a number of issues. 1) We are introducing a setcc between the result of the; intrisic call and select. 2) The intrinsic is expected to produce a i32 value; so a any extend (which becomes a zero extend) is added. We probably need some kind of target DAG combine hook to fix this. //===---------------------------------------------------------------------===//. We generate significantly worse code for this than GCC:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=21150; http://gcc.gnu.org/bugzilla/attachment.cgi?id=8701. There is also one case we do worse on PPC. //===----",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:5706,Testability,test,test,5706,"----------------------------------===//. We are currently lowering large (1MB+) memmove/memcpy to rep/stosl and rep/movsl; We should leave these as libcalls for everything over a much lower threshold,; since libc is hand tuned for medium and large mem ops (avoiding RFO for large; stores, TLB preheating, etc). //===---------------------------------------------------------------------===//. Optimize this into something reasonable:; x * copysign(1.0, y) * copysign(1.0, z). //===---------------------------------------------------------------------===//. Optimize copysign(x, *y) to use an integer load from y. //===---------------------------------------------------------------------===//. The following tests perform worse with LSR:. lambda, siod, optimizer-eval, ackermann, hash2, nestedloop, strcat, and Treesor. //===---------------------------------------------------------------------===//. Adding to the list of cmp / test poor codegen issues:. int test(__m128 *A, __m128 *B) {; if (_mm_comige_ss(*A, *B)); return 3;; else; return 4;; }. _test:; 	movl 8(%esp), %eax; 	movaps (%eax), %xmm0; 	movl 4(%esp), %eax; 	movaps (%eax), %xmm1; 	comiss %xmm0, %xmm1; 	setae %al; 	movzbl %al, %ecx; 	movl $3, %eax; 	movl $4, %edx; 	cmpl $0, %ecx; 	cmove %edx, %eax; 	ret. Note the setae, movzbl, cmpl, cmove can be replaced with a single cmovae. There; are a number of issues. 1) We are introducing a setcc between the result of the; intrisic call and select. 2) The intrinsic is expected to produce a i32 value; so a any extend (which becomes a zero extend) is added. We probably need some kind of target DAG combine hook to fix this. //===---------------------------------------------------------------------===//. We generate significantly worse code for this than GCC:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=21150; http://gcc.gnu.org/bugzilla/attachment.cgi?id=8701. There is also one case we do worse on PPC. //===---------------------------------------------------------------------===//. Fo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:6761,Testability,test,test,6761,"else; return 4;; }. _test:; 	movl 8(%esp), %eax; 	movaps (%eax), %xmm0; 	movl 4(%esp), %eax; 	movaps (%eax), %xmm1; 	comiss %xmm0, %xmm1; 	setae %al; 	movzbl %al, %ecx; 	movl $3, %eax; 	movl $4, %edx; 	cmpl $0, %ecx; 	cmove %edx, %eax; 	ret. Note the setae, movzbl, cmpl, cmove can be replaced with a single cmovae. There; are a number of issues. 1) We are introducing a setcc between the result of the; intrisic call and select. 2) The intrinsic is expected to produce a i32 value; so a any extend (which becomes a zero extend) is added. We probably need some kind of target DAG combine hook to fix this. //===---------------------------------------------------------------------===//. We generate significantly worse code for this than GCC:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=21150; http://gcc.gnu.org/bugzilla/attachment.cgi?id=8701. There is also one case we do worse on PPC. //===---------------------------------------------------------------------===//. For this:. int test(int a); {; return a * 3;; }. We currently emits; 	imull $3, 4(%esp), %eax. Perhaps this is what we really should generate is? Is imull three or four; cycles? Note: ICC generates this:; 	movl	4(%esp), %eax; 	leal	(%eax,%eax,2), %eax. The current instruction priority is based on pattern complexity. The former is; more ""complex"" because it folds a load so the latter will not be emitted. Perhaps we should use AddedComplexity to give LEA32r a higher priority? We; should always try to match LEA first since the LEA matching code does some; estimate to determine whether the match is profitable. However, if we care more about code size, then imull is better. It's two bytes; shorter than movl + leal. On a Pentium M, both variants have the same characteristics with regard; to throughput; however, the multiplication has a latency of four cycles, as; opposed to two cycles for the movl+lea variant. //===---------------------------------------------------------------------===//. It appears gcc place string da",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:11753,Testability,test,test,11753,"dition to potential cache misses, this can't help decoding as I; imagine there has to be some kind of complicated decoder reset and realignment; to grab the bytes from the next cacheline. 532 532 0x3cfc movb (1809(%esp, %esi), %bl <<<--- spans 2 64 byte lines; 942 942 0x3d03 movl %dh, (1809(%esp, %esi); 937 937 0x3d0a incl %esi; 3 3 0x3d0b cmpb %bl, %dl; 27 27 0x3d0d jnz 0x000062db <main+11707>. //===---------------------------------------------------------------------===//. In c99 mode, the preprocessor doesn't like assembly comments like #TRUNCATE. //===---------------------------------------------------------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===---------------------------------------------------------------------===//. This code:. void test(int X) {; if (X) abort();; }. is currently compiled to:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne LBB1_1; addl $12, %esp; ret; LBB1_1:; call L_abort$stub. It would be better to produce:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne L_abort$stub; addl $12, %esp; ret. This can be applied to any no-return function call that takes no arguments etc.; Alternatively, the stack save/restore logic could be shrink-wrapped, producing; something like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; subl $12, %esp; call L_abort$stub. Both are useful in different situations. Finally, it could be shrink-wrapped; and tail called, like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; pop %eax # realign stack.; call L_abort$stub. Though this probably isn't worth it. //===---------------------------------------------------------------------===//. Sometimes it is better to codegen subtractions from a constant (e.g. 7-x) with; a neg instead of a sub instruction. Consider:. int test(char ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:11913,Testability,stub,stub,11913,"of complicated decoder reset and realignment; to grab the bytes from the next cacheline. 532 532 0x3cfc movb (1809(%esp, %esi), %bl <<<--- spans 2 64 byte lines; 942 942 0x3d03 movl %dh, (1809(%esp, %esi); 937 937 0x3d0a incl %esi; 3 3 0x3d0b cmpb %bl, %dl; 27 27 0x3d0d jnz 0x000062db <main+11707>. //===---------------------------------------------------------------------===//. In c99 mode, the preprocessor doesn't like assembly comments like #TRUNCATE. //===---------------------------------------------------------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===---------------------------------------------------------------------===//. This code:. void test(int X) {; if (X) abort();; }. is currently compiled to:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne LBB1_1; addl $12, %esp; ret; LBB1_1:; call L_abort$stub. It would be better to produce:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne L_abort$stub; addl $12, %esp; ret. This can be applied to any no-return function call that takes no arguments etc.; Alternatively, the stack save/restore logic could be shrink-wrapped, producing; something like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; subl $12, %esp; call L_abort$stub. Both are useful in different situations. Finally, it could be shrink-wrapped; and tail called, like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; pop %eax # realign stack.; call L_abort$stub. Though this probably isn't worth it. //===---------------------------------------------------------------------===//. Sometimes it is better to codegen subtractions from a constant (e.g. 7-x) with; a neg instead of a sub instruction. Consider:. int test(char X) { return 7-X; }. we currently produce:; _test:; movl $7, %eax; movsbl 4(%esp), %ecx; subl %ecx, ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:12006,Testability,stub,stub,12006,"), %bl <<<--- spans 2 64 byte lines; 942 942 0x3d03 movl %dh, (1809(%esp, %esi); 937 937 0x3d0a incl %esi; 3 3 0x3d0b cmpb %bl, %dl; 27 27 0x3d0d jnz 0x000062db <main+11707>. //===---------------------------------------------------------------------===//. In c99 mode, the preprocessor doesn't like assembly comments like #TRUNCATE. //===---------------------------------------------------------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===---------------------------------------------------------------------===//. This code:. void test(int X) {; if (X) abort();; }. is currently compiled to:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne LBB1_1; addl $12, %esp; ret; LBB1_1:; call L_abort$stub. It would be better to produce:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne L_abort$stub; addl $12, %esp; ret. This can be applied to any no-return function call that takes no arguments etc.; Alternatively, the stack save/restore logic could be shrink-wrapped, producing; something like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; subl $12, %esp; call L_abort$stub. Both are useful in different situations. Finally, it could be shrink-wrapped; and tail called, like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; pop %eax # realign stack.; call L_abort$stub. Though this probably isn't worth it. //===---------------------------------------------------------------------===//. Sometimes it is better to codegen subtractions from a constant (e.g. 7-x) with; a neg instead of a sub instruction. Consider:. int test(char X) { return 7-X; }. we currently produce:; _test:; movl $7, %eax; movsbl 4(%esp), %ecx; subl %ecx, %eax; ret. We would use one fewer register if codegen'd as:. movsbl 4(%esp), %eax; 	neg %eax; add $7, %eax; ret. Note that th",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:12152,Testability,log,logic,12152,">. //===---------------------------------------------------------------------===//. In c99 mode, the preprocessor doesn't like assembly comments like #TRUNCATE. //===---------------------------------------------------------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===---------------------------------------------------------------------===//. This code:. void test(int X) {; if (X) abort();; }. is currently compiled to:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne LBB1_1; addl $12, %esp; ret; LBB1_1:; call L_abort$stub. It would be better to produce:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne L_abort$stub; addl $12, %esp; ret. This can be applied to any no-return function call that takes no arguments etc.; Alternatively, the stack save/restore logic could be shrink-wrapped, producing; something like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; subl $12, %esp; call L_abort$stub. Both are useful in different situations. Finally, it could be shrink-wrapped; and tail called, like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; pop %eax # realign stack.; call L_abort$stub. Though this probably isn't worth it. //===---------------------------------------------------------------------===//. Sometimes it is better to codegen subtractions from a constant (e.g. 7-x) with; a neg instead of a sub instruction. Consider:. int test(char X) { return 7-X; }. we currently produce:; _test:; movl $7, %eax; movsbl 4(%esp), %ecx; subl %ecx, %eax; ret. We would use one fewer register if codegen'd as:. movsbl 4(%esp), %eax; 	neg %eax; add $7, %eax; ret. Note that this isn't beneficial if the load can be folded into the sub. In; this case, we want a sub:. int test(int X) { return 7-X; }; _test:; movl $7, %eax; subl 4(%esp), %eax; ret. /",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:12297,Testability,stub,stub,12297,", the preprocessor doesn't like assembly comments like #TRUNCATE. //===---------------------------------------------------------------------===//. This could be a single 16-bit load. int f(char *p) {; if ((p[0] == 1) & (p[1] == 2)) return 1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===---------------------------------------------------------------------===//. This code:. void test(int X) {; if (X) abort();; }. is currently compiled to:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne LBB1_1; addl $12, %esp; ret; LBB1_1:; call L_abort$stub. It would be better to produce:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne L_abort$stub; addl $12, %esp; ret. This can be applied to any no-return function call that takes no arguments etc.; Alternatively, the stack save/restore logic could be shrink-wrapped, producing; something like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; subl $12, %esp; call L_abort$stub. Both are useful in different situations. Finally, it could be shrink-wrapped; and tail called, like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; pop %eax # realign stack.; call L_abort$stub. Though this probably isn't worth it. //===---------------------------------------------------------------------===//. Sometimes it is better to codegen subtractions from a constant (e.g. 7-x) with; a neg instead of a sub instruction. Consider:. int test(char X) { return 7-X; }. we currently produce:; _test:; movl $7, %eax; movsbl 4(%esp), %ecx; subl %ecx, %eax; ret. We would use one fewer register if codegen'd as:. movsbl 4(%esp), %eax; 	neg %eax; add $7, %eax; ret. Note that this isn't beneficial if the load can be folded into the sub. In; this case, we want a sub:. int test(int X) { return 7-X; }; _test:; movl $7, %eax; subl 4(%esp), %eax; ret. //===---------------------------------------------------------------------===//. Leaf functions ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:12502,Testability,stub,stub,12502,"1;; return 0;; }. //===---------------------------------------------------------------------===//. We should inline lrintf and probably other libc functions. //===---------------------------------------------------------------------===//. This code:. void test(int X) {; if (X) abort();; }. is currently compiled to:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne LBB1_1; addl $12, %esp; ret; LBB1_1:; call L_abort$stub. It would be better to produce:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne L_abort$stub; addl $12, %esp; ret. This can be applied to any no-return function call that takes no arguments etc.; Alternatively, the stack save/restore logic could be shrink-wrapped, producing; something like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; subl $12, %esp; call L_abort$stub. Both are useful in different situations. Finally, it could be shrink-wrapped; and tail called, like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; pop %eax # realign stack.; call L_abort$stub. Though this probably isn't worth it. //===---------------------------------------------------------------------===//. Sometimes it is better to codegen subtractions from a constant (e.g. 7-x) with; a neg instead of a sub instruction. Consider:. int test(char X) { return 7-X; }. we currently produce:; _test:; movl $7, %eax; movsbl 4(%esp), %ecx; subl %ecx, %eax; ret. We would use one fewer register if codegen'd as:. movsbl 4(%esp), %eax; 	neg %eax; add $7, %eax; ret. Note that this isn't beneficial if the load can be folded into the sub. In; this case, we want a sub:. int test(int X) { return 7-X; }; _test:; movl $7, %eax; subl 4(%esp), %eax; ret. //===---------------------------------------------------------------------===//. Leaf functions that require one 4-byte spill slot have a prolog like this:. _foo:; pushl %esi; subl $4, %esp; ...; and an epilog like this:; addl $4, %esp; popl %esi; ret. It would be smaller, and potentially faster, to push eax on entry and to; pop int",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:12757,Testability,test,test,12757,"f (X) abort();; }. is currently compiled to:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne LBB1_1; addl $12, %esp; ret; LBB1_1:; call L_abort$stub. It would be better to produce:. _test:; subl $12, %esp; cmpl $0, 16(%esp); jne L_abort$stub; addl $12, %esp; ret. This can be applied to any no-return function call that takes no arguments etc.; Alternatively, the stack save/restore logic could be shrink-wrapped, producing; something like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; subl $12, %esp; call L_abort$stub. Both are useful in different situations. Finally, it could be shrink-wrapped; and tail called, like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; pop %eax # realign stack.; call L_abort$stub. Though this probably isn't worth it. //===---------------------------------------------------------------------===//. Sometimes it is better to codegen subtractions from a constant (e.g. 7-x) with; a neg instead of a sub instruction. Consider:. int test(char X) { return 7-X; }. we currently produce:; _test:; movl $7, %eax; movsbl 4(%esp), %ecx; subl %ecx, %eax; ret. We would use one fewer register if codegen'd as:. movsbl 4(%esp), %eax; 	neg %eax; add $7, %eax; ret. Note that this isn't beneficial if the load can be folded into the sub. In; this case, we want a sub:. int test(int X) { return 7-X; }; _test:; movl $7, %eax; subl 4(%esp), %eax; ret. //===---------------------------------------------------------------------===//. Leaf functions that require one 4-byte spill slot have a prolog like this:. _foo:; pushl %esi; subl $4, %esp; ...; and an epilog like this:; addl $4, %esp; popl %esi; ret. It would be smaller, and potentially faster, to push eax on entry and to; pop into a dummy register instead of using addl/subl of esp. Just don't pop ; into any return registers :). //===---------------------------------------------------------------------===//. The X86 backend should fold (branch (or (setcc, setcc))) into multiple ; branches. We gene",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:13086,Testability,test,test,13086,"tively, the stack save/restore logic could be shrink-wrapped, producing; something like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; subl $12, %esp; call L_abort$stub. Both are useful in different situations. Finally, it could be shrink-wrapped; and tail called, like this:. _test:; cmpl $0, 4(%esp); jne LBB1_1; ret; LBB1_1:; pop %eax # realign stack.; call L_abort$stub. Though this probably isn't worth it. //===---------------------------------------------------------------------===//. Sometimes it is better to codegen subtractions from a constant (e.g. 7-x) with; a neg instead of a sub instruction. Consider:. int test(char X) { return 7-X; }. we currently produce:; _test:; movl $7, %eax; movsbl 4(%esp), %ecx; subl %ecx, %eax; ret. We would use one fewer register if codegen'd as:. movsbl 4(%esp), %eax; 	neg %eax; add $7, %eax; ret. Note that this isn't beneficial if the load can be folded into the sub. In; this case, we want a sub:. int test(int X) { return 7-X; }; _test:; movl $7, %eax; subl 4(%esp), %eax; ret. //===---------------------------------------------------------------------===//. Leaf functions that require one 4-byte spill slot have a prolog like this:. _foo:; pushl %esi; subl $4, %esp; ...; and an epilog like this:; addl $4, %esp; popl %esi; ret. It would be smaller, and potentially faster, to push eax on entry and to; pop into a dummy register instead of using addl/subl of esp. Just don't pop ; into any return registers :). //===---------------------------------------------------------------------===//. The X86 backend should fold (branch (or (setcc, setcc))) into multiple ; branches. We generate really poor code for:. double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. For example, the entry BB is:. _testf:; subl $20, %esp; pxor %xmm0, %xmm0; movsd 24(%esp), %xmm1; ucomisd %xmm0, %xmm1; setnp %al; sete %cl; testb %cl, %al; jne LBB1_5 # UnifiedReturnBlock; LBB1_1: # cond_true. it would be better to replace the la",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:13804,Testability,test,testf,13804,"; movl $7, %eax; movsbl 4(%esp), %ecx; subl %ecx, %eax; ret. We would use one fewer register if codegen'd as:. movsbl 4(%esp), %eax; 	neg %eax; add $7, %eax; ret. Note that this isn't beneficial if the load can be folded into the sub. In; this case, we want a sub:. int test(int X) { return 7-X; }; _test:; movl $7, %eax; subl 4(%esp), %eax; ret. //===---------------------------------------------------------------------===//. Leaf functions that require one 4-byte spill slot have a prolog like this:. _foo:; pushl %esi; subl $4, %esp; ...; and an epilog like this:; addl $4, %esp; popl %esi; ret. It would be smaller, and potentially faster, to push eax on entry and to; pop into a dummy register instead of using addl/subl of esp. Just don't pop ; into any return registers :). //===---------------------------------------------------------------------===//. The X86 backend should fold (branch (or (setcc, setcc))) into multiple ; branches. We generate really poor code for:. double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. For example, the entry BB is:. _testf:; subl $20, %esp; pxor %xmm0, %xmm0; movsd 24(%esp), %xmm1; ucomisd %xmm0, %xmm1; setnp %al; sete %cl; testb %cl, %al; jne LBB1_5 # UnifiedReturnBlock; LBB1_1: # cond_true. it would be better to replace the last four instructions with:. 	jp LBB1_1; 	je LBB1_5; LBB1_1:. We also codegen the inner ?: into a diamond:. cvtss2sd LCPI1_0(%rip), %xmm2; cvtss2sd LCPI1_1(%rip), %xmm3; ucomisd %xmm1, %xmm0; ja LBB1_3 # cond_true; LBB1_2: # cond_true; movapd %xmm3, %xmm2; LBB1_3: # cond_true; movapd %xmm2, %xmm0; ret. We should sink the load into xmm3 into the LBB1_2 block. This should; be pretty easy, and will nuke all the copies. //===---------------------------------------------------------------------===//. This:; #include <algorithm>; inline std::pair<unsigned, bool> full_add(unsigned a, unsigned b); { return std::make_pair(a + b, a + b < a); }; bool no_overflow(unsigned a, unsigned b); { return !f",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:14016,Testability,test,testb,14016,"sn't beneficial if the load can be folded into the sub. In; this case, we want a sub:. int test(int X) { return 7-X; }; _test:; movl $7, %eax; subl 4(%esp), %eax; ret. //===---------------------------------------------------------------------===//. Leaf functions that require one 4-byte spill slot have a prolog like this:. _foo:; pushl %esi; subl $4, %esp; ...; and an epilog like this:; addl $4, %esp; popl %esi; ret. It would be smaller, and potentially faster, to push eax on entry and to; pop into a dummy register instead of using addl/subl of esp. Just don't pop ; into any return registers :). //===---------------------------------------------------------------------===//. The X86 backend should fold (branch (or (setcc, setcc))) into multiple ; branches. We generate really poor code for:. double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. For example, the entry BB is:. _testf:; subl $20, %esp; pxor %xmm0, %xmm0; movsd 24(%esp), %xmm1; ucomisd %xmm0, %xmm1; setnp %al; sete %cl; testb %cl, %al; jne LBB1_5 # UnifiedReturnBlock; LBB1_1: # cond_true. it would be better to replace the last four instructions with:. 	jp LBB1_1; 	je LBB1_5; LBB1_1:. We also codegen the inner ?: into a diamond:. cvtss2sd LCPI1_0(%rip), %xmm2; cvtss2sd LCPI1_1(%rip), %xmm3; ucomisd %xmm1, %xmm0; ja LBB1_3 # cond_true; LBB1_2: # cond_true; movapd %xmm3, %xmm2; LBB1_3: # cond_true; movapd %xmm2, %xmm0; ret. We should sink the load into xmm3 into the LBB1_2 block. This should; be pretty easy, and will nuke all the copies. //===---------------------------------------------------------------------===//. This:; #include <algorithm>; inline std::pair<unsigned, bool> full_add(unsigned a, unsigned b); { return std::make_pair(a + b, a + b < a); }; bool no_overflow(unsigned a, unsigned b); { return !full_add(a, b).second; }. Should compile to:; 	addl	%esi, %edi; 	setae	%al; 	movzbl	%al, %eax; 	ret. on x86-64, instead of the rather stupid-looking:; 	addl	%esi, %edi; 	setb	%al;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:16910,Testability,test,testw,16910,"p); 	movswl	-52(%ebp), %eax; 	movl	%ecx, -84(%ebp); 	subl	%eax, -84(%ebp); 	movswl	-70(%ebp), %eax; 	movl	%ecx, -88(%ebp); 	subl	%eax, -88(%ebp); 	movswl	-50(%ebp), %eax; 	subl	%eax, %ecx; 	movl	%ecx, -76(%ebp); 	movswl	-42(%ebp), %eax; 	movl	%eax, -92(%ebp); 	movswl	-66(%ebp), %eax; 	movl	%eax, -96(%ebp); 	movw	$0, -98(%ebp). This appears to be bad because the RA is not folding the store to the stack ; slot into the movl. The above instructions could be:; 	movl $32, -80(%ebp); ...; 	movl $32, -84(%ebp); ...; This seems like a cross between remat and spill folding. This has redundant subtractions of %eax from a stack slot. However, %ecx doesn't; change, so we could simply subtract %eax from %ecx first and then use %ecx (or; vice-versa). //===---------------------------------------------------------------------===//. This code:. 	%tmp659 = icmp slt i16 %tmp654, 0		; <i1> [#uses=1]; 	br i1 %tmp659, label %cond_true662, label %cond_next715. produces this:. 	testw	%cx, %cx; 	movswl	%cx, %esi; 	jns	LBB4_109	# cond_next715. Shark tells us that using %cx in the testw instruction is sub-optimal. It; suggests using the 32-bit register (which is what ICC uses). //===---------------------------------------------------------------------===//. We compile this:. void compare (long long foo) {; if (foo < 4294967297LL); abort();; }. to:. compare:; subl $4, %esp; cmpl $0, 8(%esp); setne %al; movzbw %al, %ax; cmpl $1, 12(%esp); setg %cl; movzbw %cl, %cx; cmove %ax, %cx; testb $1, %cl; jne .LBB1_2 # UnifiedReturnBlock; .LBB1_1: # ifthen; call abort; .LBB1_2: # UnifiedReturnBlock; addl $4, %esp; ret. (also really horrible code on ppc). This is due to the expand code for 64-bit; compares. GCC produces multiple branches, which is much nicer:. compare:; subl $12, %esp; movl 20(%esp), %edx; movl 16(%esp), %eax; decl %edx; jle .L7; .L5:; addl $12, %esp; ret; .p2align 4,,7; .L7:; jl .L4; cmpl $0, %eax; .p2align 4,,8; ja .L5; .L4:; .p2align 4,,9; call abort. //===-----------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:17012,Testability,test,testw,17012,"%ebp); 	movswl	-70(%ebp), %eax; 	movl	%ecx, -88(%ebp); 	subl	%eax, -88(%ebp); 	movswl	-50(%ebp), %eax; 	subl	%eax, %ecx; 	movl	%ecx, -76(%ebp); 	movswl	-42(%ebp), %eax; 	movl	%eax, -92(%ebp); 	movswl	-66(%ebp), %eax; 	movl	%eax, -96(%ebp); 	movw	$0, -98(%ebp). This appears to be bad because the RA is not folding the store to the stack ; slot into the movl. The above instructions could be:; 	movl $32, -80(%ebp); ...; 	movl $32, -84(%ebp); ...; This seems like a cross between remat and spill folding. This has redundant subtractions of %eax from a stack slot. However, %ecx doesn't; change, so we could simply subtract %eax from %ecx first and then use %ecx (or; vice-versa). //===---------------------------------------------------------------------===//. This code:. 	%tmp659 = icmp slt i16 %tmp654, 0		; <i1> [#uses=1]; 	br i1 %tmp659, label %cond_true662, label %cond_next715. produces this:. 	testw	%cx, %cx; 	movswl	%cx, %esi; 	jns	LBB4_109	# cond_next715. Shark tells us that using %cx in the testw instruction is sub-optimal. It; suggests using the 32-bit register (which is what ICC uses). //===---------------------------------------------------------------------===//. We compile this:. void compare (long long foo) {; if (foo < 4294967297LL); abort();; }. to:. compare:; subl $4, %esp; cmpl $0, 8(%esp); setne %al; movzbw %al, %ax; cmpl $1, 12(%esp); setg %cl; movzbw %cl, %cx; cmove %ax, %cx; testb $1, %cl; jne .LBB1_2 # UnifiedReturnBlock; .LBB1_1: # ifthen; call abort; .LBB1_2: # UnifiedReturnBlock; addl $4, %esp; ret. (also really horrible code on ppc). This is due to the expand code for 64-bit; compares. GCC produces multiple branches, which is much nicer:. compare:; subl $12, %esp; movl 20(%esp), %edx; movl 16(%esp), %eax; decl %edx; jle .L7; .L5:; addl $12, %esp; ret; .p2align 4,,7; .L7:; jl .L4; cmpl $0, %eax; .p2align 4,,8; ja .L5; .L4:; .p2align 4,,9; call abort. //===---------------------------------------------------------------------===//. Tail call optimization",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:17418,Testability,test,testb,17418," movl. The above instructions could be:; 	movl $32, -80(%ebp); ...; 	movl $32, -84(%ebp); ...; This seems like a cross between remat and spill folding. This has redundant subtractions of %eax from a stack slot. However, %ecx doesn't; change, so we could simply subtract %eax from %ecx first and then use %ecx (or; vice-versa). //===---------------------------------------------------------------------===//. This code:. 	%tmp659 = icmp slt i16 %tmp654, 0		; <i1> [#uses=1]; 	br i1 %tmp659, label %cond_true662, label %cond_next715. produces this:. 	testw	%cx, %cx; 	movswl	%cx, %esi; 	jns	LBB4_109	# cond_next715. Shark tells us that using %cx in the testw instruction is sub-optimal. It; suggests using the 32-bit register (which is what ICC uses). //===---------------------------------------------------------------------===//. We compile this:. void compare (long long foo) {; if (foo < 4294967297LL); abort();; }. to:. compare:; subl $4, %esp; cmpl $0, 8(%esp); setne %al; movzbw %al, %ax; cmpl $1, 12(%esp); setg %cl; movzbw %cl, %cx; cmove %ax, %cx; testb $1, %cl; jne .LBB1_2 # UnifiedReturnBlock; .LBB1_1: # ifthen; call abort; .LBB1_2: # UnifiedReturnBlock; addl $4, %esp; ret. (also really horrible code on ppc). This is due to the expand code for 64-bit; compares. GCC produces multiple branches, which is much nicer:. compare:; subl $12, %esp; movl 20(%esp), %edx; movl 16(%esp), %eax; decl %edx; jle .L7; .L5:; addl $12, %esp; ret; .p2align 4,,7; .L7:; jl .L4; cmpl $0, %eax; .p2align 4,,8; ja .L5; .L4:; .p2align 4,,9; call abort. //===---------------------------------------------------------------------===//. Tail call optimization improvements: Tail call optimization currently; pushes all arguments on the top of the stack (their normal place for; non-tail call optimized calls) that source from the callers arguments; or that source from a virtual register (also possibly sourcing from; callers arguments).; This is done to prevent overwriting of parameters (see example; below) ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:19705,Testability,stub,stub,19705,"stack slot of callee function would overwrite; arg2 of the caller. Possible optimizations:. - Analyse the actual parameters of the callee to see which would; overwrite a caller parameter which is used by the callee and only; push them onto the top of the stack. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg1,arg2);; }. Here we don't need to write any variables to the top of the stack; since they don't overwrite each other. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg2,arg1);; }. Here we need to push the arguments because they overwrite each; other. //===---------------------------------------------------------------------===//. main (); {; int i = 0;; unsigned long int z = 0;. do {; z -= 0x00004000;; i++;; if (i > 0x00040000); abort ();; } while (z > 0);; exit (0);; }. gcc compiles this to:. _main:; 	subl	$28, %esp; 	xorl	%eax, %eax; 	jmp	L2; L3:; 	cmpl	$262144, %eax; 	je	L10; L2:; 	addl	$1, %eax; 	cmpl	$262145, %eax; 	jne	L3; 	call	L_abort$stub; L10:; 	movl	$0, (%esp); 	call	L_exit$stub. llvm:. _main:; 	subl	$12, %esp; 	movl	$1, %eax; 	movl	$16384, %ecx; LBB1_1:	# bb; 	cmpl	$262145, %eax; 	jge	LBB1_4	# cond_true; LBB1_2:	# cond_next; 	incl	%eax; 	addl	$4294950912, %ecx; 	cmpl	$16384, %ecx; 	jne	LBB1_1	# bb; LBB1_3:	# bb11; 	xorl	%eax, %eax; 	addl	$12, %esp; 	ret; LBB1_4:	# cond_true; 	call	L_abort$stub. 1. LSR should rewrite the first cmp with induction variable %ecx.; 2. DAG combiner should fold; leal 1(%eax), %edx; cmpl $262145, %edx; =>; cmpl $262144, %eax. //===---------------------------------------------------------------------===//. define i64 @test(double %X) {; 	%Y = fptosi double %X to i64; 	ret i64 %Y; }. compiles to:. _test:; 	subl	$20, %esp; 	movsd	24(%esp), %xmm0; 	movsd	%xmm0, 8(%esp); 	fldl	8(%esp); 	fisttpll	(%esp); 	movl	4(%esp), %edx; 	movl	(%esp), %eax; 	addl	$20, %esp; 	#FP_REG_KILL; 	ret. This should just fldl directly from the input stack slot. //",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:19748,Testability,stub,stub,19748,"stack slot of callee function would overwrite; arg2 of the caller. Possible optimizations:. - Analyse the actual parameters of the callee to see which would; overwrite a caller parameter which is used by the callee and only; push them onto the top of the stack. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg1,arg2);; }. Here we don't need to write any variables to the top of the stack; since they don't overwrite each other. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg2,arg1);; }. Here we need to push the arguments because they overwrite each; other. //===---------------------------------------------------------------------===//. main (); {; int i = 0;; unsigned long int z = 0;. do {; z -= 0x00004000;; i++;; if (i > 0x00040000); abort ();; } while (z > 0);; exit (0);; }. gcc compiles this to:. _main:; 	subl	$28, %esp; 	xorl	%eax, %eax; 	jmp	L2; L3:; 	cmpl	$262144, %eax; 	je	L10; L2:; 	addl	$1, %eax; 	cmpl	$262145, %eax; 	jne	L3; 	call	L_abort$stub; L10:; 	movl	$0, (%esp); 	call	L_exit$stub. llvm:. _main:; 	subl	$12, %esp; 	movl	$1, %eax; 	movl	$16384, %ecx; LBB1_1:	# bb; 	cmpl	$262145, %eax; 	jge	LBB1_4	# cond_true; LBB1_2:	# cond_next; 	incl	%eax; 	addl	$4294950912, %ecx; 	cmpl	$16384, %ecx; 	jne	LBB1_1	# bb; LBB1_3:	# bb11; 	xorl	%eax, %eax; 	addl	$12, %esp; 	ret; LBB1_4:	# cond_true; 	call	L_abort$stub. 1. LSR should rewrite the first cmp with induction variable %ecx.; 2. DAG combiner should fold; leal 1(%eax), %edx; cmpl $262145, %edx; =>; cmpl $262144, %eax. //===---------------------------------------------------------------------===//. define i64 @test(double %X) {; 	%Y = fptosi double %X to i64; 	ret i64 %Y; }. compiles to:. _test:; 	subl	$20, %esp; 	movsd	24(%esp), %xmm0; 	movsd	%xmm0, 8(%esp); 	fldl	8(%esp); 	fisttpll	(%esp); 	movl	4(%esp), %edx; 	movl	(%esp), %eax; 	addl	$20, %esp; 	#FP_REG_KILL; 	ret. This should just fldl directly from the input stack slot. //",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:20070,Testability,stub,stub,20070," callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg1,arg2);; }. Here we don't need to write any variables to the top of the stack; since they don't overwrite each other. int callee (int32 arg1, int32 arg2);; int caller (int32 arg1, int32 arg2) {; return callee(arg2,arg1);; }. Here we need to push the arguments because they overwrite each; other. //===---------------------------------------------------------------------===//. main (); {; int i = 0;; unsigned long int z = 0;. do {; z -= 0x00004000;; i++;; if (i > 0x00040000); abort ();; } while (z > 0);; exit (0);; }. gcc compiles this to:. _main:; 	subl	$28, %esp; 	xorl	%eax, %eax; 	jmp	L2; L3:; 	cmpl	$262144, %eax; 	je	L10; L2:; 	addl	$1, %eax; 	cmpl	$262145, %eax; 	jne	L3; 	call	L_abort$stub; L10:; 	movl	$0, (%esp); 	call	L_exit$stub. llvm:. _main:; 	subl	$12, %esp; 	movl	$1, %eax; 	movl	$16384, %ecx; LBB1_1:	# bb; 	cmpl	$262145, %eax; 	jge	LBB1_4	# cond_true; LBB1_2:	# cond_next; 	incl	%eax; 	addl	$4294950912, %ecx; 	cmpl	$16384, %ecx; 	jne	LBB1_1	# bb; LBB1_3:	# bb11; 	xorl	%eax, %eax; 	addl	$12, %esp; 	ret; LBB1_4:	# cond_true; 	call	L_abort$stub. 1. LSR should rewrite the first cmp with induction variable %ecx.; 2. DAG combiner should fold; leal 1(%eax), %edx; cmpl $262145, %edx; =>; cmpl $262144, %eax. //===---------------------------------------------------------------------===//. define i64 @test(double %X) {; 	%Y = fptosi double %X to i64; 	ret i64 %Y; }. compiles to:. _test:; 	subl	$20, %esp; 	movsd	24(%esp), %xmm0; 	movsd	%xmm0, 8(%esp); 	fldl	8(%esp); 	fisttpll	(%esp); 	movl	4(%esp), %edx; 	movl	(%esp), %eax; 	addl	$20, %esp; 	#FP_REG_KILL; 	ret. This should just fldl directly from the input stack slot. //===---------------------------------------------------------------------===//. This code:; int foo (int x) { return (x & 65535) | 255; }. Should compile into:. _foo:; movzwl 4(%esp), %eax; orl $255, %eax; ret. instead of:; _foo:; 	movl	$65280, %eax; 	andl	4(%esp), ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:20329,Testability,test,test,20329,"------------------------===//. main (); {; int i = 0;; unsigned long int z = 0;. do {; z -= 0x00004000;; i++;; if (i > 0x00040000); abort ();; } while (z > 0);; exit (0);; }. gcc compiles this to:. _main:; 	subl	$28, %esp; 	xorl	%eax, %eax; 	jmp	L2; L3:; 	cmpl	$262144, %eax; 	je	L10; L2:; 	addl	$1, %eax; 	cmpl	$262145, %eax; 	jne	L3; 	call	L_abort$stub; L10:; 	movl	$0, (%esp); 	call	L_exit$stub. llvm:. _main:; 	subl	$12, %esp; 	movl	$1, %eax; 	movl	$16384, %ecx; LBB1_1:	# bb; 	cmpl	$262145, %eax; 	jge	LBB1_4	# cond_true; LBB1_2:	# cond_next; 	incl	%eax; 	addl	$4294950912, %ecx; 	cmpl	$16384, %ecx; 	jne	LBB1_1	# bb; LBB1_3:	# bb11; 	xorl	%eax, %eax; 	addl	$12, %esp; 	ret; LBB1_4:	# cond_true; 	call	L_abort$stub. 1. LSR should rewrite the first cmp with induction variable %ecx.; 2. DAG combiner should fold; leal 1(%eax), %edx; cmpl $262145, %edx; =>; cmpl $262144, %eax. //===---------------------------------------------------------------------===//. define i64 @test(double %X) {; 	%Y = fptosi double %X to i64; 	ret i64 %Y; }. compiles to:. _test:; 	subl	$20, %esp; 	movsd	24(%esp), %xmm0; 	movsd	%xmm0, 8(%esp); 	fldl	8(%esp); 	fisttpll	(%esp); 	movl	4(%esp), %edx; 	movl	(%esp), %eax; 	addl	$20, %esp; 	#FP_REG_KILL; 	ret. This should just fldl directly from the input stack slot. //===---------------------------------------------------------------------===//. This code:; int foo (int x) { return (x & 65535) | 255; }. Should compile into:. _foo:; movzwl 4(%esp), %eax; orl $255, %eax; ret. instead of:; _foo:; 	movl	$65280, %eax; 	andl	4(%esp), %eax; 	orl	$255, %eax; 	ret. //===---------------------------------------------------------------------===//. We're codegen'ing multiply of long longs inefficiently:. unsigned long long LLM(unsigned long long arg1, unsigned long long arg2) {; return arg1 * arg2;; }. We compile to (fomit-frame-pointer):. _LLM:; 	pushl	%esi; 	movl	8(%esp), %ecx; 	movl	16(%esp), %esi; 	movl	%esi, %eax; 	mull	%ecx; 	imull	12(%esp), %esi; 	addl	%edx, %esi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:22863,Testability,test,testcase,22863,"------------------------------------------------------===//. We can fold a store into ""zeroing a reg"". Instead of:. xorl %eax, %eax; movl %eax, 124(%esp). we should get:. movl $0, 124(%esp). if the flags of the xor are dead. Likewise, we isel ""x<<1"" into ""add reg,reg"". If reg is spilled, this should; be folded into: shl [mem], 1. //===---------------------------------------------------------------------===//. In SSE mode, we turn abs and neg into a load from the constant pool plus a xor; or and instruction, for example:. 	xorpd	LCPI1_0, %xmm2. However, if xmm2 gets spilled, we end up with really ugly code like this:. 	movsd	(%esp), %xmm0; 	xorpd	LCPI1_0, %xmm0; 	movsd	%xmm0, (%esp). Since we 'know' that this is a 'neg', we can actually ""fold"" the spill into; the neg/abs instruction, turning it into an *integer* operation, like this:. 	xorl 2147483648, [mem+4] ## 2147483648 = (1 << 31). you could also use xorb, but xorl is less likely to lead to a partial register; stall. Here is a contrived testcase:. double a, b, c;; void test(double *P) {; double X = *P;; a = X;; bar();; X = -X;; b = X;; bar();; c = X;; }. //===---------------------------------------------------------------------===//. The generated code on x86 for checking for signed overflow on a multiply the; obvious way is much longer than it needs to be. int x(int a, int b) {; long long prod = (long long)a*b;; return prod > 0x7FFFFFFF || prod < (-0x7FFFFFFF-1);; }. See PR2053 for more details. //===---------------------------------------------------------------------===//. We should investigate using cdq/ctld (effect: edx = sar eax, 31); more aggressively; it should cost the same as a move+shift on any modern; processor, but it's a lot shorter. Downside is that it puts more; pressure on register allocation because it has fixed operands. Example:; int abs(int x) {return x < 0 ? -x : x;}. gcc compiles this to the following when using march/mtune=pentium2/3/4/m/etc.:; abs:; movl 4(%esp), %eax; cltd; xorl %edx, %e",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:22896,Testability,test,test,22896,"ld a store into ""zeroing a reg"". Instead of:. xorl %eax, %eax; movl %eax, 124(%esp). we should get:. movl $0, 124(%esp). if the flags of the xor are dead. Likewise, we isel ""x<<1"" into ""add reg,reg"". If reg is spilled, this should; be folded into: shl [mem], 1. //===---------------------------------------------------------------------===//. In SSE mode, we turn abs and neg into a load from the constant pool plus a xor; or and instruction, for example:. 	xorpd	LCPI1_0, %xmm2. However, if xmm2 gets spilled, we end up with really ugly code like this:. 	movsd	(%esp), %xmm0; 	xorpd	LCPI1_0, %xmm0; 	movsd	%xmm0, (%esp). Since we 'know' that this is a 'neg', we can actually ""fold"" the spill into; the neg/abs instruction, turning it into an *integer* operation, like this:. 	xorl 2147483648, [mem+4] ## 2147483648 = (1 << 31). you could also use xorb, but xorl is less likely to lead to a partial register; stall. Here is a contrived testcase:. double a, b, c;; void test(double *P) {; double X = *P;; a = X;; bar();; X = -X;; b = X;; bar();; c = X;; }. //===---------------------------------------------------------------------===//. The generated code on x86 for checking for signed overflow on a multiply the; obvious way is much longer than it needs to be. int x(int a, int b) {; long long prod = (long long)a*b;; return prod > 0x7FFFFFFF || prod < (-0x7FFFFFFF-1);; }. See PR2053 for more details. //===---------------------------------------------------------------------===//. We should investigate using cdq/ctld (effect: edx = sar eax, 31); more aggressively; it should cost the same as a move+shift on any modern; processor, but it's a lot shorter. Downside is that it puts more; pressure on register allocation because it has fixed operands. Example:; int abs(int x) {return x < 0 ? -x : x;}. gcc compiles this to the following when using march/mtune=pentium2/3/4/m/etc.:; abs:; movl 4(%esp), %eax; cltd; xorl %edx, %eax; subl %edx, %eax; ret. //===---------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:24576,Testability,test,testl,24576,"odern; processor, but it's a lot shorter. Downside is that it puts more; pressure on register allocation because it has fixed operands. Example:; int abs(int x) {return x < 0 ? -x : x;}. gcc compiles this to the following when using march/mtune=pentium2/3/4/m/etc.:; abs:; movl 4(%esp), %eax; cltd; xorl %edx, %eax; subl %edx, %eax; ret. //===---------------------------------------------------------------------===//. Take the following code (from ; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=16541):. extern unsigned char first_one[65536];; int FirstOnet(unsigned long long arg1); {; if (arg1 >> 48); return (first_one[arg1 >> 48]);; return 0;; }. The following code is currently generated:; FirstOnet:; movl 8(%esp), %eax; cmpl $65536, %eax; movl 4(%esp), %ecx; jb .LBB1_2 # UnifiedReturnBlock; .LBB1_1: # ifthen; shrl $16, %eax; movzbl first_one(%eax), %eax; ret; .LBB1_2: # UnifiedReturnBlock; xorl %eax, %eax; ret. We could change the ""movl 8(%esp), %eax"" into ""movzwl 10(%esp), %eax""; this; lets us change the cmpl into a testl, which is shorter, and eliminate the shift. //===---------------------------------------------------------------------===//. We compile this function:. define i32 @foo(i32 %a, i32 %b, i32 %c, i8 zeroext %d) nounwind {; entry:; 	%tmp2 = icmp eq i8 %d, 0		; <i1> [#uses=1]; 	br i1 %tmp2, label %bb7, label %bb. bb:		; preds = %entry; 	%tmp6 = add i32 %b, %a		; <i32> [#uses=1]; 	ret i32 %tmp6. bb7:		; preds = %entry; 	%tmp10 = sub i32 %a, %c		; <i32> [#uses=1]; 	ret i32 %tmp10; }. to:. foo: # @foo; # %bb.0: # %entry; 	movl	4(%esp), %ecx; 	cmpb	$0, 16(%esp); 	je	.LBB0_2; # %bb.1: # %bb; 	movl	8(%esp), %eax; 	addl	%ecx, %eax; 	ret; .LBB0_2: # %bb7; 	movl	12(%esp), %edx; 	movl	%ecx, %eax; 	subl	%edx, %eax; 	ret. There's an obviously unnecessary movl in .LBB0_2, and we could eliminate a; couple more movls by putting 4(%esp) into %eax instead of %ecx. //===---------------------------------------------------------------------===//. Take the following:. target d",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:26504,Testability,test,test,26504," the following:. target datalayout = ""e-p:32:32:32-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:32:64-f32:32:32-f64:32:64-v64:64:64-v128:128:128-a0:0:64-f80:128:128-S128""; target triple = ""i386-apple-darwin8""; @in_exit.4870.b = internal global i1 false		; <i1*> [#uses=2]; define fastcc void @abort_gzip() noreturn nounwind {; entry:; 	%tmp.b.i = load i1* @in_exit.4870.b		; <i1> [#uses=1]; 	br i1 %tmp.b.i, label %bb.i, label %bb4.i; bb.i:		; preds = %entry; 	tail call void @exit( i32 1 ) noreturn nounwind ; 	unreachable; bb4.i:		; preds = %entry; 	store i1 true, i1* @in_exit.4870.b; 	tail call void @exit( i32 1 ) noreturn nounwind ; 	unreachable; }; declare void @exit(i32) noreturn nounwind . This compiles into:; _abort_gzip: ## @abort_gzip; ## %bb.0: ## %entry; 	subl	$12, %esp; 	movb	_in_exit.4870.b, %al; 	cmpb	$1, %al; 	jne	LBB0_2. We somehow miss folding the movb into the cmpb. //===---------------------------------------------------------------------===//. We compile:. int test(int x, int y) {; return x-y-1;; }. into (-m64):. _test:; 	decl	%edi; 	movl	%edi, %eax; 	subl	%esi, %eax; 	ret. it would be better to codegen as: x+~y (notl+addl). //===---------------------------------------------------------------------===//. This code:. int foo(const char *str,...); {; __builtin_va_list a; int x;; __builtin_va_start(a,str); x = __builtin_va_arg(a,int); __builtin_va_end(a);; return x;; }. gets compiled into this on x86-64:; 	subq $200, %rsp; movaps %xmm7, 160(%rsp); movaps %xmm6, 144(%rsp); movaps %xmm5, 128(%rsp); movaps %xmm4, 112(%rsp); movaps %xmm3, 96(%rsp); movaps %xmm2, 80(%rsp); movaps %xmm1, 64(%rsp); movaps %xmm0, 48(%rsp); movq %r9, 40(%rsp); movq %r8, 32(%rsp); movq %rcx, 24(%rsp); movq %rdx, 16(%rsp); movq %rsi, 8(%rsp); leaq (%rsp), %rax; movq %rax, 192(%rsp); leaq 208(%rsp), %rax; movq %rax, 184(%rsp); movl $48, 180(%rsp); movl $8, 176(%rsp); movl 176(%rsp), %eax; cmpl $47, %eax; jbe .LBB1_3 # bb; .LBB1_1: # bb3; movq 184(%rsp), %rcx; leaq 8(%rcx), %rax; movq %rax,",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:30378,Testability,Test,Test,30378,"following:. 	movl	12(%ebp), %eax; 	movl	16(%ebp), %ecx; 	xorl	%edx, %edx; 	divl	%ecx; 	movl	8(%ebp), %eax; 	divl	%ecx; 	movl	%edx, %eax; 	ret. A similar code sequence works for division. //===---------------------------------------------------------------------===//. We currently compile this:. define i32 @func1(i32 %v1, i32 %v2) nounwind {; entry:; %t = call {i32, i1} @llvm.sadd.with.overflow.i32(i32 %v1, i32 %v2); %sum = extractvalue {i32, i1} %t, 0; %obit = extractvalue {i32, i1} %t, 1; br i1 %obit, label %overflow, label %normal; normal:; ret i32 %sum; overflow:; call void @llvm.trap(); unreachable; }; declare {i32, i1} @llvm.sadd.with.overflow.i32(i32, i32); declare void @llvm.trap(). to:. _func1:; 	movl	4(%esp), %eax; 	addl	8(%esp), %eax; 	jo	LBB1_2	## overflow; LBB1_1:	## normal; 	ret; LBB1_2:	## overflow; 	ud2. it would be nice to produce ""into"" someday. //===---------------------------------------------------------------------===//. Test instructions can be eliminated by using EFLAGS values from arithmetic; instructions. This is currently not done for mul, and, or, xor, neg, shl,; sra, srl, shld, shrd, atomic ops, and others. It is also currently not done; for read-modify-write instructions. It is also current not done if the; OF or CF flags are needed. The shift operators have the complication that when the shift count is; zero, EFLAGS is not set, so they can only subsume a test instruction if; the shift count is known to be non-zero. Also, using the EFLAGS value; from a shift is apparently very slow on some x86 implementations. In read-modify-write instructions, the root node in the isel match is; the store, and isel has no way for the use of the EFLAGS result of the; arithmetic to be remapped to the new node. Add and subtract instructions set OF on signed overflow and CF on unsiged; overflow, while test instructions always clear OF and CF. In order to; replace a test with an add or subtract in a situation where OF or CF is; needed, codegen must be able t",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:30829,Testability,test,test,30829,"vm.sadd.with.overflow.i32(i32 %v1, i32 %v2); %sum = extractvalue {i32, i1} %t, 0; %obit = extractvalue {i32, i1} %t, 1; br i1 %obit, label %overflow, label %normal; normal:; ret i32 %sum; overflow:; call void @llvm.trap(); unreachable; }; declare {i32, i1} @llvm.sadd.with.overflow.i32(i32, i32); declare void @llvm.trap(). to:. _func1:; 	movl	4(%esp), %eax; 	addl	8(%esp), %eax; 	jo	LBB1_2	## overflow; LBB1_1:	## normal; 	ret; LBB1_2:	## overflow; 	ud2. it would be nice to produce ""into"" someday. //===---------------------------------------------------------------------===//. Test instructions can be eliminated by using EFLAGS values from arithmetic; instructions. This is currently not done for mul, and, or, xor, neg, shl,; sra, srl, shld, shrd, atomic ops, and others. It is also currently not done; for read-modify-write instructions. It is also current not done if the; OF or CF flags are needed. The shift operators have the complication that when the shift count is; zero, EFLAGS is not set, so they can only subsume a test instruction if; the shift count is known to be non-zero. Also, using the EFLAGS value; from a shift is apparently very slow on some x86 implementations. In read-modify-write instructions, the root node in the isel match is; the store, and isel has no way for the use of the EFLAGS result of the; arithmetic to be remapped to the new node. Add and subtract instructions set OF on signed overflow and CF on unsiged; overflow, while test instructions always clear OF and CF. In order to; replace a test with an add or subtract in a situation where OF or CF is; needed, codegen must be able to prove that the operation cannot see; signed or unsigned overflow, respectively. //===---------------------------------------------------------------------===//. memcpy/memmove do not lower to SSE copies when possible. A silly example is:; define <16 x float> @foo(<16 x float> %A) nounwind {; 	%tmp = alloca <16 x float>, align 16; 	%tmp2 = alloca <16 x float>, align 16; 	",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:31264,Testability,test,test,31264,"verflow; 	ud2. it would be nice to produce ""into"" someday. //===---------------------------------------------------------------------===//. Test instructions can be eliminated by using EFLAGS values from arithmetic; instructions. This is currently not done for mul, and, or, xor, neg, shl,; sra, srl, shld, shrd, atomic ops, and others. It is also currently not done; for read-modify-write instructions. It is also current not done if the; OF or CF flags are needed. The shift operators have the complication that when the shift count is; zero, EFLAGS is not set, so they can only subsume a test instruction if; the shift count is known to be non-zero. Also, using the EFLAGS value; from a shift is apparently very slow on some x86 implementations. In read-modify-write instructions, the root node in the isel match is; the store, and isel has no way for the use of the EFLAGS result of the; arithmetic to be remapped to the new node. Add and subtract instructions set OF on signed overflow and CF on unsiged; overflow, while test instructions always clear OF and CF. In order to; replace a test with an add or subtract in a situation where OF or CF is; needed, codegen must be able to prove that the operation cannot see; signed or unsigned overflow, respectively. //===---------------------------------------------------------------------===//. memcpy/memmove do not lower to SSE copies when possible. A silly example is:; define <16 x float> @foo(<16 x float> %A) nounwind {; 	%tmp = alloca <16 x float>, align 16; 	%tmp2 = alloca <16 x float>, align 16; 	store <16 x float> %A, <16 x float>* %tmp; 	%s = bitcast <16 x float>* %tmp to i8*; 	%s2 = bitcast <16 x float>* %tmp2 to i8*; 	call void @llvm.memcpy.i64(i8* %s, i8* %s2, i64 64, i32 16); 	%R = load <16 x float>* %tmp2; 	ret <16 x float> %R; }. declare void @llvm.memcpy.i64(i8* nocapture, i8* nocapture, i64, i32) nounwind. which compiles to:. _foo:; 	subl	$140, %esp; 	movaps	%xmm3, 112(%esp); 	movaps	%xmm2, 96(%esp); 	movaps	%xmm1, 80(%e",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:31329,Testability,test,test,31329,"liminated by using EFLAGS values from arithmetic; instructions. This is currently not done for mul, and, or, xor, neg, shl,; sra, srl, shld, shrd, atomic ops, and others. It is also currently not done; for read-modify-write instructions. It is also current not done if the; OF or CF flags are needed. The shift operators have the complication that when the shift count is; zero, EFLAGS is not set, so they can only subsume a test instruction if; the shift count is known to be non-zero. Also, using the EFLAGS value; from a shift is apparently very slow on some x86 implementations. In read-modify-write instructions, the root node in the isel match is; the store, and isel has no way for the use of the EFLAGS result of the; arithmetic to be remapped to the new node. Add and subtract instructions set OF on signed overflow and CF on unsiged; overflow, while test instructions always clear OF and CF. In order to; replace a test with an add or subtract in a situation where OF or CF is; needed, codegen must be able to prove that the operation cannot see; signed or unsigned overflow, respectively. //===---------------------------------------------------------------------===//. memcpy/memmove do not lower to SSE copies when possible. A silly example is:; define <16 x float> @foo(<16 x float> %A) nounwind {; 	%tmp = alloca <16 x float>, align 16; 	%tmp2 = alloca <16 x float>, align 16; 	store <16 x float> %A, <16 x float>* %tmp; 	%s = bitcast <16 x float>* %tmp to i8*; 	%s2 = bitcast <16 x float>* %tmp2 to i8*; 	call void @llvm.memcpy.i64(i8* %s, i8* %s2, i64 64, i32 16); 	%R = load <16 x float>* %tmp2; 	ret <16 x float> %R; }. declare void @llvm.memcpy.i64(i8* nocapture, i8* nocapture, i64, i32) nounwind. which compiles to:. _foo:; 	subl	$140, %esp; 	movaps	%xmm3, 112(%esp); 	movaps	%xmm2, 96(%esp); 	movaps	%xmm1, 80(%esp); 	movaps	%xmm0, 64(%esp); 	movl	60(%esp), %eax; 	movl	%eax, 124(%esp); 	movl	56(%esp), %eax; 	movl	%eax, 120(%esp); 	movl	52(%esp), %eax; <many many more 32-bit ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:33343,Testability,Test,Testcase,33343,"p), %eax; 	movl	%eax, 120(%esp); 	movl	52(%esp), %eax; <many many more 32-bit copies>; 	movaps	(%esp), %xmm0; 	movaps	16(%esp), %xmm1; 	movaps	32(%esp), %xmm2; 	movaps	48(%esp), %xmm3; 	addl	$140, %esp; 	ret. On Nehalem, it may even be cheaper to just use movups when unaligned than to; fall back to lower-granularity chunks. //===---------------------------------------------------------------------===//. Implement processor-specific optimizations for parity with GCC on these; processors. GCC does two optimizations:. 1. ix86_pad_returns inserts a noop before ret instructions if immediately; preceded by a conditional branch or is the target of a jump.; 2. ix86_avoid_jump_misspredicts inserts noops in cases where a 16-byte block of; code contains more than 3 branches.; ; The first one is done for all AMDs, Core2, and ""Generic""; The second one is done for: Atom, Pentium Pro, all AMDs, Pentium 4, Nocona,; Core 2, and ""Generic"". //===---------------------------------------------------------------------===//; Testcase:; int x(int a) { return (a&0xf0)>>4; }. Current output:; 	movl	4(%esp), %eax; 	shrl	$4, %eax; 	andl	$15, %eax; 	ret. Ideal output:; 	movzbl	4(%esp), %eax; 	shrl	$4, %eax; 	ret. //===---------------------------------------------------------------------===//. Re-implement atomic builtins __sync_add_and_fetch() and __sync_sub_and_fetch; properly. When the return value is not used (i.e. only care about the value in the; memory), x86 does not have to use add to implement these. Instead, it can use; add, sub, inc, dec instructions with the ""lock"" prefix. This is currently implemented using a bit of instruction selection trick. The; issue is the target independent pattern produces one output and a chain and we; want to map it into one that just output a chain. The current trick is to select; it into a MERGE_VALUES with the first definition being an implicit_def. The; proper solution is to add new ISD opcodes for the no-output variant. DAG; combiner can then transform",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:36306,Testability,test,testb,36306,"abi says:. Booleans, when stored in a memory object, are stored as single byte objects the; value of which is always 0 (false) or 1 (true). We are not using this fact:. int bar(_Bool *a) { return *a; }. define i32 @bar(i8* nocapture %a) nounwind readonly optsize {; %1 = load i8* %a, align 1, !tbaa !0; %tmp = and i8 %1, 1; %2 = zext i8 %tmp to i32; ret i32 %2; }. bar:; movb (%rdi), %al; andb $1, %al; movzbl %al, %eax; ret. GCC produces. bar:; movzbl (%rdi), %eax; ret. //===---------------------------------------------------------------------===//. Take the following C code:; int f(int a, int b) { return (unsigned char)a == (unsigned char)b; }. We generate the following IR with clang:; define i32 @f(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = xor i32 %b, %a ; <i32> [#uses=1]; %tmp6 = and i32 %tmp, 255 ; <i32> [#uses=1]; %cmp = icmp eq i32 %tmp6, 0 ; <i1> [#uses=1]; %conv5 = zext i1 %cmp to i32 ; <i32> [#uses=1]; ret i32 %conv5; }. And the following x86 code:; 	xorl	%esi, %edi; 	testb	$-1, %dil; 	sete	%al; 	movzbl	%al, %eax; 	ret. A cmpb instead of the xorl+testb would be one instruction shorter. //===---------------------------------------------------------------------===//. Given the following C code:; int f(int a, int b) { return (signed char)a == (signed char)b; }. We generate the following IR with clang:; define i32 @f(i32 %a, i32 %b) nounwind readnone {; entry:; %sext = shl i32 %a, 24 ; <i32> [#uses=1]; %conv1 = ashr i32 %sext, 24 ; <i32> [#uses=1]; %sext6 = shl i32 %b, 24 ; <i32> [#uses=1]; %conv4 = ashr i32 %sext6, 24 ; <i32> [#uses=1]; %cmp = icmp eq i32 %conv1, %conv4 ; <i1> [#uses=1]; %conv5 = zext i1 %cmp to i32 ; <i32> [#uses=1]; ret i32 %conv5; }. And the following x86 code:; 	movsbl	%sil, %eax; 	movsbl	%dil, %ecx; 	cmpl	%eax, %ecx; 	sete	%al; 	movzbl	%al, %eax; 	ret. It should be possible to eliminate the sign extensions. //===---------------------------------------------------------------------===//. LLVM misses a load+store narrowing opportunity",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:36386,Testability,test,testb,36386,"ts the; value of which is always 0 (false) or 1 (true). We are not using this fact:. int bar(_Bool *a) { return *a; }. define i32 @bar(i8* nocapture %a) nounwind readonly optsize {; %1 = load i8* %a, align 1, !tbaa !0; %tmp = and i8 %1, 1; %2 = zext i8 %tmp to i32; ret i32 %2; }. bar:; movb (%rdi), %al; andb $1, %al; movzbl %al, %eax; ret. GCC produces. bar:; movzbl (%rdi), %eax; ret. //===---------------------------------------------------------------------===//. Take the following C code:; int f(int a, int b) { return (unsigned char)a == (unsigned char)b; }. We generate the following IR with clang:; define i32 @f(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = xor i32 %b, %a ; <i32> [#uses=1]; %tmp6 = and i32 %tmp, 255 ; <i32> [#uses=1]; %cmp = icmp eq i32 %tmp6, 0 ; <i1> [#uses=1]; %conv5 = zext i1 %cmp to i32 ; <i32> [#uses=1]; ret i32 %conv5; }. And the following x86 code:; 	xorl	%esi, %edi; 	testb	$-1, %dil; 	sete	%al; 	movzbl	%al, %eax; 	ret. A cmpb instead of the xorl+testb would be one instruction shorter. //===---------------------------------------------------------------------===//. Given the following C code:; int f(int a, int b) { return (signed char)a == (signed char)b; }. We generate the following IR with clang:; define i32 @f(i32 %a, i32 %b) nounwind readnone {; entry:; %sext = shl i32 %a, 24 ; <i32> [#uses=1]; %conv1 = ashr i32 %sext, 24 ; <i32> [#uses=1]; %sext6 = shl i32 %b, 24 ; <i32> [#uses=1]; %conv4 = ashr i32 %sext6, 24 ; <i32> [#uses=1]; %cmp = icmp eq i32 %conv1, %conv4 ; <i1> [#uses=1]; %conv5 = zext i1 %cmp to i32 ; <i32> [#uses=1]; ret i32 %conv5; }. And the following x86 code:; 	movsbl	%sil, %eax; 	movsbl	%dil, %ecx; 	cmpl	%eax, %ecx; 	sete	%al; 	movzbl	%al, %eax; 	ret. It should be possible to eliminate the sign extensions. //===---------------------------------------------------------------------===//. LLVM misses a load+store narrowing opportunity in this code:. %struct.bf = type { i64, i16, i16, i32 }. @bfi = external global %st",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:38797,Testability,test,testl,38797,"1; %5 = load %struct.bf** @bfi, align 8 ; <%struct.bf*> [#uses=1]; %6 = getelementptr %struct.bf* %5, i64 0, i32 1 ; <i16*> [#uses=1]; %7 = bitcast i16* %6 to i32* ; <i32*> [#uses=2]; %8 = load i32* %7, align 1 ; <i32> [#uses=1]; %9 = and i32 %8, -131073 ; <i32> [#uses=1]; store i32 %9, i32* %7, align 1; ret void; }. LLVM currently emits this:. movq bfi(%rip), %rax; andl $-65537, 8(%rax); movq bfi(%rip), %rax; andl $-131073, 8(%rax); ret. It could narrow the loads and stores to emit this:. movq bfi(%rip), %rax; andb $-2, 10(%rax); movq bfi(%rip), %rax; andb $-3, 10(%rax); ret. The trouble is that there is a TokenFactor between the store and the; load, making it non-trivial to determine if there's anything between; the load and the store which would prohibit narrowing. //===---------------------------------------------------------------------===//. This code:; void foo(unsigned x) {; if (x == 0) bar();; else if (x == 1) qux();; }. currently compiles into:; _foo:; 	movl	4(%esp), %eax; 	cmpl	$1, %eax; 	je	LBB0_3; 	testl	%eax, %eax; 	jne	LBB0_4. the testl could be removed:; _foo:; 	movl	4(%esp), %eax; 	cmpl	$1, %eax; 	je	LBB0_3; 	jb	LBB0_4. 0 is the only unsigned number < 1. //===---------------------------------------------------------------------===//. This code:. %0 = type { i32, i1 }. define i32 @add32carry(i32 %sum, i32 %x) nounwind readnone ssp {; entry:; %uadd = tail call %0 @llvm.uadd.with.overflow.i32(i32 %sum, i32 %x); %cmp = extractvalue %0 %uadd, 1; %inc = zext i1 %cmp to i32; %add = add i32 %x, %sum; %z.0 = add i32 %add, %inc; ret i32 %z.0; }. declare %0 @llvm.uadd.with.overflow.i32(i32, i32) nounwind readnone. compiles to:. _add32carry: ## @add32carry; 	addl	%esi, %edi; 	sbbl	%ecx, %ecx; 	movl	%edi, %eax; 	subl	%ecx, %eax; 	ret. But it could be:. _add32carry:; 	leal	(%rsi,%rdi), %eax; 	cmpl	%esi, %eax; 	adcl	$0, %eax; 	ret. //===---------------------------------------------------------------------===//. The hot loop of 256.bzip2 contains code that looks a ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:38832,Testability,test,testl,38832," 0, i32 1 ; <i16*> [#uses=1]; %7 = bitcast i16* %6 to i32* ; <i32*> [#uses=2]; %8 = load i32* %7, align 1 ; <i32> [#uses=1]; %9 = and i32 %8, -131073 ; <i32> [#uses=1]; store i32 %9, i32* %7, align 1; ret void; }. LLVM currently emits this:. movq bfi(%rip), %rax; andl $-65537, 8(%rax); movq bfi(%rip), %rax; andl $-131073, 8(%rax); ret. It could narrow the loads and stores to emit this:. movq bfi(%rip), %rax; andb $-2, 10(%rax); movq bfi(%rip), %rax; andb $-3, 10(%rax); ret. The trouble is that there is a TokenFactor between the store and the; load, making it non-trivial to determine if there's anything between; the load and the store which would prohibit narrowing. //===---------------------------------------------------------------------===//. This code:; void foo(unsigned x) {; if (x == 0) bar();; else if (x == 1) qux();; }. currently compiles into:; _foo:; 	movl	4(%esp), %eax; 	cmpl	$1, %eax; 	je	LBB0_3; 	testl	%eax, %eax; 	jne	LBB0_4. the testl could be removed:; _foo:; 	movl	4(%esp), %eax; 	cmpl	$1, %eax; 	je	LBB0_3; 	jb	LBB0_4. 0 is the only unsigned number < 1. //===---------------------------------------------------------------------===//. This code:. %0 = type { i32, i1 }. define i32 @add32carry(i32 %sum, i32 %x) nounwind readnone ssp {; entry:; %uadd = tail call %0 @llvm.uadd.with.overflow.i32(i32 %sum, i32 %x); %cmp = extractvalue %0 %uadd, 1; %inc = zext i1 %cmp to i32; %add = add i32 %x, %sum; %z.0 = add i32 %add, %inc; ret i32 %z.0; }. declare %0 @llvm.uadd.with.overflow.i32(i32, i32) nounwind readnone. compiles to:. _add32carry: ## @add32carry; 	addl	%esi, %edi; 	sbbl	%ecx, %ecx; 	movl	%edi, %eax; 	subl	%ecx, %eax; 	ret. But it could be:. _add32carry:; 	leal	(%rsi,%rdi), %eax; 	cmpl	%esi, %eax; 	adcl	$0, %eax; 	ret. //===---------------------------------------------------------------------===//. The hot loop of 256.bzip2 contains code that looks a bit like this:. int foo(char *P, char *Q, int x, int y) {; if (P[0] != Q[0]); return P[0] < Q[0];; if (P[1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:42434,Testability,test,testl,42434,"the movb+andb+movzbl sequence. //===---------------------------------------------------------------------===//. For the following:; struct u1 {; float x, y;; };; float foo(struct u1 u) {; return u.x + u.y;; }. We currently generate:; 	movdqa	%xmm0, %xmm1; 	pshufd	$1, %xmm0, %xmm0 # xmm0 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm0; 	ret. We could save an instruction here by commuting the addss. //===---------------------------------------------------------------------===//. This (from PR9661):. float clamp_float(float a) {; if (a > 1.0f); return 1.0f;; else if (a < 0.0f); return 0.0f;; else; return a;; }. Could compile to:. clamp_float: # @clamp_float; movss .LCPI0_0(%rip), %xmm1; minss %xmm1, %xmm0; pxor %xmm1, %xmm1; maxss %xmm1, %xmm0; ret. with -ffast-math. //===---------------------------------------------------------------------===//. This function (from PR9803):. int clamp2(int a) {; if (a > 5); a = 5;; if (a < 0) ; return 0;; return a;; }. Compiles to:. _clamp2: ## @clamp2; pushq %rbp; movq %rsp, %rbp; cmpl $5, %edi; movl $5, %ecx; cmovlel %edi, %ecx; testl %ecx, %ecx; movl $0, %eax; cmovnsl %ecx, %eax; popq %rbp; ret. The move of 0 could be scheduled above the test to make it is xor reg,reg. //===---------------------------------------------------------------------===//. GCC PR48986. We currently compile this:. void bar(void);; void yyy(int* p) {; if (__sync_fetch_and_add(p, -1) == 1); bar();; }. into:; 	movl	$-1, %eax; 	lock; 	xaddl	%eax, (%rdi); 	cmpl	$1, %eax; 	je	LBB0_2. Instead we could generate:. 	lock; 	dec %rdi; 	je LBB0_2. The trick is to match ""fetch_and_add(X, -C) == C"". //===---------------------------------------------------------------------===//. unsigned t(unsigned a, unsigned b) {; return a <= b ? 5 : -5;; }. We generate:; 	movl	$5, %ecx; 	cmpl	%esi, %edi; 	movl	$-5, %eax; 	cmovbel	%ecx, %eax. GCC:; 	cmpl	%edi, %esi; 	sbbl	%eax, %eax; 	andl	$-10, %eax; 	addl	$5, %eax. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:42546,Testability,test,test,42546,"the movb+andb+movzbl sequence. //===---------------------------------------------------------------------===//. For the following:; struct u1 {; float x, y;; };; float foo(struct u1 u) {; return u.x + u.y;; }. We currently generate:; 	movdqa	%xmm0, %xmm1; 	pshufd	$1, %xmm0, %xmm0 # xmm0 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm0; 	ret. We could save an instruction here by commuting the addss. //===---------------------------------------------------------------------===//. This (from PR9661):. float clamp_float(float a) {; if (a > 1.0f); return 1.0f;; else if (a < 0.0f); return 0.0f;; else; return a;; }. Could compile to:. clamp_float: # @clamp_float; movss .LCPI0_0(%rip), %xmm1; minss %xmm1, %xmm0; pxor %xmm1, %xmm1; maxss %xmm1, %xmm0; ret. with -ffast-math. //===---------------------------------------------------------------------===//. This function (from PR9803):. int clamp2(int a) {; if (a > 5); a = 5;; if (a < 0) ; return 0;; return a;; }. Compiles to:. _clamp2: ## @clamp2; pushq %rbp; movq %rsp, %rbp; cmpl $5, %edi; movl $5, %ecx; cmovlel %edi, %ecx; testl %ecx, %ecx; movl $0, %eax; cmovnsl %ecx, %eax; popq %rbp; ret. The move of 0 could be scheduled above the test to make it is xor reg,reg. //===---------------------------------------------------------------------===//. GCC PR48986. We currently compile this:. void bar(void);; void yyy(int* p) {; if (__sync_fetch_and_add(p, -1) == 1); bar();; }. into:; 	movl	$-1, %eax; 	lock; 	xaddl	%eax, (%rdi); 	cmpl	$1, %eax; 	je	LBB0_2. Instead we could generate:. 	lock; 	dec %rdi; 	je LBB0_2. The trick is to match ""fetch_and_add(X, -C) == C"". //===---------------------------------------------------------------------===//. unsigned t(unsigned a, unsigned b) {; return a <= b ? 5 : -5;; }. We generate:; 	movl	$5, %ecx; 	cmpl	%esi, %edi; 	movl	$-5, %eax; 	cmovbel	%ecx, %eax. GCC:; 	cmpl	%edi, %esi; 	sbbl	%eax, %eax; 	andl	$-10, %eax; 	addl	$5, %eax. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:3685,Usability,clear,clearbit,3685,"load into a compare. The; pattern is written as (cmp reg, (load p)). Because the compare isn't; commutative, it is not matched with the load on both sides. The dag combiner; should be made smart enough to canonicalize the load into the RHS of a compare; when it can invert the result of the compare for free. //===---------------------------------------------------------------------===//. In many cases, LLVM generates code like this:. _test:; movl 8(%esp), %eax; cmpl %eax, 4(%esp); setl %al; movzbl %al, %eax; ret. on some processors (which ones?), it is more efficient to do this:. _test:; movl 8(%esp), %ebx; xor %eax, %eax; cmpl %ebx, 4(%esp); setl %al; ret. Doing this correctly is tricky though, as the xor clobbers the flags. //===---------------------------------------------------------------------===//. We should generate bts/btr/etc instructions on targets where they are cheap or; when codesize is important. e.g., for:. void setbit(int *target, int bit) {; *target |= (1 << bit);; }; void clearbit(int *target, int bit) {; *target &= ~(1 << bit);; }. //===---------------------------------------------------------------------===//. Instead of the following for memset char*, 1, 10:. 	movl $16843009, 4(%edx); 	movl $16843009, (%edx); 	movw $257, 8(%edx). It might be better to generate. 	movl $16843009, %eax; 	movl %eax, 4(%edx); 	movl %eax, (%edx); 	movw al, 8(%edx); 	; when we can spare a register. It reduces code size. //===---------------------------------------------------------------------===//. Evaluate what the best way to codegen sdiv X, (2^C) is. For X/8, we currently; get this:. define i32 @test1(i32 %X) {; %Y = sdiv i32 %X, 8; ret i32 %Y; }. _test1:; movl 4(%esp), %eax; movl %eax, %ecx; sarl $31, %ecx; shrl $29, %ecx; addl %ecx, %eax; sarl $3, %eax; ret. GCC knows several different ways to codegen it, one of which is this:. _test1:; movl 4(%esp), %eax; cmpl $-1, %eax; leal 7(%eax), %ecx; cmovle %ecx, %eax; sarl $3, %eax; ret. which is probably slower, but it's",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:9750,Usability,simpl,simplifications,9750,"by -loop-reduce and -indvars. //===---------------------------------------------------------------------===//. u32 to float conversion improvement:. float uint32_2_float( unsigned u ) {; float fl = (int) (u & 0xffff);; float fh = (int) (u >> 16);; fh *= 0x1.0p16f;; return fh + fl;; }. 00000000 subl $0x04,%esp; 00000003 movl 0x08(%esp,1),%eax; 00000007 movl %eax,%ecx; 00000009 shrl $0x10,%ecx; 0000000c cvtsi2ss %ecx,%xmm0; 00000010 andl $0x0000ffff,%eax; 00000015 cvtsi2ss %eax,%xmm1; 00000019 mulss 0x00000078,%xmm0; 00000021 addss %xmm1,%xmm0; 00000025 movss %xmm0,(%esp,1); 0000002a flds (%esp,1); 0000002d addl $0x04,%esp; 00000030 ret. //===---------------------------------------------------------------------===//. When using fastcc abi, align stack slot of argument of type double on 8 byte; boundary to improve performance. //===---------------------------------------------------------------------===//. GCC's ix86_expand_int_movcc function (in i386.c) has a ton of interesting; simplifications for integer ""x cmp y ? a : b"". //===---------------------------------------------------------------------===//. Consider the expansion of:. define i32 @test3(i32 %X) {; %tmp1 = urem i32 %X, 255; ret i32 %tmp1; }. Currently it compiles to:. ...; movl $2155905153, %ecx; movl 8(%esp), %esi; movl %esi, %eax; mull %ecx; ... This could be ""reassociated"" into:. movl $2155905153, %eax; movl 8(%esp), %ecx; mull %ecx. to avoid the copy. In fact, the existing two-address stuff would do this; except that mul isn't a commutative 2-addr instruction. I guess this has; to be done at isel time based on the #uses to mul?. //===---------------------------------------------------------------------===//. Make sure the instruction which starts a loop does not cross a cacheline; boundary. This requires knowning the exact length of each machine instruction.; That is somewhat complicated, but doable. Example 256.bzip2:. In the new trace, the hot loop has an instruction which crosses a cacheline; boundar",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:16615,Usability,simpl,simply,16615,"; 	%tmp614615 = sext i16 %tmp101 to i32		; <i32> [#uses=1]; 	%tmp621622 = sext i16 %tmp104 to i32		; <i32> [#uses=1]; 	%tmp623 = sub i32 32, %tmp621622		; <i32> [#uses=1]; 	br label %bb114. produces:. LBB3_5:	# bb114.preheader; 	movswl	-68(%ebp), %eax; 	movl	$32, %ecx; 	movl	%ecx, -80(%ebp); 	subl	%eax, -80(%ebp); 	movswl	-52(%ebp), %eax; 	movl	%ecx, -84(%ebp); 	subl	%eax, -84(%ebp); 	movswl	-70(%ebp), %eax; 	movl	%ecx, -88(%ebp); 	subl	%eax, -88(%ebp); 	movswl	-50(%ebp), %eax; 	subl	%eax, %ecx; 	movl	%ecx, -76(%ebp); 	movswl	-42(%ebp), %eax; 	movl	%eax, -92(%ebp); 	movswl	-66(%ebp), %eax; 	movl	%eax, -96(%ebp); 	movw	$0, -98(%ebp). This appears to be bad because the RA is not folding the store to the stack ; slot into the movl. The above instructions could be:; 	movl $32, -80(%ebp); ...; 	movl $32, -84(%ebp); ...; This seems like a cross between remat and spill folding. This has redundant subtractions of %eax from a stack slot. However, %ecx doesn't; change, so we could simply subtract %eax from %ecx first and then use %ecx (or; vice-versa). //===---------------------------------------------------------------------===//. This code:. 	%tmp659 = icmp slt i16 %tmp654, 0		; <i1> [#uses=1]; 	br i1 %tmp659, label %cond_true662, label %cond_next715. produces this:. 	testw	%cx, %cx; 	movswl	%cx, %esi; 	jns	LBB4_109	# cond_next715. Shark tells us that using %cx in the testw instruction is sub-optimal. It; suggests using the 32-bit register (which is what ICC uses). //===---------------------------------------------------------------------===//. We compile this:. void compare (long long foo) {; if (foo < 4294967297LL); abort();; }. to:. compare:; subl $4, %esp; cmpl $0, 8(%esp); setne %al; movzbw %al, %ax; cmpl $1, 12(%esp); setg %cl; movzbw %cl, %cx; cmove %ax, %cx; testb $1, %cl; jne .LBB1_2 # UnifiedReturnBlock; .LBB1_1: # ifthen; call abort; .LBB1_2: # UnifiedReturnBlock; addl $4, %esp; ret. (also really horrible code on ppc). This is due to the expand code for 64-bit; c",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:31289,Usability,clear,clear,31289,"verflow; 	ud2. it would be nice to produce ""into"" someday. //===---------------------------------------------------------------------===//. Test instructions can be eliminated by using EFLAGS values from arithmetic; instructions. This is currently not done for mul, and, or, xor, neg, shl,; sra, srl, shld, shrd, atomic ops, and others. It is also currently not done; for read-modify-write instructions. It is also current not done if the; OF or CF flags are needed. The shift operators have the complication that when the shift count is; zero, EFLAGS is not set, so they can only subsume a test instruction if; the shift count is known to be non-zero. Also, using the EFLAGS value; from a shift is apparently very slow on some x86 implementations. In read-modify-write instructions, the root node in the isel match is; the store, and isel has no way for the use of the EFLAGS result of the; arithmetic to be remapped to the new node. Add and subtract instructions set OF on signed overflow and CF on unsiged; overflow, while test instructions always clear OF and CF. In order to; replace a test with an add or subtract in a situation where OF or CF is; needed, codegen must be able to prove that the operation cannot see; signed or unsigned overflow, respectively. //===---------------------------------------------------------------------===//. memcpy/memmove do not lower to SSE copies when possible. A silly example is:; define <16 x float> @foo(<16 x float> %A) nounwind {; 	%tmp = alloca <16 x float>, align 16; 	%tmp2 = alloca <16 x float>, align 16; 	store <16 x float> %A, <16 x float>* %tmp; 	%s = bitcast <16 x float>* %tmp to i8*; 	%s2 = bitcast <16 x float>* %tmp2 to i8*; 	call void @llvm.memcpy.i64(i8* %s, i8* %s2, i64 64, i32 16); 	%R = load <16 x float>* %tmp2; 	ret <16 x float> %R; }. declare void @llvm.memcpy.i64(i8* nocapture, i8* nocapture, i64, i32) nounwind. which compiles to:. _foo:; 	subl	$140, %esp; 	movaps	%xmm3, 112(%esp); 	movaps	%xmm2, 96(%esp); 	movaps	%xmm1, 80(%e",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/DirectX/DXILWriter/CMakeLists.txt:168,Integrability,DEPEND,DEPENDS,168,include_directories(${CMAKE_CURRENT_SOURCE_DIR}/../). add_llvm_component_library(LLVMDXILBitWriter; DXILBitcodeWriter.cpp; DXILValueEnumerator.cpp; DXILWriterPass.cpp. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; BitWriter; Core; DirectXPointerTypeAnalysis; MC; Object; Support; TargetParser; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/DirectX/DXILWriter/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/DirectX/DXILWriter/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/AsmParser/CMakeLists.txt:98,Energy Efficiency,Power,PowerPCDesc,98,add_llvm_component_library(LLVMPowerPCAsmParser; PPCAsmParser.cpp. LINK_COMPONENTS; MC; MCParser; PowerPCDesc; PowerPCInfo; Support. ADD_TO_COMPONENT; PowerPC; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/AsmParser/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/AsmParser/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/AsmParser/CMakeLists.txt:111,Energy Efficiency,Power,PowerPCInfo,111,add_llvm_component_library(LLVMPowerPCAsmParser; PPCAsmParser.cpp. LINK_COMPONENTS; MC; MCParser; PowerPCDesc; PowerPCInfo; Support. ADD_TO_COMPONENT; PowerPC; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/AsmParser/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/AsmParser/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/AsmParser/CMakeLists.txt:151,Energy Efficiency,Power,PowerPC,151,add_llvm_component_library(LLVMPowerPCAsmParser; PPCAsmParser.cpp. LINK_COMPONENTS; MC; MCParser; PowerPCDesc; PowerPCInfo; Support. ADD_TO_COMPONENT; PowerPC; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/AsmParser/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/AsmParser/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/Disassembler/CMakeLists.txt:110,Energy Efficiency,Power,PowerPCInfo,110,add_llvm_component_library(LLVMPowerPCDisassembler; PPCDisassembler.cpp. LINK_COMPONENTS; MCDisassembler; MC; PowerPCInfo; Support. ADD_TO_COMPONENT; PowerPC; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/Disassembler/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/Disassembler/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/Disassembler/CMakeLists.txt:150,Energy Efficiency,Power,PowerPC,150,add_llvm_component_library(LLVMPowerPCDisassembler; PPCDisassembler.cpp. LINK_COMPONENTS; MCDisassembler; MC; PowerPCInfo; Support. ADD_TO_COMPONENT; PowerPC; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/Disassembler/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/Disassembler/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/CMakeLists.txt:319,Energy Efficiency,Power,PowerPCInfo,319,add_llvm_component_library(LLVMPowerPCDesc; PPCAsmBackend.cpp; PPCInstPrinter.cpp; PPCMCTargetDesc.cpp; PPCMCAsmInfo.cpp; PPCMCCodeEmitter.cpp; PPCMCExpr.cpp; PPCPredicates.cpp; PPCELFObjectWriter.cpp; PPCXCOFFObjectWriter.cpp; PPCELFStreamer.cpp; PPCXCOFFStreamer.cpp. LINK_COMPONENTS; BinaryFormat; CodeGenTypes; MC; PowerPCInfo; Support; TargetParser. ADD_TO_COMPONENT; PowerPC; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/CMakeLists.txt:373,Energy Efficiency,Power,PowerPC,373,add_llvm_component_library(LLVMPowerPCDesc; PPCAsmBackend.cpp; PPCInstPrinter.cpp; PPCMCTargetDesc.cpp; PPCMCAsmInfo.cpp; PPCMCCodeEmitter.cpp; PPCMCExpr.cpp; PPCPredicates.cpp; PPCELFObjectWriter.cpp; PPCXCOFFObjectWriter.cpp; PPCELFStreamer.cpp; PPCXCOFFStreamer.cpp. LINK_COMPONENTS; BinaryFormat; CodeGenTypes; MC; PowerPCInfo; Support; TargetParser. ADD_TO_COMPONENT; PowerPC; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/TargetInfo/CMakeLists.txt:44,Energy Efficiency,Power,PowerPCTargetInfo,44,add_llvm_component_library(LLVMPowerPCInfo; PowerPCTargetInfo.cpp. LINK_COMPONENTS; MC; Support. ADD_TO_COMPONENT; PowerPC; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/TargetInfo/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/TargetInfo/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/TargetInfo/CMakeLists.txt:115,Energy Efficiency,Power,PowerPC,115,add_llvm_component_library(LLVMPowerPCInfo; PowerPCTargetInfo.cpp. LINK_COMPONENTS; MC; Support. ADD_TO_COMPONENT; PowerPC; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/TargetInfo/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/TargetInfo/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Testing/Annotations/CMakeLists.txt:344,Testability,Test,Testing,344,"# Do not build unittest libraries automatically, they will be pulled in; # by unittests if these are built.; if (NOT ${LLVM_INSTALL_GTEST}); set (BUILDTREE_ONLY BUILDTREE_ONLY); set(EXCLUDE_FROM_ALL ON); endif(). add_llvm_library(LLVMTestingAnnotations; Annotations.cpp. ${BUILDTREE_ONLY}. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Testing/Support. LINK_COMPONENTS; Support; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Testing/Annotations/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Testing/Annotations/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt:250,Availability,Error,Error,250,"# Do not build unittest libraries automatically, they will be pulled in; # by unittests if these are built.; if (NOT ${LLVM_INSTALL_GTEST}); set (BUILDTREE_ONLY BUILDTREE_ONLY); set(EXCLUDE_FROM_ALL ON); endif(). add_llvm_library(LLVMTestingSupport; Error.cpp; SupportHelpers.cpp. ${BUILDTREE_ONLY}. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Testing/Support. LINK_COMPONENTS; Support; ). target_link_libraries(LLVMTestingSupport PRIVATE llvm_gtest). # This is to avoid the error in gtest-death-test-internal.h; # (150,16): error: 'Create' overrides a member function but; # is not marked 'override' [-Werror,-Wsuggest-override]; # during self-compile on Windows. if (HOST_WINNT AND ""${CMAKE_CXX_COMPILER_ID}"" MATCHES ""Clang"" ); SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -Wno-suggest-override""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt:485,Availability,error,error,485,"# Do not build unittest libraries automatically, they will be pulled in; # by unittests if these are built.; if (NOT ${LLVM_INSTALL_GTEST}); set (BUILDTREE_ONLY BUILDTREE_ONLY); set(EXCLUDE_FROM_ALL ON); endif(). add_llvm_library(LLVMTestingSupport; Error.cpp; SupportHelpers.cpp. ${BUILDTREE_ONLY}. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Testing/Support. LINK_COMPONENTS; Support; ). target_link_libraries(LLVMTestingSupport PRIVATE llvm_gtest). # This is to avoid the error in gtest-death-test-internal.h; # (150,16): error: 'Create' overrides a member function but; # is not marked 'override' [-Werror,-Wsuggest-override]; # during self-compile on Windows. if (HOST_WINNT AND ""${CMAKE_CXX_COMPILER_ID}"" MATCHES ""Clang"" ); SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -Wno-suggest-override""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt:535,Availability,error,error,535,"# Do not build unittest libraries automatically, they will be pulled in; # by unittests if these are built.; if (NOT ${LLVM_INSTALL_GTEST}); set (BUILDTREE_ONLY BUILDTREE_ONLY); set(EXCLUDE_FROM_ALL ON); endif(). add_llvm_library(LLVMTestingSupport; Error.cpp; SupportHelpers.cpp. ${BUILDTREE_ONLY}. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Testing/Support. LINK_COMPONENTS; Support; ). target_link_libraries(LLVMTestingSupport PRIVATE llvm_gtest). # This is to avoid the error in gtest-death-test-internal.h; # (150,16): error: 'Create' overrides a member function but; # is not marked 'override' [-Werror,-Wsuggest-override]; # during self-compile on Windows. if (HOST_WINNT AND ""${CMAKE_CXX_COMPILER_ID}"" MATCHES ""Clang"" ); SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -Wno-suggest-override""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt:475,Safety,avoid,avoid,475,"# Do not build unittest libraries automatically, they will be pulled in; # by unittests if these are built.; if (NOT ${LLVM_INSTALL_GTEST}); set (BUILDTREE_ONLY BUILDTREE_ONLY); set(EXCLUDE_FROM_ALL ON); endif(). add_llvm_library(LLVMTestingSupport; Error.cpp; SupportHelpers.cpp. ${BUILDTREE_ONLY}. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Testing/Support. LINK_COMPONENTS; Support; ). target_link_libraries(LLVMTestingSupport PRIVATE llvm_gtest). # This is to avoid the error in gtest-death-test-internal.h; # (150,16): error: 'Create' overrides a member function but; # is not marked 'override' [-Werror,-Wsuggest-override]; # during self-compile on Windows. if (HOST_WINNT AND ""${CMAKE_CXX_COMPILER_ID}"" MATCHES ""Clang"" ); SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -Wno-suggest-override""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt:354,Testability,Test,Testing,354,"# Do not build unittest libraries automatically, they will be pulled in; # by unittests if these are built.; if (NOT ${LLVM_INSTALL_GTEST}); set (BUILDTREE_ONLY BUILDTREE_ONLY); set(EXCLUDE_FROM_ALL ON); endif(). add_llvm_library(LLVMTestingSupport; Error.cpp; SupportHelpers.cpp. ${BUILDTREE_ONLY}. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Testing/Support. LINK_COMPONENTS; Support; ). target_link_libraries(LLVMTestingSupport PRIVATE llvm_gtest). # This is to avoid the error in gtest-death-test-internal.h; # (150,16): error: 'Create' overrides a member function but; # is not marked 'override' [-Werror,-Wsuggest-override]; # during self-compile on Windows. if (HOST_WINNT AND ""${CMAKE_CXX_COMPILER_ID}"" MATCHES ""Clang"" ); SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -Wno-suggest-override""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt:506,Testability,test,test-internal,506,"# Do not build unittest libraries automatically, they will be pulled in; # by unittests if these are built.; if (NOT ${LLVM_INSTALL_GTEST}); set (BUILDTREE_ONLY BUILDTREE_ONLY); set(EXCLUDE_FROM_ALL ON); endif(). add_llvm_library(LLVMTestingSupport; Error.cpp; SupportHelpers.cpp. ${BUILDTREE_ONLY}. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Testing/Support. LINK_COMPONENTS; Support; ). target_link_libraries(LLVMTestingSupport PRIVATE llvm_gtest). # This is to avoid the error in gtest-death-test-internal.h; # (150,16): error: 'Create' overrides a member function but; # is not marked 'override' [-Werror,-Wsuggest-override]; # during self-compile on Windows. if (HOST_WINNT AND ""${CMAKE_CXX_COMPILER_ID}"" MATCHES ""Clang"" ); SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -Wno-suggest-override""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Testing/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ToolDrivers/llvm-lib/CMakeLists.txt:274,Integrability,DEPEND,DEPENDS,274,set(LLVM_LINK_COMPONENTS; BinaryFormat; BitReader; Object; Option; Support; ). set(LLVM_TARGET_DEFINITIONS Options.td); tablegen(LLVM Options.inc -gen-opt-parser-defs); add_public_tablegen_target(LibOptionsTableGen). add_llvm_component_library(LLVMLibDriver; LibDriver.cpp. DEPENDS; intrinsics_gen. LINK_COMPONENTS; BinaryFormat; BitReader; Object; Option; Support; TargetParser; ); add_dependencies(LLVMLibDriver LibOptionsTableGen); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ToolDrivers/llvm-lib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ToolDrivers/llvm-lib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/CMakeLists.txt:233,Integrability,DEPEND,DEPENDS,233,add_llvm_component_library(LLVMAggressiveInstCombine; AggressiveInstCombine.cpp; TruncInstCombine.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/AggressiveInstCombine. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; Core; Support; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/CFGuard/CMakeLists.txt:119,Integrability,DEPEND,DEPENDS,119,add_llvm_component_library(LLVMCFGuard; CFGuard.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Core; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/CFGuard/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/CFGuard/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CMakeLists.txt:241,Integrability,DEPEND,DEPENDS,241,add_llvm_component_library(LLVMCoroutines; Coroutines.cpp; CoroCleanup.cpp; CoroConditionalWrapper.cpp; CoroEarly.cpp; CoroElide.cpp; CoroFrame.cpp; CoroSplit.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/Coroutines. DEPENDS; intrinsics_gen; LLVMAnalysis. LINK_COMPONENTS; Analysis; Core; IPO; Scalar; Support; TransformUtils; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Hello/CMakeLists.txt:384,Integrability,DEPEND,DEPENDS,384,"# If we don't need RTTI or EH, there's no reason to export anything; # from the hello plugin.; if( NOT LLVM_REQUIRES_RTTI ); if( NOT LLVM_REQUIRES_EH ); set(LLVM_EXPORTED_SYMBOL_FILE ${CMAKE_CURRENT_SOURCE_DIR}/Hello.exports); endif(); endif(). if(WIN32 OR CYGWIN OR ZOS); set(LLVM_LINK_COMPONENTS Core Support); endif(). add_llvm_library( LLVMHello MODULE BUILDTREE_ONLY; Hello.cpp. DEPENDS; intrinsics_gen; PLUGIN_TOOL; opt; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Hello/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Hello/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Hello/CMakeLists.txt:86,Modifiability,plugin,plugin,86,"# If we don't need RTTI or EH, there's no reason to export anything; # from the hello plugin.; if( NOT LLVM_REQUIRES_RTTI ); if( NOT LLVM_REQUIRES_EH ); set(LLVM_EXPORTED_SYMBOL_FILE ${CMAKE_CURRENT_SOURCE_DIR}/Hello.exports); endif(); endif(). if(WIN32 OR CYGWIN OR ZOS); set(LLVM_LINK_COMPONENTS Core Support); endif(). add_llvm_library( LLVMHello MODULE BUILDTREE_ONLY; Hello.cpp. DEPENDS; intrinsics_gen; PLUGIN_TOOL; opt; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Hello/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Hello/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/HipStdPar/CMakeLists.txt:133,Integrability,DEPEND,DEPENDS,133,add_llvm_component_library(LLVMHipStdPar; HipStdPar.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/HipStdPar. DEPENDS; intrinsics_gen; LLVMAnalysis. COMPONENT_NAME; HipStdPar. LINK_COMPONENTS; Analysis; Core; Support; TransformUtils); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/HipStdPar/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/HipStdPar/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/CMakeLists.txt:540,Integrability,DEPEND,DEPENDS,540,add_llvm_component_library(LLVMInstCombine; InstructionCombining.cpp; InstCombineAddSub.cpp; InstCombineAtomicRMW.cpp; InstCombineAndOrXor.cpp; InstCombineCalls.cpp; InstCombineCasts.cpp; InstCombineCompares.cpp; InstCombineLoadStoreAlloca.cpp; InstCombineMulDivRem.cpp; InstCombineNegator.cpp; InstCombinePHI.cpp; InstCombineSelect.cpp; InstCombineShifts.cpp; InstCombineSimplifyDemanded.cpp; InstCombineVectorOps.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/InstCombine. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; Core; Support; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/CMakeLists.txt:594,Integrability,DEPEND,DEPENDS,594,add_llvm_component_library(LLVMInstrumentation; AddressSanitizer.cpp; BoundsChecking.cpp; CGProfile.cpp; ControlHeightReduction.cpp; DataFlowSanitizer.cpp; GCOVProfiling.cpp; BlockCoverageInference.cpp; MemProfiler.cpp; MemorySanitizer.cpp; IndirectCallPromotion.cpp; Instrumentation.cpp; InstrOrderFile.cpp; InstrProfiling.cpp; KCFI.cpp; PGOInstrumentation.cpp; PGOMemOPSizeOpt.cpp; PoisonChecking.cpp; SanitizerCoverage.cpp; SanitizerBinaryMetadata.cpp; ValueProfileCollector.cpp; ThreadSanitizer.cpp; HWAddressSanitizer.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; Core; Demangle; MC; Support; TargetParser; TransformUtils; ProfileData; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/CMakeLists.txt:404,Security,Sanitiz,SanitizerCoverage,404,add_llvm_component_library(LLVMInstrumentation; AddressSanitizer.cpp; BoundsChecking.cpp; CGProfile.cpp; ControlHeightReduction.cpp; DataFlowSanitizer.cpp; GCOVProfiling.cpp; BlockCoverageInference.cpp; MemProfiler.cpp; MemorySanitizer.cpp; IndirectCallPromotion.cpp; Instrumentation.cpp; InstrOrderFile.cpp; InstrProfiling.cpp; KCFI.cpp; PGOInstrumentation.cpp; PGOMemOPSizeOpt.cpp; PoisonChecking.cpp; SanitizerCoverage.cpp; SanitizerBinaryMetadata.cpp; ValueProfileCollector.cpp; ThreadSanitizer.cpp; HWAddressSanitizer.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; Core; Demangle; MC; Support; TargetParser; TransformUtils; ProfileData; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/CMakeLists.txt:427,Security,Sanitiz,SanitizerBinaryMetadata,427,add_llvm_component_library(LLVMInstrumentation; AddressSanitizer.cpp; BoundsChecking.cpp; CGProfile.cpp; ControlHeightReduction.cpp; DataFlowSanitizer.cpp; GCOVProfiling.cpp; BlockCoverageInference.cpp; MemProfiler.cpp; MemorySanitizer.cpp; IndirectCallPromotion.cpp; Instrumentation.cpp; InstrOrderFile.cpp; InstrProfiling.cpp; KCFI.cpp; PGOInstrumentation.cpp; PGOMemOPSizeOpt.cpp; PoisonChecking.cpp; SanitizerCoverage.cpp; SanitizerBinaryMetadata.cpp; ValueProfileCollector.cpp; ThreadSanitizer.cpp; HWAddressSanitizer.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; Core; Demangle; MC; Support; TargetParser; TransformUtils; ProfileData; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/CMakeLists.txt:1040,Integrability,DEPEND,DEPENDS,1040,add_llvm_component_library(LLVMipo; AlwaysInliner.cpp; Annotation2Metadata.cpp; ArgumentPromotion.cpp; Attributor.cpp; AttributorAttributes.cpp; BarrierNoopPass.cpp; BlockExtractor.cpp; CalledValuePropagation.cpp; ConstantMerge.cpp; CrossDSOCFI.cpp; DeadArgumentElimination.cpp; ElimAvailExtern.cpp; EmbedBitcodePass.cpp; ExtractGV.cpp; ForceFunctionAttrs.cpp; FunctionAttrs.cpp; FunctionImport.cpp; FunctionSpecialization.cpp; GlobalDCE.cpp; GlobalOpt.cpp; GlobalSplit.cpp; HotColdSplitting.cpp; IPO.cpp; IROutliner.cpp; InferFunctionAttrs.cpp; Inliner.cpp; Internalize.cpp; LoopExtractor.cpp; LowerTypeTests.cpp; MemProfContextDisambiguation.cpp; MergeFunctions.cpp; ModuleInliner.cpp; OpenMPOpt.cpp; PartialInlining.cpp; SampleContextTracker.cpp; SampleProfile.cpp; SampleProfileProbe.cpp; SCCP.cpp; StripDeadPrototypes.cpp; StripSymbols.cpp; SyntheticCountsPropagation.cpp; ThinLTOBitcodeWriter.cpp; WholeProgramDevirt.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/IPO. DEPENDS; intrinsics_gen; omp_gen. COMPONENT_NAME; IPO. LINK_COMPONENTS; AggressiveInstCombine; Analysis; BitReader; BitWriter; Core; FrontendOpenMP; InstCombine; IRReader; Linker; Object; ProfileData; Scalar; Support; TargetParser; TransformUtils; Vectorize; Instrumentation; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/IPO/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/CMakeLists.txt:133,Integrability,Depend,DependencyAnalysis,133,add_llvm_component_library(LLVMObjCARCOpts; ObjCARC.cpp; ObjCARCOpts.cpp; ObjCARCExpand.cpp; ObjCARCAPElim.cpp; ObjCARCContract.cpp; DependencyAnalysis.cpp; ProvenanceAnalysis.cpp; ProvenanceAnalysisEvaluator.cpp; PtrState.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms. DEPENDS; intrinsics_gen. COMPONENT_NAME; ObjCARC. LINK_COMPONENTS; Analysis; Core; Support; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/CMakeLists.txt:294,Integrability,DEPEND,DEPENDS,294,add_llvm_component_library(LLVMObjCARCOpts; ObjCARC.cpp; ObjCARCOpts.cpp; ObjCARCExpand.cpp; ObjCARCAPElim.cpp; ObjCARCContract.cpp; DependencyAnalysis.cpp; ProvenanceAnalysis.cpp; ProvenanceAnalysisEvaluator.cpp; PtrState.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms. DEPENDS; intrinsics_gen. COMPONENT_NAME; ObjCARC. LINK_COMPONENTS; Analysis; Core; Support; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/CMakeLists.txt:1835,Integrability,DEPEND,DEPENDS,1835,add_llvm_component_library(LLVMScalarOpts; ADCE.cpp; AlignmentFromAssumptions.cpp; AnnotationRemarks.cpp; BDCE.cpp; CallSiteSplitting.cpp; ConstantHoisting.cpp; ConstraintElimination.cpp; CorrelatedValuePropagation.cpp; DCE.cpp; DeadStoreElimination.cpp; DFAJumpThreading.cpp; DivRemPairs.cpp; EarlyCSE.cpp; FlattenCFGPass.cpp; Float2Int.cpp; GuardWidening.cpp; GVN.cpp; GVNHoist.cpp; GVNSink.cpp; IVUsersPrinter.cpp; InductiveRangeCheckElimination.cpp; IndVarSimplify.cpp; InferAddressSpaces.cpp; InferAlignment.cpp; InstSimplifyPass.cpp; JumpThreading.cpp; LICM.cpp; LoopAccessAnalysisPrinter.cpp; LoopBoundSplit.cpp; LoopSink.cpp; LoopDeletion.cpp; LoopDataPrefetch.cpp; LoopDistribute.cpp; LoopFuse.cpp; LoopIdiomRecognize.cpp; LoopInstSimplify.cpp; LoopInterchange.cpp; LoopFlatten.cpp; LoopLoadElimination.cpp; LoopPassManager.cpp; LoopPredication.cpp; LoopRerollPass.cpp; LoopRotation.cpp; LoopSimplifyCFG.cpp; LoopStrengthReduce.cpp; LoopUnrollPass.cpp; LoopUnrollAndJamPass.cpp; LoopVersioningLICM.cpp; LowerAtomicPass.cpp; LowerConstantIntrinsics.cpp; LowerExpectIntrinsic.cpp; LowerGuardIntrinsic.cpp; LowerMatrixIntrinsics.cpp; LowerWidenableCondition.cpp; MakeGuardsExplicit.cpp; MemCpyOptimizer.cpp; MergeICmps.cpp; MergedLoadStoreMotion.cpp; NaryReassociate.cpp; NewGVN.cpp; PartiallyInlineLibCalls.cpp; PlaceSafepoints.cpp; Reassociate.cpp; Reg2Mem.cpp; RewriteStatepointsForGC.cpp; SCCP.cpp; SROA.cpp; Scalar.cpp; Scalarizer.cpp; ScalarizeMaskedMemIntrin.cpp; SeparateConstOffsetFromGEP.cpp; SimpleLoopUnswitch.cpp; SimplifyCFGPass.cpp; Sink.cpp; SpeculativeExecution.cpp; StraightLineStrengthReduce.cpp; StructurizeCFG.cpp; TailRecursionElimination.cpp; TLSVariableHoist.cpp; WarnMissedTransforms.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/Scalar. DEPENDS; intrinsics_gen. COMPONENT_NAME; Scalar. LINK_COMPONENTS; AggressiveInstCombine; Analysis; Core; InstCombine; Support; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Scalar/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/CMakeLists.txt:1370,Modifiability,Rewrite,RewriteStatepointsForGC,1370,add_llvm_component_library(LLVMScalarOpts; ADCE.cpp; AlignmentFromAssumptions.cpp; AnnotationRemarks.cpp; BDCE.cpp; CallSiteSplitting.cpp; ConstantHoisting.cpp; ConstraintElimination.cpp; CorrelatedValuePropagation.cpp; DCE.cpp; DeadStoreElimination.cpp; DFAJumpThreading.cpp; DivRemPairs.cpp; EarlyCSE.cpp; FlattenCFGPass.cpp; Float2Int.cpp; GuardWidening.cpp; GVN.cpp; GVNHoist.cpp; GVNSink.cpp; IVUsersPrinter.cpp; InductiveRangeCheckElimination.cpp; IndVarSimplify.cpp; InferAddressSpaces.cpp; InferAlignment.cpp; InstSimplifyPass.cpp; JumpThreading.cpp; LICM.cpp; LoopAccessAnalysisPrinter.cpp; LoopBoundSplit.cpp; LoopSink.cpp; LoopDeletion.cpp; LoopDataPrefetch.cpp; LoopDistribute.cpp; LoopFuse.cpp; LoopIdiomRecognize.cpp; LoopInstSimplify.cpp; LoopInterchange.cpp; LoopFlatten.cpp; LoopLoadElimination.cpp; LoopPassManager.cpp; LoopPredication.cpp; LoopRerollPass.cpp; LoopRotation.cpp; LoopSimplifyCFG.cpp; LoopStrengthReduce.cpp; LoopUnrollPass.cpp; LoopUnrollAndJamPass.cpp; LoopVersioningLICM.cpp; LowerAtomicPass.cpp; LowerConstantIntrinsics.cpp; LowerExpectIntrinsic.cpp; LowerGuardIntrinsic.cpp; LowerMatrixIntrinsics.cpp; LowerWidenableCondition.cpp; MakeGuardsExplicit.cpp; MemCpyOptimizer.cpp; MergeICmps.cpp; MergedLoadStoreMotion.cpp; NaryReassociate.cpp; NewGVN.cpp; PartiallyInlineLibCalls.cpp; PlaceSafepoints.cpp; Reassociate.cpp; Reg2Mem.cpp; RewriteStatepointsForGC.cpp; SCCP.cpp; SROA.cpp; Scalar.cpp; Scalarizer.cpp; ScalarizeMaskedMemIntrin.cpp; SeparateConstOffsetFromGEP.cpp; SimpleLoopUnswitch.cpp; SimplifyCFGPass.cpp; Sink.cpp; SpeculativeExecution.cpp; StraightLineStrengthReduce.cpp; StructurizeCFG.cpp; TailRecursionElimination.cpp; TLSVariableHoist.cpp; WarnMissedTransforms.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/Scalar. DEPENDS; intrinsics_gen. COMPONENT_NAME; Scalar. LINK_COMPONENTS; AggressiveInstCombine; Analysis; Core; InstCombine; Support; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Scalar/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/CMakeLists.txt:1509,Usability,Simpl,SimpleLoopUnswitch,1509,add_llvm_component_library(LLVMScalarOpts; ADCE.cpp; AlignmentFromAssumptions.cpp; AnnotationRemarks.cpp; BDCE.cpp; CallSiteSplitting.cpp; ConstantHoisting.cpp; ConstraintElimination.cpp; CorrelatedValuePropagation.cpp; DCE.cpp; DeadStoreElimination.cpp; DFAJumpThreading.cpp; DivRemPairs.cpp; EarlyCSE.cpp; FlattenCFGPass.cpp; Float2Int.cpp; GuardWidening.cpp; GVN.cpp; GVNHoist.cpp; GVNSink.cpp; IVUsersPrinter.cpp; InductiveRangeCheckElimination.cpp; IndVarSimplify.cpp; InferAddressSpaces.cpp; InferAlignment.cpp; InstSimplifyPass.cpp; JumpThreading.cpp; LICM.cpp; LoopAccessAnalysisPrinter.cpp; LoopBoundSplit.cpp; LoopSink.cpp; LoopDeletion.cpp; LoopDataPrefetch.cpp; LoopDistribute.cpp; LoopFuse.cpp; LoopIdiomRecognize.cpp; LoopInstSimplify.cpp; LoopInterchange.cpp; LoopFlatten.cpp; LoopLoadElimination.cpp; LoopPassManager.cpp; LoopPredication.cpp; LoopRerollPass.cpp; LoopRotation.cpp; LoopSimplifyCFG.cpp; LoopStrengthReduce.cpp; LoopUnrollPass.cpp; LoopUnrollAndJamPass.cpp; LoopVersioningLICM.cpp; LowerAtomicPass.cpp; LowerConstantIntrinsics.cpp; LowerExpectIntrinsic.cpp; LowerGuardIntrinsic.cpp; LowerMatrixIntrinsics.cpp; LowerWidenableCondition.cpp; MakeGuardsExplicit.cpp; MemCpyOptimizer.cpp; MergeICmps.cpp; MergedLoadStoreMotion.cpp; NaryReassociate.cpp; NewGVN.cpp; PartiallyInlineLibCalls.cpp; PlaceSafepoints.cpp; Reassociate.cpp; Reg2Mem.cpp; RewriteStatepointsForGC.cpp; SCCP.cpp; SROA.cpp; Scalar.cpp; Scalarizer.cpp; ScalarizeMaskedMemIntrin.cpp; SeparateConstOffsetFromGEP.cpp; SimpleLoopUnswitch.cpp; SimplifyCFGPass.cpp; Sink.cpp; SpeculativeExecution.cpp; StraightLineStrengthReduce.cpp; StructurizeCFG.cpp; TailRecursionElimination.cpp; TLSVariableHoist.cpp; WarnMissedTransforms.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/Scalar. DEPENDS; intrinsics_gen. COMPONENT_NAME; Scalar. LINK_COMPONENTS; AggressiveInstCombine; Analysis; Core; InstCombine; Support; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Scalar/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/CMakeLists.txt:1533,Usability,Simpl,SimplifyCFGPass,1533,add_llvm_component_library(LLVMScalarOpts; ADCE.cpp; AlignmentFromAssumptions.cpp; AnnotationRemarks.cpp; BDCE.cpp; CallSiteSplitting.cpp; ConstantHoisting.cpp; ConstraintElimination.cpp; CorrelatedValuePropagation.cpp; DCE.cpp; DeadStoreElimination.cpp; DFAJumpThreading.cpp; DivRemPairs.cpp; EarlyCSE.cpp; FlattenCFGPass.cpp; Float2Int.cpp; GuardWidening.cpp; GVN.cpp; GVNHoist.cpp; GVNSink.cpp; IVUsersPrinter.cpp; InductiveRangeCheckElimination.cpp; IndVarSimplify.cpp; InferAddressSpaces.cpp; InferAlignment.cpp; InstSimplifyPass.cpp; JumpThreading.cpp; LICM.cpp; LoopAccessAnalysisPrinter.cpp; LoopBoundSplit.cpp; LoopSink.cpp; LoopDeletion.cpp; LoopDataPrefetch.cpp; LoopDistribute.cpp; LoopFuse.cpp; LoopIdiomRecognize.cpp; LoopInstSimplify.cpp; LoopInterchange.cpp; LoopFlatten.cpp; LoopLoadElimination.cpp; LoopPassManager.cpp; LoopPredication.cpp; LoopRerollPass.cpp; LoopRotation.cpp; LoopSimplifyCFG.cpp; LoopStrengthReduce.cpp; LoopUnrollPass.cpp; LoopUnrollAndJamPass.cpp; LoopVersioningLICM.cpp; LowerAtomicPass.cpp; LowerConstantIntrinsics.cpp; LowerExpectIntrinsic.cpp; LowerGuardIntrinsic.cpp; LowerMatrixIntrinsics.cpp; LowerWidenableCondition.cpp; MakeGuardsExplicit.cpp; MemCpyOptimizer.cpp; MergeICmps.cpp; MergedLoadStoreMotion.cpp; NaryReassociate.cpp; NewGVN.cpp; PartiallyInlineLibCalls.cpp; PlaceSafepoints.cpp; Reassociate.cpp; Reg2Mem.cpp; RewriteStatepointsForGC.cpp; SCCP.cpp; SROA.cpp; Scalar.cpp; Scalarizer.cpp; ScalarizeMaskedMemIntrin.cpp; SeparateConstOffsetFromGEP.cpp; SimpleLoopUnswitch.cpp; SimplifyCFGPass.cpp; Sink.cpp; SpeculativeExecution.cpp; StraightLineStrengthReduce.cpp; StructurizeCFG.cpp; TailRecursionElimination.cpp; TLSVariableHoist.cpp; WarnMissedTransforms.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/Scalar. DEPENDS; intrinsics_gen. COMPONENT_NAME; Scalar. LINK_COMPONENTS; AggressiveInstCombine; Analysis; Core; InstCombine; Support; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Scalar/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt:728,Integrability,Inject,InjectTLIMappings,728,add_llvm_component_library(LLVMTransformUtils; AddDiscriminators.cpp; AMDGPUEmitPrintf.cpp; ASanStackFrameLayout.cpp; AssumeBundleBuilder.cpp; BasicBlockUtils.cpp; BreakCriticalEdges.cpp; BuildLibCalls.cpp; BypassSlowDivision.cpp; CallPromotionUtils.cpp; CallGraphUpdater.cpp; CanonicalizeAliases.cpp; CanonicalizeFreezeInLoops.cpp; CloneFunction.cpp; CloneModule.cpp; CodeExtractor.cpp; CodeLayout.cpp; CodeMoverUtils.cpp; CtorUtils.cpp; CountVisits.cpp; Debugify.cpp; DemoteRegToStack.cpp; DXILUpgrade.cpp; EntryExitInstrumenter.cpp; EscapeEnumerator.cpp; Evaluator.cpp; FixIrreducible.cpp; FlattenCFG.cpp; FunctionComparator.cpp; FunctionImportUtils.cpp; GlobalStatus.cpp; GuardUtils.cpp; HelloWorld.cpp; InlineFunction.cpp; InjectTLIMappings.cpp; InstructionNamer.cpp; IntegerDivision.cpp; LCSSA.cpp; LibCallsShrinkWrap.cpp; Local.cpp; LoopConstrainer.cpp; LoopPeel.cpp; LoopRotationUtils.cpp; LoopSimplify.cpp; LoopUnroll.cpp; LoopUnrollAndJam.cpp; LoopUnrollRuntime.cpp; LoopUtils.cpp; LoopVersioning.cpp; LowerAtomic.cpp; LowerGlobalDtors.cpp; LowerIFunc.cpp; LowerInvoke.cpp; LowerMemIntrinsics.cpp; LowerSwitch.cpp; MatrixUtils.cpp; MemoryOpRemark.cpp; MemoryTaggingSupport.cpp; Mem2Reg.cpp; MetaRenamer.cpp; MisExpect.cpp; ModuleUtils.cpp; MoveAutoInit.cpp; NameAnonGlobals.cpp; PredicateInfo.cpp; PromoteMemoryToRegister.cpp; RelLookupTableConverter.cpp; ScalarEvolutionExpander.cpp; SCCPSolver.cpp; StripGCRelocates.cpp; SSAUpdater.cpp; SSAUpdaterBulk.cpp; SampleProfileInference.cpp; SampleProfileLoaderBaseUtil.cpp; SanitizerStats.cpp; SimplifyCFG.cpp; SimplifyIndVar.cpp; SimplifyLibCalls.cpp; SizeOpts.cpp; SplitModule.cpp; StripNonLineTableDebugInfo.cpp; SymbolRewriter.cpp; UnifyFunctionExitNodes.cpp; UnifyLoopExits.cpp; Utils.cpp; ValueMapper.cpp; VNCoercion.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/Utils. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; Core; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt:1898,Integrability,DEPEND,DEPENDS,1898,add_llvm_component_library(LLVMTransformUtils; AddDiscriminators.cpp; AMDGPUEmitPrintf.cpp; ASanStackFrameLayout.cpp; AssumeBundleBuilder.cpp; BasicBlockUtils.cpp; BreakCriticalEdges.cpp; BuildLibCalls.cpp; BypassSlowDivision.cpp; CallPromotionUtils.cpp; CallGraphUpdater.cpp; CanonicalizeAliases.cpp; CanonicalizeFreezeInLoops.cpp; CloneFunction.cpp; CloneModule.cpp; CodeExtractor.cpp; CodeLayout.cpp; CodeMoverUtils.cpp; CtorUtils.cpp; CountVisits.cpp; Debugify.cpp; DemoteRegToStack.cpp; DXILUpgrade.cpp; EntryExitInstrumenter.cpp; EscapeEnumerator.cpp; Evaluator.cpp; FixIrreducible.cpp; FlattenCFG.cpp; FunctionComparator.cpp; FunctionImportUtils.cpp; GlobalStatus.cpp; GuardUtils.cpp; HelloWorld.cpp; InlineFunction.cpp; InjectTLIMappings.cpp; InstructionNamer.cpp; IntegerDivision.cpp; LCSSA.cpp; LibCallsShrinkWrap.cpp; Local.cpp; LoopConstrainer.cpp; LoopPeel.cpp; LoopRotationUtils.cpp; LoopSimplify.cpp; LoopUnroll.cpp; LoopUnrollAndJam.cpp; LoopUnrollRuntime.cpp; LoopUtils.cpp; LoopVersioning.cpp; LowerAtomic.cpp; LowerGlobalDtors.cpp; LowerIFunc.cpp; LowerInvoke.cpp; LowerMemIntrinsics.cpp; LowerSwitch.cpp; MatrixUtils.cpp; MemoryOpRemark.cpp; MemoryTaggingSupport.cpp; Mem2Reg.cpp; MetaRenamer.cpp; MisExpect.cpp; ModuleUtils.cpp; MoveAutoInit.cpp; NameAnonGlobals.cpp; PredicateInfo.cpp; PromoteMemoryToRegister.cpp; RelLookupTableConverter.cpp; ScalarEvolutionExpander.cpp; SCCPSolver.cpp; StripGCRelocates.cpp; SSAUpdater.cpp; SSAUpdaterBulk.cpp; SampleProfileInference.cpp; SampleProfileLoaderBaseUtil.cpp; SanitizerStats.cpp; SimplifyCFG.cpp; SimplifyIndVar.cpp; SimplifyLibCalls.cpp; SizeOpts.cpp; SplitModule.cpp; StripNonLineTableDebugInfo.cpp; SymbolRewriter.cpp; UnifyFunctionExitNodes.cpp; UnifyLoopExits.cpp; Utils.cpp; ValueMapper.cpp; VNCoercion.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/Utils. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; Core; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt:728,Security,Inject,InjectTLIMappings,728,add_llvm_component_library(LLVMTransformUtils; AddDiscriminators.cpp; AMDGPUEmitPrintf.cpp; ASanStackFrameLayout.cpp; AssumeBundleBuilder.cpp; BasicBlockUtils.cpp; BreakCriticalEdges.cpp; BuildLibCalls.cpp; BypassSlowDivision.cpp; CallPromotionUtils.cpp; CallGraphUpdater.cpp; CanonicalizeAliases.cpp; CanonicalizeFreezeInLoops.cpp; CloneFunction.cpp; CloneModule.cpp; CodeExtractor.cpp; CodeLayout.cpp; CodeMoverUtils.cpp; CtorUtils.cpp; CountVisits.cpp; Debugify.cpp; DemoteRegToStack.cpp; DXILUpgrade.cpp; EntryExitInstrumenter.cpp; EscapeEnumerator.cpp; Evaluator.cpp; FixIrreducible.cpp; FlattenCFG.cpp; FunctionComparator.cpp; FunctionImportUtils.cpp; GlobalStatus.cpp; GuardUtils.cpp; HelloWorld.cpp; InlineFunction.cpp; InjectTLIMappings.cpp; InstructionNamer.cpp; IntegerDivision.cpp; LCSSA.cpp; LibCallsShrinkWrap.cpp; Local.cpp; LoopConstrainer.cpp; LoopPeel.cpp; LoopRotationUtils.cpp; LoopSimplify.cpp; LoopUnroll.cpp; LoopUnrollAndJam.cpp; LoopUnrollRuntime.cpp; LoopUtils.cpp; LoopVersioning.cpp; LowerAtomic.cpp; LowerGlobalDtors.cpp; LowerIFunc.cpp; LowerInvoke.cpp; LowerMemIntrinsics.cpp; LowerSwitch.cpp; MatrixUtils.cpp; MemoryOpRemark.cpp; MemoryTaggingSupport.cpp; Mem2Reg.cpp; MetaRenamer.cpp; MisExpect.cpp; ModuleUtils.cpp; MoveAutoInit.cpp; NameAnonGlobals.cpp; PredicateInfo.cpp; PromoteMemoryToRegister.cpp; RelLookupTableConverter.cpp; ScalarEvolutionExpander.cpp; SCCPSolver.cpp; StripGCRelocates.cpp; SSAUpdater.cpp; SSAUpdaterBulk.cpp; SampleProfileInference.cpp; SampleProfileLoaderBaseUtil.cpp; SanitizerStats.cpp; SimplifyCFG.cpp; SimplifyIndVar.cpp; SimplifyLibCalls.cpp; SizeOpts.cpp; SplitModule.cpp; StripNonLineTableDebugInfo.cpp; SymbolRewriter.cpp; UnifyFunctionExitNodes.cpp; UnifyLoopExits.cpp; Utils.cpp; ValueMapper.cpp; VNCoercion.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/Utils. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; Core; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt:1530,Security,Sanitiz,SanitizerStats,1530,add_llvm_component_library(LLVMTransformUtils; AddDiscriminators.cpp; AMDGPUEmitPrintf.cpp; ASanStackFrameLayout.cpp; AssumeBundleBuilder.cpp; BasicBlockUtils.cpp; BreakCriticalEdges.cpp; BuildLibCalls.cpp; BypassSlowDivision.cpp; CallPromotionUtils.cpp; CallGraphUpdater.cpp; CanonicalizeAliases.cpp; CanonicalizeFreezeInLoops.cpp; CloneFunction.cpp; CloneModule.cpp; CodeExtractor.cpp; CodeLayout.cpp; CodeMoverUtils.cpp; CtorUtils.cpp; CountVisits.cpp; Debugify.cpp; DemoteRegToStack.cpp; DXILUpgrade.cpp; EntryExitInstrumenter.cpp; EscapeEnumerator.cpp; Evaluator.cpp; FixIrreducible.cpp; FlattenCFG.cpp; FunctionComparator.cpp; FunctionImportUtils.cpp; GlobalStatus.cpp; GuardUtils.cpp; HelloWorld.cpp; InlineFunction.cpp; InjectTLIMappings.cpp; InstructionNamer.cpp; IntegerDivision.cpp; LCSSA.cpp; LibCallsShrinkWrap.cpp; Local.cpp; LoopConstrainer.cpp; LoopPeel.cpp; LoopRotationUtils.cpp; LoopSimplify.cpp; LoopUnroll.cpp; LoopUnrollAndJam.cpp; LoopUnrollRuntime.cpp; LoopUtils.cpp; LoopVersioning.cpp; LowerAtomic.cpp; LowerGlobalDtors.cpp; LowerIFunc.cpp; LowerInvoke.cpp; LowerMemIntrinsics.cpp; LowerSwitch.cpp; MatrixUtils.cpp; MemoryOpRemark.cpp; MemoryTaggingSupport.cpp; Mem2Reg.cpp; MetaRenamer.cpp; MisExpect.cpp; ModuleUtils.cpp; MoveAutoInit.cpp; NameAnonGlobals.cpp; PredicateInfo.cpp; PromoteMemoryToRegister.cpp; RelLookupTableConverter.cpp; ScalarEvolutionExpander.cpp; SCCPSolver.cpp; StripGCRelocates.cpp; SSAUpdater.cpp; SSAUpdaterBulk.cpp; SampleProfileInference.cpp; SampleProfileLoaderBaseUtil.cpp; SanitizerStats.cpp; SimplifyCFG.cpp; SimplifyIndVar.cpp; SimplifyLibCalls.cpp; SizeOpts.cpp; SplitModule.cpp; StripNonLineTableDebugInfo.cpp; SymbolRewriter.cpp; UnifyFunctionExitNodes.cpp; UnifyLoopExits.cpp; Utils.cpp; ValueMapper.cpp; VNCoercion.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/Utils. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; Core; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt:1550,Usability,Simpl,SimplifyCFG,1550,add_llvm_component_library(LLVMTransformUtils; AddDiscriminators.cpp; AMDGPUEmitPrintf.cpp; ASanStackFrameLayout.cpp; AssumeBundleBuilder.cpp; BasicBlockUtils.cpp; BreakCriticalEdges.cpp; BuildLibCalls.cpp; BypassSlowDivision.cpp; CallPromotionUtils.cpp; CallGraphUpdater.cpp; CanonicalizeAliases.cpp; CanonicalizeFreezeInLoops.cpp; CloneFunction.cpp; CloneModule.cpp; CodeExtractor.cpp; CodeLayout.cpp; CodeMoverUtils.cpp; CtorUtils.cpp; CountVisits.cpp; Debugify.cpp; DemoteRegToStack.cpp; DXILUpgrade.cpp; EntryExitInstrumenter.cpp; EscapeEnumerator.cpp; Evaluator.cpp; FixIrreducible.cpp; FlattenCFG.cpp; FunctionComparator.cpp; FunctionImportUtils.cpp; GlobalStatus.cpp; GuardUtils.cpp; HelloWorld.cpp; InlineFunction.cpp; InjectTLIMappings.cpp; InstructionNamer.cpp; IntegerDivision.cpp; LCSSA.cpp; LibCallsShrinkWrap.cpp; Local.cpp; LoopConstrainer.cpp; LoopPeel.cpp; LoopRotationUtils.cpp; LoopSimplify.cpp; LoopUnroll.cpp; LoopUnrollAndJam.cpp; LoopUnrollRuntime.cpp; LoopUtils.cpp; LoopVersioning.cpp; LowerAtomic.cpp; LowerGlobalDtors.cpp; LowerIFunc.cpp; LowerInvoke.cpp; LowerMemIntrinsics.cpp; LowerSwitch.cpp; MatrixUtils.cpp; MemoryOpRemark.cpp; MemoryTaggingSupport.cpp; Mem2Reg.cpp; MetaRenamer.cpp; MisExpect.cpp; ModuleUtils.cpp; MoveAutoInit.cpp; NameAnonGlobals.cpp; PredicateInfo.cpp; PromoteMemoryToRegister.cpp; RelLookupTableConverter.cpp; ScalarEvolutionExpander.cpp; SCCPSolver.cpp; StripGCRelocates.cpp; SSAUpdater.cpp; SSAUpdaterBulk.cpp; SampleProfileInference.cpp; SampleProfileLoaderBaseUtil.cpp; SanitizerStats.cpp; SimplifyCFG.cpp; SimplifyIndVar.cpp; SimplifyLibCalls.cpp; SizeOpts.cpp; SplitModule.cpp; StripNonLineTableDebugInfo.cpp; SymbolRewriter.cpp; UnifyFunctionExitNodes.cpp; UnifyLoopExits.cpp; Utils.cpp; ValueMapper.cpp; VNCoercion.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/Utils. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; Core; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt:1567,Usability,Simpl,SimplifyIndVar,1567,add_llvm_component_library(LLVMTransformUtils; AddDiscriminators.cpp; AMDGPUEmitPrintf.cpp; ASanStackFrameLayout.cpp; AssumeBundleBuilder.cpp; BasicBlockUtils.cpp; BreakCriticalEdges.cpp; BuildLibCalls.cpp; BypassSlowDivision.cpp; CallPromotionUtils.cpp; CallGraphUpdater.cpp; CanonicalizeAliases.cpp; CanonicalizeFreezeInLoops.cpp; CloneFunction.cpp; CloneModule.cpp; CodeExtractor.cpp; CodeLayout.cpp; CodeMoverUtils.cpp; CtorUtils.cpp; CountVisits.cpp; Debugify.cpp; DemoteRegToStack.cpp; DXILUpgrade.cpp; EntryExitInstrumenter.cpp; EscapeEnumerator.cpp; Evaluator.cpp; FixIrreducible.cpp; FlattenCFG.cpp; FunctionComparator.cpp; FunctionImportUtils.cpp; GlobalStatus.cpp; GuardUtils.cpp; HelloWorld.cpp; InlineFunction.cpp; InjectTLIMappings.cpp; InstructionNamer.cpp; IntegerDivision.cpp; LCSSA.cpp; LibCallsShrinkWrap.cpp; Local.cpp; LoopConstrainer.cpp; LoopPeel.cpp; LoopRotationUtils.cpp; LoopSimplify.cpp; LoopUnroll.cpp; LoopUnrollAndJam.cpp; LoopUnrollRuntime.cpp; LoopUtils.cpp; LoopVersioning.cpp; LowerAtomic.cpp; LowerGlobalDtors.cpp; LowerIFunc.cpp; LowerInvoke.cpp; LowerMemIntrinsics.cpp; LowerSwitch.cpp; MatrixUtils.cpp; MemoryOpRemark.cpp; MemoryTaggingSupport.cpp; Mem2Reg.cpp; MetaRenamer.cpp; MisExpect.cpp; ModuleUtils.cpp; MoveAutoInit.cpp; NameAnonGlobals.cpp; PredicateInfo.cpp; PromoteMemoryToRegister.cpp; RelLookupTableConverter.cpp; ScalarEvolutionExpander.cpp; SCCPSolver.cpp; StripGCRelocates.cpp; SSAUpdater.cpp; SSAUpdaterBulk.cpp; SampleProfileInference.cpp; SampleProfileLoaderBaseUtil.cpp; SanitizerStats.cpp; SimplifyCFG.cpp; SimplifyIndVar.cpp; SimplifyLibCalls.cpp; SizeOpts.cpp; SplitModule.cpp; StripNonLineTableDebugInfo.cpp; SymbolRewriter.cpp; UnifyFunctionExitNodes.cpp; UnifyLoopExits.cpp; Utils.cpp; ValueMapper.cpp; VNCoercion.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/Utils. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; Core; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt:1587,Usability,Simpl,SimplifyLibCalls,1587,add_llvm_component_library(LLVMTransformUtils; AddDiscriminators.cpp; AMDGPUEmitPrintf.cpp; ASanStackFrameLayout.cpp; AssumeBundleBuilder.cpp; BasicBlockUtils.cpp; BreakCriticalEdges.cpp; BuildLibCalls.cpp; BypassSlowDivision.cpp; CallPromotionUtils.cpp; CallGraphUpdater.cpp; CanonicalizeAliases.cpp; CanonicalizeFreezeInLoops.cpp; CloneFunction.cpp; CloneModule.cpp; CodeExtractor.cpp; CodeLayout.cpp; CodeMoverUtils.cpp; CtorUtils.cpp; CountVisits.cpp; Debugify.cpp; DemoteRegToStack.cpp; DXILUpgrade.cpp; EntryExitInstrumenter.cpp; EscapeEnumerator.cpp; Evaluator.cpp; FixIrreducible.cpp; FlattenCFG.cpp; FunctionComparator.cpp; FunctionImportUtils.cpp; GlobalStatus.cpp; GuardUtils.cpp; HelloWorld.cpp; InlineFunction.cpp; InjectTLIMappings.cpp; InstructionNamer.cpp; IntegerDivision.cpp; LCSSA.cpp; LibCallsShrinkWrap.cpp; Local.cpp; LoopConstrainer.cpp; LoopPeel.cpp; LoopRotationUtils.cpp; LoopSimplify.cpp; LoopUnroll.cpp; LoopUnrollAndJam.cpp; LoopUnrollRuntime.cpp; LoopUtils.cpp; LoopVersioning.cpp; LowerAtomic.cpp; LowerGlobalDtors.cpp; LowerIFunc.cpp; LowerInvoke.cpp; LowerMemIntrinsics.cpp; LowerSwitch.cpp; MatrixUtils.cpp; MemoryOpRemark.cpp; MemoryTaggingSupport.cpp; Mem2Reg.cpp; MetaRenamer.cpp; MisExpect.cpp; ModuleUtils.cpp; MoveAutoInit.cpp; NameAnonGlobals.cpp; PredicateInfo.cpp; PromoteMemoryToRegister.cpp; RelLookupTableConverter.cpp; ScalarEvolutionExpander.cpp; SCCPSolver.cpp; StripGCRelocates.cpp; SSAUpdater.cpp; SSAUpdaterBulk.cpp; SampleProfileInference.cpp; SampleProfileLoaderBaseUtil.cpp; SanitizerStats.cpp; SimplifyCFG.cpp; SimplifyIndVar.cpp; SimplifyLibCalls.cpp; SizeOpts.cpp; SplitModule.cpp; StripNonLineTableDebugInfo.cpp; SymbolRewriter.cpp; UnifyFunctionExitNodes.cpp; UnifyLoopExits.cpp; Utils.cpp; ValueMapper.cpp; VNCoercion.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/Utils. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; Core; Support; TargetParser; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/CMakeLists.txt:412,Integrability,DEPEND,DEPENDS,412,add_llvm_component_library(LLVMVectorize; LoadStoreVectorizer.cpp; LoopVectorizationLegality.cpp; LoopVectorize.cpp; SLPVectorizer.cpp; Vectorize.cpp; VectorCombine.cpp; VPlan.cpp; VPlanAnalysis.cpp; VPlanHCFGBuilder.cpp; VPlanRecipes.cpp; VPlanSLP.cpp; VPlanTransforms.cpp; VPlanVerifier.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/Vectorize. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; Core; Support; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/CMakeLists.txt:42,Performance,Load,LoadStoreVectorizer,42,add_llvm_component_library(LLVMVectorize; LoadStoreVectorizer.cpp; LoopVectorizationLegality.cpp; LoopVectorize.cpp; SLPVectorizer.cpp; Vectorize.cpp; VectorCombine.cpp; VPlan.cpp; VPlanAnalysis.cpp; VPlanHCFGBuilder.cpp; VPlanRecipes.cpp; VPlanSLP.cpp; VPlanTransforms.cpp; VPlanVerifier.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Transforms/Vectorize. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; Core; Support; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint/CMakeLists.txt:497,Integrability,DEPEND,DEPENDS,497,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitWriter; CodeGen; Extensions; Core; IPO; IRReader; AggressiveInstCombine; InstCombine; Instrumentation; Linker; ObjCARCOpts; ScalarOpts; Support; Target; TargetParser; TransformUtils; Vectorize; ). add_llvm_tool(bugpoint; BugDriver.cpp; CrashDebugger.cpp; ExecutionDriver.cpp; ExtractFunction.cpp; FindBugs.cpp; Miscompilation.cpp; OptimizerDriver.cpp; ToolRunner.cpp; bugpoint.cpp. DEPENDS; intrinsics_gen; SUPPORT_PLUGINS; ); export_executable_symbols_for_plugins(bugpoint); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/bugpoint/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint/CMakeLists.txt:446,Performance,Optimiz,OptimizerDriver,446,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitWriter; CodeGen; Extensions; Core; IPO; IRReader; AggressiveInstCombine; InstCombine; Instrumentation; Linker; ObjCARCOpts; ScalarOpts; Support; Target; TargetParser; TransformUtils; Vectorize; ). add_llvm_tool(bugpoint; BugDriver.cpp; CrashDebugger.cpp; ExecutionDriver.cpp; ExtractFunction.cpp; FindBugs.cpp; Miscompilation.cpp; OptimizerDriver.cpp; ToolRunner.cpp; bugpoint.cpp. DEPENDS; intrinsics_gen; SUPPORT_PLUGINS; ); export_executable_symbols_for_plugins(bugpoint); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/bugpoint/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint-passes/CMakeLists.txt:455,Integrability,DEPEND,DEPENDS,455,"if( NOT LLVM_BUILD_TOOLS ); set(EXCLUDE_FROM_ALL ON); endif(). # If we don't need RTTI or EH, there's no reason to export anything; # from this plugin.; if( NOT LLVM_REQUIRES_RTTI ); if( NOT LLVM_REQUIRES_EH ); set(LLVM_EXPORTED_SYMBOL_FILE ${CMAKE_CURRENT_SOURCE_DIR}/bugpoint.exports); endif(); endif(). if(WIN32 OR CYGWIN OR ZOS); set(LLVM_LINK_COMPONENTS Core Support); endif(). add_llvm_library( BugpointPasses MODULE BUILDTREE_ONLY; TestPasses.cpp. DEPENDS; intrinsics_gen; bugpoint; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/bugpoint-passes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint-passes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint-passes/CMakeLists.txt:144,Modifiability,plugin,plugin,144,"if( NOT LLVM_BUILD_TOOLS ); set(EXCLUDE_FROM_ALL ON); endif(). # If we don't need RTTI or EH, there's no reason to export anything; # from this plugin.; if( NOT LLVM_REQUIRES_RTTI ); if( NOT LLVM_REQUIRES_EH ); set(LLVM_EXPORTED_SYMBOL_FILE ${CMAKE_CURRENT_SOURCE_DIR}/bugpoint.exports); endif(); endif(). if(WIN32 OR CYGWIN OR ZOS); set(LLVM_LINK_COMPONENTS Core Support); endif(). add_llvm_library( BugpointPasses MODULE BUILDTREE_ONLY; TestPasses.cpp. DEPENDS; intrinsics_gen; bugpoint; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/bugpoint-passes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint-passes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint-passes/CMakeLists.txt:439,Testability,Test,TestPasses,439,"if( NOT LLVM_BUILD_TOOLS ); set(EXCLUDE_FROM_ALL ON); endif(). # If we don't need RTTI or EH, there's no reason to export anything; # from this plugin.; if( NOT LLVM_REQUIRES_RTTI ); if( NOT LLVM_REQUIRES_EH ); set(LLVM_EXPORTED_SYMBOL_FILE ${CMAKE_CURRENT_SOURCE_DIR}/bugpoint.exports); endif(); endif(). if(WIN32 OR CYGWIN OR ZOS); set(LLVM_LINK_COMPONENTS Core Support); endif(). add_llvm_library( BugpointPasses MODULE BUILDTREE_ONLY; TestPasses.cpp. DEPENDS; intrinsics_gen; bugpoint; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/bugpoint-passes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint-passes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/CMakeLists.txt:585,Integrability,DEPEND,DEPENDS,585,"set(LLVM_TARGET_DEFINITIONS Options.td); tablegen(LLVM Options.inc -gen-opt-parser-defs); add_public_tablegen_target(DsymutilTableGen). set(LLVM_LINK_COMPONENTS; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; AsmPrinter; CodeGen; CodeGenTypes; DWARFLinker; DWARFLinkerClassic; DWARFLinkerParallel; DebugInfoDWARF; MC; Object; Option; Remarks; Support; Target; TargetParser; ). add_llvm_tool(dsymutil; dsymutil.cpp; BinaryHolder.cpp; CFBundle.cpp; DebugMap.cpp; DwarfLinkerForBinary.cpp; MachODebugMapParser.cpp; MachOUtils.cpp; Reproducer.cpp; RelocationMap.cpp; SymbolMap.cpp. DEPENDS; intrinsics_gen; ${tablegen_deps}; DsymutilTableGen; GENERATE_DRIVER; ). if(APPLE AND NOT LLVM_TOOL_LLVM_DRIVER_BUILD); target_link_libraries(dsymutil PRIVATE ""-framework CoreFoundation""); endif(APPLE AND NOT LLVM_TOOL_LLVM_DRIVER_BUILD). # target_link_libraries(dsymutil PRIVATE ${LLVM_ATOMIC_LIB}); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/dsymutil/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt:205,Integrability,message,message,205,"option(LLVM_INCLUDE_DXIL_TESTS ""Include DXIL tests"" Off); mark_as_advanced(LLVM_INCLUDE_DXIL_TESTS). if (NOT LLVM_INCLUDE_DXIL_TESTS); return(); endif (). if (NOT ""DirectX"" IN_LIST LLVM_TARGETS_TO_BUILD); message(FATAL_ERROR ""Building dxil-dis tests is unsupported without the DirectX target""); endif (). if (CMAKE_HOST_UNIX); set(LLVM_LINK_OR_COPY create_symlink); else (); set(LLVM_LINK_OR_COPY copy); endif (). if (DXIL_DIS); add_custom_target(dxil-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${DXIL_DIS}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/dxil-dis${CMAKE_EXECUTABLE_SUFFIX}""); return(); endif (). include(ExternalProject). set(SOURCE_DIR ${CMAKE_CURRENT_BINARY_DIR}/DXC-src); set(BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/DXC-bins); set(GIT_SETTINGS GIT_REPOSITORY https://github.com/microsoft/DirectXShaderCompiler.git). if (DXC_SOURCE_DIR); set(SOURCE_DIR ${DXC_SOURCE_DIR}); unset(GIT_SETTINGS); endif (). ExternalProject_Add(DXC; ${GIT_SETTINGS}; SOURCE_DIR ${SOURCE_DIR}; BINARY_DIR ${BINARY_DIR}; CMAKE_ARGS -C ${SOURCE_DIR}/cmake/caches/PredefinedParams.cmake -DLLVM_INCLUDE_TESTS=On; BUILD_COMMAND ${CMAKE_COMMAND} --build ${BINARY_DIR} --target llvm-dis; BUILD_BYPRODUCTS ${BINARY_DIR}/bin/llvm-dis; INSTALL_COMMAND """"; ). add_custom_target(dxil-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/bin/llvm-dis${CMAKE_EXECUTABLE_SUFFIX}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/dxil-dis${CMAKE_EXECUTABLE_SUFFIX}""; DEPENDS DXC; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt:1441,Integrability,DEPEND,DEPENDS,1441,"option(LLVM_INCLUDE_DXIL_TESTS ""Include DXIL tests"" Off); mark_as_advanced(LLVM_INCLUDE_DXIL_TESTS). if (NOT LLVM_INCLUDE_DXIL_TESTS); return(); endif (). if (NOT ""DirectX"" IN_LIST LLVM_TARGETS_TO_BUILD); message(FATAL_ERROR ""Building dxil-dis tests is unsupported without the DirectX target""); endif (). if (CMAKE_HOST_UNIX); set(LLVM_LINK_OR_COPY create_symlink); else (); set(LLVM_LINK_OR_COPY copy); endif (). if (DXIL_DIS); add_custom_target(dxil-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${DXIL_DIS}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/dxil-dis${CMAKE_EXECUTABLE_SUFFIX}""); return(); endif (). include(ExternalProject). set(SOURCE_DIR ${CMAKE_CURRENT_BINARY_DIR}/DXC-src); set(BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/DXC-bins); set(GIT_SETTINGS GIT_REPOSITORY https://github.com/microsoft/DirectXShaderCompiler.git). if (DXC_SOURCE_DIR); set(SOURCE_DIR ${DXC_SOURCE_DIR}); unset(GIT_SETTINGS); endif (). ExternalProject_Add(DXC; ${GIT_SETTINGS}; SOURCE_DIR ${SOURCE_DIR}; BINARY_DIR ${BINARY_DIR}; CMAKE_ARGS -C ${SOURCE_DIR}/cmake/caches/PredefinedParams.cmake -DLLVM_INCLUDE_TESTS=On; BUILD_COMMAND ${CMAKE_COMMAND} --build ${BINARY_DIR} --target llvm-dis; BUILD_BYPRODUCTS ${BINARY_DIR}/bin/llvm-dis; INSTALL_COMMAND """"; ). add_custom_target(dxil-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/bin/llvm-dis${CMAKE_EXECUTABLE_SUFFIX}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/dxil-dis${CMAKE_EXECUTABLE_SUFFIX}""; DEPENDS DXC; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt:1046,Performance,cache,caches,1046,"option(LLVM_INCLUDE_DXIL_TESTS ""Include DXIL tests"" Off); mark_as_advanced(LLVM_INCLUDE_DXIL_TESTS). if (NOT LLVM_INCLUDE_DXIL_TESTS); return(); endif (). if (NOT ""DirectX"" IN_LIST LLVM_TARGETS_TO_BUILD); message(FATAL_ERROR ""Building dxil-dis tests is unsupported without the DirectX target""); endif (). if (CMAKE_HOST_UNIX); set(LLVM_LINK_OR_COPY create_symlink); else (); set(LLVM_LINK_OR_COPY copy); endif (). if (DXIL_DIS); add_custom_target(dxil-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${DXIL_DIS}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/dxil-dis${CMAKE_EXECUTABLE_SUFFIX}""); return(); endif (). include(ExternalProject). set(SOURCE_DIR ${CMAKE_CURRENT_BINARY_DIR}/DXC-src); set(BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/DXC-bins); set(GIT_SETTINGS GIT_REPOSITORY https://github.com/microsoft/DirectXShaderCompiler.git). if (DXC_SOURCE_DIR); set(SOURCE_DIR ${DXC_SOURCE_DIR}); unset(GIT_SETTINGS); endif (). ExternalProject_Add(DXC; ${GIT_SETTINGS}; SOURCE_DIR ${SOURCE_DIR}; BINARY_DIR ${BINARY_DIR}; CMAKE_ARGS -C ${SOURCE_DIR}/cmake/caches/PredefinedParams.cmake -DLLVM_INCLUDE_TESTS=On; BUILD_COMMAND ${CMAKE_COMMAND} --build ${BINARY_DIR} --target llvm-dis; BUILD_BYPRODUCTS ${BINARY_DIR}/bin/llvm-dis; INSTALL_COMMAND """"; ). add_custom_target(dxil-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/bin/llvm-dis${CMAKE_EXECUTABLE_SUFFIX}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/dxil-dis${CMAKE_EXECUTABLE_SUFFIX}""; DEPENDS DXC; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt:45,Testability,test,tests,45,"option(LLVM_INCLUDE_DXIL_TESTS ""Include DXIL tests"" Off); mark_as_advanced(LLVM_INCLUDE_DXIL_TESTS). if (NOT LLVM_INCLUDE_DXIL_TESTS); return(); endif (). if (NOT ""DirectX"" IN_LIST LLVM_TARGETS_TO_BUILD); message(FATAL_ERROR ""Building dxil-dis tests is unsupported without the DirectX target""); endif (). if (CMAKE_HOST_UNIX); set(LLVM_LINK_OR_COPY create_symlink); else (); set(LLVM_LINK_OR_COPY copy); endif (). if (DXIL_DIS); add_custom_target(dxil-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${DXIL_DIS}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/dxil-dis${CMAKE_EXECUTABLE_SUFFIX}""); return(); endif (). include(ExternalProject). set(SOURCE_DIR ${CMAKE_CURRENT_BINARY_DIR}/DXC-src); set(BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/DXC-bins); set(GIT_SETTINGS GIT_REPOSITORY https://github.com/microsoft/DirectXShaderCompiler.git). if (DXC_SOURCE_DIR); set(SOURCE_DIR ${DXC_SOURCE_DIR}); unset(GIT_SETTINGS); endif (). ExternalProject_Add(DXC; ${GIT_SETTINGS}; SOURCE_DIR ${SOURCE_DIR}; BINARY_DIR ${BINARY_DIR}; CMAKE_ARGS -C ${SOURCE_DIR}/cmake/caches/PredefinedParams.cmake -DLLVM_INCLUDE_TESTS=On; BUILD_COMMAND ${CMAKE_COMMAND} --build ${BINARY_DIR} --target llvm-dis; BUILD_BYPRODUCTS ${BINARY_DIR}/bin/llvm-dis; INSTALL_COMMAND """"; ). add_custom_target(dxil-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/bin/llvm-dis${CMAKE_EXECUTABLE_SUFFIX}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/dxil-dis${CMAKE_EXECUTABLE_SUFFIX}""; DEPENDS DXC; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt:244,Testability,test,tests,244,"option(LLVM_INCLUDE_DXIL_TESTS ""Include DXIL tests"" Off); mark_as_advanced(LLVM_INCLUDE_DXIL_TESTS). if (NOT LLVM_INCLUDE_DXIL_TESTS); return(); endif (). if (NOT ""DirectX"" IN_LIST LLVM_TARGETS_TO_BUILD); message(FATAL_ERROR ""Building dxil-dis tests is unsupported without the DirectX target""); endif (). if (CMAKE_HOST_UNIX); set(LLVM_LINK_OR_COPY create_symlink); else (); set(LLVM_LINK_OR_COPY copy); endif (). if (DXIL_DIS); add_custom_target(dxil-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${DXIL_DIS}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/dxil-dis${CMAKE_EXECUTABLE_SUFFIX}""); return(); endif (). include(ExternalProject). set(SOURCE_DIR ${CMAKE_CURRENT_BINARY_DIR}/DXC-src); set(BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/DXC-bins); set(GIT_SETTINGS GIT_REPOSITORY https://github.com/microsoft/DirectXShaderCompiler.git). if (DXC_SOURCE_DIR); set(SOURCE_DIR ${DXC_SOURCE_DIR}); unset(GIT_SETTINGS); endif (). ExternalProject_Add(DXC; ${GIT_SETTINGS}; SOURCE_DIR ${SOURCE_DIR}; BINARY_DIR ${BINARY_DIR}; CMAKE_ARGS -C ${SOURCE_DIR}/cmake/caches/PredefinedParams.cmake -DLLVM_INCLUDE_TESTS=On; BUILD_COMMAND ${CMAKE_COMMAND} --build ${BINARY_DIR} --target llvm-dis; BUILD_BYPRODUCTS ${BINARY_DIR}/bin/llvm-dis; INSTALL_COMMAND """"; ). add_custom_target(dxil-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/bin/llvm-dis${CMAKE_EXECUTABLE_SUFFIX}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/dxil-dis${CMAKE_EXECUTABLE_SUFFIX}""; DEPENDS DXC; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dxil-dis/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/gold/CMakeLists.txt:306,Modifiability,plugin,plugin,306,set(LLVM_EXPORTED_SYMBOL_FILE ${CMAKE_CURRENT_SOURCE_DIR}/gold.exports). if( LLVM_ENABLE_PIC AND LLVM_BINUTILS_INCDIR ); include_directories( ${LLVM_BINUTILS_INCDIR} ). set(LLVM_LINK_COMPONENTS; ${LLVM_TARGETS_TO_BUILD}; Linker; LTO; BitWriter; IPO; TargetParser; ). add_llvm_library(LLVMgold MODULE; gold-plugin.cpp; ). endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/gold/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/gold/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/gold/README.txt:436,Availability,error,error,436,"The LLVM Gold LTO Plugin; ========================. This directory contains a plugin that is designed to work with binutils; gold linker. At present time, this is not the default linker in; binutils, and the default build of gold does not support plugins. See docs/GoldPlugin.html for complete build and usage instructions. NOTE: libLTO and LLVMgold aren't built without PIC because they would fail; to link on x86-64 with a relocation error: PIC and non-PIC can't be combined.; As an alternative to passing --enable-pic, you can use 'make ENABLE_PIC=1' in; your entire LLVM build.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/gold/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/gold/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/gold/README.txt:18,Modifiability,Plugin,Plugin,18,"The LLVM Gold LTO Plugin; ========================. This directory contains a plugin that is designed to work with binutils; gold linker. At present time, this is not the default linker in; binutils, and the default build of gold does not support plugins. See docs/GoldPlugin.html for complete build and usage instructions. NOTE: libLTO and LLVMgold aren't built without PIC because they would fail; to link on x86-64 with a relocation error: PIC and non-PIC can't be combined.; As an alternative to passing --enable-pic, you can use 'make ENABLE_PIC=1' in; your entire LLVM build.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/gold/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/gold/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/gold/README.txt:78,Modifiability,plugin,plugin,78,"The LLVM Gold LTO Plugin; ========================. This directory contains a plugin that is designed to work with binutils; gold linker. At present time, this is not the default linker in; binutils, and the default build of gold does not support plugins. See docs/GoldPlugin.html for complete build and usage instructions. NOTE: libLTO and LLVMgold aren't built without PIC because they would fail; to link on x86-64 with a relocation error: PIC and non-PIC can't be combined.; As an alternative to passing --enable-pic, you can use 'make ENABLE_PIC=1' in; your entire LLVM build.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/gold/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/gold/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/gold/README.txt:247,Modifiability,plugin,plugins,247,"The LLVM Gold LTO Plugin; ========================. This directory contains a plugin that is designed to work with binutils; gold linker. At present time, this is not the default linker in; binutils, and the default build of gold does not support plugins. See docs/GoldPlugin.html for complete build and usage instructions. NOTE: libLTO and LLVMgold aren't built without PIC because they would fail; to link on x86-64 with a relocation error: PIC and non-PIC can't be combined.; As an alternative to passing --enable-pic, you can use 'make ENABLE_PIC=1' in; your entire LLVM build.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/gold/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/gold/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llc/CMakeLists.txt:338,Integrability,DEPEND,DEPENDS,338,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; AsmPrinter; CodeGen; CodeGenTypes; Core; IRPrinter; IRReader; MC; MIRParser; Passes; Remarks; ScalarOpts; SelectionDAG; Support; Target; TargetParser; TransformUtils; Vectorize; ). add_llvm_tool(llc; llc.cpp; NewPMDriver.cpp. DEPENDS; intrinsics_gen; SUPPORT_PLUGINS; ). export_executable_symbols_for_plugins(llc); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llc/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llc/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/lli/CMakeLists.txt:767,Integrability,DEPEND,DEPENDS,767,if ( LLVM_INCLUDE_UTILS ); add_subdirectory(ChildTarget); endif(). set(LLVM_LINK_COMPONENTS; CodeGen; Core; ExecutionEngine; IRReader; Interpreter; JITLink; MC; MCJIT; Object; OrcJIT; OrcDebugging; OrcShared; OrcTargetProcess; Passes; RuntimeDyld; SelectionDAG; Support; Target; TargetParser; TransformUtils; native; ). if( LLVM_USE_OPROFILE ); set(LLVM_LINK_COMPONENTS; ${LLVM_LINK_COMPONENTS}; OProfileJIT; ); endif( LLVM_USE_OPROFILE ). if( LLVM_USE_INTEL_JITEVENTS ); set(LLVM_LINK_COMPONENTS; ${LLVM_LINK_COMPONENTS}; DebugInfoDWARF; IntelJITEvents; Object; ); endif( LLVM_USE_INTEL_JITEVENTS ). if( LLVM_USE_PERF ); set(LLVM_LINK_COMPONENTS; ${LLVM_LINK_COMPONENTS}; DebugInfoDWARF; PerfJITEvents; Object; ); endif( LLVM_USE_PERF ). add_llvm_tool(lli; lli.cpp. DEPENDS; intrinsics_gen; ). export_executable_symbols(lli); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/lli/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/lli/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-ar/CMakeLists.txt:198,Integrability,DEPEND,DEPENDS,198,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsDescs; AllTargetsInfos; BinaryFormat; Core; DlltoolDriver; LibDriver; Object; Support; TargetParser; ). add_llvm_tool(llvm-ar; llvm-ar.cpp. DEPENDS; intrinsics_gen; GENERATE_DRIVER; ). add_llvm_tool_symlink(llvm-ranlib llvm-ar); add_llvm_tool_symlink(llvm-lib llvm-ar); add_llvm_tool_symlink(llvm-dlltool llvm-ar). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(ar llvm-ar); add_llvm_tool_symlink(dlltool llvm-ar); add_llvm_tool_symlink(ranlib llvm-ar); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-ar/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-ar/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-as/CMakeLists.txt:102,Integrability,DEPEND,DEPENDS,102,set(LLVM_LINK_COMPONENTS; AsmParser; BitWriter; Core; Support; ). add_llvm_tool(llvm-as; llvm-as.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-as/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-as/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-bcanalyzer/CMakeLists.txt:118,Integrability,DEPEND,DEPENDS,118,set(LLVM_LINK_COMPONENTS; BitReader; BitstreamReader; Support; ). add_llvm_tool(llvm-bcanalyzer; llvm-bcanalyzer.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-bcanalyzer/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-bcanalyzer/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-c-test/CMakeLists.txt:354,Availability,failure,failures,354,"set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsDisassemblers; AllTargetsInfos; BitReader; Core; MCDisassembler; Object; Support; Target; ). # We should only have llvm-c-test use libLLVM if libLLVM is built with the; # default list of components. Using libLLVM with custom components can result in; # build failures. set (USE_LLVM_DYLIB FALSE). if (TARGET LLVM); set (USE_LLVM_DYLIB TRUE); if (DEFINED LLVM_DYLIB_COMPONENTS); foreach(c in ${LLVM_LINK_COMPONENTS}); list(FIND LLVM_DYLIB_COMPONENTS ${c} C_IDX); if (C_IDX EQUAL -1); set(USE_LLVM_DYLIB FALSE); break(); endif(); endforeach(); endif(); endif(). if(USE_LLVM_DYLIB); set(LLVM_LINK_COMPONENTS); endif(). if (LLVM_COMPILER_IS_GCC_COMPATIBLE); set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -std=gnu99 -Wstrict-prototypes""); endif (). add_llvm_tool(llvm-c-test; attributes.c; calc.c; debuginfo.c; diagnostic.c; disassemble.c; echo.cpp; helpers.c; include-all.c; main.c; module.c; metadata.c; object.c; targets.c; ). if(USE_LLVM_DYLIB); target_link_libraries(llvm-c-test LLVM); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-c-test/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-c-test/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-c-test/CMakeLists.txt:920,Availability,echo,echo,920,"set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsDisassemblers; AllTargetsInfos; BitReader; Core; MCDisassembler; Object; Support; Target; ). # We should only have llvm-c-test use libLLVM if libLLVM is built with the; # default list of components. Using libLLVM with custom components can result in; # build failures. set (USE_LLVM_DYLIB FALSE). if (TARGET LLVM); set (USE_LLVM_DYLIB TRUE); if (DEFINED LLVM_DYLIB_COMPONENTS); foreach(c in ${LLVM_LINK_COMPONENTS}); list(FIND LLVM_DYLIB_COMPONENTS ${c} C_IDX); if (C_IDX EQUAL -1); set(USE_LLVM_DYLIB FALSE); break(); endif(); endforeach(); endif(); endif(). if(USE_LLVM_DYLIB); set(LLVM_LINK_COMPONENTS); endif(). if (LLVM_COMPILER_IS_GCC_COMPATIBLE); set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -std=gnu99 -Wstrict-prototypes""); endif (). add_llvm_tool(llvm-c-test; attributes.c; calc.c; debuginfo.c; diagnostic.c; disassemble.c; echo.cpp; helpers.c; include-all.c; main.c; module.c; metadata.c; object.c; targets.c; ). if(USE_LLVM_DYLIB); target_link_libraries(llvm-c-test LLVM); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-c-test/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-c-test/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-c-test/CMakeLists.txt:217,Testability,test,test,217,"set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsDisassemblers; AllTargetsInfos; BitReader; Core; MCDisassembler; Object; Support; Target; ). # We should only have llvm-c-test use libLLVM if libLLVM is built with the; # default list of components. Using libLLVM with custom components can result in; # build failures. set (USE_LLVM_DYLIB FALSE). if (TARGET LLVM); set (USE_LLVM_DYLIB TRUE); if (DEFINED LLVM_DYLIB_COMPONENTS); foreach(c in ${LLVM_LINK_COMPONENTS}); list(FIND LLVM_DYLIB_COMPONENTS ${c} C_IDX); if (C_IDX EQUAL -1); set(USE_LLVM_DYLIB FALSE); break(); endif(); endforeach(); endif(); endif(). if(USE_LLVM_DYLIB); set(LLVM_LINK_COMPONENTS); endif(). if (LLVM_COMPILER_IS_GCC_COMPATIBLE); set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -std=gnu99 -Wstrict-prototypes""); endif (). add_llvm_tool(llvm-c-test; attributes.c; calc.c; debuginfo.c; diagnostic.c; disassemble.c; echo.cpp; helpers.c; include-all.c; main.c; module.c; metadata.c; object.c; targets.c; ). if(USE_LLVM_DYLIB); target_link_libraries(llvm-c-test LLVM); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-c-test/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-c-test/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-c-test/CMakeLists.txt:850,Testability,test,test,850,"set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsDisassemblers; AllTargetsInfos; BitReader; Core; MCDisassembler; Object; Support; Target; ). # We should only have llvm-c-test use libLLVM if libLLVM is built with the; # default list of components. Using libLLVM with custom components can result in; # build failures. set (USE_LLVM_DYLIB FALSE). if (TARGET LLVM); set (USE_LLVM_DYLIB TRUE); if (DEFINED LLVM_DYLIB_COMPONENTS); foreach(c in ${LLVM_LINK_COMPONENTS}); list(FIND LLVM_DYLIB_COMPONENTS ${c} C_IDX); if (C_IDX EQUAL -1); set(USE_LLVM_DYLIB FALSE); break(); endif(); endforeach(); endif(); endif(). if(USE_LLVM_DYLIB); set(LLVM_LINK_COMPONENTS); endif(). if (LLVM_COMPILER_IS_GCC_COMPATIBLE); set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -std=gnu99 -Wstrict-prototypes""); endif (). add_llvm_tool(llvm-c-test; attributes.c; calc.c; debuginfo.c; diagnostic.c; disassemble.c; echo.cpp; helpers.c; include-all.c; main.c; module.c; metadata.c; object.c; targets.c; ). if(USE_LLVM_DYLIB); target_link_libraries(llvm-c-test LLVM); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-c-test/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-c-test/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-c-test/CMakeLists.txt:1059,Testability,test,test,1059,"set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsDisassemblers; AllTargetsInfos; BitReader; Core; MCDisassembler; Object; Support; Target; ). # We should only have llvm-c-test use libLLVM if libLLVM is built with the; # default list of components. Using libLLVM with custom components can result in; # build failures. set (USE_LLVM_DYLIB FALSE). if (TARGET LLVM); set (USE_LLVM_DYLIB TRUE); if (DEFINED LLVM_DYLIB_COMPONENTS); foreach(c in ${LLVM_LINK_COMPONENTS}); list(FIND LLVM_DYLIB_COMPONENTS ${c} C_IDX); if (C_IDX EQUAL -1); set(USE_LLVM_DYLIB FALSE); break(); endif(); endforeach(); endif(); endif(). if(USE_LLVM_DYLIB); set(LLVM_LINK_COMPONENTS); endif(). if (LLVM_COMPILER_IS_GCC_COMPATIBLE); set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -std=gnu99 -Wstrict-prototypes""); endif (). add_llvm_tool(llvm-c-test; attributes.c; calc.c; debuginfo.c; diagnostic.c; disassemble.c; echo.cpp; helpers.c; include-all.c; main.c; module.c; metadata.c; object.c; targets.c; ). if(USE_LLVM_DYLIB); target_link_libraries(llvm-c-test LLVM); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-c-test/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-c-test/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:3376,Integrability,depend,dependency,3376,"SRC_DIR}); set(LLVM_OBJ_ROOT ${LLVM_BINARY_DIR}); set(LLVM_CPPFLAGS ""${LLVM_DEFINITIONS}""); set(LLVM_CFLAGS ""${LLVM_C_STD_FLAG} ${LLVM_DEFINITIONS}""); # The language standard potentially affects the ABI/API of LLVM, so we want; # to make sure it is reported by llvm-config.; set(LLVM_CXXFLAGS ""${CMAKE_CXX${CMAKE_CXX_STANDARD}_STANDARD_COMPILE_OPTION} ${LLVM_CXX_STDLIB_FLAG} ${COMPILE_FLAGS} ${LLVM_DEFINITIONS}""); set(LLVM_BUILD_SYSTEM cmake); set(LLVM_HAS_RTTI ${LLVM_CONFIG_HAS_RTTI}); set(LLVM_DYLIB_VERSION ""${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX}""). # Use the C++ link flags, since they should be a superset of C link flags.; set(LLVM_LDFLAGS ""${CMAKE_CXX_LINK_FLAGS}""); set(LLVM_BUILDMODE ${CMAKE_BUILD_TYPE}); set(LLVM_SYSTEM_LIBS ${SYSTEM_LIBS}); string(REPLACE "";"" "" "" LLVM_TARGETS_BUILT ""${LLVM_TARGETS_TO_BUILD}""); llvm_canonicalize_cmake_booleans(; LLVM_BUILD_LLVM_DYLIB; LLVM_LINK_LLVM_DYLIB; LLVM_HAS_RTTI; BUILD_SHARED_LIBS); llvm_expand_pseudo_components(LLVM_DYLIB_COMPONENTS_expanded ""${LLVM_DYLIB_COMPONENTS}""); configure_file(${BUILDVARIABLES_SRCPATH} ${BUILDVARIABLES_OBJPATH} @ONLY). # Set build-time environment(s).; add_compile_definitions(CMAKE_CFG_INTDIR=""$<CONFIG>""). if(LLVM_ENABLE_MODULES); target_compile_options(llvm-config PUBLIC; ""-fmodules-ignore-macro=CMAKE_CFG_INTDIR""; ); endif(). # Add the dependency on the generation step.; add_file_dependencies(${CMAKE_CURRENT_SOURCE_DIR}/llvm-config.cpp ${BUILDVARIABLES_OBJPATH}). if(CMAKE_CROSSCOMPILING); if (LLVM_NATIVE_TOOL_DIR AND NOT LLVM_CONFIG_PATH); if (EXISTS ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); set(LLVM_CONFIG_PATH ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); endif(); endif(). if (NOT LLVM_CONFIG_PATH); build_native_tool(llvm-config LLVM_CONFIG_PATH); set(LLVM_CONFIG_PATH ""${LLVM_CONFIG_PATH}"" CACHE STRING """"). add_custom_target(NativeLLVMConfig DEPENDS ${LLVM_CONFIG_PATH}); add_dependencies(llvm-config NativeLLVMConfig); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:3946,Integrability,DEPEND,DEPENDS,3946,"SRC_DIR}); set(LLVM_OBJ_ROOT ${LLVM_BINARY_DIR}); set(LLVM_CPPFLAGS ""${LLVM_DEFINITIONS}""); set(LLVM_CFLAGS ""${LLVM_C_STD_FLAG} ${LLVM_DEFINITIONS}""); # The language standard potentially affects the ABI/API of LLVM, so we want; # to make sure it is reported by llvm-config.; set(LLVM_CXXFLAGS ""${CMAKE_CXX${CMAKE_CXX_STANDARD}_STANDARD_COMPILE_OPTION} ${LLVM_CXX_STDLIB_FLAG} ${COMPILE_FLAGS} ${LLVM_DEFINITIONS}""); set(LLVM_BUILD_SYSTEM cmake); set(LLVM_HAS_RTTI ${LLVM_CONFIG_HAS_RTTI}); set(LLVM_DYLIB_VERSION ""${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX}""). # Use the C++ link flags, since they should be a superset of C link flags.; set(LLVM_LDFLAGS ""${CMAKE_CXX_LINK_FLAGS}""); set(LLVM_BUILDMODE ${CMAKE_BUILD_TYPE}); set(LLVM_SYSTEM_LIBS ${SYSTEM_LIBS}); string(REPLACE "";"" "" "" LLVM_TARGETS_BUILT ""${LLVM_TARGETS_TO_BUILD}""); llvm_canonicalize_cmake_booleans(; LLVM_BUILD_LLVM_DYLIB; LLVM_LINK_LLVM_DYLIB; LLVM_HAS_RTTI; BUILD_SHARED_LIBS); llvm_expand_pseudo_components(LLVM_DYLIB_COMPONENTS_expanded ""${LLVM_DYLIB_COMPONENTS}""); configure_file(${BUILDVARIABLES_SRCPATH} ${BUILDVARIABLES_OBJPATH} @ONLY). # Set build-time environment(s).; add_compile_definitions(CMAKE_CFG_INTDIR=""$<CONFIG>""). if(LLVM_ENABLE_MODULES); target_compile_options(llvm-config PUBLIC; ""-fmodules-ignore-macro=CMAKE_CFG_INTDIR""; ); endif(). # Add the dependency on the generation step.; add_file_dependencies(${CMAKE_CURRENT_SOURCE_DIR}/llvm-config.cpp ${BUILDVARIABLES_OBJPATH}). if(CMAKE_CROSSCOMPILING); if (LLVM_NATIVE_TOOL_DIR AND NOT LLVM_CONFIG_PATH); if (EXISTS ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); set(LLVM_CONFIG_PATH ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); endif(); endif(). if (NOT LLVM_CONFIG_PATH); build_native_tool(llvm-config LLVM_CONFIG_PATH); set(LLVM_CONFIG_PATH ""${LLVM_CONFIG_PATH}"" CACHE STRING """"). add_custom_target(NativeLLVMConfig DEPENDS ${LLVM_CONFIG_PATH}); add_dependencies(llvm-config NativeLLVMConfig); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:222,Modifiability,config,config,222,"set(LLVM_LINK_COMPONENTS; Support; TargetParser; ). set(BUILDVARIABLES_SRCPATH ${CMAKE_CURRENT_SOURCE_DIR}/BuildVariables.inc.in); set(BUILDVARIABLES_OBJPATH ${CMAKE_CURRENT_BINARY_DIR}/BuildVariables.inc). # Add the llvm-config tool.; add_llvm_tool(llvm-config; llvm-config.cpp; # This utility doesn't use much of LLVM, so linking a shared library for the; # entire thing is overkill. Avoiding that especially saves on build time when cross; # compiling LLVM and building both cross and native `llvm-config`s. We don't; # want to build an entire native libLLVM.so in addition to the cross one just; # for the native `llvm-config`!; DISABLE_LLVM_LINK_LLVM_DYLIB; ). # Compute the substitution values for various items.; get_property(SUPPORT_SYSTEM_LIBS TARGET LLVMSupport PROPERTY LLVM_SYSTEM_LIBS); get_property(WINDOWSMANIFEST_SYSTEM_LIBS TARGET LLVMWindowsManifest PROPERTY LLVM_SYSTEM_LIBS). foreach(l ${SUPPORT_SYSTEM_LIBS} ${WINDOWSMANIFEST_SYSTEM_LIBS}); if(MSVC); if(IS_ABSOLUTE ${l}); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}.lib""); endif(); else(); if (l MATCHES ""^-""); # If it's an option, pass it without changes.; set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); # Otherwise assume it's a library name we need to link with.; if(IS_ABSOLUTE ${l}); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""-l${l}""); endif(); endif(); endif(); endforeach(). string(REPLACE "";"" "" "" SYSTEM_LIBS ""${SYSTEM_LIBS}""). # Fetch target specific compile options, e.g. RTTI option; get_property(COMPILE_FLAGS TARGET llvm-config PROPERTY COMPILE_FLAGS). # NOTE: We don't want to start extracting any random C/CXX flags that the; # user may add that could affect the ABI. We only want to extract flags; # that have been added by the LLVM build system.; string(REGEX MATCH ""-stdlib=[^ ]+"" LLVM_CXX_STDLIB_FLAG ${CMAKE_CXX_FLAGS}); string(REGEX MATCH ""-std=[^ ]+"" LLVM_C_STD_FLAG ${CMAKE_C_FLAGS}). # Use configure_file to create BuildVaria",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:255,Modifiability,config,config,255,"set(LLVM_LINK_COMPONENTS; Support; TargetParser; ). set(BUILDVARIABLES_SRCPATH ${CMAKE_CURRENT_SOURCE_DIR}/BuildVariables.inc.in); set(BUILDVARIABLES_OBJPATH ${CMAKE_CURRENT_BINARY_DIR}/BuildVariables.inc). # Add the llvm-config tool.; add_llvm_tool(llvm-config; llvm-config.cpp; # This utility doesn't use much of LLVM, so linking a shared library for the; # entire thing is overkill. Avoiding that especially saves on build time when cross; # compiling LLVM and building both cross and native `llvm-config`s. We don't; # want to build an entire native libLLVM.so in addition to the cross one just; # for the native `llvm-config`!; DISABLE_LLVM_LINK_LLVM_DYLIB; ). # Compute the substitution values for various items.; get_property(SUPPORT_SYSTEM_LIBS TARGET LLVMSupport PROPERTY LLVM_SYSTEM_LIBS); get_property(WINDOWSMANIFEST_SYSTEM_LIBS TARGET LLVMWindowsManifest PROPERTY LLVM_SYSTEM_LIBS). foreach(l ${SUPPORT_SYSTEM_LIBS} ${WINDOWSMANIFEST_SYSTEM_LIBS}); if(MSVC); if(IS_ABSOLUTE ${l}); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}.lib""); endif(); else(); if (l MATCHES ""^-""); # If it's an option, pass it without changes.; set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); # Otherwise assume it's a library name we need to link with.; if(IS_ABSOLUTE ${l}); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""-l${l}""); endif(); endif(); endif(); endforeach(). string(REPLACE "";"" "" "" SYSTEM_LIBS ""${SYSTEM_LIBS}""). # Fetch target specific compile options, e.g. RTTI option; get_property(COMPILE_FLAGS TARGET llvm-config PROPERTY COMPILE_FLAGS). # NOTE: We don't want to start extracting any random C/CXX flags that the; # user may add that could affect the ABI. We only want to extract flags; # that have been added by the LLVM build system.; string(REGEX MATCH ""-stdlib=[^ ]+"" LLVM_CXX_STDLIB_FLAG ${CMAKE_CXX_FLAGS}); string(REGEX MATCH ""-std=[^ ]+"" LLVM_C_STD_FLAG ${CMAKE_C_FLAGS}). # Use configure_file to create BuildVaria",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:268,Modifiability,config,config,268,"set(LLVM_LINK_COMPONENTS; Support; TargetParser; ). set(BUILDVARIABLES_SRCPATH ${CMAKE_CURRENT_SOURCE_DIR}/BuildVariables.inc.in); set(BUILDVARIABLES_OBJPATH ${CMAKE_CURRENT_BINARY_DIR}/BuildVariables.inc). # Add the llvm-config tool.; add_llvm_tool(llvm-config; llvm-config.cpp; # This utility doesn't use much of LLVM, so linking a shared library for the; # entire thing is overkill. Avoiding that especially saves on build time when cross; # compiling LLVM and building both cross and native `llvm-config`s. We don't; # want to build an entire native libLLVM.so in addition to the cross one just; # for the native `llvm-config`!; DISABLE_LLVM_LINK_LLVM_DYLIB; ). # Compute the substitution values for various items.; get_property(SUPPORT_SYSTEM_LIBS TARGET LLVMSupport PROPERTY LLVM_SYSTEM_LIBS); get_property(WINDOWSMANIFEST_SYSTEM_LIBS TARGET LLVMWindowsManifest PROPERTY LLVM_SYSTEM_LIBS). foreach(l ${SUPPORT_SYSTEM_LIBS} ${WINDOWSMANIFEST_SYSTEM_LIBS}); if(MSVC); if(IS_ABSOLUTE ${l}); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}.lib""); endif(); else(); if (l MATCHES ""^-""); # If it's an option, pass it without changes.; set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); # Otherwise assume it's a library name we need to link with.; if(IS_ABSOLUTE ${l}); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""-l${l}""); endif(); endif(); endif(); endforeach(). string(REPLACE "";"" "" "" SYSTEM_LIBS ""${SYSTEM_LIBS}""). # Fetch target specific compile options, e.g. RTTI option; get_property(COMPILE_FLAGS TARGET llvm-config PROPERTY COMPILE_FLAGS). # NOTE: We don't want to start extracting any random C/CXX flags that the; # user may add that could affect the ABI. We only want to extract flags; # that have been added by the LLVM build system.; string(REGEX MATCH ""-stdlib=[^ ]+"" LLVM_CXX_STDLIB_FLAG ${CMAKE_CXX_FLAGS}); string(REGEX MATCH ""-std=[^ ]+"" LLVM_C_STD_FLAG ${CMAKE_C_FLAGS}). # Use configure_file to create BuildVaria",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:501,Modifiability,config,config,501,"set(LLVM_LINK_COMPONENTS; Support; TargetParser; ). set(BUILDVARIABLES_SRCPATH ${CMAKE_CURRENT_SOURCE_DIR}/BuildVariables.inc.in); set(BUILDVARIABLES_OBJPATH ${CMAKE_CURRENT_BINARY_DIR}/BuildVariables.inc). # Add the llvm-config tool.; add_llvm_tool(llvm-config; llvm-config.cpp; # This utility doesn't use much of LLVM, so linking a shared library for the; # entire thing is overkill. Avoiding that especially saves on build time when cross; # compiling LLVM and building both cross and native `llvm-config`s. We don't; # want to build an entire native libLLVM.so in addition to the cross one just; # for the native `llvm-config`!; DISABLE_LLVM_LINK_LLVM_DYLIB; ). # Compute the substitution values for various items.; get_property(SUPPORT_SYSTEM_LIBS TARGET LLVMSupport PROPERTY LLVM_SYSTEM_LIBS); get_property(WINDOWSMANIFEST_SYSTEM_LIBS TARGET LLVMWindowsManifest PROPERTY LLVM_SYSTEM_LIBS). foreach(l ${SUPPORT_SYSTEM_LIBS} ${WINDOWSMANIFEST_SYSTEM_LIBS}); if(MSVC); if(IS_ABSOLUTE ${l}); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}.lib""); endif(); else(); if (l MATCHES ""^-""); # If it's an option, pass it without changes.; set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); # Otherwise assume it's a library name we need to link with.; if(IS_ABSOLUTE ${l}); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""-l${l}""); endif(); endif(); endif(); endforeach(). string(REPLACE "";"" "" "" SYSTEM_LIBS ""${SYSTEM_LIBS}""). # Fetch target specific compile options, e.g. RTTI option; get_property(COMPILE_FLAGS TARGET llvm-config PROPERTY COMPILE_FLAGS). # NOTE: We don't want to start extracting any random C/CXX flags that the; # user may add that could affect the ABI. We only want to extract flags; # that have been added by the LLVM build system.; string(REGEX MATCH ""-stdlib=[^ ]+"" LLVM_CXX_STDLIB_FLAG ${CMAKE_CXX_FLAGS}); string(REGEX MATCH ""-std=[^ ]+"" LLVM_C_STD_FLAG ${CMAKE_C_FLAGS}). # Use configure_file to create BuildVaria",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:623,Modifiability,config,config,623,"set(LLVM_LINK_COMPONENTS; Support; TargetParser; ). set(BUILDVARIABLES_SRCPATH ${CMAKE_CURRENT_SOURCE_DIR}/BuildVariables.inc.in); set(BUILDVARIABLES_OBJPATH ${CMAKE_CURRENT_BINARY_DIR}/BuildVariables.inc). # Add the llvm-config tool.; add_llvm_tool(llvm-config; llvm-config.cpp; # This utility doesn't use much of LLVM, so linking a shared library for the; # entire thing is overkill. Avoiding that especially saves on build time when cross; # compiling LLVM and building both cross and native `llvm-config`s. We don't; # want to build an entire native libLLVM.so in addition to the cross one just; # for the native `llvm-config`!; DISABLE_LLVM_LINK_LLVM_DYLIB; ). # Compute the substitution values for various items.; get_property(SUPPORT_SYSTEM_LIBS TARGET LLVMSupport PROPERTY LLVM_SYSTEM_LIBS); get_property(WINDOWSMANIFEST_SYSTEM_LIBS TARGET LLVMWindowsManifest PROPERTY LLVM_SYSTEM_LIBS). foreach(l ${SUPPORT_SYSTEM_LIBS} ${WINDOWSMANIFEST_SYSTEM_LIBS}); if(MSVC); if(IS_ABSOLUTE ${l}); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}.lib""); endif(); else(); if (l MATCHES ""^-""); # If it's an option, pass it without changes.; set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); # Otherwise assume it's a library name we need to link with.; if(IS_ABSOLUTE ${l}); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""-l${l}""); endif(); endif(); endif(); endforeach(). string(REPLACE "";"" "" "" SYSTEM_LIBS ""${SYSTEM_LIBS}""). # Fetch target specific compile options, e.g. RTTI option; get_property(COMPILE_FLAGS TARGET llvm-config PROPERTY COMPILE_FLAGS). # NOTE: We don't want to start extracting any random C/CXX flags that the; # user may add that could affect the ABI. We only want to extract flags; # that have been added by the LLVM build system.; string(REGEX MATCH ""-stdlib=[^ ]+"" LLVM_CXX_STDLIB_FLAG ${CMAKE_CXX_FLAGS}); string(REGEX MATCH ""-std=[^ ]+"" LLVM_C_STD_FLAG ${CMAKE_C_FLAGS}). # Use configure_file to create BuildVaria",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:1586,Modifiability,config,config,1586,"n to the cross one just; # for the native `llvm-config`!; DISABLE_LLVM_LINK_LLVM_DYLIB; ). # Compute the substitution values for various items.; get_property(SUPPORT_SYSTEM_LIBS TARGET LLVMSupport PROPERTY LLVM_SYSTEM_LIBS); get_property(WINDOWSMANIFEST_SYSTEM_LIBS TARGET LLVMWindowsManifest PROPERTY LLVM_SYSTEM_LIBS). foreach(l ${SUPPORT_SYSTEM_LIBS} ${WINDOWSMANIFEST_SYSTEM_LIBS}); if(MSVC); if(IS_ABSOLUTE ${l}); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}.lib""); endif(); else(); if (l MATCHES ""^-""); # If it's an option, pass it without changes.; set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); # Otherwise assume it's a library name we need to link with.; if(IS_ABSOLUTE ${l}); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""-l${l}""); endif(); endif(); endif(); endforeach(). string(REPLACE "";"" "" "" SYSTEM_LIBS ""${SYSTEM_LIBS}""). # Fetch target specific compile options, e.g. RTTI option; get_property(COMPILE_FLAGS TARGET llvm-config PROPERTY COMPILE_FLAGS). # NOTE: We don't want to start extracting any random C/CXX flags that the; # user may add that could affect the ABI. We only want to extract flags; # that have been added by the LLVM build system.; string(REGEX MATCH ""-stdlib=[^ ]+"" LLVM_CXX_STDLIB_FLAG ${CMAKE_CXX_FLAGS}); string(REGEX MATCH ""-std=[^ ]+"" LLVM_C_STD_FLAG ${CMAKE_C_FLAGS}). # Use configure_file to create BuildVariables.inc.; set(LLVM_SRC_ROOT ${LLVM_MAIN_SRC_DIR}); set(LLVM_OBJ_ROOT ${LLVM_BINARY_DIR}); set(LLVM_CPPFLAGS ""${LLVM_DEFINITIONS}""); set(LLVM_CFLAGS ""${LLVM_C_STD_FLAG} ${LLVM_DEFINITIONS}""); # The language standard potentially affects the ABI/API of LLVM, so we want; # to make sure it is reported by llvm-config.; set(LLVM_CXXFLAGS ""${CMAKE_CXX${CMAKE_CXX_STANDARD}_STANDARD_COMPILE_OPTION} ${LLVM_CXX_STDLIB_FLAG} ${COMPILE_FLAGS} ${LLVM_DEFINITIONS}""); set(LLVM_BUILD_SYSTEM cmake); set(LLVM_HAS_RTTI ${LLVM_CONFIG_HAS_RTTI}); set(LLVM_DYLIB_VERSION ""${LLVM_VERSION_MAJO",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:2308,Modifiability,config,config,2308,"hanges.; set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); # Otherwise assume it's a library name we need to link with.; if(IS_ABSOLUTE ${l}); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""-l${l}""); endif(); endif(); endif(); endforeach(). string(REPLACE "";"" "" "" SYSTEM_LIBS ""${SYSTEM_LIBS}""). # Fetch target specific compile options, e.g. RTTI option; get_property(COMPILE_FLAGS TARGET llvm-config PROPERTY COMPILE_FLAGS). # NOTE: We don't want to start extracting any random C/CXX flags that the; # user may add that could affect the ABI. We only want to extract flags; # that have been added by the LLVM build system.; string(REGEX MATCH ""-stdlib=[^ ]+"" LLVM_CXX_STDLIB_FLAG ${CMAKE_CXX_FLAGS}); string(REGEX MATCH ""-std=[^ ]+"" LLVM_C_STD_FLAG ${CMAKE_C_FLAGS}). # Use configure_file to create BuildVariables.inc.; set(LLVM_SRC_ROOT ${LLVM_MAIN_SRC_DIR}); set(LLVM_OBJ_ROOT ${LLVM_BINARY_DIR}); set(LLVM_CPPFLAGS ""${LLVM_DEFINITIONS}""); set(LLVM_CFLAGS ""${LLVM_C_STD_FLAG} ${LLVM_DEFINITIONS}""); # The language standard potentially affects the ABI/API of LLVM, so we want; # to make sure it is reported by llvm-config.; set(LLVM_CXXFLAGS ""${CMAKE_CXX${CMAKE_CXX_STANDARD}_STANDARD_COMPILE_OPTION} ${LLVM_CXX_STDLIB_FLAG} ${COMPILE_FLAGS} ${LLVM_DEFINITIONS}""); set(LLVM_BUILD_SYSTEM cmake); set(LLVM_HAS_RTTI ${LLVM_CONFIG_HAS_RTTI}); set(LLVM_DYLIB_VERSION ""${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX}""). # Use the C++ link flags, since they should be a superset of C link flags.; set(LLVM_LDFLAGS ""${CMAKE_CXX_LINK_FLAGS}""); set(LLVM_BUILDMODE ${CMAKE_BUILD_TYPE}); set(LLVM_SYSTEM_LIBS ${SYSTEM_LIBS}); string(REPLACE "";"" "" "" LLVM_TARGETS_BUILT ""${LLVM_TARGETS_TO_BUILD}""); llvm_canonicalize_cmake_booleans(; LLVM_BUILD_LLVM_DYLIB; LLVM_LINK_LLVM_DYLIB; LLVM_HAS_RTTI; BUILD_SHARED_LIBS); llvm_expand_pseudo_components(LLVM_DYLIB_COMPONENTS_expanded ""${LLVM_DYLIB_COMPONENTS}""); configure_file(${BUILDVARIABLES_SRCPATH} ${BUILDVARIABLES_OBJPATH} @ONLY). # Set bui",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:3232,Modifiability,CONFIG,CONFIG,3232,"SRC_DIR}); set(LLVM_OBJ_ROOT ${LLVM_BINARY_DIR}); set(LLVM_CPPFLAGS ""${LLVM_DEFINITIONS}""); set(LLVM_CFLAGS ""${LLVM_C_STD_FLAG} ${LLVM_DEFINITIONS}""); # The language standard potentially affects the ABI/API of LLVM, so we want; # to make sure it is reported by llvm-config.; set(LLVM_CXXFLAGS ""${CMAKE_CXX${CMAKE_CXX_STANDARD}_STANDARD_COMPILE_OPTION} ${LLVM_CXX_STDLIB_FLAG} ${COMPILE_FLAGS} ${LLVM_DEFINITIONS}""); set(LLVM_BUILD_SYSTEM cmake); set(LLVM_HAS_RTTI ${LLVM_CONFIG_HAS_RTTI}); set(LLVM_DYLIB_VERSION ""${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX}""). # Use the C++ link flags, since they should be a superset of C link flags.; set(LLVM_LDFLAGS ""${CMAKE_CXX_LINK_FLAGS}""); set(LLVM_BUILDMODE ${CMAKE_BUILD_TYPE}); set(LLVM_SYSTEM_LIBS ${SYSTEM_LIBS}); string(REPLACE "";"" "" "" LLVM_TARGETS_BUILT ""${LLVM_TARGETS_TO_BUILD}""); llvm_canonicalize_cmake_booleans(; LLVM_BUILD_LLVM_DYLIB; LLVM_LINK_LLVM_DYLIB; LLVM_HAS_RTTI; BUILD_SHARED_LIBS); llvm_expand_pseudo_components(LLVM_DYLIB_COMPONENTS_expanded ""${LLVM_DYLIB_COMPONENTS}""); configure_file(${BUILDVARIABLES_SRCPATH} ${BUILDVARIABLES_OBJPATH} @ONLY). # Set build-time environment(s).; add_compile_definitions(CMAKE_CFG_INTDIR=""$<CONFIG>""). if(LLVM_ENABLE_MODULES); target_compile_options(llvm-config PUBLIC; ""-fmodules-ignore-macro=CMAKE_CFG_INTDIR""; ); endif(). # Add the dependency on the generation step.; add_file_dependencies(${CMAKE_CURRENT_SOURCE_DIR}/llvm-config.cpp ${BUILDVARIABLES_OBJPATH}). if(CMAKE_CROSSCOMPILING); if (LLVM_NATIVE_TOOL_DIR AND NOT LLVM_CONFIG_PATH); if (EXISTS ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); set(LLVM_CONFIG_PATH ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); endif(); endif(). if (NOT LLVM_CONFIG_PATH); build_native_tool(llvm-config LLVM_CONFIG_PATH); set(LLVM_CONFIG_PATH ""${LLVM_CONFIG_PATH}"" CACHE STRING """"). add_custom_target(NativeLLVMConfig DEPENDS ${LLVM_CONFIG_PATH}); add_dependencies(llvm-config NativeLLVMConfig); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:3296,Modifiability,config,config,3296,"SRC_DIR}); set(LLVM_OBJ_ROOT ${LLVM_BINARY_DIR}); set(LLVM_CPPFLAGS ""${LLVM_DEFINITIONS}""); set(LLVM_CFLAGS ""${LLVM_C_STD_FLAG} ${LLVM_DEFINITIONS}""); # The language standard potentially affects the ABI/API of LLVM, so we want; # to make sure it is reported by llvm-config.; set(LLVM_CXXFLAGS ""${CMAKE_CXX${CMAKE_CXX_STANDARD}_STANDARD_COMPILE_OPTION} ${LLVM_CXX_STDLIB_FLAG} ${COMPILE_FLAGS} ${LLVM_DEFINITIONS}""); set(LLVM_BUILD_SYSTEM cmake); set(LLVM_HAS_RTTI ${LLVM_CONFIG_HAS_RTTI}); set(LLVM_DYLIB_VERSION ""${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX}""). # Use the C++ link flags, since they should be a superset of C link flags.; set(LLVM_LDFLAGS ""${CMAKE_CXX_LINK_FLAGS}""); set(LLVM_BUILDMODE ${CMAKE_BUILD_TYPE}); set(LLVM_SYSTEM_LIBS ${SYSTEM_LIBS}); string(REPLACE "";"" "" "" LLVM_TARGETS_BUILT ""${LLVM_TARGETS_TO_BUILD}""); llvm_canonicalize_cmake_booleans(; LLVM_BUILD_LLVM_DYLIB; LLVM_LINK_LLVM_DYLIB; LLVM_HAS_RTTI; BUILD_SHARED_LIBS); llvm_expand_pseudo_components(LLVM_DYLIB_COMPONENTS_expanded ""${LLVM_DYLIB_COMPONENTS}""); configure_file(${BUILDVARIABLES_SRCPATH} ${BUILDVARIABLES_OBJPATH} @ONLY). # Set build-time environment(s).; add_compile_definitions(CMAKE_CFG_INTDIR=""$<CONFIG>""). if(LLVM_ENABLE_MODULES); target_compile_options(llvm-config PUBLIC; ""-fmodules-ignore-macro=CMAKE_CFG_INTDIR""; ); endif(). # Add the dependency on the generation step.; add_file_dependencies(${CMAKE_CURRENT_SOURCE_DIR}/llvm-config.cpp ${BUILDVARIABLES_OBJPATH}). if(CMAKE_CROSSCOMPILING); if (LLVM_NATIVE_TOOL_DIR AND NOT LLVM_CONFIG_PATH); if (EXISTS ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); set(LLVM_CONFIG_PATH ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); endif(); endif(). if (NOT LLVM_CONFIG_PATH); build_native_tool(llvm-config LLVM_CONFIG_PATH); set(LLVM_CONFIG_PATH ""${LLVM_CONFIG_PATH}"" CACHE STRING """"). add_custom_target(NativeLLVMConfig DEPENDS ${LLVM_CONFIG_PATH}); add_dependencies(llvm-config NativeLLVMConfig); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:3467,Modifiability,config,config,3467,"SRC_DIR}); set(LLVM_OBJ_ROOT ${LLVM_BINARY_DIR}); set(LLVM_CPPFLAGS ""${LLVM_DEFINITIONS}""); set(LLVM_CFLAGS ""${LLVM_C_STD_FLAG} ${LLVM_DEFINITIONS}""); # The language standard potentially affects the ABI/API of LLVM, so we want; # to make sure it is reported by llvm-config.; set(LLVM_CXXFLAGS ""${CMAKE_CXX${CMAKE_CXX_STANDARD}_STANDARD_COMPILE_OPTION} ${LLVM_CXX_STDLIB_FLAG} ${COMPILE_FLAGS} ${LLVM_DEFINITIONS}""); set(LLVM_BUILD_SYSTEM cmake); set(LLVM_HAS_RTTI ${LLVM_CONFIG_HAS_RTTI}); set(LLVM_DYLIB_VERSION ""${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX}""). # Use the C++ link flags, since they should be a superset of C link flags.; set(LLVM_LDFLAGS ""${CMAKE_CXX_LINK_FLAGS}""); set(LLVM_BUILDMODE ${CMAKE_BUILD_TYPE}); set(LLVM_SYSTEM_LIBS ${SYSTEM_LIBS}); string(REPLACE "";"" "" "" LLVM_TARGETS_BUILT ""${LLVM_TARGETS_TO_BUILD}""); llvm_canonicalize_cmake_booleans(; LLVM_BUILD_LLVM_DYLIB; LLVM_LINK_LLVM_DYLIB; LLVM_HAS_RTTI; BUILD_SHARED_LIBS); llvm_expand_pseudo_components(LLVM_DYLIB_COMPONENTS_expanded ""${LLVM_DYLIB_COMPONENTS}""); configure_file(${BUILDVARIABLES_SRCPATH} ${BUILDVARIABLES_OBJPATH} @ONLY). # Set build-time environment(s).; add_compile_definitions(CMAKE_CFG_INTDIR=""$<CONFIG>""). if(LLVM_ENABLE_MODULES); target_compile_options(llvm-config PUBLIC; ""-fmodules-ignore-macro=CMAKE_CFG_INTDIR""; ); endif(). # Add the dependency on the generation step.; add_file_dependencies(${CMAKE_CURRENT_SOURCE_DIR}/llvm-config.cpp ${BUILDVARIABLES_OBJPATH}). if(CMAKE_CROSSCOMPILING); if (LLVM_NATIVE_TOOL_DIR AND NOT LLVM_CONFIG_PATH); if (EXISTS ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); set(LLVM_CONFIG_PATH ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); endif(); endif(). if (NOT LLVM_CONFIG_PATH); build_native_tool(llvm-config LLVM_CONFIG_PATH); set(LLVM_CONFIG_PATH ""${LLVM_CONFIG_PATH}"" CACHE STRING """"). add_custom_target(NativeLLVMConfig DEPENDS ${LLVM_CONFIG_PATH}); add_dependencies(llvm-config NativeLLVMConfig); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:3625,Modifiability,config,config,3625,"SRC_DIR}); set(LLVM_OBJ_ROOT ${LLVM_BINARY_DIR}); set(LLVM_CPPFLAGS ""${LLVM_DEFINITIONS}""); set(LLVM_CFLAGS ""${LLVM_C_STD_FLAG} ${LLVM_DEFINITIONS}""); # The language standard potentially affects the ABI/API of LLVM, so we want; # to make sure it is reported by llvm-config.; set(LLVM_CXXFLAGS ""${CMAKE_CXX${CMAKE_CXX_STANDARD}_STANDARD_COMPILE_OPTION} ${LLVM_CXX_STDLIB_FLAG} ${COMPILE_FLAGS} ${LLVM_DEFINITIONS}""); set(LLVM_BUILD_SYSTEM cmake); set(LLVM_HAS_RTTI ${LLVM_CONFIG_HAS_RTTI}); set(LLVM_DYLIB_VERSION ""${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX}""). # Use the C++ link flags, since they should be a superset of C link flags.; set(LLVM_LDFLAGS ""${CMAKE_CXX_LINK_FLAGS}""); set(LLVM_BUILDMODE ${CMAKE_BUILD_TYPE}); set(LLVM_SYSTEM_LIBS ${SYSTEM_LIBS}); string(REPLACE "";"" "" "" LLVM_TARGETS_BUILT ""${LLVM_TARGETS_TO_BUILD}""); llvm_canonicalize_cmake_booleans(; LLVM_BUILD_LLVM_DYLIB; LLVM_LINK_LLVM_DYLIB; LLVM_HAS_RTTI; BUILD_SHARED_LIBS); llvm_expand_pseudo_components(LLVM_DYLIB_COMPONENTS_expanded ""${LLVM_DYLIB_COMPONENTS}""); configure_file(${BUILDVARIABLES_SRCPATH} ${BUILDVARIABLES_OBJPATH} @ONLY). # Set build-time environment(s).; add_compile_definitions(CMAKE_CFG_INTDIR=""$<CONFIG>""). if(LLVM_ENABLE_MODULES); target_compile_options(llvm-config PUBLIC; ""-fmodules-ignore-macro=CMAKE_CFG_INTDIR""; ); endif(). # Add the dependency on the generation step.; add_file_dependencies(${CMAKE_CURRENT_SOURCE_DIR}/llvm-config.cpp ${BUILDVARIABLES_OBJPATH}). if(CMAKE_CROSSCOMPILING); if (LLVM_NATIVE_TOOL_DIR AND NOT LLVM_CONFIG_PATH); if (EXISTS ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); set(LLVM_CONFIG_PATH ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); endif(); endif(). if (NOT LLVM_CONFIG_PATH); build_native_tool(llvm-config LLVM_CONFIG_PATH); set(LLVM_CONFIG_PATH ""${LLVM_CONFIG_PATH}"" CACHE STRING """"). add_custom_target(NativeLLVMConfig DEPENDS ${LLVM_CONFIG_PATH}); add_dependencies(llvm-config NativeLLVMConfig); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:3716,Modifiability,config,config,3716,"SRC_DIR}); set(LLVM_OBJ_ROOT ${LLVM_BINARY_DIR}); set(LLVM_CPPFLAGS ""${LLVM_DEFINITIONS}""); set(LLVM_CFLAGS ""${LLVM_C_STD_FLAG} ${LLVM_DEFINITIONS}""); # The language standard potentially affects the ABI/API of LLVM, so we want; # to make sure it is reported by llvm-config.; set(LLVM_CXXFLAGS ""${CMAKE_CXX${CMAKE_CXX_STANDARD}_STANDARD_COMPILE_OPTION} ${LLVM_CXX_STDLIB_FLAG} ${COMPILE_FLAGS} ${LLVM_DEFINITIONS}""); set(LLVM_BUILD_SYSTEM cmake); set(LLVM_HAS_RTTI ${LLVM_CONFIG_HAS_RTTI}); set(LLVM_DYLIB_VERSION ""${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX}""). # Use the C++ link flags, since they should be a superset of C link flags.; set(LLVM_LDFLAGS ""${CMAKE_CXX_LINK_FLAGS}""); set(LLVM_BUILDMODE ${CMAKE_BUILD_TYPE}); set(LLVM_SYSTEM_LIBS ${SYSTEM_LIBS}); string(REPLACE "";"" "" "" LLVM_TARGETS_BUILT ""${LLVM_TARGETS_TO_BUILD}""); llvm_canonicalize_cmake_booleans(; LLVM_BUILD_LLVM_DYLIB; LLVM_LINK_LLVM_DYLIB; LLVM_HAS_RTTI; BUILD_SHARED_LIBS); llvm_expand_pseudo_components(LLVM_DYLIB_COMPONENTS_expanded ""${LLVM_DYLIB_COMPONENTS}""); configure_file(${BUILDVARIABLES_SRCPATH} ${BUILDVARIABLES_OBJPATH} @ONLY). # Set build-time environment(s).; add_compile_definitions(CMAKE_CFG_INTDIR=""$<CONFIG>""). if(LLVM_ENABLE_MODULES); target_compile_options(llvm-config PUBLIC; ""-fmodules-ignore-macro=CMAKE_CFG_INTDIR""; ); endif(). # Add the dependency on the generation step.; add_file_dependencies(${CMAKE_CURRENT_SOURCE_DIR}/llvm-config.cpp ${BUILDVARIABLES_OBJPATH}). if(CMAKE_CROSSCOMPILING); if (LLVM_NATIVE_TOOL_DIR AND NOT LLVM_CONFIG_PATH); if (EXISTS ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); set(LLVM_CONFIG_PATH ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); endif(); endif(). if (NOT LLVM_CONFIG_PATH); build_native_tool(llvm-config LLVM_CONFIG_PATH); set(LLVM_CONFIG_PATH ""${LLVM_CONFIG_PATH}"" CACHE STRING """"). add_custom_target(NativeLLVMConfig DEPENDS ${LLVM_CONFIG_PATH}); add_dependencies(llvm-config NativeLLVMConfig); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:3824,Modifiability,config,config,3824,"SRC_DIR}); set(LLVM_OBJ_ROOT ${LLVM_BINARY_DIR}); set(LLVM_CPPFLAGS ""${LLVM_DEFINITIONS}""); set(LLVM_CFLAGS ""${LLVM_C_STD_FLAG} ${LLVM_DEFINITIONS}""); # The language standard potentially affects the ABI/API of LLVM, so we want; # to make sure it is reported by llvm-config.; set(LLVM_CXXFLAGS ""${CMAKE_CXX${CMAKE_CXX_STANDARD}_STANDARD_COMPILE_OPTION} ${LLVM_CXX_STDLIB_FLAG} ${COMPILE_FLAGS} ${LLVM_DEFINITIONS}""); set(LLVM_BUILD_SYSTEM cmake); set(LLVM_HAS_RTTI ${LLVM_CONFIG_HAS_RTTI}); set(LLVM_DYLIB_VERSION ""${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX}""). # Use the C++ link flags, since they should be a superset of C link flags.; set(LLVM_LDFLAGS ""${CMAKE_CXX_LINK_FLAGS}""); set(LLVM_BUILDMODE ${CMAKE_BUILD_TYPE}); set(LLVM_SYSTEM_LIBS ${SYSTEM_LIBS}); string(REPLACE "";"" "" "" LLVM_TARGETS_BUILT ""${LLVM_TARGETS_TO_BUILD}""); llvm_canonicalize_cmake_booleans(; LLVM_BUILD_LLVM_DYLIB; LLVM_LINK_LLVM_DYLIB; LLVM_HAS_RTTI; BUILD_SHARED_LIBS); llvm_expand_pseudo_components(LLVM_DYLIB_COMPONENTS_expanded ""${LLVM_DYLIB_COMPONENTS}""); configure_file(${BUILDVARIABLES_SRCPATH} ${BUILDVARIABLES_OBJPATH} @ONLY). # Set build-time environment(s).; add_compile_definitions(CMAKE_CFG_INTDIR=""$<CONFIG>""). if(LLVM_ENABLE_MODULES); target_compile_options(llvm-config PUBLIC; ""-fmodules-ignore-macro=CMAKE_CFG_INTDIR""; ); endif(). # Add the dependency on the generation step.; add_file_dependencies(${CMAKE_CURRENT_SOURCE_DIR}/llvm-config.cpp ${BUILDVARIABLES_OBJPATH}). if(CMAKE_CROSSCOMPILING); if (LLVM_NATIVE_TOOL_DIR AND NOT LLVM_CONFIG_PATH); if (EXISTS ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); set(LLVM_CONFIG_PATH ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); endif(); endif(). if (NOT LLVM_CONFIG_PATH); build_native_tool(llvm-config LLVM_CONFIG_PATH); set(LLVM_CONFIG_PATH ""${LLVM_CONFIG_PATH}"" CACHE STRING """"). add_custom_target(NativeLLVMConfig DEPENDS ${LLVM_CONFIG_PATH}); add_dependencies(llvm-config NativeLLVMConfig); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:3998,Modifiability,config,config,3998,"SRC_DIR}); set(LLVM_OBJ_ROOT ${LLVM_BINARY_DIR}); set(LLVM_CPPFLAGS ""${LLVM_DEFINITIONS}""); set(LLVM_CFLAGS ""${LLVM_C_STD_FLAG} ${LLVM_DEFINITIONS}""); # The language standard potentially affects the ABI/API of LLVM, so we want; # to make sure it is reported by llvm-config.; set(LLVM_CXXFLAGS ""${CMAKE_CXX${CMAKE_CXX_STANDARD}_STANDARD_COMPILE_OPTION} ${LLVM_CXX_STDLIB_FLAG} ${COMPILE_FLAGS} ${LLVM_DEFINITIONS}""); set(LLVM_BUILD_SYSTEM cmake); set(LLVM_HAS_RTTI ${LLVM_CONFIG_HAS_RTTI}); set(LLVM_DYLIB_VERSION ""${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX}""). # Use the C++ link flags, since they should be a superset of C link flags.; set(LLVM_LDFLAGS ""${CMAKE_CXX_LINK_FLAGS}""); set(LLVM_BUILDMODE ${CMAKE_BUILD_TYPE}); set(LLVM_SYSTEM_LIBS ${SYSTEM_LIBS}); string(REPLACE "";"" "" "" LLVM_TARGETS_BUILT ""${LLVM_TARGETS_TO_BUILD}""); llvm_canonicalize_cmake_booleans(; LLVM_BUILD_LLVM_DYLIB; LLVM_LINK_LLVM_DYLIB; LLVM_HAS_RTTI; BUILD_SHARED_LIBS); llvm_expand_pseudo_components(LLVM_DYLIB_COMPONENTS_expanded ""${LLVM_DYLIB_COMPONENTS}""); configure_file(${BUILDVARIABLES_SRCPATH} ${BUILDVARIABLES_OBJPATH} @ONLY). # Set build-time environment(s).; add_compile_definitions(CMAKE_CFG_INTDIR=""$<CONFIG>""). if(LLVM_ENABLE_MODULES); target_compile_options(llvm-config PUBLIC; ""-fmodules-ignore-macro=CMAKE_CFG_INTDIR""; ); endif(). # Add the dependency on the generation step.; add_file_dependencies(${CMAKE_CURRENT_SOURCE_DIR}/llvm-config.cpp ${BUILDVARIABLES_OBJPATH}). if(CMAKE_CROSSCOMPILING); if (LLVM_NATIVE_TOOL_DIR AND NOT LLVM_CONFIG_PATH); if (EXISTS ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); set(LLVM_CONFIG_PATH ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); endif(); endif(). if (NOT LLVM_CONFIG_PATH); build_native_tool(llvm-config LLVM_CONFIG_PATH); set(LLVM_CONFIG_PATH ""${LLVM_CONFIG_PATH}"" CACHE STRING """"). add_custom_target(NativeLLVMConfig DEPENDS ${LLVM_CONFIG_PATH}); add_dependencies(llvm-config NativeLLVMConfig); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:3893,Performance,CACHE,CACHE,3893,"SRC_DIR}); set(LLVM_OBJ_ROOT ${LLVM_BINARY_DIR}); set(LLVM_CPPFLAGS ""${LLVM_DEFINITIONS}""); set(LLVM_CFLAGS ""${LLVM_C_STD_FLAG} ${LLVM_DEFINITIONS}""); # The language standard potentially affects the ABI/API of LLVM, so we want; # to make sure it is reported by llvm-config.; set(LLVM_CXXFLAGS ""${CMAKE_CXX${CMAKE_CXX_STANDARD}_STANDARD_COMPILE_OPTION} ${LLVM_CXX_STDLIB_FLAG} ${COMPILE_FLAGS} ${LLVM_DEFINITIONS}""); set(LLVM_BUILD_SYSTEM cmake); set(LLVM_HAS_RTTI ${LLVM_CONFIG_HAS_RTTI}); set(LLVM_DYLIB_VERSION ""${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX}""). # Use the C++ link flags, since they should be a superset of C link flags.; set(LLVM_LDFLAGS ""${CMAKE_CXX_LINK_FLAGS}""); set(LLVM_BUILDMODE ${CMAKE_BUILD_TYPE}); set(LLVM_SYSTEM_LIBS ${SYSTEM_LIBS}); string(REPLACE "";"" "" "" LLVM_TARGETS_BUILT ""${LLVM_TARGETS_TO_BUILD}""); llvm_canonicalize_cmake_booleans(; LLVM_BUILD_LLVM_DYLIB; LLVM_LINK_LLVM_DYLIB; LLVM_HAS_RTTI; BUILD_SHARED_LIBS); llvm_expand_pseudo_components(LLVM_DYLIB_COMPONENTS_expanded ""${LLVM_DYLIB_COMPONENTS}""); configure_file(${BUILDVARIABLES_SRCPATH} ${BUILDVARIABLES_OBJPATH} @ONLY). # Set build-time environment(s).; add_compile_definitions(CMAKE_CFG_INTDIR=""$<CONFIG>""). if(LLVM_ENABLE_MODULES); target_compile_options(llvm-config PUBLIC; ""-fmodules-ignore-macro=CMAKE_CFG_INTDIR""; ); endif(). # Add the dependency on the generation step.; add_file_dependencies(${CMAKE_CURRENT_SOURCE_DIR}/llvm-config.cpp ${BUILDVARIABLES_OBJPATH}). if(CMAKE_CROSSCOMPILING); if (LLVM_NATIVE_TOOL_DIR AND NOT LLVM_CONFIG_PATH); if (EXISTS ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); set(LLVM_CONFIG_PATH ""${LLVM_NATIVE_TOOL_DIR}/llvm-config${LLVM_HOST_EXECUTABLE_SUFFIX}""); endif(); endif(). if (NOT LLVM_CONFIG_PATH); build_native_tool(llvm-config LLVM_CONFIG_PATH); set(LLVM_CONFIG_PATH ""${LLVM_CONFIG_PATH}"" CACHE STRING """"). add_custom_target(NativeLLVMConfig DEPENDS ${LLVM_CONFIG_PATH}); add_dependencies(llvm-config NativeLLVMConfig); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt:386,Safety,Avoid,Avoiding,386,"set(LLVM_LINK_COMPONENTS; Support; TargetParser; ). set(BUILDVARIABLES_SRCPATH ${CMAKE_CURRENT_SOURCE_DIR}/BuildVariables.inc.in); set(BUILDVARIABLES_OBJPATH ${CMAKE_CURRENT_BINARY_DIR}/BuildVariables.inc). # Add the llvm-config tool.; add_llvm_tool(llvm-config; llvm-config.cpp; # This utility doesn't use much of LLVM, so linking a shared library for the; # entire thing is overkill. Avoiding that especially saves on build time when cross; # compiling LLVM and building both cross and native `llvm-config`s. We don't; # want to build an entire native libLLVM.so in addition to the cross one just; # for the native `llvm-config`!; DISABLE_LLVM_LINK_LLVM_DYLIB; ). # Compute the substitution values for various items.; get_property(SUPPORT_SYSTEM_LIBS TARGET LLVMSupport PROPERTY LLVM_SYSTEM_LIBS); get_property(WINDOWSMANIFEST_SYSTEM_LIBS TARGET LLVMWindowsManifest PROPERTY LLVM_SYSTEM_LIBS). foreach(l ${SUPPORT_SYSTEM_LIBS} ${WINDOWSMANIFEST_SYSTEM_LIBS}); if(MSVC); if(IS_ABSOLUTE ${l}); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}.lib""); endif(); else(); if (l MATCHES ""^-""); # If it's an option, pass it without changes.; set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); # Otherwise assume it's a library name we need to link with.; if(IS_ABSOLUTE ${l}); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""${l}""); else(); set(SYSTEM_LIBS ${SYSTEM_LIBS} ""-l${l}""); endif(); endif(); endif(); endforeach(). string(REPLACE "";"" "" "" SYSTEM_LIBS ""${SYSTEM_LIBS}""). # Fetch target specific compile options, e.g. RTTI option; get_property(COMPILE_FLAGS TARGET llvm-config PROPERTY COMPILE_FLAGS). # NOTE: We don't want to start extracting any random C/CXX flags that the; # user may add that could affect the ABI. We only want to extract flags; # that have been added by the LLVM build system.; string(REGEX MATCH ""-stdlib=[^ ]+"" LLVM_CXX_STDLIB_FLAG ${CMAKE_CXX_FLAGS}); string(REGEX MATCH ""-std=[^ ]+"" LLVM_C_STD_FLAG ${CMAKE_C_FLAGS}). # Use configure_file to create BuildVaria",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-config/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-cov/CMakeLists.txt:353,Testability,Test,TestingSupport,353,set(LLVM_LINK_COMPONENTS; Core; Support; Object; Coverage; ProfileData; TargetParser; ). add_llvm_tool(llvm-cov; llvm-cov.cpp; gcov.cpp; CodeCoverage.cpp; CoverageExporterJson.cpp; CoverageExporterLcov.cpp; CoverageFilters.cpp; CoverageReport.cpp; CoverageSummaryInfo.cpp; SourceCoverageView.cpp; SourceCoverageViewHTML.cpp; SourceCoverageViewText.cpp; TestingSupport.cpp; ). target_link_libraries(llvm-cov PRIVATE LLVMDebuginfod); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-cov/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-cov/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-cxxdump/CMakeLists.txt:113,Availability,Error,Error,113,set(LLVM_LINK_COMPONENTS; AllTargetsInfos; MC; Object; Support; ). add_llvm_tool(llvm-cxxdump; llvm-cxxdump.cpp; Error.cpp; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-cxxdump/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-cxxdump/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-cxxfilt/CMakeLists.txt:249,Integrability,DEPEND,DEPENDS,249,set(LLVM_LINK_COMPONENTS; Demangle; Option; Support; TargetParser; ). set(LLVM_TARGET_DEFINITIONS Opts.td); tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(CxxfiltOptsTableGen). add_llvm_tool(llvm-cxxfilt; llvm-cxxfilt.cpp. DEPENDS; CxxfiltOptsTableGen; GENERATE_DRIVER; ). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(c++filt llvm-cxxfilt); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-cxxfilt/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-cxxfilt/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:4613,Energy Efficiency,efficient,efficient,4613,"---------------------===//; // LVDoubleMap to return optional<ValueType> instead of null pointer.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1294164. The more idiomatic LLVM way to handle this would be to have 'find '; return Optional<ValueType>. //===----------------------------------------------------------------------===//; // Pass references instead of pointers (Comparison functions).; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125782#inline-1293920. In the comparison functions, pass references instead of pointers (when; pointers cannot be null). //===----------------------------------------------------------------------===//; // Use StringMap where possible.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1294211. LLVM has a StringMap class that is advertised as more efficient than; std::map<std::string, ValueType>. Mainly it does fewer allocations; because the key is not a std::string. Replace the use of std::map<std::string, ValueType> with String Map.; One specific case is the LVSymbolNames definitions. //===----------------------------------------------------------------------===//; // Calculate unique offset for CodeView elements.; //===----------------------------------------------------------------------===//; In order to have the same logical functionality as the ELF Reader, such; as:. - find scopes contribution to debug info; - sort by its physical location. The logical elements must have an unique offset (similar like the DWARF; DIE offset). //===----------------------------------------------------------------------===//; // Move 'initializeFileAndStringTables' to the COFF Library.; //===----------------------------------------------------------------------===//; There is some code in the CodeView reader that was extracted/adapted; fro",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:5598,Energy Efficiency,adapt,adapted,5598,"e-1294211. LLVM has a StringMap class that is advertised as more efficient than; std::map<std::string, ValueType>. Mainly it does fewer allocations; because the key is not a std::string. Replace the use of std::map<std::string, ValueType> with String Map.; One specific case is the LVSymbolNames definitions. //===----------------------------------------------------------------------===//; // Calculate unique offset for CodeView elements.; //===----------------------------------------------------------------------===//; In order to have the same logical functionality as the ELF Reader, such; as:. - find scopes contribution to debug info; - sort by its physical location. The logical elements must have an unique offset (similar like the DWARF; DIE offset). //===----------------------------------------------------------------------===//; // Move 'initializeFileAndStringTables' to the COFF Library.; //===----------------------------------------------------------------------===//; There is some code in the CodeView reader that was extracted/adapted; from 'tools/llvm-readobj/COFFDumper.cpp' that can be moved to the COFF; library. We had a similar case with code shared with llvm-pdbutil that was moved; to the PDB library: https://reviews.llvm.org/D122226. //===----------------------------------------------------------------------===//; // Move 'getSymbolKindName'/'formatRegisterId' to the CodeView Library.; //===----------------------------------------------------------------------===//; There is some code in the CodeView reader that was extracted/adapted; from 'lib/DebugInfo/CodeView/SymbolDumper.cpp' that can be used. //===----------------------------------------------------------------------===//; // Use of std::unordered_set instead of std::set.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125784#inline-1221421. Replace the std::set usage for DeducedScopes, UnresolvedScopes and; IdentifiedNamespaces with std",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:6113,Energy Efficiency,adapt,adapted,6113,"==//; In order to have the same logical functionality as the ELF Reader, such; as:. - find scopes contribution to debug info; - sort by its physical location. The logical elements must have an unique offset (similar like the DWARF; DIE offset). //===----------------------------------------------------------------------===//; // Move 'initializeFileAndStringTables' to the COFF Library.; //===----------------------------------------------------------------------===//; There is some code in the CodeView reader that was extracted/adapted; from 'tools/llvm-readobj/COFFDumper.cpp' that can be moved to the COFF; library. We had a similar case with code shared with llvm-pdbutil that was moved; to the PDB library: https://reviews.llvm.org/D122226. //===----------------------------------------------------------------------===//; // Move 'getSymbolKindName'/'formatRegisterId' to the CodeView Library.; //===----------------------------------------------------------------------===//; There is some code in the CodeView reader that was extracted/adapted; from 'lib/DebugInfo/CodeView/SymbolDumper.cpp' that can be used. //===----------------------------------------------------------------------===//; // Use of std::unordered_set instead of std::set.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125784#inline-1221421. Replace the std::set usage for DeducedScopes, UnresolvedScopes and; IdentifiedNamespaces with std::unordered_set and get the benefit; of the O(1) while inserting/searching, as the order is not important. //===----------------------------------------------------------------------===//; // Optimize 'LVNamespaceDeduction::find' funtion.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125784#inline-1296195. Optimize the 'find' method to use the proposed code:. LVStringRefs::iterator Iter = std::find_if(Components.begin(), Components.end(),; [](StringRe",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:8128,Energy Efficiency,allocate,allocate,8128,"tifiedNamespaces.end();; });; LVStringRefs::size_type FirstNonNamespace = std::distance(Components.begin(), Iter);. //===----------------------------------------------------------------------===//; // Move all the printing support to a common module.; //===----------------------------------------------------------------------===//; Factor out printing functionality from the logical elements into a; common module. //===----------------------------------------------------------------------===//; // Refactor 'LVBinaryReader::processLines'.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1246155; https://reviews.llvm.org/D137156. During the traversal of the debug information sections, we created the; logical lines representing the disassembled instructions from the text; section and the logical lines representing the line records from the; debug line section. Using the ranges associated with the logical scopes,; we will allocate those logical lines to their logical scopes. Consider the case when any of those lines become orphans, causing; incorrect scope parent for disassembly or line records. //===----------------------------------------------------------------------===//; // Add support for '-ffunction-sections'.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1295012. Only linked executables are handled. It does not support relocatable; files compiled with -ffunction-sections. //===----------------------------------------------------------------------===//; // Add support for DWARF v5 .debug_names section.; // Add support for CodeView public symbols stream.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1294142. The ELF and CodeView readers use the public names information to create; the instructions (LVLineAssembler). Instead of relying on DWAR",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:2133,Modifiability,Rewrite,Rewrite,2133,"st instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch between %d/%x format strings and uint64_t type.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137400; https://github.com/llvm/llvm-project/issues/58758. Incorrect printing of uint64_t on 32-bit platforms.; Add the PRIx64 specifier to the printing code (format()). //===----------------------------------------------------------------------===//; // Remove 'LVScope::Children' container.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1373902. Use a chaining iterator over the other containers rather than keep a; separate container 'Children' that mirrors their contents. //===-------------",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:5598,Modifiability,adapt,adapted,5598,"e-1294211. LLVM has a StringMap class that is advertised as more efficient than; std::map<std::string, ValueType>. Mainly it does fewer allocations; because the key is not a std::string. Replace the use of std::map<std::string, ValueType> with String Map.; One specific case is the LVSymbolNames definitions. //===----------------------------------------------------------------------===//; // Calculate unique offset for CodeView elements.; //===----------------------------------------------------------------------===//; In order to have the same logical functionality as the ELF Reader, such; as:. - find scopes contribution to debug info; - sort by its physical location. The logical elements must have an unique offset (similar like the DWARF; DIE offset). //===----------------------------------------------------------------------===//; // Move 'initializeFileAndStringTables' to the COFF Library.; //===----------------------------------------------------------------------===//; There is some code in the CodeView reader that was extracted/adapted; from 'tools/llvm-readobj/COFFDumper.cpp' that can be moved to the COFF; library. We had a similar case with code shared with llvm-pdbutil that was moved; to the PDB library: https://reviews.llvm.org/D122226. //===----------------------------------------------------------------------===//; // Move 'getSymbolKindName'/'formatRegisterId' to the CodeView Library.; //===----------------------------------------------------------------------===//; There is some code in the CodeView reader that was extracted/adapted; from 'lib/DebugInfo/CodeView/SymbolDumper.cpp' that can be used. //===----------------------------------------------------------------------===//; // Use of std::unordered_set instead of std::set.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125784#inline-1221421. Replace the std::set usage for DeducedScopes, UnresolvedScopes and; IdentifiedNamespaces with std",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:6113,Modifiability,adapt,adapted,6113,"==//; In order to have the same logical functionality as the ELF Reader, such; as:. - find scopes contribution to debug info; - sort by its physical location. The logical elements must have an unique offset (similar like the DWARF; DIE offset). //===----------------------------------------------------------------------===//; // Move 'initializeFileAndStringTables' to the COFF Library.; //===----------------------------------------------------------------------===//; There is some code in the CodeView reader that was extracted/adapted; from 'tools/llvm-readobj/COFFDumper.cpp' that can be moved to the COFF; library. We had a similar case with code shared with llvm-pdbutil that was moved; to the PDB library: https://reviews.llvm.org/D122226. //===----------------------------------------------------------------------===//; // Move 'getSymbolKindName'/'formatRegisterId' to the CodeView Library.; //===----------------------------------------------------------------------===//; There is some code in the CodeView reader that was extracted/adapted; from 'lib/DebugInfo/CodeView/SymbolDumper.cpp' that can be used. //===----------------------------------------------------------------------===//; // Use of std::unordered_set instead of std::set.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125784#inline-1221421. Replace the std::set usage for DeducedScopes, UnresolvedScopes and; IdentifiedNamespaces with std::unordered_set and get the benefit; of the O(1) while inserting/searching, as the order is not important. //===----------------------------------------------------------------------===//; // Optimize 'LVNamespaceDeduction::find' funtion.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125784#inline-1296195. Optimize the 'find' method to use the proposed code:. LVStringRefs::iterator Iter = std::find_if(Components.begin(), Components.end(),; [](StringRe",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:7625,Modifiability,Refactor,Refactor,7625," inserting/searching, as the order is not important. //===----------------------------------------------------------------------===//; // Optimize 'LVNamespaceDeduction::find' funtion.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125784#inline-1296195. Optimize the 'find' method to use the proposed code:. LVStringRefs::iterator Iter = std::find_if(Components.begin(), Components.end(),; [](StringRef Name) {; return IdentifiedNamespaces.find(Name) == IdentifiedNamespaces.end();; });; LVStringRefs::size_type FirstNonNamespace = std::distance(Components.begin(), Iter);. //===----------------------------------------------------------------------===//; // Move all the printing support to a common module.; //===----------------------------------------------------------------------===//; Factor out printing functionality from the logical elements into a; common module. //===----------------------------------------------------------------------===//; // Refactor 'LVBinaryReader::processLines'.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1246155; https://reviews.llvm.org/D137156. During the traversal of the debug information sections, we created the; logical lines representing the disassembled instructions from the text; section and the logical lines representing the line records from the; debug line section. Using the ranges associated with the logical scopes,; we will allocate those logical lines to their logical scopes. Consider the case when any of those lines become orphans, causing; incorrect scope parent for disassembly or line records. //===----------------------------------------------------------------------===//; // Add support for '-ffunction-sections'.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1295012. Only linked executables are handled. It d",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:10057,Modifiability,Extend,Extended,10057,"---------------------------------===//; https://reviews.llvm.org/D125783#inline-1295012. Only linked executables are handled. It does not support relocatable; files compiled with -ffunction-sections. //===----------------------------------------------------------------------===//; // Add support for DWARF v5 .debug_names section.; // Add support for CodeView public symbols stream.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1294142. The ELF and CodeView readers use the public names information to create; the instructions (LVLineAssembler). Instead of relying on DWARF section; names (.debug_pubnames, .debug_names) and CodeView public symbol stream; (S_PUB32), the readers collects the needed information while processing; the debug information. If the object file supports the above section names and stream, use them; to create the public names. //===----------------------------------------------------------------------===//; // Add support for some extra DWARF locations.; //===----------------------------------------------------------------------===//; The following DWARF debug location operands are not supported:. - DW_OP_const_type; - DW_OP_entry_value; - DW_OP_implicit_value. //===----------------------------------------------------------------------===//; // Add support for additional binary formats.; //===----------------------------------------------------------------------===//; - WebAssembly (Wasm).; https://github.com/llvm/llvm-project/issues/57040#issuecomment-1211336680. - Extended COFF (XCOFF). //===----------------------------------------------------------------------===//; // Add support for JSON or YAML.; //===----------------------------------------------------------------------===//; The logical view uses its own and non-standard free form text when; displaying information on logical elements. //===----------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:6740,Performance,Optimiz,Optimize,6740,"shared with llvm-pdbutil that was moved; to the PDB library: https://reviews.llvm.org/D122226. //===----------------------------------------------------------------------===//; // Move 'getSymbolKindName'/'formatRegisterId' to the CodeView Library.; //===----------------------------------------------------------------------===//; There is some code in the CodeView reader that was extracted/adapted; from 'lib/DebugInfo/CodeView/SymbolDumper.cpp' that can be used. //===----------------------------------------------------------------------===//; // Use of std::unordered_set instead of std::set.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125784#inline-1221421. Replace the std::set usage for DeducedScopes, UnresolvedScopes and; IdentifiedNamespaces with std::unordered_set and get the benefit; of the O(1) while inserting/searching, as the order is not important. //===----------------------------------------------------------------------===//; // Optimize 'LVNamespaceDeduction::find' funtion.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125784#inline-1296195. Optimize the 'find' method to use the proposed code:. LVStringRefs::iterator Iter = std::find_if(Components.begin(), Components.end(),; [](StringRef Name) {; return IdentifiedNamespaces.find(Name) == IdentifiedNamespaces.end();; });; LVStringRefs::size_type FirstNonNamespace = std::distance(Components.begin(), Iter);. //===----------------------------------------------------------------------===//; // Move all the printing support to a common module.; //===----------------------------------------------------------------------===//; Factor out printing functionality from the logical elements into a; common module. //===----------------------------------------------------------------------===//; // Refactor 'LVBinaryReader::processLines'.; //===------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:6919,Performance,Optimiz,Optimize,6919,"o the CodeView Library.; //===----------------------------------------------------------------------===//; There is some code in the CodeView reader that was extracted/adapted; from 'lib/DebugInfo/CodeView/SymbolDumper.cpp' that can be used. //===----------------------------------------------------------------------===//; // Use of std::unordered_set instead of std::set.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125784#inline-1221421. Replace the std::set usage for DeducedScopes, UnresolvedScopes and; IdentifiedNamespaces with std::unordered_set and get the benefit; of the O(1) while inserting/searching, as the order is not important. //===----------------------------------------------------------------------===//; // Optimize 'LVNamespaceDeduction::find' funtion.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125784#inline-1296195. Optimize the 'find' method to use the proposed code:. LVStringRefs::iterator Iter = std::find_if(Components.begin(), Components.end(),; [](StringRef Name) {; return IdentifiedNamespaces.find(Name) == IdentifiedNamespaces.end();; });; LVStringRefs::size_type FirstNonNamespace = std::distance(Components.begin(), Iter);. //===----------------------------------------------------------------------===//; // Move all the printing support to a common module.; //===----------------------------------------------------------------------===//; Factor out printing functionality from the logical elements into a; common module. //===----------------------------------------------------------------------===//; // Refactor 'LVBinaryReader::processLines'.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1246155; https://reviews.llvm.org/D137156. During the traversal of the debug information sections, we created the; logical lines representing the disassembl",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:973,Security,expose,expose,973,"//===- llvm/tools/llvm-debuginfo-analyzer/README.txt ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains notes collected during the development, review and test.; // It describes limitations, know issues and future work.; //; //===----------------------------------------------------------------------===//. //===----------------------------------------------------------------------===//; // Remove the use of macros in 'LVReader.h' that describe the bumpallocators.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.;",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:993,Security,hash,hasher,993,"//===- llvm/tools/llvm-debuginfo-analyzer/README.txt ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains notes collected during the development, review and test.; // It describes limitations, know issues and future work.; //; //===----------------------------------------------------------------------===//. //===----------------------------------------------------------------------===//; // Remove the use of macros in 'LVReader.h' that describe the bumpallocators.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.;",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1410,Security,expose,exposed,1410,"and test.; // It describes limitations, know issues and future work.; //; //===----------------------------------------------------------------------===//. //===----------------------------------------------------------------------===//; // Remove the use of macros in 'LVReader.h' that describe the bumpallocators.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch bet",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1512,Security,validat,validate,1512,"and test.; // It describes limitations, know issues and future work.; //; //===----------------------------------------------------------------------===//. //===----------------------------------------------------------------------===//; // Remove the use of macros in 'LVReader.h' that describe the bumpallocators.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch bet",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:453,Testability,test,test,453,"//===- llvm/tools/llvm-debuginfo-analyzer/README.txt ----------------------===//; //; // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains notes collected during the development, review and test.; // It describes limitations, know issues and future work.; //; //===----------------------------------------------------------------------===//. //===----------------------------------------------------------------------===//; // Remove the use of macros in 'LVReader.h' that describe the bumpallocators.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.;",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1173,Testability,test,test,1173,"LVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains notes collected during the development, review and test.; // It describes limitations, know issues and future work.; //; //===----------------------------------------------------------------------===//. //===----------------------------------------------------------------------===//; // Remove the use of macros in 'LVReader.h' that describe the bumpallocators.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unitte",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1196,Testability,test,test,1196,"LVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains notes collected during the development, review and test.; // It describes limitations, know issues and future work.; //; //===----------------------------------------------------------------------===//. //===----------------------------------------------------------------------===//; // Remove the use of macros in 'LVReader.h' that describe the bumpallocators.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unitte",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1209,Testability,log,logical,1209,"LVM Exceptions.; // See https://llvm.org/LICENSE.txt for license information.; // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception; //; //===----------------------------------------------------------------------===//; //; // This file contains notes collected during the development, review and test.; // It describes limitations, know issues and future work.; //; //===----------------------------------------------------------------------===//. //===----------------------------------------------------------------------===//; // Remove the use of macros in 'LVReader.h' that describe the bumpallocators.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unitte",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1503,Testability,test,tests,1503,"and test.; // It describes limitations, know issues and future work.; //; //===----------------------------------------------------------------------===//. //===----------------------------------------------------------------------===//; // Remove the use of macros in 'LVReader.h' that describe the bumpallocators.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch bet",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1525,Testability,log,logical,1525,"and test.; // It describes limitations, know issues and future work.; //; //===----------------------------------------------------------------------===//. //===----------------------------------------------------------------------===//; // Remove the use of macros in 'LVReader.h' that describe the bumpallocators.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch bet",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1603,Testability,Log,LogicalView,1603,"-----------===//. //===----------------------------------------------------------------------===//; // Remove the use of macros in 'LVReader.h' that describe the bumpallocators.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch between %d/%x format strings and uint64_t type.; //===----------------------------------------------------------------------===//; https://rev",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1677,Testability,Log,LogicalView,1677,"------------===//; // Remove the use of macros in 'LVReader.h' that describe the bumpallocators.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch between %d/%x format strings and uint64_t type.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137400; https://github.com/llvm/llvm-project/issues/58758. Incorre",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1717,Testability,test,tests,1717,"he bumpallocators.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch between %d/%x format strings and uint64_t type.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137400; https://github.com/llvm/llvm-project/issues/58758. Incorrect printing of uint64_t on 32-bit platforms.; Add the PRIx64 specifier to the",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1743,Testability,test,test,1743,"he bumpallocators.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch between %d/%x format strings and uint64_t type.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137400; https://github.com/llvm/llvm-project/issues/58758. Incorrect printing of uint64_t on 32-bit platforms.; Add the PRIx64 specifier to the",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1758,Testability,Log,LogicalView,1758,"he bumpallocators.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch between %d/%x format strings and uint64_t type.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137400; https://github.com/llvm/llvm-project/issues/58758. Incorrect printing of uint64_t on 32-bit platforms.; Add the PRIx64 specifier to the",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1785,Testability,test,test,1785,"--------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch between %d/%x format strings and uint64_t type.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137400; https://github.com/llvm/llvm-project/issues/58758. Incorrect printing of uint64_t on 32-bit platforms.; Add the PRIx64 specifier to the printing code (format()). //===--------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1809,Testability,test,test,1809,"--------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch between %d/%x format strings and uint64_t type.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137400; https://github.com/llvm/llvm-project/issues/58758. Incorrect printing of uint64_t on 32-bit platforms.; Add the PRIx64 specifier to the printing code (format()). //===--------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1824,Testability,Log,LogicalView,1824,"--------------------------===//; https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch between %d/%x format strings and uint64_t type.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137400; https://github.com/llvm/llvm-project/issues/58758. Incorrect printing of uint64_t on 32-bit platforms.; Add the PRIx64 specifier to the printing code (format()). //===--------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1846,Testability,test,test,1846,"https://reviews.llvm.org/D137933#inline-1389904. Use a standard (or LLVM) map with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch between %d/%x format strings and uint64_t type.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137400; https://github.com/llvm/llvm-project/issues/58758. Incorrect printing of uint64_t on 32-bit platforms.; Add the PRIx64 specifier to the printing code (format()). //===----------------------------------------------------------------------=",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:1994,Testability,test,tests,1994," with typeinfo (would need a specialization; to expose equality and hasher) for the allocators and the creation; functions could be a function template. //===----------------------------------------------------------------------===//; // Use a lit test instead of a unit test for the logical readers.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch between %d/%x format strings and uint64_t type.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137400; https://github.com/llvm/llvm-project/issues/58758. Incorrect printing of uint64_t on 32-bit platforms.; Add the PRIx64 specifier to the printing code (format()). //===----------------------------------------------------------------------===//; // Remove 'LVScope::Children' container.; //===------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:2295,Testability,test,test,2295,"s://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch between %d/%x format strings and uint64_t type.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137400; https://github.com/llvm/llvm-project/issues/58758. Incorrect printing of uint64_t on 32-bit platforms.; Add the PRIx64 specifier to the printing code (format()). //===----------------------------------------------------------------------===//; // Remove 'LVScope::Children' container.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1373902. Use a chaining iterator over the other containers rather than keep a; separate container 'Children' that mirrors their contents. //===----------------------------------------------------------------------===//; // Use TableGen for command line options.; //===--------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:2318,Testability,test,test,2318,"s://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch between %d/%x format strings and uint64_t type.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137400; https://github.com/llvm/llvm-project/issues/58758. Incorrect printing of uint64_t on 32-bit platforms.; Add the PRIx64 specifier to the printing code (format()). //===----------------------------------------------------------------------===//; // Remove 'LVScope::Children' container.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1373902. Use a chaining iterator over the other containers rather than keep a; separate container 'Children' that mirrors their contents. //===----------------------------------------------------------------------===//; // Use TableGen for command line options.; //===--------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:2331,Testability,log,logical,2331,"s://reviews.llvm.org/D125783#inline-1324376. As the DebugInfoLogicalView library is sufficiently exposed via the; llvm-debuginfo-analyzer tool, follow the LLVM general approach and; use LIT tests to validate the logical readers. Convert the unitests:; llvm-project/llvm/unittests/DebugInfo/LogicalView/CodeViewReaderTest.cpp; llvm-project/llvm/unittests/DebugInfo/LogicalView/ELFReaderTest.cpp. into LIT tests:; llvm-project/llvm/test/DebugInfo/LogicalView/CodeViewReader.test; llvm-project/llvm/test/DebugInfo/LogicalView/ELFReader.test. //===----------------------------------------------------------------------===//; // Eliminate calls to 'getInputFileDirectory()' in the unit tests.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1324359. Rewrite the unittests 'LFReaderTest' and 'CodeViewReaderTest'to eliminate; the call:. getInputFileDirectory(). as use of that call is discouraged. See: Use a lit test instead of a unit test for the logical readers. //===----------------------------------------------------------------------===//; // Fix mismatch between %d/%x format strings and uint64_t type.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137400; https://github.com/llvm/llvm-project/issues/58758. Incorrect printing of uint64_t on 32-bit platforms.; Add the PRIx64 specifier to the printing code (format()). //===----------------------------------------------------------------------===//; // Remove 'LVScope::Children' container.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D137933#inline-1373902. Use a chaining iterator over the other containers rather than keep a; separate container 'Children' that mirrors their contents. //===----------------------------------------------------------------------===//; // Use TableGen for command line options.; //===--------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:5098,Testability,log,logical,5098,"n functions).; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125782#inline-1293920. In the comparison functions, pass references instead of pointers (when; pointers cannot be null). //===----------------------------------------------------------------------===//; // Use StringMap where possible.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1294211. LLVM has a StringMap class that is advertised as more efficient than; std::map<std::string, ValueType>. Mainly it does fewer allocations; because the key is not a std::string. Replace the use of std::map<std::string, ValueType> with String Map.; One specific case is the LVSymbolNames definitions. //===----------------------------------------------------------------------===//; // Calculate unique offset for CodeView elements.; //===----------------------------------------------------------------------===//; In order to have the same logical functionality as the ELF Reader, such; as:. - find scopes contribution to debug info; - sort by its physical location. The logical elements must have an unique offset (similar like the DWARF; DIE offset). //===----------------------------------------------------------------------===//; // Move 'initializeFileAndStringTables' to the COFF Library.; //===----------------------------------------------------------------------===//; There is some code in the CodeView reader that was extracted/adapted; from 'tools/llvm-readobj/COFFDumper.cpp' that can be moved to the COFF; library. We had a similar case with code shared with llvm-pdbutil that was moved; to the PDB library: https://reviews.llvm.org/D122226. //===----------------------------------------------------------------------===//; // Move 'getSymbolKindName'/'formatRegisterId' to the CodeView Library.; //===----------------------------------------------------------------------===//; There is some cod",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:5229,Testability,log,logical,5229,"of pointers (when; pointers cannot be null). //===----------------------------------------------------------------------===//; // Use StringMap where possible.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1294211. LLVM has a StringMap class that is advertised as more efficient than; std::map<std::string, ValueType>. Mainly it does fewer allocations; because the key is not a std::string. Replace the use of std::map<std::string, ValueType> with String Map.; One specific case is the LVSymbolNames definitions. //===----------------------------------------------------------------------===//; // Calculate unique offset for CodeView elements.; //===----------------------------------------------------------------------===//; In order to have the same logical functionality as the ELF Reader, such; as:. - find scopes contribution to debug info; - sort by its physical location. The logical elements must have an unique offset (similar like the DWARF; DIE offset). //===----------------------------------------------------------------------===//; // Move 'initializeFileAndStringTables' to the COFF Library.; //===----------------------------------------------------------------------===//; There is some code in the CodeView reader that was extracted/adapted; from 'tools/llvm-readobj/COFFDumper.cpp' that can be moved to the COFF; library. We had a similar case with code shared with llvm-pdbutil that was moved; to the PDB library: https://reviews.llvm.org/D122226. //===----------------------------------------------------------------------===//; // Move 'getSymbolKindName'/'formatRegisterId' to the CodeView Library.; //===----------------------------------------------------------------------===//; There is some code in the CodeView reader that was extracted/adapted; from 'lib/DebugInfo/CodeView/SymbolDumper.cpp' that can be used. //===----------------------------------------------------------------------===//",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:7500,Testability,log,logical,7500,"ce the std::set usage for DeducedScopes, UnresolvedScopes and; IdentifiedNamespaces with std::unordered_set and get the benefit; of the O(1) while inserting/searching, as the order is not important. //===----------------------------------------------------------------------===//; // Optimize 'LVNamespaceDeduction::find' funtion.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125784#inline-1296195. Optimize the 'find' method to use the proposed code:. LVStringRefs::iterator Iter = std::find_if(Components.begin(), Components.end(),; [](StringRef Name) {; return IdentifiedNamespaces.find(Name) == IdentifiedNamespaces.end();; });; LVStringRefs::size_type FirstNonNamespace = std::distance(Components.begin(), Iter);. //===----------------------------------------------------------------------===//; // Move all the printing support to a common module.; //===----------------------------------------------------------------------===//; Factor out printing functionality from the logical elements into a; common module. //===----------------------------------------------------------------------===//; // Refactor 'LVBinaryReader::processLines'.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1246155; https://reviews.llvm.org/D137156. During the traversal of the debug information sections, we created the; logical lines representing the disassembled instructions from the text; section and the logical lines representing the line records from the; debug line section. Using the ranges associated with the logical scopes,; we will allocate those logical lines to their logical scopes. Consider the case when any of those lines become orphans, causing; incorrect scope parent for disassembly or line records. //===----------------------------------------------------------------------===//; // Add support for '-ffunction-sections'.; //===---------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:7904,Testability,log,logical,7904," use the proposed code:. LVStringRefs::iterator Iter = std::find_if(Components.begin(), Components.end(),; [](StringRef Name) {; return IdentifiedNamespaces.find(Name) == IdentifiedNamespaces.end();; });; LVStringRefs::size_type FirstNonNamespace = std::distance(Components.begin(), Iter);. //===----------------------------------------------------------------------===//; // Move all the printing support to a common module.; //===----------------------------------------------------------------------===//; Factor out printing functionality from the logical elements into a; common module. //===----------------------------------------------------------------------===//; // Refactor 'LVBinaryReader::processLines'.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1246155; https://reviews.llvm.org/D137156. During the traversal of the debug information sections, we created the; logical lines representing the disassembled instructions from the text; section and the logical lines representing the line records from the; debug line section. Using the ranges associated with the logical scopes,; we will allocate those logical lines to their logical scopes. Consider the case when any of those lines become orphans, causing; incorrect scope parent for disassembly or line records. //===----------------------------------------------------------------------===//; // Add support for '-ffunction-sections'.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1295012. Only linked executables are handled. It does not support relocatable; files compiled with -ffunction-sections. //===----------------------------------------------------------------------===//; // Add support for DWARF v5 .debug_names section.; // Add support for CodeView public symbols stream.; //===----------------------------------------------------------------------===//; https://r",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:7992,Testability,log,logical,7992," use the proposed code:. LVStringRefs::iterator Iter = std::find_if(Components.begin(), Components.end(),; [](StringRef Name) {; return IdentifiedNamespaces.find(Name) == IdentifiedNamespaces.end();; });; LVStringRefs::size_type FirstNonNamespace = std::distance(Components.begin(), Iter);. //===----------------------------------------------------------------------===//; // Move all the printing support to a common module.; //===----------------------------------------------------------------------===//; Factor out printing functionality from the logical elements into a; common module. //===----------------------------------------------------------------------===//; // Refactor 'LVBinaryReader::processLines'.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1246155; https://reviews.llvm.org/D137156. During the traversal of the debug information sections, we created the; logical lines representing the disassembled instructions from the text; section and the logical lines representing the line records from the; debug line section. Using the ranges associated with the logical scopes,; we will allocate those logical lines to their logical scopes. Consider the case when any of those lines become orphans, causing; incorrect scope parent for disassembly or line records. //===----------------------------------------------------------------------===//; // Add support for '-ffunction-sections'.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1295012. Only linked executables are handled. It does not support relocatable; files compiled with -ffunction-sections. //===----------------------------------------------------------------------===//; // Add support for DWARF v5 .debug_names section.; // Add support for CodeView public symbols stream.; //===----------------------------------------------------------------------===//; https://r",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:8103,Testability,log,logical,8103,"tifiedNamespaces.end();; });; LVStringRefs::size_type FirstNonNamespace = std::distance(Components.begin(), Iter);. //===----------------------------------------------------------------------===//; // Move all the printing support to a common module.; //===----------------------------------------------------------------------===//; Factor out printing functionality from the logical elements into a; common module. //===----------------------------------------------------------------------===//; // Refactor 'LVBinaryReader::processLines'.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1246155; https://reviews.llvm.org/D137156. During the traversal of the debug information sections, we created the; logical lines representing the disassembled instructions from the text; section and the logical lines representing the line records from the; debug line section. Using the ranges associated with the logical scopes,; we will allocate those logical lines to their logical scopes. Consider the case when any of those lines become orphans, causing; incorrect scope parent for disassembly or line records. //===----------------------------------------------------------------------===//; // Add support for '-ffunction-sections'.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1295012. Only linked executables are handled. It does not support relocatable; files compiled with -ffunction-sections. //===----------------------------------------------------------------------===//; // Add support for DWARF v5 .debug_names section.; // Add support for CodeView public symbols stream.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1294142. The ELF and CodeView readers use the public names information to create; the instructions (LVLineAssembler). Instead of relying on DWAR",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:8143,Testability,log,logical,8143,"tifiedNamespaces.end();; });; LVStringRefs::size_type FirstNonNamespace = std::distance(Components.begin(), Iter);. //===----------------------------------------------------------------------===//; // Move all the printing support to a common module.; //===----------------------------------------------------------------------===//; Factor out printing functionality from the logical elements into a; common module. //===----------------------------------------------------------------------===//; // Refactor 'LVBinaryReader::processLines'.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1246155; https://reviews.llvm.org/D137156. During the traversal of the debug information sections, we created the; logical lines representing the disassembled instructions from the text; section and the logical lines representing the line records from the; debug line section. Using the ranges associated with the logical scopes,; we will allocate those logical lines to their logical scopes. Consider the case when any of those lines become orphans, causing; incorrect scope parent for disassembly or line records. //===----------------------------------------------------------------------===//; // Add support for '-ffunction-sections'.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1295012. Only linked executables are handled. It does not support relocatable; files compiled with -ffunction-sections. //===----------------------------------------------------------------------===//; // Add support for DWARF v5 .debug_names section.; // Add support for CodeView public symbols stream.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1294142. The ELF and CodeView readers use the public names information to create; the instructions (LVLineAssembler). Instead of relying on DWAR",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:8166,Testability,log,logical,8166,"tifiedNamespaces.end();; });; LVStringRefs::size_type FirstNonNamespace = std::distance(Components.begin(), Iter);. //===----------------------------------------------------------------------===//; // Move all the printing support to a common module.; //===----------------------------------------------------------------------===//; Factor out printing functionality from the logical elements into a; common module. //===----------------------------------------------------------------------===//; // Refactor 'LVBinaryReader::processLines'.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1246155; https://reviews.llvm.org/D137156. During the traversal of the debug information sections, we created the; logical lines representing the disassembled instructions from the text; section and the logical lines representing the line records from the; debug line section. Using the ranges associated with the logical scopes,; we will allocate those logical lines to their logical scopes. Consider the case when any of those lines become orphans, causing; incorrect scope parent for disassembly or line records. //===----------------------------------------------------------------------===//; // Add support for '-ffunction-sections'.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1295012. Only linked executables are handled. It does not support relocatable; files compiled with -ffunction-sections. //===----------------------------------------------------------------------===//; // Add support for DWARF v5 .debug_names section.; // Add support for CodeView public symbols stream.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1294142. The ELF and CodeView readers use the public names information to create; the instructions (LVLineAssembler). Instead of relying on DWAR",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:10282,Testability,log,logical,10282,"---------------------------------===//; https://reviews.llvm.org/D125783#inline-1295012. Only linked executables are handled. It does not support relocatable; files compiled with -ffunction-sections. //===----------------------------------------------------------------------===//; // Add support for DWARF v5 .debug_names section.; // Add support for CodeView public symbols stream.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1294142. The ELF and CodeView readers use the public names information to create; the instructions (LVLineAssembler). Instead of relying on DWARF section; names (.debug_pubnames, .debug_names) and CodeView public symbol stream; (S_PUB32), the readers collects the needed information while processing; the debug information. If the object file supports the above section names and stream, use them; to create the public names. //===----------------------------------------------------------------------===//; // Add support for some extra DWARF locations.; //===----------------------------------------------------------------------===//; The following DWARF debug location operands are not supported:. - DW_OP_const_type; - DW_OP_entry_value; - DW_OP_implicit_value. //===----------------------------------------------------------------------===//; // Add support for additional binary formats.; //===----------------------------------------------------------------------===//; - WebAssembly (Wasm).; https://github.com/llvm/llvm-project/issues/57040#issuecomment-1211336680. - Extended COFF (XCOFF). //===----------------------------------------------------------------------===//; // Add support for JSON or YAML.; //===----------------------------------------------------------------------===//; The logical view uses its own and non-standard free form text when; displaying information on logical elements. //===----------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt:10372,Testability,log,logical,10372,"---------------------------------===//; https://reviews.llvm.org/D125783#inline-1295012. Only linked executables are handled. It does not support relocatable; files compiled with -ffunction-sections. //===----------------------------------------------------------------------===//; // Add support for DWARF v5 .debug_names section.; // Add support for CodeView public symbols stream.; //===----------------------------------------------------------------------===//; https://reviews.llvm.org/D125783#inline-1294142. The ELF and CodeView readers use the public names information to create; the instructions (LVLineAssembler). Instead of relying on DWARF section; names (.debug_pubnames, .debug_names) and CodeView public symbol stream; (S_PUB32), the readers collects the needed information while processing; the debug information. If the object file supports the above section names and stream, use them; to create the public names. //===----------------------------------------------------------------------===//; // Add support for some extra DWARF locations.; //===----------------------------------------------------------------------===//; The following DWARF debug location operands are not supported:. - DW_OP_const_type; - DW_OP_entry_value; - DW_OP_implicit_value. //===----------------------------------------------------------------------===//; // Add support for additional binary formats.; //===----------------------------------------------------------------------===//; - WebAssembly (Wasm).; https://github.com/llvm/llvm-project/issues/57040#issuecomment-1211336680. - Extended COFF (XCOFF). //===----------------------------------------------------------------------===//; // Add support for JSON or YAML.; //===----------------------------------------------------------------------===//; The logical view uses its own and non-standard free form text when; displaying information on logical elements. //===----------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfod/CMakeLists.txt:234,Integrability,DEPEND,DEPENDS,234,set(LLVM_LINK_COMPONENTS; Option; Support; ); set(LLVM_TARGET_DEFINITIONS Opts.td); tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(DebugInfodOptsTableGen). add_llvm_tool(llvm-debuginfod; llvm-debuginfod.cpp. DEPENDS; DebugInfodOptsTableGen; GENERATE_DRIVER; ). if(NOT LLVM_TOOL_LLVM_DRIVER_BUILD); target_link_libraries(llvm-debuginfod PRIVATE LLVMDebuginfod); endif(). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(debuginfod llvm-debuginfod); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-debuginfod/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfod/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-diff/CMakeLists.txt:94,Integrability,DEPEND,DEPENDS,94,set(LLVM_LINK_COMPONENTS; Core; IRReader; Support; ). add_llvm_tool(llvm-diff; llvm-diff.cpp. DEPENDS; intrinsics_gen; ). add_subdirectory(lib); target_link_libraries(llvm-diff PRIVATE LLVMDiff); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-diff/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-diff/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-dis/CMakeLists.txt:93,Integrability,DEPEND,DEPENDS,93,set(LLVM_LINK_COMPONENTS; BitReader; Core; Support; ). add_llvm_tool(llvm-dis; llvm-dis.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-dis/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-dis/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-driver/CMakeLists.txt:1495,Deployability,install,install,1495,"get_property(LLVM_COMMON_DEPENDS GLOBAL PROPERTY LLVM_DRIVER_DEPS); get_property(LLVM_DRIVER_OBJLIBS GLOBAL PROPERTY LLVM_DRIVER_OBJLIBS). get_property(LLVM_DRIVER_TOOLS GLOBAL PROPERTY LLVM_DRIVER_TOOLS). list(SORT LLVM_DRIVER_TOOLS); list(REVERSE LLVM_DRIVER_TOOLS); foreach(tool ${LLVM_DRIVER_TOOLS}); string(REPLACE ""-"" ""_"" tool_entry ${tool}); get_property(tool_aliases GLOBAL PROPERTY LLVM_DRIVER_TOOL_ALIASES_${tool}); foreach(alias ${tool_aliases}); set_property(GLOBAL APPEND PROPERTY LLVM_DRIVER_TOOL_SYMLINKS ${alias}); string(REPLACE ""llvm-"" """" alias ${alias}); set(def_decl ""${def_decl}LLVM_DRIVER_TOOL(\""${alias}\"", ${tool_entry})\n""); endforeach(); endforeach(). file(WRITE; ""${CMAKE_CURRENT_BINARY_DIR}/LLVMDriverTools.def""; ""${def_decl}#undef LLVM_DRIVER_TOOL\n""). target_include_directories(llvm-driver PRIVATE ${CMAKE_CURRENT_BINARY_DIR}); target_sources(llvm-driver PRIVATE llvm-driver.cpp). set_target_properties(llvm-driver PROPERTIES OUTPUT_NAME llvm). target_link_libraries(llvm-driver PUBLIC ${LLVM_DRIVER_OBJLIBS}); target_link_libraries(llvm-driver PUBLIC LLVMDebuginfod). if(APPLE); # dsymutil uses some CoreFoundation stuff on Darwin...; target_link_libraries(llvm-driver PRIVATE ""-framework CoreFoundation""); endif(APPLE). macro(generate_driver_tool_targets); get_property(LLVM_DRIVER_TOOL_SYMLINKS GLOBAL PROPERTY LLVM_DRIVER_TOOL_SYMLINKS); foreach(name ${LLVM_DRIVER_TOOL_SYMLINKS}); add_llvm_tool_symlink(${name} llvm-driver ALWAYS_GENERATE); # Always generate install targets; llvm_install_symlink(LLVM ${name} llvm-driver ALWAYS_GENERATE); endforeach(); endmacro(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-driver/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-driver/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-dwarfutil/CMakeLists.txt:435,Integrability,DEPEND,DEPENDS,435,set(LLVM_TARGET_DEFINITIONS Options.td); tablegen(LLVM Options.inc -gen-opt-parser-defs); add_public_tablegen_target(DwarfutilTableGen). set(LLVM_LINK_COMPONENTS; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; CodeGenTypes; DWARFLinker; DWARFLinkerClassic; DWARFLinkerParallel; DebugInfoDWARF; MC; ObjCopy; Object; Option; Support; Target; TargetParser; ). add_llvm_tool(llvm-dwarfutil; llvm-dwarfutil.cpp; DebugInfoLinker.cpp. DEPENDS; intrinsics_gen; ${tablegen_deps}; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-dwarfutil/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-dwarfutil/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-dwp/CMakeLists.txt:314,Integrability,DEPEND,DEPENDS,314,set(LLVM_LINK_COMPONENTS; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; DebugInfoDWARF; DWP; MC; Object; Option; Support; TargetParser; ). set(LLVM_TARGET_DEFINITIONS Opts.td); tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(DwpOptsTableGen). add_llvm_tool(llvm-dwp; llvm-dwp.cpp. DEPENDS; intrinsics_gen; DwpOptsTableGen; GENERATE_DRIVER; ). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(dwp llvm-dwp); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-dwp/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-dwp/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/CMakeLists.txt:259,Integrability,DEPEND,DEPENDS,259,"set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsDisassemblers; AllTargetsInfos; CodeGenTypes; MC; MCParser; Support; TargetParser; ). add_llvm_tool(llvm-exegesis; DISABLE_LLVM_LINK_LLVM_DYLIB; llvm-exegesis.cpp. DEPENDS; intrinsics_gen; ). # Has side effect of defining LLVM_EXEGESIS_TARGETS; add_subdirectory(lib). # Link all enabled exegesis targets; set(libs); foreach(t ${LLVM_EXEGESIS_TARGETS}); string(STRIP ${t} t); list(APPEND libs ""LLVMExegesis${t}""); endforeach(). target_link_libraries(llvm-exegesis PRIVATE; LLVMExegesis; ${libs}; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-exegesis/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-extract/CMakeLists.txt:145,Integrability,DEPEND,DEPENDS,145,set(LLVM_LINK_COMPONENTS; Analysis; BitWriter; Core; IPO; IRReader; IRPrinter; Passes; Support; ). add_llvm_tool(llvm-extract; llvm-extract.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-extract/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-extract/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-gsymutil/CMakeLists.txt:311,Integrability,DEPEND,DEPENDS,311,set(LLVM_LINK_COMPONENTS; ${LLVM_TARGETS_TO_BUILD}; DebugInfoDWARF; DebugInfoGSYM; MC; Object; Option; Support; TargetParser; ). set(LLVM_TARGET_DEFINITIONS Opts.td); tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(GSYMUtilOptsTableGen). add_llvm_tool(llvm-gsymutil; llvm-gsymutil.cpp. DEPENDS; GSYMUtilOptsTableGen. GENERATE_DRIVER; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-gsymutil/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-gsymutil/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-ifs/CMakeLists.txt:263,Availability,Error,ErrorCollector,263,set(LLVM_LINK_COMPONENTS; BinaryFormat; InterfaceStub; ObjectYAML; Option; Support; TargetParser; TextAPI; ). set(LLVM_TARGET_DEFINITIONS Opts.td); tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(IFSOptsTableGen). add_llvm_tool(llvm-ifs; ErrorCollector.cpp; llvm-ifs.cpp. DEPENDS; IFSOptsTableGen. GENERATE_DRIVER; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-ifs/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-ifs/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-ifs/CMakeLists.txt:40,Integrability,Interface,InterfaceStub,40,set(LLVM_LINK_COMPONENTS; BinaryFormat; InterfaceStub; ObjectYAML; Option; Support; TargetParser; TextAPI; ). set(LLVM_TARGET_DEFINITIONS Opts.td); tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(IFSOptsTableGen). add_llvm_tool(llvm-ifs; ErrorCollector.cpp; llvm-ifs.cpp. DEPENDS; IFSOptsTableGen. GENERATE_DRIVER; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-ifs/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-ifs/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-ifs/CMakeLists.txt:297,Integrability,DEPEND,DEPENDS,297,set(LLVM_LINK_COMPONENTS; BinaryFormat; InterfaceStub; ObjectYAML; Option; Support; TargetParser; TextAPI; ). set(LLVM_TARGET_DEFINITIONS Opts.td); tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(IFSOptsTableGen). add_llvm_tool(llvm-ifs; ErrorCollector.cpp; llvm-ifs.cpp. DEPENDS; IFSOptsTableGen. GENERATE_DRIVER; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-ifs/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-ifs/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-libtool-darwin/CMakeLists.txt:322,Integrability,DEPEND,DEPENDS,322,set(LLVM_LINK_COMPONENTS; BinaryFormat; Core; Object; Option; Support; TargetParser; TextAPI; ${LLVM_TARGETS_TO_BUILD}; ). set(LLVM_TARGET_DEFINITIONS Opts.td); tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(LibtoolDarwinOptsTableGen). add_llvm_tool(llvm-libtool-darwin; llvm-libtool-darwin.cpp. DEPENDS; LibtoolDarwinOptsTableGen; GENERATE_DRIVER; ). if(LLVM_INSTALL_CCTOOLS_SYMLINKS); add_llvm_tool_symlink(libtool llvm-libtool-darwin); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-libtool-darwin/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-libtool-darwin/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-link/CMakeLists.txt:167,Integrability,DEPEND,DEPENDS,167,set(LLVM_LINK_COMPONENTS; BinaryFormat; BitReader; BitWriter; Core; IRReader; Linker; Object; Support; TransformUtils; IPO; ). add_llvm_tool(llvm-link; llvm-link.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-link/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-link/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-lipo/CMakeLists.txt:301,Integrability,DEPEND,DEPENDS,301,set(LLVM_LINK_COMPONENTS; ${LLVM_TARGETS_TO_BUILD}; Object; Option; Support; TargetParser; TextAPI; Core; BinaryFormat; ). set(LLVM_TARGET_DEFINITIONS LipoOpts.td); tablegen(LLVM LipoOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(LipoOptsTableGen). add_llvm_tool(llvm-lipo; llvm-lipo.cpp; DEPENDS; LipoOptsTableGen; GENERATE_DRIVER; ). if(LLVM_INSTALL_CCTOOLS_SYMLINKS); add_llvm_tool_symlink(lipo llvm-lipo); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-lipo/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-lipo/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-lto/CMakeLists.txt:243,Integrability,DEPEND,DEPENDS,243,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; BitReader; BitWriter; CodeGen; Core; IRReader; IPO; LTO; MC; Object; Support; Target; TargetParser; ). add_llvm_tool(llvm-lto; llvm-lto.cpp. DEPENDS intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-lto/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-lto/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-lto2/CMakeLists.txt:235,Integrability,DEPEND,DEPENDS,235,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; BitReader; CodeGen; Core; Linker; LTO; MC; Object; Passes; Support; Target; TargetParser; ). add_llvm_tool(llvm-lto2; llvm-lto2.cpp. DEPENDS; intrinsics_gen; ); export_executable_symbols_for_plugins(llvm-lto2); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-lto2/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-lto2/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-mca/CMakeLists.txt:316,Deployability,Pipeline,PipelinePrinter,316,include_directories(include). set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsMCAs # CustomBehaviour and InstrPostProcess; AllTargetsDescs; AllTargetsDisassemblers; AllTargetsInfos; MCA; MC; MCParser; Support; TargetParser; ). add_llvm_tool(llvm-mca; llvm-mca.cpp; CodeRegion.cpp; CodeRegionGenerator.cpp; PipelinePrinter.cpp; Views/BottleneckAnalysis.cpp; Views/DispatchStatistics.cpp; Views/InstructionInfoView.cpp; Views/InstructionView.cpp; Views/RegisterFileStatistics.cpp; Views/ResourcePressureView.cpp; Views/RetireControlUnitStatistics.cpp; Views/SchedulerStatistics.cpp; Views/SummaryView.cpp; Views/TimelineView.cpp; ). set(LLVM_MCA_SOURCE_DIR ${CURRENT_SOURCE_DIR}); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-mca/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-mca/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-mca/CMakeLists.txt:566,Energy Efficiency,Schedul,SchedulerStatistics,566,include_directories(include). set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsMCAs # CustomBehaviour and InstrPostProcess; AllTargetsDescs; AllTargetsDisassemblers; AllTargetsInfos; MCA; MC; MCParser; Support; TargetParser; ). add_llvm_tool(llvm-mca; llvm-mca.cpp; CodeRegion.cpp; CodeRegionGenerator.cpp; PipelinePrinter.cpp; Views/BottleneckAnalysis.cpp; Views/DispatchStatistics.cpp; Views/InstructionInfoView.cpp; Views/InstructionView.cpp; Views/RegisterFileStatistics.cpp; Views/ResourcePressureView.cpp; Views/RetireControlUnitStatistics.cpp; Views/SchedulerStatistics.cpp; Views/SummaryView.cpp; Views/TimelineView.cpp; ). set(LLVM_MCA_SOURCE_DIR ${CURRENT_SOURCE_DIR}); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-mca/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-mca/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-mca/CMakeLists.txt:343,Performance,Bottleneck,BottleneckAnalysis,343,include_directories(include). set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsMCAs # CustomBehaviour and InstrPostProcess; AllTargetsDescs; AllTargetsDisassemblers; AllTargetsInfos; MCA; MC; MCParser; Support; TargetParser; ). add_llvm_tool(llvm-mca; llvm-mca.cpp; CodeRegion.cpp; CodeRegionGenerator.cpp; PipelinePrinter.cpp; Views/BottleneckAnalysis.cpp; Views/DispatchStatistics.cpp; Views/InstructionInfoView.cpp; Views/InstructionView.cpp; Views/RegisterFileStatistics.cpp; Views/ResourcePressureView.cpp; Views/RetireControlUnitStatistics.cpp; Views/SchedulerStatistics.cpp; Views/SummaryView.cpp; Views/TimelineView.cpp; ). set(LLVM_MCA_SOURCE_DIR ${CURRENT_SOURCE_DIR}); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-mca/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-mca/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-ml/CMakeLists.txt:333,Integrability,DEPEND,DEPENDS,333,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsDescs; AllTargetsDisassemblers; AllTargetsInfos; MC; MCParser; Option; Support; TargetParser; ). set(LLVM_TARGET_DEFINITIONS Opts.td). tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(MLTableGen). add_llvm_tool(llvm-ml; llvm-ml.cpp; Disassembler.cpp; DEPENDS; MLTableGen; GENERATE_DRIVER; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-ml/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-ml/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-mt/CMakeLists.txt:223,Integrability,DEPEND,DEPENDS,223,set(LLVM_LINK_COMPONENTS; Option; Support; WindowsManifest; ). set(LLVM_TARGET_DEFINITIONS Opts.td). tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(MtTableGen). add_llvm_tool(llvm-mt; llvm-mt.cpp; DEPENDS; MtTableGen; GENERATE_DRIVER; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-mt/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-mt/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-nm/CMakeLists.txt:338,Integrability,DEPEND,DEPENDS,338,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsDescs; AllTargetsInfos; BinaryFormat; Core; Demangle; Object; Option; Support; Symbolize; TargetParser; TextAPI; ). set(LLVM_TARGET_DEFINITIONS Opts.td); tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(NmOptsTableGen). add_llvm_tool(llvm-nm; llvm-nm.cpp. DEPENDS; NmOptsTableGen; intrinsics_gen; GENERATE_DRIVER; ). setup_host_tool(llvm-nm LLVM_NM llvm_nm_exe llvm_nm_target). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(nm llvm-nm); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-nm/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-nm/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt:270,Deployability,Install,InstallNameToolOpts,270,set(LLVM_LINK_COMPONENTS; Object; ObjCopy; Option; Support; TargetParser; MC; BinaryFormat; ). set(LLVM_TARGET_DEFINITIONS ObjcopyOpts.td); tablegen(LLVM ObjcopyOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(ObjcopyOptsTableGen). set(LLVM_TARGET_DEFINITIONS InstallNameToolOpts.td); tablegen(LLVM InstallNameToolOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(InstallNameToolOptsTableGen). set(LLVM_TARGET_DEFINITIONS BitcodeStripOpts.td); tablegen(LLVM BitcodeStripOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(BitcodeStripOptsTableGen). set(LLVM_TARGET_DEFINITIONS StripOpts.td); tablegen(LLVM StripOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(StripOptsTableGen). add_llvm_tool(llvm-objcopy; ObjcopyOptions.cpp; llvm-objcopy.cpp; DEPENDS; ObjcopyOptsTableGen; InstallNameToolOptsTableGen; StripOptsTableGen; GENERATE_DRIVER; ). add_llvm_tool_symlink(llvm-install-name-tool llvm-objcopy); add_llvm_tool_symlink(llvm-bitcode-strip llvm-objcopy); add_llvm_tool_symlink(llvm-strip llvm-objcopy). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(objcopy llvm-objcopy); add_llvm_tool_symlink(strip llvm-objcopy); endif(). if(LLVM_INSTALL_CCTOOLS_SYMLINKS); add_llvm_tool_symlink(install_name_tool llvm-install-name-tool); add_llvm_tool_symlink(bitcode_strip llvm-bitcode-strip); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt:309,Deployability,Install,InstallNameToolOpts,309,set(LLVM_LINK_COMPONENTS; Object; ObjCopy; Option; Support; TargetParser; MC; BinaryFormat; ). set(LLVM_TARGET_DEFINITIONS ObjcopyOpts.td); tablegen(LLVM ObjcopyOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(ObjcopyOptsTableGen). set(LLVM_TARGET_DEFINITIONS InstallNameToolOpts.td); tablegen(LLVM InstallNameToolOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(InstallNameToolOptsTableGen). set(LLVM_TARGET_DEFINITIONS BitcodeStripOpts.td); tablegen(LLVM BitcodeStripOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(BitcodeStripOptsTableGen). set(LLVM_TARGET_DEFINITIONS StripOpts.td); tablegen(LLVM StripOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(StripOptsTableGen). add_llvm_tool(llvm-objcopy; ObjcopyOptions.cpp; llvm-objcopy.cpp; DEPENDS; ObjcopyOptsTableGen; InstallNameToolOptsTableGen; StripOptsTableGen; GENERATE_DRIVER; ). add_llvm_tool_symlink(llvm-install-name-tool llvm-objcopy); add_llvm_tool_symlink(llvm-bitcode-strip llvm-objcopy); add_llvm_tool_symlink(llvm-strip llvm-objcopy). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(objcopy llvm-objcopy); add_llvm_tool_symlink(strip llvm-objcopy); endif(). if(LLVM_INSTALL_CCTOOLS_SYMLINKS); add_llvm_tool_symlink(install_name_tool llvm-install-name-tool); add_llvm_tool_symlink(bitcode_strip llvm-bitcode-strip); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt:383,Deployability,Install,InstallNameToolOptsTableGen,383,set(LLVM_LINK_COMPONENTS; Object; ObjCopy; Option; Support; TargetParser; MC; BinaryFormat; ). set(LLVM_TARGET_DEFINITIONS ObjcopyOpts.td); tablegen(LLVM ObjcopyOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(ObjcopyOptsTableGen). set(LLVM_TARGET_DEFINITIONS InstallNameToolOpts.td); tablegen(LLVM InstallNameToolOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(InstallNameToolOptsTableGen). set(LLVM_TARGET_DEFINITIONS BitcodeStripOpts.td); tablegen(LLVM BitcodeStripOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(BitcodeStripOptsTableGen). set(LLVM_TARGET_DEFINITIONS StripOpts.td); tablegen(LLVM StripOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(StripOptsTableGen). add_llvm_tool(llvm-objcopy; ObjcopyOptions.cpp; llvm-objcopy.cpp; DEPENDS; ObjcopyOptsTableGen; InstallNameToolOptsTableGen; StripOptsTableGen; GENERATE_DRIVER; ). add_llvm_tool_symlink(llvm-install-name-tool llvm-objcopy); add_llvm_tool_symlink(llvm-bitcode-strip llvm-objcopy); add_llvm_tool_symlink(llvm-strip llvm-objcopy). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(objcopy llvm-objcopy); add_llvm_tool_symlink(strip llvm-objcopy); endif(). if(LLVM_INSTALL_CCTOOLS_SYMLINKS); add_llvm_tool_symlink(install_name_tool llvm-install-name-tool); add_llvm_tool_symlink(bitcode_strip llvm-bitcode-strip); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt:812,Deployability,Install,InstallNameToolOptsTableGen,812,set(LLVM_LINK_COMPONENTS; Object; ObjCopy; Option; Support; TargetParser; MC; BinaryFormat; ). set(LLVM_TARGET_DEFINITIONS ObjcopyOpts.td); tablegen(LLVM ObjcopyOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(ObjcopyOptsTableGen). set(LLVM_TARGET_DEFINITIONS InstallNameToolOpts.td); tablegen(LLVM InstallNameToolOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(InstallNameToolOptsTableGen). set(LLVM_TARGET_DEFINITIONS BitcodeStripOpts.td); tablegen(LLVM BitcodeStripOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(BitcodeStripOptsTableGen). set(LLVM_TARGET_DEFINITIONS StripOpts.td); tablegen(LLVM StripOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(StripOptsTableGen). add_llvm_tool(llvm-objcopy; ObjcopyOptions.cpp; llvm-objcopy.cpp; DEPENDS; ObjcopyOptsTableGen; InstallNameToolOptsTableGen; StripOptsTableGen; GENERATE_DRIVER; ). add_llvm_tool_symlink(llvm-install-name-tool llvm-objcopy); add_llvm_tool_symlink(llvm-bitcode-strip llvm-objcopy); add_llvm_tool_symlink(llvm-strip llvm-objcopy). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(objcopy llvm-objcopy); add_llvm_tool_symlink(strip llvm-objcopy); endif(). if(LLVM_INSTALL_CCTOOLS_SYMLINKS); add_llvm_tool_symlink(install_name_tool llvm-install-name-tool); add_llvm_tool_symlink(bitcode_strip llvm-bitcode-strip); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt:907,Deployability,install,install-name-tool,907,set(LLVM_LINK_COMPONENTS; Object; ObjCopy; Option; Support; TargetParser; MC; BinaryFormat; ). set(LLVM_TARGET_DEFINITIONS ObjcopyOpts.td); tablegen(LLVM ObjcopyOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(ObjcopyOptsTableGen). set(LLVM_TARGET_DEFINITIONS InstallNameToolOpts.td); tablegen(LLVM InstallNameToolOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(InstallNameToolOptsTableGen). set(LLVM_TARGET_DEFINITIONS BitcodeStripOpts.td); tablegen(LLVM BitcodeStripOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(BitcodeStripOptsTableGen). set(LLVM_TARGET_DEFINITIONS StripOpts.td); tablegen(LLVM StripOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(StripOptsTableGen). add_llvm_tool(llvm-objcopy; ObjcopyOptions.cpp; llvm-objcopy.cpp; DEPENDS; ObjcopyOptsTableGen; InstallNameToolOptsTableGen; StripOptsTableGen; GENERATE_DRIVER; ). add_llvm_tool_symlink(llvm-install-name-tool llvm-objcopy); add_llvm_tool_symlink(llvm-bitcode-strip llvm-objcopy); add_llvm_tool_symlink(llvm-strip llvm-objcopy). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(objcopy llvm-objcopy); add_llvm_tool_symlink(strip llvm-objcopy); endif(). if(LLVM_INSTALL_CCTOOLS_SYMLINKS); add_llvm_tool_symlink(install_name_tool llvm-install-name-tool); add_llvm_tool_symlink(bitcode_strip llvm-bitcode-strip); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt:1257,Deployability,install,install-name-tool,1257,set(LLVM_LINK_COMPONENTS; Object; ObjCopy; Option; Support; TargetParser; MC; BinaryFormat; ). set(LLVM_TARGET_DEFINITIONS ObjcopyOpts.td); tablegen(LLVM ObjcopyOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(ObjcopyOptsTableGen). set(LLVM_TARGET_DEFINITIONS InstallNameToolOpts.td); tablegen(LLVM InstallNameToolOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(InstallNameToolOptsTableGen). set(LLVM_TARGET_DEFINITIONS BitcodeStripOpts.td); tablegen(LLVM BitcodeStripOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(BitcodeStripOptsTableGen). set(LLVM_TARGET_DEFINITIONS StripOpts.td); tablegen(LLVM StripOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(StripOptsTableGen). add_llvm_tool(llvm-objcopy; ObjcopyOptions.cpp; llvm-objcopy.cpp; DEPENDS; ObjcopyOptsTableGen; InstallNameToolOptsTableGen; StripOptsTableGen; GENERATE_DRIVER; ). add_llvm_tool_symlink(llvm-install-name-tool llvm-objcopy); add_llvm_tool_symlink(llvm-bitcode-strip llvm-objcopy); add_llvm_tool_symlink(llvm-strip llvm-objcopy). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(objcopy llvm-objcopy); add_llvm_tool_symlink(strip llvm-objcopy); endif(). if(LLVM_INSTALL_CCTOOLS_SYMLINKS); add_llvm_tool_symlink(install_name_tool llvm-install-name-tool); add_llvm_tool_symlink(bitcode_strip llvm-bitcode-strip); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt:782,Integrability,DEPEND,DEPENDS,782,set(LLVM_LINK_COMPONENTS; Object; ObjCopy; Option; Support; TargetParser; MC; BinaryFormat; ). set(LLVM_TARGET_DEFINITIONS ObjcopyOpts.td); tablegen(LLVM ObjcopyOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(ObjcopyOptsTableGen). set(LLVM_TARGET_DEFINITIONS InstallNameToolOpts.td); tablegen(LLVM InstallNameToolOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(InstallNameToolOptsTableGen). set(LLVM_TARGET_DEFINITIONS BitcodeStripOpts.td); tablegen(LLVM BitcodeStripOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(BitcodeStripOptsTableGen). set(LLVM_TARGET_DEFINITIONS StripOpts.td); tablegen(LLVM StripOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(StripOptsTableGen). add_llvm_tool(llvm-objcopy; ObjcopyOptions.cpp; llvm-objcopy.cpp; DEPENDS; ObjcopyOptsTableGen; InstallNameToolOptsTableGen; StripOptsTableGen; GENERATE_DRIVER; ). add_llvm_tool_symlink(llvm-install-name-tool llvm-objcopy); add_llvm_tool_symlink(llvm-bitcode-strip llvm-objcopy); add_llvm_tool_symlink(llvm-strip llvm-objcopy). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(objcopy llvm-objcopy); add_llvm_tool_symlink(strip llvm-objcopy); endif(). if(LLVM_INSTALL_CCTOOLS_SYMLINKS); add_llvm_tool_symlink(install_name_tool llvm-install-name-tool); add_llvm_tool_symlink(bitcode_strip llvm-bitcode-strip); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objcopy/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objdump/CMakeLists.txt:653,Integrability,DEPEND,DEPENDS,653,set(LLVM_LINK_COMPONENTS; AllTargetsDescs; AllTargetsDisassemblers; AllTargetsInfos; BinaryFormat; DebugInfoBTF; DebugInfoDWARF; Demangle; MC; MCDisassembler; Object; Option; Support; Symbolize; TargetParser; ). set(LLVM_TARGET_DEFINITIONS ObjdumpOpts.td); tablegen(LLVM ObjdumpOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(ObjdumpOptsTableGen). set(LLVM_TARGET_DEFINITIONS OtoolOpts.td); tablegen(LLVM OtoolOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(OtoolOptsTableGen). add_llvm_tool(llvm-objdump; llvm-objdump.cpp; SourcePrinter.cpp; COFFDump.cpp; ELFDump.cpp; MachODump.cpp; OffloadDump.cpp; WasmDump.cpp; XCOFFDump.cpp; DEPENDS; ObjdumpOptsTableGen; OtoolOptsTableGen; GENERATE_DRIVER; ). if(NOT LLVM_TOOL_LLVM_DRIVER_BUILD); target_link_libraries(llvm-objdump PRIVATE LLVMDebuginfod); endif(). add_llvm_tool_symlink(llvm-otool llvm-objdump). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(objdump llvm-objdump); endif(). if(LLVM_INSTALL_CCTOOLS_SYMLINKS); add_llvm_tool_symlink(otool llvm-otool); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-objdump/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objdump/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profdata/CMakeLists.txt:113,Integrability,DEPEND,DEPENDS,113,set(LLVM_LINK_COMPONENTS; Core; Object; ProfileData; Support; ). add_llvm_tool(llvm-profdata; llvm-profdata.cpp. DEPENDS; intrinsics_gen; GENERATE_DRIVER; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-profdata/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profdata/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-rc/CMakeLists.txt:507,Integrability,DEPEND,DEPENDS,507,set(LLVM_LINK_COMPONENTS; Object; Option; Support; TargetParser; ). set(LLVM_TARGET_DEFINITIONS Opts.td); tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(RcOptsTableGen). set(LLVM_TARGET_DEFINITIONS WindresOpts.td); tablegen(LLVM WindresOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(WindresOptsTableGen). add_llvm_tool(llvm-rc; llvm-rc.cpp; ResourceFileWriter.cpp; ResourceScriptCppFilter.cpp; ResourceScriptParser.cpp; ResourceScriptStmt.cpp; ResourceScriptToken.cpp; DEPENDS; WindresOptsTableGen; GENERATE_DRIVER; ). add_llvm_tool_symlink(llvm-windres llvm-rc). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(windres llvm-rc); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-rc/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-rc/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-readobj/CMakeLists.txt:491,Integrability,DEPEND,DEPENDS,491,set(LLVM_LINK_COMPONENTS; BinaryFormat; DebugInfoCodeView; DebugInfoDWARF; Demangle; Object; Option; Support; TargetParser; ). set(LLVM_TARGET_DEFINITIONS Opts.td); tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(ReadobjOptsTableGen). add_llvm_tool(llvm-readobj; ARMWinEHPrinter.cpp; COFFDumper.cpp; COFFImportDumper.cpp; ELFDumper.cpp; llvm-readobj.cpp; MachODumper.cpp; ObjDumper.cpp; WasmDumper.cpp; Win64EHDumper.cpp; WindowsResourceDumper.cpp; XCOFFDumper.cpp; DEPENDS; ReadobjOptsTableGen; GENERATE_DRIVER; ). setup_host_tool(llvm-readobj LLVM_READOBJ llvm_readobj_exe llvm_readobj_target). add_llvm_tool_symlink(llvm-readelf llvm-readobj). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(readelf llvm-readobj); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-readobj/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-readobj/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-readtapi/CMakeLists.txt:304,Integrability,DEPEND,DEPENDS,304,set(LLVM_LINK_COMPONENTS; BinaryFormat; Object; Support; Option; TextAPI; TextAPIBinaryReader; ). set(LLVM_TARGET_DEFINITIONS TapiOpts.td); tablegen(LLVM TapiOpts.inc -gen-opt-parser-defs); add_public_tablegen_target(ReadTAPIOptsTableGen). add_llvm_tool(llvm-readtapi; llvm-readtapi.cpp; DiffEngine.cpp. DEPENDS; ReadTAPIOptsTableGen; ). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(readtapi llvm-readtapi); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-readtapi/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-readtapi/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:270,Energy Efficiency,reduce,reduce,270,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:296,Energy Efficiency,Reduce,ReducerWorkItem,296,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:376,Energy Efficiency,Reduce,ReduceAliases,376,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:402,Energy Efficiency,Reduce,ReduceArguments,402,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:430,Energy Efficiency,Reduce,ReduceAttributes,430,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:459,Energy Efficiency,Reduce,ReduceBasicBlocks,459,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:489,Energy Efficiency,Reduce,ReduceDIMetadata,489,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:518,Energy Efficiency,Reduce,ReduceDPValues,518,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:545,Energy Efficiency,Reduce,ReduceFunctionBodies,545,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:578,Energy Efficiency,Reduce,ReduceFunctions,578,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:606,Energy Efficiency,Reduce,ReduceGlobalObjects,606,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:638,Energy Efficiency,Reduce,ReduceGlobalValues,638,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:669,Energy Efficiency,Reduce,ReduceGlobalVarInitializers,669,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:709,Energy Efficiency,Reduce,ReduceGlobalVars,709,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:738,Energy Efficiency,Reduce,ReduceInstructions,738,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:769,Energy Efficiency,Reduce,ReduceInstructionFlags,769,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:804,Energy Efficiency,Reduce,ReduceInvokes,804,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:830,Energy Efficiency,Reduce,ReduceMetadata,830,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:857,Energy Efficiency,Reduce,ReduceModuleData,857,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:886,Energy Efficiency,Reduce,ReduceMemoryOperations,886,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:921,Energy Efficiency,Reduce,ReduceOperandBundles,921,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:954,Energy Efficiency,Reduce,ReduceOpcodes,954,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:980,Energy Efficiency,Reduce,ReduceSpecialGlobals,980,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:1013,Energy Efficiency,Reduce,ReduceOperands,1013,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:1040,Energy Efficiency,Reduce,ReduceOperandsSkip,1040,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:1071,Energy Efficiency,Reduce,ReduceOperandsToArgs,1071,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:1104,Energy Efficiency,Reduce,ReduceInstructionsMIR,1104,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:1138,Energy Efficiency,Reduce,ReduceInstructionFlagsMIR,1138,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:1176,Energy Efficiency,Reduce,ReduceIRReferences,1176,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:1207,Energy Efficiency,Reduce,ReduceVirtualRegisters,1207,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:1242,Energy Efficiency,Reduce,ReduceRegisterMasks,1242,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:1274,Energy Efficiency,Reduce,ReduceRegisterDefs,1274,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:1305,Energy Efficiency,Reduce,ReduceRegisterUses,1305,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:1336,Energy Efficiency,Reduce,ReduceUsingSimplifyCFG,1336,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:1453,Energy Efficiency,reduce,reduce,1453,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:1465,Integrability,DEPEND,DEPENDS,1465,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:317,Testability,Test,TestRunner,317,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt:1395,Usability,Simpl,SimplifyInstructions,1395,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; Analysis; BitReader; BitWriter; CodeGen; CodeGenTypes; Core; IPO; IRReader; MC; MIRParser; Passes; Support; Target; TargetParser; TransformUtils; ). add_llvm_tool(llvm-reduce; DeltaManager.cpp; ReducerWorkItem.cpp; TestRunner.cpp; deltas/Delta.cpp; deltas/Utils.cpp; deltas/ReduceAliases.cpp; deltas/ReduceArguments.cpp; deltas/ReduceAttributes.cpp; deltas/ReduceBasicBlocks.cpp; deltas/ReduceDIMetadata.cpp; deltas/ReduceDPValues.cpp; deltas/ReduceFunctionBodies.cpp; deltas/ReduceFunctions.cpp; deltas/ReduceGlobalObjects.cpp; deltas/ReduceGlobalValues.cpp; deltas/ReduceGlobalVarInitializers.cpp; deltas/ReduceGlobalVars.cpp; deltas/ReduceInstructions.cpp; deltas/ReduceInstructionFlags.cpp; deltas/ReduceInvokes.cpp; deltas/ReduceMetadata.cpp; deltas/ReduceModuleData.cpp; deltas/ReduceMemoryOperations.cpp; deltas/ReduceOperandBundles.cpp; deltas/ReduceOpcodes.cpp; deltas/ReduceSpecialGlobals.cpp; deltas/ReduceOperands.cpp; deltas/ReduceOperandsSkip.cpp; deltas/ReduceOperandsToArgs.cpp; deltas/ReduceInstructionsMIR.cpp; deltas/ReduceInstructionFlagsMIR.cpp; deltas/ReduceIRReferences.cpp; deltas/ReduceVirtualRegisters.cpp; deltas/ReduceRegisterMasks.cpp; deltas/ReduceRegisterDefs.cpp; deltas/ReduceRegisterUses.cpp; deltas/ReduceUsingSimplifyCFG.cpp; deltas/RunIRPasses.cpp; deltas/SimplifyInstructions.cpp; deltas/StripDebugInfo.cpp; llvm-reduce.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-reduce/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt:2432,Energy Efficiency,reduce,reduce,2432," OUTPUT_NAME LLVM ${INSTALL_WITH_TOOLCHAIN} ${SOURCES}); # Add symlink for backwards compatibility with old library name; llvm_install_library_symlink(LLVM-${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX} $<TARGET_FILE_NAME:LLVM> SHARED FULL_DEST COMPONENT LLVM); endif(). list(REMOVE_DUPLICATES LIB_NAMES); if(""${CMAKE_SYSTEM_NAME}"" STREQUAL ""Darwin""); set(LIB_NAMES -Wl,-all_load ${LIB_NAMES}); else(); configure_file(; ${CMAKE_CURRENT_SOURCE_DIR}/simple_version_script.map.in; ${LLVM_LIBRARY_DIR}/tools/llvm-shlib/simple_version_script.map). # GNU ld doesn't resolve symbols in the version script.; set(LIB_NAMES -Wl,--whole-archive ${LIB_NAMES} -Wl,--no-whole-archive); if (NOT LLVM_LINKER_IS_SOLARISLD AND NOT MINGW); # Solaris ld does not accept global: *; so there is no way to version *all* global symbols; set(LIB_NAMES -Wl,--version-script,${LLVM_LIBRARY_DIR}/tools/llvm-shlib/simple_version_script.map ${LIB_NAMES}); endif(); if (NOT MINGW AND NOT LLVM_LINKER_IS_SOLARISLD_ILLUMOS); # Optimize function calls for default visibility definitions to avoid PLT and; # reduce dynamic relocations.; # Note: for -fno-pic default, the address of a function may be different from; # inside and outside libLLVM.so.; target_link_options(LLVM PRIVATE LINKER:-Bsymbolic-functions); endif(); endif(). target_link_libraries(LLVM PRIVATE ${LIB_NAMES}). if(LLVM_ENABLE_THREADS AND NOT HAVE_CXX_ATOMICS64_WITHOUT_LIB); target_link_libraries(LLVM PUBLIC atomic); endif(). if (APPLE); set_property(TARGET LLVM APPEND_STRING PROPERTY; LINK_FLAGS; "" -compatibility_version 1 -current_version ${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}.${LLVM_VERSION_PATCH}""); endif(). if(TARGET libLLVMExports); add_dependencies(LLVM libLLVMExports); endif(); endif(). if(LLVM_BUILD_LLVM_C_DYLIB AND NOT MSVC); if(NOT APPLE); message(FATAL_ERROR ""Generating libLLVM-c is only supported on Darwin""); endif(). if(NOT LLVM_BUILD_LLVM_DYLIB); message(FATAL_ERROR ""Generating libLLVM-c requires LLVM_BUILD_LLVM_C_DYLIB on Darwin""); endif",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt:316,Integrability,message,message,316,"# This tool creates a shared library from the LLVM libraries. Generating this; # library is enabled by setting LLVM_BUILD_LLVM_DYLIB=yes on the CMake; # commandline. By default the shared library only exports the LLVM C API. set(SOURCES; libllvm.cpp; ). if(LLVM_LINK_LLVM_DYLIB AND LLVM_DYLIB_EXPORTED_SYMBOL_FILE); message(WARNING ""Using LLVM_LINK_LLVM_DYLIB with LLVM_DYLIB_EXPORTED_SYMBOL_FILE may not work. Use at your own risk.""); endif(). if(LLVM_BUILD_LLVM_DYLIB); if(MSVC); message(FATAL_ERROR ""Generating libLLVM is not supported on MSVC""); endif(); if(ZOS); message(FATAL_ERROR ""Generating libLLVM is not supported on z/OS""); endif(). llvm_map_components_to_libnames(LIB_NAMES ${LLVM_DYLIB_COMPONENTS}). # Exclude libLLVMTableGen for the following reasons:; # - it is only used by internal *-tblgen utilities;; # - it pollutes the global options space.; list(REMOVE_ITEM LIB_NAMES ""LLVMTableGen""). if(LLVM_DYLIB_EXPORTED_SYMBOL_FILE); set(LLVM_EXPORTED_SYMBOL_FILE ${LLVM_DYLIB_EXPORTED_SYMBOL_FILE}); add_custom_target(libLLVMExports DEPENDS ${LLVM_EXPORTED_SYMBOL_FILE}); endif(). if (LLVM_LINK_LLVM_DYLIB); set(INSTALL_WITH_TOOLCHAIN INSTALL_WITH_TOOLCHAIN); endif(); if (WIN32); add_llvm_library(LLVM SHARED DISABLE_LLVM_LINK_LLVM_DYLIB SONAME ${INSTALL_WITH_TOOLCHAIN} ${SOURCES}); else(); add_llvm_library(LLVM SHARED DISABLE_LLVM_LINK_LLVM_DYLIB OUTPUT_NAME LLVM ${INSTALL_WITH_TOOLCHAIN} ${SOURCES}); # Add symlink for backwards compatibility with old library name; llvm_install_library_symlink(LLVM-${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX} $<TARGET_FILE_NAME:LLVM> SHARED FULL_DEST COMPONENT LLVM); endif(). list(REMOVE_DUPLICATES LIB_NAMES); if(""${CMAKE_SYSTEM_NAME}"" STREQUAL ""Darwin""); set(LIB_NAMES -Wl,-all_load ${LIB_NAMES}); else(); configure_file(; ${CMAKE_CURRENT_SOURCE_DIR}/simple_version_script.map.in; ${LLVM_LIBRARY_DIR}/tools/llvm-shlib/simple_version_script.map). # GNU ld doesn't resolve symbols in the version script.; set(LIB_NAMES -Wl,--whole-archive ${LIB_NA",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt:482,Integrability,message,message,482,"# This tool creates a shared library from the LLVM libraries. Generating this; # library is enabled by setting LLVM_BUILD_LLVM_DYLIB=yes on the CMake; # commandline. By default the shared library only exports the LLVM C API. set(SOURCES; libllvm.cpp; ). if(LLVM_LINK_LLVM_DYLIB AND LLVM_DYLIB_EXPORTED_SYMBOL_FILE); message(WARNING ""Using LLVM_LINK_LLVM_DYLIB with LLVM_DYLIB_EXPORTED_SYMBOL_FILE may not work. Use at your own risk.""); endif(). if(LLVM_BUILD_LLVM_DYLIB); if(MSVC); message(FATAL_ERROR ""Generating libLLVM is not supported on MSVC""); endif(); if(ZOS); message(FATAL_ERROR ""Generating libLLVM is not supported on z/OS""); endif(). llvm_map_components_to_libnames(LIB_NAMES ${LLVM_DYLIB_COMPONENTS}). # Exclude libLLVMTableGen for the following reasons:; # - it is only used by internal *-tblgen utilities;; # - it pollutes the global options space.; list(REMOVE_ITEM LIB_NAMES ""LLVMTableGen""). if(LLVM_DYLIB_EXPORTED_SYMBOL_FILE); set(LLVM_EXPORTED_SYMBOL_FILE ${LLVM_DYLIB_EXPORTED_SYMBOL_FILE}); add_custom_target(libLLVMExports DEPENDS ${LLVM_EXPORTED_SYMBOL_FILE}); endif(). if (LLVM_LINK_LLVM_DYLIB); set(INSTALL_WITH_TOOLCHAIN INSTALL_WITH_TOOLCHAIN); endif(); if (WIN32); add_llvm_library(LLVM SHARED DISABLE_LLVM_LINK_LLVM_DYLIB SONAME ${INSTALL_WITH_TOOLCHAIN} ${SOURCES}); else(); add_llvm_library(LLVM SHARED DISABLE_LLVM_LINK_LLVM_DYLIB OUTPUT_NAME LLVM ${INSTALL_WITH_TOOLCHAIN} ${SOURCES}); # Add symlink for backwards compatibility with old library name; llvm_install_library_symlink(LLVM-${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX} $<TARGET_FILE_NAME:LLVM> SHARED FULL_DEST COMPONENT LLVM); endif(). list(REMOVE_DUPLICATES LIB_NAMES); if(""${CMAKE_SYSTEM_NAME}"" STREQUAL ""Darwin""); set(LIB_NAMES -Wl,-all_load ${LIB_NAMES}); else(); configure_file(; ${CMAKE_CURRENT_SOURCE_DIR}/simple_version_script.map.in; ${LLVM_LIBRARY_DIR}/tools/llvm-shlib/simple_version_script.map). # GNU ld doesn't resolve symbols in the version script.; set(LIB_NAMES -Wl,--whole-archive ${LIB_NA",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt:568,Integrability,message,message,568,"# This tool creates a shared library from the LLVM libraries. Generating this; # library is enabled by setting LLVM_BUILD_LLVM_DYLIB=yes on the CMake; # commandline. By default the shared library only exports the LLVM C API. set(SOURCES; libllvm.cpp; ). if(LLVM_LINK_LLVM_DYLIB AND LLVM_DYLIB_EXPORTED_SYMBOL_FILE); message(WARNING ""Using LLVM_LINK_LLVM_DYLIB with LLVM_DYLIB_EXPORTED_SYMBOL_FILE may not work. Use at your own risk.""); endif(). if(LLVM_BUILD_LLVM_DYLIB); if(MSVC); message(FATAL_ERROR ""Generating libLLVM is not supported on MSVC""); endif(); if(ZOS); message(FATAL_ERROR ""Generating libLLVM is not supported on z/OS""); endif(). llvm_map_components_to_libnames(LIB_NAMES ${LLVM_DYLIB_COMPONENTS}). # Exclude libLLVMTableGen for the following reasons:; # - it is only used by internal *-tblgen utilities;; # - it pollutes the global options space.; list(REMOVE_ITEM LIB_NAMES ""LLVMTableGen""). if(LLVM_DYLIB_EXPORTED_SYMBOL_FILE); set(LLVM_EXPORTED_SYMBOL_FILE ${LLVM_DYLIB_EXPORTED_SYMBOL_FILE}); add_custom_target(libLLVMExports DEPENDS ${LLVM_EXPORTED_SYMBOL_FILE}); endif(). if (LLVM_LINK_LLVM_DYLIB); set(INSTALL_WITH_TOOLCHAIN INSTALL_WITH_TOOLCHAIN); endif(); if (WIN32); add_llvm_library(LLVM SHARED DISABLE_LLVM_LINK_LLVM_DYLIB SONAME ${INSTALL_WITH_TOOLCHAIN} ${SOURCES}); else(); add_llvm_library(LLVM SHARED DISABLE_LLVM_LINK_LLVM_DYLIB OUTPUT_NAME LLVM ${INSTALL_WITH_TOOLCHAIN} ${SOURCES}); # Add symlink for backwards compatibility with old library name; llvm_install_library_symlink(LLVM-${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX} $<TARGET_FILE_NAME:LLVM> SHARED FULL_DEST COMPONENT LLVM); endif(). list(REMOVE_DUPLICATES LIB_NAMES); if(""${CMAKE_SYSTEM_NAME}"" STREQUAL ""Darwin""); set(LIB_NAMES -Wl,-all_load ${LIB_NAMES}); else(); configure_file(; ${CMAKE_CURRENT_SOURCE_DIR}/simple_version_script.map.in; ${LLVM_LIBRARY_DIR}/tools/llvm-shlib/simple_version_script.map). # GNU ld doesn't resolve symbols in the version script.; set(LIB_NAMES -Wl,--whole-archive ${LIB_NA",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt:1045,Integrability,DEPEND,DEPENDS,1045,"# This tool creates a shared library from the LLVM libraries. Generating this; # library is enabled by setting LLVM_BUILD_LLVM_DYLIB=yes on the CMake; # commandline. By default the shared library only exports the LLVM C API. set(SOURCES; libllvm.cpp; ). if(LLVM_LINK_LLVM_DYLIB AND LLVM_DYLIB_EXPORTED_SYMBOL_FILE); message(WARNING ""Using LLVM_LINK_LLVM_DYLIB with LLVM_DYLIB_EXPORTED_SYMBOL_FILE may not work. Use at your own risk.""); endif(). if(LLVM_BUILD_LLVM_DYLIB); if(MSVC); message(FATAL_ERROR ""Generating libLLVM is not supported on MSVC""); endif(); if(ZOS); message(FATAL_ERROR ""Generating libLLVM is not supported on z/OS""); endif(). llvm_map_components_to_libnames(LIB_NAMES ${LLVM_DYLIB_COMPONENTS}). # Exclude libLLVMTableGen for the following reasons:; # - it is only used by internal *-tblgen utilities;; # - it pollutes the global options space.; list(REMOVE_ITEM LIB_NAMES ""LLVMTableGen""). if(LLVM_DYLIB_EXPORTED_SYMBOL_FILE); set(LLVM_EXPORTED_SYMBOL_FILE ${LLVM_DYLIB_EXPORTED_SYMBOL_FILE}); add_custom_target(libLLVMExports DEPENDS ${LLVM_EXPORTED_SYMBOL_FILE}); endif(). if (LLVM_LINK_LLVM_DYLIB); set(INSTALL_WITH_TOOLCHAIN INSTALL_WITH_TOOLCHAIN); endif(); if (WIN32); add_llvm_library(LLVM SHARED DISABLE_LLVM_LINK_LLVM_DYLIB SONAME ${INSTALL_WITH_TOOLCHAIN} ${SOURCES}); else(); add_llvm_library(LLVM SHARED DISABLE_LLVM_LINK_LLVM_DYLIB OUTPUT_NAME LLVM ${INSTALL_WITH_TOOLCHAIN} ${SOURCES}); # Add symlink for backwards compatibility with old library name; llvm_install_library_symlink(LLVM-${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX} $<TARGET_FILE_NAME:LLVM> SHARED FULL_DEST COMPONENT LLVM); endif(). list(REMOVE_DUPLICATES LIB_NAMES); if(""${CMAKE_SYSTEM_NAME}"" STREQUAL ""Darwin""); set(LIB_NAMES -Wl,-all_load ${LIB_NAMES}); else(); configure_file(; ${CMAKE_CURRENT_SOURCE_DIR}/simple_version_script.map.in; ${LLVM_LIBRARY_DIR}/tools/llvm-shlib/simple_version_script.map). # GNU ld doesn't resolve symbols in the version script.; set(LIB_NAMES -Wl,--whole-archive ${LIB_NA",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt:3157,Integrability,message,message,3157,"ls; set(LIB_NAMES -Wl,--version-script,${LLVM_LIBRARY_DIR}/tools/llvm-shlib/simple_version_script.map ${LIB_NAMES}); endif(); if (NOT MINGW AND NOT LLVM_LINKER_IS_SOLARISLD_ILLUMOS); # Optimize function calls for default visibility definitions to avoid PLT and; # reduce dynamic relocations.; # Note: for -fno-pic default, the address of a function may be different from; # inside and outside libLLVM.so.; target_link_options(LLVM PRIVATE LINKER:-Bsymbolic-functions); endif(); endif(). target_link_libraries(LLVM PRIVATE ${LIB_NAMES}). if(LLVM_ENABLE_THREADS AND NOT HAVE_CXX_ATOMICS64_WITHOUT_LIB); target_link_libraries(LLVM PUBLIC atomic); endif(). if (APPLE); set_property(TARGET LLVM APPEND_STRING PROPERTY; LINK_FLAGS; "" -compatibility_version 1 -current_version ${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}.${LLVM_VERSION_PATCH}""); endif(). if(TARGET libLLVMExports); add_dependencies(LLVM libLLVMExports); endif(); endif(). if(LLVM_BUILD_LLVM_C_DYLIB AND NOT MSVC); if(NOT APPLE); message(FATAL_ERROR ""Generating libLLVM-c is only supported on Darwin""); endif(). if(NOT LLVM_BUILD_LLVM_DYLIB); message(FATAL_ERROR ""Generating libLLVM-c requires LLVM_BUILD_LLVM_C_DYLIB on Darwin""); endif(). # To get the export list for a single llvm library:; # nm ${LIB_PATH} | awk ""/T _LLVM/ { print $3 }"" | sort -u | sed -e ""s/^_//g"" > ${LIB_PATH}.exports. set(LLVM_EXPORTED_SYMBOL_FILE ${LLVM_BINARY_DIR}/libllvm-c.exports). set(LIB_DIR ${LLVM_LIBRARY_DIR}); set(LIB_NAME ${LIB_DIR}/${CMAKE_SHARED_LIBRARY_PREFIX}LLVM); set(LIB_PATH ${LIB_NAME}${CMAKE_SHARED_LIBRARY_SUFFIX}); set(LIB_EXPORTS_PATH ${LIB_NAME}.exports); list(APPEND LLVM_DYLIB_REQUIRED_EXPORTS ${LIB_EXPORTS_PATH}). add_custom_command(OUTPUT ${LLVM_EXPORTED_SYMBOL_FILE}; COMMAND nm ${LIB_PATH} | awk ""/T _LLVM/ || /T LLVM/ { print $3 }"" | sort -u | sed -e ""s/^_//g"" > ${LLVM_EXPORTED_SYMBOL_FILE}; WORKING_DIRECTORY ${LIB_DIR}; DEPENDS LLVM; COMMENT ""Generating Export list for LLVM...""; VERBATIM ). add_custom_target(libLLVMCExports DEPE",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt:3270,Integrability,message,message,3270,"MINGW AND NOT LLVM_LINKER_IS_SOLARISLD_ILLUMOS); # Optimize function calls for default visibility definitions to avoid PLT and; # reduce dynamic relocations.; # Note: for -fno-pic default, the address of a function may be different from; # inside and outside libLLVM.so.; target_link_options(LLVM PRIVATE LINKER:-Bsymbolic-functions); endif(); endif(). target_link_libraries(LLVM PRIVATE ${LIB_NAMES}). if(LLVM_ENABLE_THREADS AND NOT HAVE_CXX_ATOMICS64_WITHOUT_LIB); target_link_libraries(LLVM PUBLIC atomic); endif(). if (APPLE); set_property(TARGET LLVM APPEND_STRING PROPERTY; LINK_FLAGS; "" -compatibility_version 1 -current_version ${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}.${LLVM_VERSION_PATCH}""); endif(). if(TARGET libLLVMExports); add_dependencies(LLVM libLLVMExports); endif(); endif(). if(LLVM_BUILD_LLVM_C_DYLIB AND NOT MSVC); if(NOT APPLE); message(FATAL_ERROR ""Generating libLLVM-c is only supported on Darwin""); endif(). if(NOT LLVM_BUILD_LLVM_DYLIB); message(FATAL_ERROR ""Generating libLLVM-c requires LLVM_BUILD_LLVM_C_DYLIB on Darwin""); endif(). # To get the export list for a single llvm library:; # nm ${LIB_PATH} | awk ""/T _LLVM/ { print $3 }"" | sort -u | sed -e ""s/^_//g"" > ${LIB_PATH}.exports. set(LLVM_EXPORTED_SYMBOL_FILE ${LLVM_BINARY_DIR}/libllvm-c.exports). set(LIB_DIR ${LLVM_LIBRARY_DIR}); set(LIB_NAME ${LIB_DIR}/${CMAKE_SHARED_LIBRARY_PREFIX}LLVM); set(LIB_PATH ${LIB_NAME}${CMAKE_SHARED_LIBRARY_SUFFIX}); set(LIB_EXPORTS_PATH ${LIB_NAME}.exports); list(APPEND LLVM_DYLIB_REQUIRED_EXPORTS ${LIB_EXPORTS_PATH}). add_custom_command(OUTPUT ${LLVM_EXPORTED_SYMBOL_FILE}; COMMAND nm ${LIB_PATH} | awk ""/T _LLVM/ || /T LLVM/ { print $3 }"" | sort -u | sed -e ""s/^_//g"" > ${LLVM_EXPORTED_SYMBOL_FILE}; WORKING_DIRECTORY ${LIB_DIR}; DEPENDS LLVM; COMMENT ""Generating Export list for LLVM...""; VERBATIM ). add_custom_target(libLLVMCExports DEPENDS ${LLVM_EXPORTED_SYMBOL_FILE}). add_llvm_library(LLVM-C SHARED ${SOURCES} INSTALL_WITH_TOOLCHAIN). target_link_libraries(LLVM-C PU",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt:4059,Integrability,DEPEND,DEPENDS,4059,"R}.${LLVM_VERSION_PATCH}""); endif(). if(TARGET libLLVMExports); add_dependencies(LLVM libLLVMExports); endif(); endif(). if(LLVM_BUILD_LLVM_C_DYLIB AND NOT MSVC); if(NOT APPLE); message(FATAL_ERROR ""Generating libLLVM-c is only supported on Darwin""); endif(). if(NOT LLVM_BUILD_LLVM_DYLIB); message(FATAL_ERROR ""Generating libLLVM-c requires LLVM_BUILD_LLVM_C_DYLIB on Darwin""); endif(). # To get the export list for a single llvm library:; # nm ${LIB_PATH} | awk ""/T _LLVM/ { print $3 }"" | sort -u | sed -e ""s/^_//g"" > ${LIB_PATH}.exports. set(LLVM_EXPORTED_SYMBOL_FILE ${LLVM_BINARY_DIR}/libllvm-c.exports). set(LIB_DIR ${LLVM_LIBRARY_DIR}); set(LIB_NAME ${LIB_DIR}/${CMAKE_SHARED_LIBRARY_PREFIX}LLVM); set(LIB_PATH ${LIB_NAME}${CMAKE_SHARED_LIBRARY_SUFFIX}); set(LIB_EXPORTS_PATH ${LIB_NAME}.exports); list(APPEND LLVM_DYLIB_REQUIRED_EXPORTS ${LIB_EXPORTS_PATH}). add_custom_command(OUTPUT ${LLVM_EXPORTED_SYMBOL_FILE}; COMMAND nm ${LIB_PATH} | awk ""/T _LLVM/ || /T LLVM/ { print $3 }"" | sort -u | sed -e ""s/^_//g"" > ${LLVM_EXPORTED_SYMBOL_FILE}; WORKING_DIRECTORY ${LIB_DIR}; DEPENDS LLVM; COMMENT ""Generating Export list for LLVM...""; VERBATIM ). add_custom_target(libLLVMCExports DEPENDS ${LLVM_EXPORTED_SYMBOL_FILE}). add_llvm_library(LLVM-C SHARED ${SOURCES} INSTALL_WITH_TOOLCHAIN). target_link_libraries(LLVM-C PUBLIC LLVM); add_dependencies(LLVM-C libLLVMCExports). set_property(TARGET LLVM-C APPEND_STRING PROPERTY; LINK_FLAGS; "" -compatibility_version 1 -current_version ${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}.${LLVM_VERSION_PATCH} -Wl,-reexport_library ${LIB_PATH}""); endif(). if(LLVM_BUILD_LLVM_C_DYLIB AND MSVC); # Build the LLVM-C.dll library that exports the C API. set(LLVM_LINK_COMPONENTS; ${LLVM_DYLIB_COMPONENTS}; ). llvm_map_components_to_libnames(LIB_NAMES ${LLVM_DYLIB_COMPONENTS}); list(REMOVE_DUPLICATES LIB_NAMES). # The python script needs to know whether symbols are prefixed with underscores or not.; if(LLVM_HOST_TRIPLE MATCHES ""i?86-.*win.*""); set(GEN_UNDERSCORE ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt:4165,Integrability,DEPEND,DEPENDS,4165,"ATAL_ERROR ""Generating libLLVM-c is only supported on Darwin""); endif(). if(NOT LLVM_BUILD_LLVM_DYLIB); message(FATAL_ERROR ""Generating libLLVM-c requires LLVM_BUILD_LLVM_C_DYLIB on Darwin""); endif(). # To get the export list for a single llvm library:; # nm ${LIB_PATH} | awk ""/T _LLVM/ { print $3 }"" | sort -u | sed -e ""s/^_//g"" > ${LIB_PATH}.exports. set(LLVM_EXPORTED_SYMBOL_FILE ${LLVM_BINARY_DIR}/libllvm-c.exports). set(LIB_DIR ${LLVM_LIBRARY_DIR}); set(LIB_NAME ${LIB_DIR}/${CMAKE_SHARED_LIBRARY_PREFIX}LLVM); set(LIB_PATH ${LIB_NAME}${CMAKE_SHARED_LIBRARY_SUFFIX}); set(LIB_EXPORTS_PATH ${LIB_NAME}.exports); list(APPEND LLVM_DYLIB_REQUIRED_EXPORTS ${LIB_EXPORTS_PATH}). add_custom_command(OUTPUT ${LLVM_EXPORTED_SYMBOL_FILE}; COMMAND nm ${LIB_PATH} | awk ""/T _LLVM/ || /T LLVM/ { print $3 }"" | sort -u | sed -e ""s/^_//g"" > ${LLVM_EXPORTED_SYMBOL_FILE}; WORKING_DIRECTORY ${LIB_DIR}; DEPENDS LLVM; COMMENT ""Generating Export list for LLVM...""; VERBATIM ). add_custom_target(libLLVMCExports DEPENDS ${LLVM_EXPORTED_SYMBOL_FILE}). add_llvm_library(LLVM-C SHARED ${SOURCES} INSTALL_WITH_TOOLCHAIN). target_link_libraries(LLVM-C PUBLIC LLVM); add_dependencies(LLVM-C libLLVMCExports). set_property(TARGET LLVM-C APPEND_STRING PROPERTY; LINK_FLAGS; "" -compatibility_version 1 -current_version ${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}.${LLVM_VERSION_PATCH} -Wl,-reexport_library ${LIB_PATH}""); endif(). if(LLVM_BUILD_LLVM_C_DYLIB AND MSVC); # Build the LLVM-C.dll library that exports the C API. set(LLVM_LINK_COMPONENTS; ${LLVM_DYLIB_COMPONENTS}; ). llvm_map_components_to_libnames(LIB_NAMES ${LLVM_DYLIB_COMPONENTS}); list(REMOVE_DUPLICATES LIB_NAMES). # The python script needs to know whether symbols are prefixed with underscores or not.; if(LLVM_HOST_TRIPLE MATCHES ""i?86-.*win.*""); set(GEN_UNDERSCORE ""--underscore""); else(); set(GEN_UNDERSCORE """"); endif(). # Set this name here, not used in multi conf loop,; # but add script will pick the right one.; set(LIBSFILE ${LLVM_BINARY_DIR}/${",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt:6606,Integrability,DEPEND,DEPENDS,6606," """"); endif(). # Set this name here, not used in multi conf loop,; # but add script will pick the right one.; set(LIBSFILE ${LLVM_BINARY_DIR}/${CMAKE_CFG_INTDIR}/libllvm-c.args). # Get the full name to the libs so the python script understands them.; foreach(lib ${LIB_NAMES}); list(APPEND FULL_LIB_NAMES ${LLVM_LIBRARY_DIR}/${lib}.lib); endforeach(). # Need to separate lib names with newlines.; string(REPLACE "";"" ""\n"" FILE_CONTENT ""${FULL_LIB_NAMES}""). if(NOT ""${CMAKE_CFG_INTDIR}"" STREQUAL "".""); foreach(BUILD_MODE ${CMAKE_CONFIGURATION_TYPES}); # Replace the special string with a per config directory.; string(REPLACE ${CMAKE_CFG_INTDIR} ${BUILD_MODE} PER_CONF_CONTENT ""${FILE_CONTENT}""). # Write out the full lib names into file to be read by the python script.; # One libsfile per build, the add_custom_command should expand; # ${CMAKE_CFG_INTDIR} correctly and select the right one.; file(WRITE ${LLVM_BINARY_DIR}/${BUILD_MODE}/libllvm-c.args ""${PER_CONF_CONTENT}""); endforeach(); else(); # Write out the full lib names into file to be read by the python script.; file(WRITE ${LIBSFILE} ""${FILE_CONTENT}""); endif(). # Generate the exports file dynamically.; set(GEN_SCRIPT ${CMAKE_CURRENT_SOURCE_DIR}/gen-msvc-exports.py). set(LLVM_EXPORTED_SYMBOL_FILE ${LLVM_BINARY_DIR}/${CMAKE_CFG_INTDIR}/libllvm-c.exports); get_host_tool_path(llvm-nm LLVM_NM llvm_nm_exe llvm_nm_target). add_custom_command(OUTPUT ${LLVM_EXPORTED_SYMBOL_FILE}; COMMAND ""${Python3_EXECUTABLE}"" ${GEN_SCRIPT} --libsfile ${LIBSFILE} ${GEN_UNDERSCORE} --nm ""${llvm_nm_exe}"" -o ${LLVM_EXPORTED_SYMBOL_FILE}; DEPENDS ${LIB_NAMES} ${llvm_nm_target}; COMMENT ""Generating export list for LLVM-C""; VERBATIM ). # Finally link the target.; add_llvm_library(LLVM-C SHARED INSTALL_WITH_TOOLCHAIN ${SOURCES} DEPENDS intrinsics_gen). if (LLVM_INTEGRATED_CRT_ALLOC AND MSVC); # Make sure we search LLVMSupport first, before the CRT libs; set(CMAKE_SHARED_LINKER_FLAGS ""${CMAKE_SHARED_LINKER_FLAGS} -INCLUDE:malloc""); endif(); ; endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt:6796,Integrability,DEPEND,DEPENDS,6796," """"); endif(). # Set this name here, not used in multi conf loop,; # but add script will pick the right one.; set(LIBSFILE ${LLVM_BINARY_DIR}/${CMAKE_CFG_INTDIR}/libllvm-c.args). # Get the full name to the libs so the python script understands them.; foreach(lib ${LIB_NAMES}); list(APPEND FULL_LIB_NAMES ${LLVM_LIBRARY_DIR}/${lib}.lib); endforeach(). # Need to separate lib names with newlines.; string(REPLACE "";"" ""\n"" FILE_CONTENT ""${FULL_LIB_NAMES}""). if(NOT ""${CMAKE_CFG_INTDIR}"" STREQUAL "".""); foreach(BUILD_MODE ${CMAKE_CONFIGURATION_TYPES}); # Replace the special string with a per config directory.; string(REPLACE ${CMAKE_CFG_INTDIR} ${BUILD_MODE} PER_CONF_CONTENT ""${FILE_CONTENT}""). # Write out the full lib names into file to be read by the python script.; # One libsfile per build, the add_custom_command should expand; # ${CMAKE_CFG_INTDIR} correctly and select the right one.; file(WRITE ${LLVM_BINARY_DIR}/${BUILD_MODE}/libllvm-c.args ""${PER_CONF_CONTENT}""); endforeach(); else(); # Write out the full lib names into file to be read by the python script.; file(WRITE ${LIBSFILE} ""${FILE_CONTENT}""); endif(). # Generate the exports file dynamically.; set(GEN_SCRIPT ${CMAKE_CURRENT_SOURCE_DIR}/gen-msvc-exports.py). set(LLVM_EXPORTED_SYMBOL_FILE ${LLVM_BINARY_DIR}/${CMAKE_CFG_INTDIR}/libllvm-c.exports); get_host_tool_path(llvm-nm LLVM_NM llvm_nm_exe llvm_nm_target). add_custom_command(OUTPUT ${LLVM_EXPORTED_SYMBOL_FILE}; COMMAND ""${Python3_EXECUTABLE}"" ${GEN_SCRIPT} --libsfile ${LIBSFILE} ${GEN_UNDERSCORE} --nm ""${llvm_nm_exe}"" -o ${LLVM_EXPORTED_SYMBOL_FILE}; DEPENDS ${LIB_NAMES} ${llvm_nm_target}; COMMENT ""Generating export list for LLVM-C""; VERBATIM ). # Finally link the target.; add_llvm_library(LLVM-C SHARED INSTALL_WITH_TOOLCHAIN ${SOURCES} DEPENDS intrinsics_gen). if (LLVM_INTEGRATED_CRT_ALLOC AND MSVC); # Make sure we search LLVMSupport first, before the CRT libs; set(CMAKE_SHARED_LINKER_FLAGS ""${CMAKE_SHARED_LINKER_FLAGS} -INCLUDE:malloc""); endif(); ; endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt:5613,Modifiability,config,config,5613," if(LLVM_BUILD_LLVM_C_DYLIB AND MSVC); # Build the LLVM-C.dll library that exports the C API. set(LLVM_LINK_COMPONENTS; ${LLVM_DYLIB_COMPONENTS}; ). llvm_map_components_to_libnames(LIB_NAMES ${LLVM_DYLIB_COMPONENTS}); list(REMOVE_DUPLICATES LIB_NAMES). # The python script needs to know whether symbols are prefixed with underscores or not.; if(LLVM_HOST_TRIPLE MATCHES ""i?86-.*win.*""); set(GEN_UNDERSCORE ""--underscore""); else(); set(GEN_UNDERSCORE """"); endif(). # Set this name here, not used in multi conf loop,; # but add script will pick the right one.; set(LIBSFILE ${LLVM_BINARY_DIR}/${CMAKE_CFG_INTDIR}/libllvm-c.args). # Get the full name to the libs so the python script understands them.; foreach(lib ${LIB_NAMES}); list(APPEND FULL_LIB_NAMES ${LLVM_LIBRARY_DIR}/${lib}.lib); endforeach(). # Need to separate lib names with newlines.; string(REPLACE "";"" ""\n"" FILE_CONTENT ""${FULL_LIB_NAMES}""). if(NOT ""${CMAKE_CFG_INTDIR}"" STREQUAL "".""); foreach(BUILD_MODE ${CMAKE_CONFIGURATION_TYPES}); # Replace the special string with a per config directory.; string(REPLACE ${CMAKE_CFG_INTDIR} ${BUILD_MODE} PER_CONF_CONTENT ""${FILE_CONTENT}""). # Write out the full lib names into file to be read by the python script.; # One libsfile per build, the add_custom_command should expand; # ${CMAKE_CFG_INTDIR} correctly and select the right one.; file(WRITE ${LLVM_BINARY_DIR}/${BUILD_MODE}/libllvm-c.args ""${PER_CONF_CONTENT}""); endforeach(); else(); # Write out the full lib names into file to be read by the python script.; file(WRITE ${LIBSFILE} ""${FILE_CONTENT}""); endif(). # Generate the exports file dynamically.; set(GEN_SCRIPT ${CMAKE_CURRENT_SOURCE_DIR}/gen-msvc-exports.py). set(LLVM_EXPORTED_SYMBOL_FILE ${LLVM_BINARY_DIR}/${CMAKE_CFG_INTDIR}/libllvm-c.exports); get_host_tool_path(llvm-nm LLVM_NM llvm_nm_exe llvm_nm_target). add_custom_command(OUTPUT ${LLVM_EXPORTED_SYMBOL_FILE}; COMMAND ""${Python3_EXECUTABLE}"" ${GEN_SCRIPT} --libsfile ${LIBSFILE} ${GEN_UNDERSCORE} --nm ""${llvm_nm_exe}"" -o",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt:2353,Performance,Optimiz,Optimize,2353," OUTPUT_NAME LLVM ${INSTALL_WITH_TOOLCHAIN} ${SOURCES}); # Add symlink for backwards compatibility with old library name; llvm_install_library_symlink(LLVM-${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX} $<TARGET_FILE_NAME:LLVM> SHARED FULL_DEST COMPONENT LLVM); endif(). list(REMOVE_DUPLICATES LIB_NAMES); if(""${CMAKE_SYSTEM_NAME}"" STREQUAL ""Darwin""); set(LIB_NAMES -Wl,-all_load ${LIB_NAMES}); else(); configure_file(; ${CMAKE_CURRENT_SOURCE_DIR}/simple_version_script.map.in; ${LLVM_LIBRARY_DIR}/tools/llvm-shlib/simple_version_script.map). # GNU ld doesn't resolve symbols in the version script.; set(LIB_NAMES -Wl,--whole-archive ${LIB_NAMES} -Wl,--no-whole-archive); if (NOT LLVM_LINKER_IS_SOLARISLD AND NOT MINGW); # Solaris ld does not accept global: *; so there is no way to version *all* global symbols; set(LIB_NAMES -Wl,--version-script,${LLVM_LIBRARY_DIR}/tools/llvm-shlib/simple_version_script.map ${LIB_NAMES}); endif(); if (NOT MINGW AND NOT LLVM_LINKER_IS_SOLARISLD_ILLUMOS); # Optimize function calls for default visibility definitions to avoid PLT and; # reduce dynamic relocations.; # Note: for -fno-pic default, the address of a function may be different from; # inside and outside libLLVM.so.; target_link_options(LLVM PRIVATE LINKER:-Bsymbolic-functions); endif(); endif(). target_link_libraries(LLVM PRIVATE ${LIB_NAMES}). if(LLVM_ENABLE_THREADS AND NOT HAVE_CXX_ATOMICS64_WITHOUT_LIB); target_link_libraries(LLVM PUBLIC atomic); endif(). if (APPLE); set_property(TARGET LLVM APPEND_STRING PROPERTY; LINK_FLAGS; "" -compatibility_version 1 -current_version ${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}.${LLVM_VERSION_PATCH}""); endif(). if(TARGET libLLVMExports); add_dependencies(LLVM libLLVMExports); endif(); endif(). if(LLVM_BUILD_LLVM_C_DYLIB AND NOT MSVC); if(NOT APPLE); message(FATAL_ERROR ""Generating libLLVM-c is only supported on Darwin""); endif(). if(NOT LLVM_BUILD_LLVM_DYLIB); message(FATAL_ERROR ""Generating libLLVM-c requires LLVM_BUILD_LLVM_C_DYLIB on Darwin""); endif",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt:427,Safety,risk,risk,427,"# This tool creates a shared library from the LLVM libraries. Generating this; # library is enabled by setting LLVM_BUILD_LLVM_DYLIB=yes on the CMake; # commandline. By default the shared library only exports the LLVM C API. set(SOURCES; libllvm.cpp; ). if(LLVM_LINK_LLVM_DYLIB AND LLVM_DYLIB_EXPORTED_SYMBOL_FILE); message(WARNING ""Using LLVM_LINK_LLVM_DYLIB with LLVM_DYLIB_EXPORTED_SYMBOL_FILE may not work. Use at your own risk.""); endif(). if(LLVM_BUILD_LLVM_DYLIB); if(MSVC); message(FATAL_ERROR ""Generating libLLVM is not supported on MSVC""); endif(); if(ZOS); message(FATAL_ERROR ""Generating libLLVM is not supported on z/OS""); endif(). llvm_map_components_to_libnames(LIB_NAMES ${LLVM_DYLIB_COMPONENTS}). # Exclude libLLVMTableGen for the following reasons:; # - it is only used by internal *-tblgen utilities;; # - it pollutes the global options space.; list(REMOVE_ITEM LIB_NAMES ""LLVMTableGen""). if(LLVM_DYLIB_EXPORTED_SYMBOL_FILE); set(LLVM_EXPORTED_SYMBOL_FILE ${LLVM_DYLIB_EXPORTED_SYMBOL_FILE}); add_custom_target(libLLVMExports DEPENDS ${LLVM_EXPORTED_SYMBOL_FILE}); endif(). if (LLVM_LINK_LLVM_DYLIB); set(INSTALL_WITH_TOOLCHAIN INSTALL_WITH_TOOLCHAIN); endif(); if (WIN32); add_llvm_library(LLVM SHARED DISABLE_LLVM_LINK_LLVM_DYLIB SONAME ${INSTALL_WITH_TOOLCHAIN} ${SOURCES}); else(); add_llvm_library(LLVM SHARED DISABLE_LLVM_LINK_LLVM_DYLIB OUTPUT_NAME LLVM ${INSTALL_WITH_TOOLCHAIN} ${SOURCES}); # Add symlink for backwards compatibility with old library name; llvm_install_library_symlink(LLVM-${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX} $<TARGET_FILE_NAME:LLVM> SHARED FULL_DEST COMPONENT LLVM); endif(). list(REMOVE_DUPLICATES LIB_NAMES); if(""${CMAKE_SYSTEM_NAME}"" STREQUAL ""Darwin""); set(LIB_NAMES -Wl,-all_load ${LIB_NAMES}); else(); configure_file(; ${CMAKE_CURRENT_SOURCE_DIR}/simple_version_script.map.in; ${LLVM_LIBRARY_DIR}/tools/llvm-shlib/simple_version_script.map). # GNU ld doesn't resolve symbols in the version script.; set(LIB_NAMES -Wl,--whole-archive ${LIB_NA",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt:2415,Safety,avoid,avoid,2415," OUTPUT_NAME LLVM ${INSTALL_WITH_TOOLCHAIN} ${SOURCES}); # Add symlink for backwards compatibility with old library name; llvm_install_library_symlink(LLVM-${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX} $<TARGET_FILE_NAME:LLVM> SHARED FULL_DEST COMPONENT LLVM); endif(). list(REMOVE_DUPLICATES LIB_NAMES); if(""${CMAKE_SYSTEM_NAME}"" STREQUAL ""Darwin""); set(LIB_NAMES -Wl,-all_load ${LIB_NAMES}); else(); configure_file(; ${CMAKE_CURRENT_SOURCE_DIR}/simple_version_script.map.in; ${LLVM_LIBRARY_DIR}/tools/llvm-shlib/simple_version_script.map). # GNU ld doesn't resolve symbols in the version script.; set(LIB_NAMES -Wl,--whole-archive ${LIB_NAMES} -Wl,--no-whole-archive); if (NOT LLVM_LINKER_IS_SOLARISLD AND NOT MINGW); # Solaris ld does not accept global: *; so there is no way to version *all* global symbols; set(LIB_NAMES -Wl,--version-script,${LLVM_LIBRARY_DIR}/tools/llvm-shlib/simple_version_script.map ${LIB_NAMES}); endif(); if (NOT MINGW AND NOT LLVM_LINKER_IS_SOLARISLD_ILLUMOS); # Optimize function calls for default visibility definitions to avoid PLT and; # reduce dynamic relocations.; # Note: for -fno-pic default, the address of a function may be different from; # inside and outside libLLVM.so.; target_link_options(LLVM PRIVATE LINKER:-Bsymbolic-functions); endif(); endif(). target_link_libraries(LLVM PRIVATE ${LIB_NAMES}). if(LLVM_ENABLE_THREADS AND NOT HAVE_CXX_ATOMICS64_WITHOUT_LIB); target_link_libraries(LLVM PUBLIC atomic); endif(). if (APPLE); set_property(TARGET LLVM APPEND_STRING PROPERTY; LINK_FLAGS; "" -compatibility_version 1 -current_version ${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}.${LLVM_VERSION_PATCH}""); endif(). if(TARGET libLLVMExports); add_dependencies(LLVM libLLVMExports); endif(); endif(). if(LLVM_BUILD_LLVM_C_DYLIB AND NOT MSVC); if(NOT APPLE); message(FATAL_ERROR ""Generating libLLVM-c is only supported on Darwin""); endif(). if(NOT LLVM_BUILD_LLVM_DYLIB); message(FATAL_ERROR ""Generating libLLVM-c requires LLVM_BUILD_LLVM_C_DYLIB on Darwin""); endif",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-shlib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-size/CMakeLists.txt:238,Integrability,DEPEND,DEPENDS,238,set(LLVM_LINK_COMPONENTS; Object; Option; Support; TargetParser; ). set(LLVM_TARGET_DEFINITIONS Opts.td); tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(SizeOptsTableGen). add_llvm_tool(llvm-size; llvm-size.cpp; DEPENDS; SizeOptsTableGen; GENERATE_DRIVER; ). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(size llvm-size); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-size/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-size/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-split/CMakeLists.txt:123,Integrability,DEPEND,DEPENDS,123,set(LLVM_LINK_COMPONENTS; TransformUtils; BitWriter; Core; IRReader; Support; ). add_llvm_tool(llvm-split; llvm-split.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-split/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-split/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-stress/CMakeLists.txt:88,Integrability,DEPEND,DEPENDS,88,set(LLVM_LINK_COMPONENTS; Core; Support; ). add_llvm_tool(llvm-stress; llvm-stress.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-stress/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-stress/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-strings/CMakeLists.txt:239,Integrability,DEPEND,DEPENDS,239,set(LLVM_LINK_COMPONENTS; Core; Object; Option; Support; ). set(LLVM_TARGET_DEFINITIONS Opts.td); tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(StringsOptsTableGen). add_llvm_tool(llvm-strings; llvm-strings.cpp; DEPENDS; StringsOptsTableGen; ). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(strings llvm-strings); endif(); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-strings/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-strings/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-symbolizer/CMakeLists.txt:561,Integrability,DEPEND,DEPENDS,561,"# FIXME: As we plan to execute llvm-symbolizer binary from compiler-rt; # libraries, it has to be compiled for all supported targets (x86_64, i386 etc).; # This means that we need LLVM libraries to be compiled for these; # targets as well. Currently, there is no support for such a build strategy. set(LLVM_TARGET_DEFINITIONS Opts.td); tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(SymbolizerOptsTableGen). set(LLVM_LINK_COMPONENTS; Demangle; Object; Option; Support; Symbolize; ). add_llvm_tool(llvm-symbolizer; llvm-symbolizer.cpp. DEPENDS; SymbolizerOptsTableGen; GENERATE_DRIVER; ). if(NOT LLVM_TOOL_LLVM_DRIVER_BUILD); target_link_libraries(llvm-symbolizer PRIVATE LLVMDebuginfod); endif(). add_llvm_tool_symlink(llvm-addr2line llvm-symbolizer). if(LLVM_INSTALL_BINUTILS_SYMLINKS); add_llvm_tool_symlink(addr2line llvm-symbolizer); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-symbolizer/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-symbolizer/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-tli-checker/CMakeLists.txt:358,Integrability,DEPEND,DEPENDS,358,set(LLVM_LINK_COMPONENTS; Analysis; BinaryFormat; BitReader; BitstreamReader; Core; Demangle; MC; MCParser; Object; Option; Remarks; Support; TargetParser; TextAPI; ). set(LLVM_TARGET_DEFINITIONS Opts.td); tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(TLICheckerOptsTableGen). add_llvm_tool(llvm-tli-checker; llvm-tli-checker.cpp. DEPENDS; TLICheckerOptsTableGen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-tli-checker/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-tli-checker/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/lto/CMakeLists.txt:603,Deployability,install,install,603,"set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsDisassemblers; AllTargetsInfos; BitReader; Core; CodeGen; LTO; MC; MCDisassembler; Support; Target; ). set(SOURCES; LTODisassembler.cpp; lto.cpp; ). set(LLVM_EXPORTED_SYMBOL_FILE ${CMAKE_CURRENT_SOURCE_DIR}/lto.exports). if(CMAKE_SYSTEM_NAME STREQUAL AIX); set(LTO_LIBRARY_TYPE MODULE); set(LTO_LIBRARY_NAME libLTO); else(); set(LTO_LIBRARY_TYPE SHARED); set(LTO_LIBRARY_NAME LTO); endif(). add_llvm_library(${LTO_LIBRARY_NAME} ${LTO_LIBRARY_TYPE} INSTALL_WITH_TOOLCHAIN; ${SOURCES} DEPENDS intrinsics_gen). install(FILES ${LLVM_MAIN_INCLUDE_DIR}/llvm-c/lto.h; DESTINATION ""${CMAKE_INSTALL_INCLUDEDIR}/llvm-c""; COMPONENT LTO). if (APPLE); set(LTO_VERSION ${LLVM_VERSION_MAJOR}); if(LLVM_LTO_VERSION_OFFSET); math(EXPR LTO_VERSION ""${LLVM_VERSION_MAJOR} + ${LLVM_LTO_VERSION_OFFSET}""); endif(); set_property(TARGET LTO APPEND_STRING PROPERTY; LINK_FLAGS; "" -compatibility_version 1 -current_version ${LTO_VERSION}.${LLVM_VERSION_MINOR}.${LLVM_VERSION_PATCH}""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/lto/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/lto/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/lto/CMakeLists.txt:578,Integrability,DEPEND,DEPENDS,578,"set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsDisassemblers; AllTargetsInfos; BitReader; Core; CodeGen; LTO; MC; MCDisassembler; Support; Target; ). set(SOURCES; LTODisassembler.cpp; lto.cpp; ). set(LLVM_EXPORTED_SYMBOL_FILE ${CMAKE_CURRENT_SOURCE_DIR}/lto.exports). if(CMAKE_SYSTEM_NAME STREQUAL AIX); set(LTO_LIBRARY_TYPE MODULE); set(LTO_LIBRARY_NAME libLTO); else(); set(LTO_LIBRARY_TYPE SHARED); set(LTO_LIBRARY_NAME LTO); endif(). add_llvm_library(${LTO_LIBRARY_NAME} ${LTO_LIBRARY_TYPE} INSTALL_WITH_TOOLCHAIN; ${SOURCES} DEPENDS intrinsics_gen). install(FILES ${LLVM_MAIN_INCLUDE_DIR}/llvm-c/lto.h; DESTINATION ""${CMAKE_INSTALL_INCLUDEDIR}/llvm-c""; COMPONENT LTO). if (APPLE); set(LTO_VERSION ${LLVM_VERSION_MAJOR}); if(LLVM_LTO_VERSION_OFFSET); math(EXPR LTO_VERSION ""${LLVM_VERSION_MAJOR} + ${LLVM_LTO_VERSION_OFFSET}""); endif(); set_property(TARGET LTO APPEND_STRING PROPERTY; LINK_FLAGS; "" -compatibility_version 1 -current_version ${LTO_VERSION}.${LLVM_VERSION_MINOR}.${LLVM_VERSION_PATCH}""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/lto/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/lto/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt/CMakeLists.txt:413,Integrability,DEPEND,DEPENDS,413,set(LLVM_LINK_COMPONENTS; AllTargetsAsmParsers; AllTargetsCodeGens; AllTargetsDescs; AllTargetsInfos; AggressiveInstCombine; Analysis; AsmParser; BitWriter; CFGuard; CodeGen; Core; Coroutines; Extensions; IPO; IRReader; IRPrinter; InstCombine; Instrumentation; MC; ObjCARCOpts; Remarks; ScalarOpts; Support; Target; TargetParser; TransformUtils; Vectorize; Passes; ). add_llvm_tool(opt; NewPMDriver.cpp; opt.cpp. DEPENDS; intrinsics_gen; SUPPORT_PLUGINS; ); export_executable_symbols_for_plugins(opt); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/opt/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/CMakeLists.txt:129,Deployability,install,install,129,"set (files; ""opt-diff.py""; ""opt-stats.py""; ""opt-viewer.py""; ""optpmap.py""; ""optrecord.py""; ""style.css""). foreach (file ${files}); install(PROGRAMS ${file}; DESTINATION ""${CMAKE_INSTALL_DATADIR}/opt-viewer""; COMPONENT opt-viewer); endforeach (file). add_custom_target(opt-viewer DEPENDS ${files}); if(NOT LLVM_ENABLE_IDE); add_llvm_install_targets(""install-opt-viewer""; DEPENDS opt-viewer; COMPONENT opt-viewer); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/opt-viewer/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/CMakeLists.txt:347,Deployability,install,install-opt-viewer,347,"set (files; ""opt-diff.py""; ""opt-stats.py""; ""opt-viewer.py""; ""optpmap.py""; ""optrecord.py""; ""style.css""). foreach (file ${files}); install(PROGRAMS ${file}; DESTINATION ""${CMAKE_INSTALL_DATADIR}/opt-viewer""; COMPONENT opt-viewer); endforeach (file). add_custom_target(opt-viewer DEPENDS ${files}); if(NOT LLVM_ENABLE_IDE); add_llvm_install_targets(""install-opt-viewer""; DEPENDS opt-viewer; COMPONENT opt-viewer); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/opt-viewer/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/CMakeLists.txt:277,Integrability,DEPEND,DEPENDS,277,"set (files; ""opt-diff.py""; ""opt-stats.py""; ""opt-viewer.py""; ""optpmap.py""; ""optrecord.py""; ""style.css""). foreach (file ${files}); install(PROGRAMS ${file}; DESTINATION ""${CMAKE_INSTALL_DATADIR}/opt-viewer""; COMPONENT opt-viewer); endforeach (file). add_custom_target(opt-viewer DEPENDS ${files}); if(NOT LLVM_ENABLE_IDE); add_llvm_install_targets(""install-opt-viewer""; DEPENDS opt-viewer; COMPONENT opt-viewer); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/opt-viewer/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/CMakeLists.txt:368,Integrability,DEPEND,DEPENDS,368,"set (files; ""opt-diff.py""; ""opt-stats.py""; ""opt-viewer.py""; ""optpmap.py""; ""optrecord.py""; ""style.css""). foreach (file ${files}); install(PROGRAMS ${file}; DESTINATION ""${CMAKE_INSTALL_DATADIR}/opt-viewer""; COMPONENT opt-viewer); endforeach (file). add_custom_target(opt-viewer DEPENDS ${files}); if(NOT LLVM_ENABLE_IDE); add_llvm_install_targets(""install-opt-viewer""; DEPENDS opt-viewer; COMPONENT opt-viewer); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/opt-viewer/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt-viewer/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/remarks-shlib/CMakeLists.txt:542,Deployability,install,install,542,"# Building shared libraries requires PIC objects.; if(LLVM_ENABLE_PIC). set(LLVM_LINK_COMPONENTS; Remarks; ). set(SOURCES; libremarks.cpp; ). if (NOT (BUILD_SHARED_LIBS OR LLVM_LINK_LLVM_DYLIB)); set(LLVM_EXPORTED_SYMBOL_FILE ${CMAKE_CURRENT_SOURCE_DIR}/Remarks.exports); endif(). add_llvm_library(Remarks SHARED INSTALL_WITH_TOOLCHAIN ${SOURCES}). if (LLVM_INTEGRATED_CRT_ALLOC AND MSVC); # Make sure we search LLVMSupport first, before the CRT libs; set(CMAKE_SHARED_LINKER_FLAGS ""${CMAKE_SHARED_LINKER_FLAGS} -INCLUDE:malloc""); endif(); ; install(FILES ${LLVM_MAIN_INCLUDE_DIR}/llvm-c/Remarks.h; DESTINATION ""${CMAKE_INSTALL_INCLUDEDIR}/llvm-c""; COMPONENT Remarks). if (APPLE); set(REMARKS_VERSION ${LLVM_VERSION_MAJOR}); set_property(TARGET Remarks APPEND_STRING PROPERTY; LINK_FLAGS; "" -compatibility_version 1 -current_version ${REMARKS_VERSION}.${LLVM_VERSION_MINOR}.${LLVM_VERSION_PATCH}""); endif(). endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/remarks-shlib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/remarks-shlib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/sancov/CMakeLists.txt:324,Integrability,DEPEND,DEPENDS,324,set(LLVM_LINK_COMPONENTS; AllTargetsDescs; AllTargetsDisassemblers; AllTargetsInfos; MC; MCDisassembler; Object; Option; Support; Symbolize; TargetParser; ). set(LLVM_TARGET_DEFINITIONS Opts.td); tablegen(LLVM Opts.inc -gen-opt-parser-defs); add_public_tablegen_target(SancovOptsTableGen). add_llvm_tool(sancov; sancov.cpp. DEPENDS; SancovOptsTableGen; GENERATE_DRIVER; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/sancov/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/sancov/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/sanstats/CMakeLists.txt:87,Integrability,DEPEND,DEPENDS,87,set(LLVM_LINK_COMPONENTS; Support; Symbolize; ). add_llvm_tool(sanstats; sanstats.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/sanstats/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/sanstats/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt:1188,Deployability,update,update,1188,"SPIRV_TOOLS_TESTS). if (NOT LLVM_INCLUDE_SPIRV_TOOLS_TESTS); return(); endif (). if (NOT ""SPIRV"" IN_LIST LLVM_TARGETS_TO_BUILD); message(FATAL_ERROR ""Building SPIRV-Tools tests is unsupported without the SPIR-V target""); endif (). # SPIRV_DIS and SPIRV_VAL variables can be used to provide paths to existing; # spirv-dis and spirv-val binaries, respectively. Otherwise, build them from; # SPIRV-Tools source.; if (NOT SPIRV_DIS OR NOT SPIRV_VAL); include(ExternalProject). set(BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/SPIRVTools-bin). ExternalProject_Add(SPIRVTools; GIT_REPOSITORY https://github.com/KhronosGroup/SPIRV-Tools.git; GIT_TAG main; BINARY_DIR ${BINARY_DIR}; BUILD_COMMAND ${CMAKE_COMMAND} --build ${BINARY_DIR} --target spirv-dis spirv-val; BUILD_BYPRODUCTS ${BINARY_DIR}/tools/spirv-dis ${BINARY_DIR}/tools/spirv-val; DOWNLOAD_COMMAND git clone https://github.com/KhronosGroup/SPIRV-Tools.git SPIRVTools &&; cd SPIRVTools &&; ${Python3_EXECUTABLE} utils/git-sync-deps; UPDATE_COMMAND git pull origin main &&; ${Python3_EXECUTABLE} utils/git-sync-deps; # Don't auto-update on every build.; UPDATE_DISCONNECTED 1; # Allow manual updating with an explicit SPIRVTools-update target.; STEP_TARGETS update; # Install handled below.; INSTALL_COMMAND """"; ); endif (). if (CMAKE_HOST_UNIX); set(LLVM_LINK_OR_COPY create_symlink); else (); set(LLVM_LINK_OR_COPY copy); endif (). # Link the provided or just built spirv-dis and spirv-val binaries.; if (SPIRV_DIS); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${SPIRV_DIS}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""); else (); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/tools/spirv-dis"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""; DEPENDS SPIRVTools; ); endif (). if (SPIRV_VAL); add_custom_target(spirv-val; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${SPIRV_VAL}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-val""); else (); add_custom_target(spirv-val; COMMAND ${C",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt:1287,Deployability,update,update,1287,"RROR ""Building SPIRV-Tools tests is unsupported without the SPIR-V target""); endif (). # SPIRV_DIS and SPIRV_VAL variables can be used to provide paths to existing; # spirv-dis and spirv-val binaries, respectively. Otherwise, build them from; # SPIRV-Tools source.; if (NOT SPIRV_DIS OR NOT SPIRV_VAL); include(ExternalProject). set(BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/SPIRVTools-bin). ExternalProject_Add(SPIRVTools; GIT_REPOSITORY https://github.com/KhronosGroup/SPIRV-Tools.git; GIT_TAG main; BINARY_DIR ${BINARY_DIR}; BUILD_COMMAND ${CMAKE_COMMAND} --build ${BINARY_DIR} --target spirv-dis spirv-val; BUILD_BYPRODUCTS ${BINARY_DIR}/tools/spirv-dis ${BINARY_DIR}/tools/spirv-val; DOWNLOAD_COMMAND git clone https://github.com/KhronosGroup/SPIRV-Tools.git SPIRVTools &&; cd SPIRVTools &&; ${Python3_EXECUTABLE} utils/git-sync-deps; UPDATE_COMMAND git pull origin main &&; ${Python3_EXECUTABLE} utils/git-sync-deps; # Don't auto-update on every build.; UPDATE_DISCONNECTED 1; # Allow manual updating with an explicit SPIRVTools-update target.; STEP_TARGETS update; # Install handled below.; INSTALL_COMMAND """"; ); endif (). if (CMAKE_HOST_UNIX); set(LLVM_LINK_OR_COPY create_symlink); else (); set(LLVM_LINK_OR_COPY copy); endif (). # Link the provided or just built spirv-dis and spirv-val binaries.; if (SPIRV_DIS); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${SPIRV_DIS}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""); else (); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/tools/spirv-dis"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""; DEPENDS SPIRVTools; ); endif (). if (SPIRV_VAL); add_custom_target(spirv-val; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${SPIRV_VAL}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-val""); else (); add_custom_target(spirv-val; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/tools/spirv-val"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-val""; DEPENDS SPIRVTools; ); endif ();",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt:1316,Deployability,update,update,1316,"OR ""Building SPIRV-Tools tests is unsupported without the SPIR-V target""); endif (). # SPIRV_DIS and SPIRV_VAL variables can be used to provide paths to existing; # spirv-dis and spirv-val binaries, respectively. Otherwise, build them from; # SPIRV-Tools source.; if (NOT SPIRV_DIS OR NOT SPIRV_VAL); include(ExternalProject). set(BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/SPIRVTools-bin). ExternalProject_Add(SPIRVTools; GIT_REPOSITORY https://github.com/KhronosGroup/SPIRV-Tools.git; GIT_TAG main; BINARY_DIR ${BINARY_DIR}; BUILD_COMMAND ${CMAKE_COMMAND} --build ${BINARY_DIR} --target spirv-dis spirv-val; BUILD_BYPRODUCTS ${BINARY_DIR}/tools/spirv-dis ${BINARY_DIR}/tools/spirv-val; DOWNLOAD_COMMAND git clone https://github.com/KhronosGroup/SPIRV-Tools.git SPIRVTools &&; cd SPIRVTools &&; ${Python3_EXECUTABLE} utils/git-sync-deps; UPDATE_COMMAND git pull origin main &&; ${Python3_EXECUTABLE} utils/git-sync-deps; # Don't auto-update on every build.; UPDATE_DISCONNECTED 1; # Allow manual updating with an explicit SPIRVTools-update target.; STEP_TARGETS update; # Install handled below.; INSTALL_COMMAND """"; ); endif (). if (CMAKE_HOST_UNIX); set(LLVM_LINK_OR_COPY create_symlink); else (); set(LLVM_LINK_OR_COPY copy); endif (). # Link the provided or just built spirv-dis and spirv-val binaries.; if (SPIRV_DIS); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${SPIRV_DIS}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""); else (); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/tools/spirv-dis"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""; DEPENDS SPIRVTools; ); endif (). if (SPIRV_VAL); add_custom_target(spirv-val; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${SPIRV_VAL}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-val""); else (); add_custom_target(spirv-val; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/tools/spirv-val"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-val""; DEPENDS SPIRVTools; ); endif (); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt:1326,Deployability,Install,Install,1326,"OR ""Building SPIRV-Tools tests is unsupported without the SPIR-V target""); endif (). # SPIRV_DIS and SPIRV_VAL variables can be used to provide paths to existing; # spirv-dis and spirv-val binaries, respectively. Otherwise, build them from; # SPIRV-Tools source.; if (NOT SPIRV_DIS OR NOT SPIRV_VAL); include(ExternalProject). set(BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/SPIRVTools-bin). ExternalProject_Add(SPIRVTools; GIT_REPOSITORY https://github.com/KhronosGroup/SPIRV-Tools.git; GIT_TAG main; BINARY_DIR ${BINARY_DIR}; BUILD_COMMAND ${CMAKE_COMMAND} --build ${BINARY_DIR} --target spirv-dis spirv-val; BUILD_BYPRODUCTS ${BINARY_DIR}/tools/spirv-dis ${BINARY_DIR}/tools/spirv-val; DOWNLOAD_COMMAND git clone https://github.com/KhronosGroup/SPIRV-Tools.git SPIRVTools &&; cd SPIRVTools &&; ${Python3_EXECUTABLE} utils/git-sync-deps; UPDATE_COMMAND git pull origin main &&; ${Python3_EXECUTABLE} utils/git-sync-deps; # Don't auto-update on every build.; UPDATE_DISCONNECTED 1; # Allow manual updating with an explicit SPIRVTools-update target.; STEP_TARGETS update; # Install handled below.; INSTALL_COMMAND """"; ); endif (). if (CMAKE_HOST_UNIX); set(LLVM_LINK_OR_COPY create_symlink); else (); set(LLVM_LINK_OR_COPY copy); endif (). # Link the provided or just built spirv-dis and spirv-val binaries.; if (SPIRV_DIS); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${SPIRV_DIS}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""); else (); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/tools/spirv-dis"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""; DEPENDS SPIRVTools; ); endif (). if (SPIRV_VAL); add_custom_target(spirv-val; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${SPIRV_VAL}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-val""); else (); add_custom_target(spirv-val; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/tools/spirv-val"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-val""; DEPENDS SPIRVTools; ); endif (); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt:240,Integrability,message,message,240,"option(LLVM_INCLUDE_SPIRV_TOOLS_TESTS ""Include tests that use SPIRV-Tools"" Off); mark_as_advanced(LLVM_INCLUDE_SPIRV_TOOLS_TESTS). if (NOT LLVM_INCLUDE_SPIRV_TOOLS_TESTS); return(); endif (). if (NOT ""SPIRV"" IN_LIST LLVM_TARGETS_TO_BUILD); message(FATAL_ERROR ""Building SPIRV-Tools tests is unsupported without the SPIR-V target""); endif (). # SPIRV_DIS and SPIRV_VAL variables can be used to provide paths to existing; # spirv-dis and spirv-val binaries, respectively. Otherwise, build them from; # SPIRV-Tools source.; if (NOT SPIRV_DIS OR NOT SPIRV_VAL); include(ExternalProject). set(BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/SPIRVTools-bin). ExternalProject_Add(SPIRVTools; GIT_REPOSITORY https://github.com/KhronosGroup/SPIRV-Tools.git; GIT_TAG main; BINARY_DIR ${BINARY_DIR}; BUILD_COMMAND ${CMAKE_COMMAND} --build ${BINARY_DIR} --target spirv-dis spirv-val; BUILD_BYPRODUCTS ${BINARY_DIR}/tools/spirv-dis ${BINARY_DIR}/tools/spirv-val; DOWNLOAD_COMMAND git clone https://github.com/KhronosGroup/SPIRV-Tools.git SPIRVTools &&; cd SPIRVTools &&; ${Python3_EXECUTABLE} utils/git-sync-deps; UPDATE_COMMAND git pull origin main &&; ${Python3_EXECUTABLE} utils/git-sync-deps; # Don't auto-update on every build.; UPDATE_DISCONNECTED 1; # Allow manual updating with an explicit SPIRVTools-update target.; STEP_TARGETS update; # Install handled below.; INSTALL_COMMAND """"; ); endif (). if (CMAKE_HOST_UNIX); set(LLVM_LINK_OR_COPY create_symlink); else (); set(LLVM_LINK_OR_COPY copy); endif (). # Link the provided or just built spirv-dis and spirv-val binaries.; if (SPIRV_DIS); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${SPIRV_DIS}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""); else (); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/tools/spirv-dis"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""; DEPENDS SPIRVTools; ); endif (). if (SPIRV_VAL); add_custom_target(spirv-val; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt:1876,Integrability,DEPEND,DEPENDS,1876,"OR ""Building SPIRV-Tools tests is unsupported without the SPIR-V target""); endif (). # SPIRV_DIS and SPIRV_VAL variables can be used to provide paths to existing; # spirv-dis and spirv-val binaries, respectively. Otherwise, build them from; # SPIRV-Tools source.; if (NOT SPIRV_DIS OR NOT SPIRV_VAL); include(ExternalProject). set(BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/SPIRVTools-bin). ExternalProject_Add(SPIRVTools; GIT_REPOSITORY https://github.com/KhronosGroup/SPIRV-Tools.git; GIT_TAG main; BINARY_DIR ${BINARY_DIR}; BUILD_COMMAND ${CMAKE_COMMAND} --build ${BINARY_DIR} --target spirv-dis spirv-val; BUILD_BYPRODUCTS ${BINARY_DIR}/tools/spirv-dis ${BINARY_DIR}/tools/spirv-val; DOWNLOAD_COMMAND git clone https://github.com/KhronosGroup/SPIRV-Tools.git SPIRVTools &&; cd SPIRVTools &&; ${Python3_EXECUTABLE} utils/git-sync-deps; UPDATE_COMMAND git pull origin main &&; ${Python3_EXECUTABLE} utils/git-sync-deps; # Don't auto-update on every build.; UPDATE_DISCONNECTED 1; # Allow manual updating with an explicit SPIRVTools-update target.; STEP_TARGETS update; # Install handled below.; INSTALL_COMMAND """"; ); endif (). if (CMAKE_HOST_UNIX); set(LLVM_LINK_OR_COPY create_symlink); else (); set(LLVM_LINK_OR_COPY copy); endif (). # Link the provided or just built spirv-dis and spirv-val binaries.; if (SPIRV_DIS); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${SPIRV_DIS}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""); else (); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/tools/spirv-dis"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""; DEPENDS SPIRVTools; ); endif (). if (SPIRV_VAL); add_custom_target(spirv-val; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${SPIRV_VAL}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-val""); else (); add_custom_target(spirv-val; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/tools/spirv-val"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-val""; DEPENDS SPIRVTools; ); endif (); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt:2224,Integrability,DEPEND,DEPENDS,2224,"OR ""Building SPIRV-Tools tests is unsupported without the SPIR-V target""); endif (). # SPIRV_DIS and SPIRV_VAL variables can be used to provide paths to existing; # spirv-dis and spirv-val binaries, respectively. Otherwise, build them from; # SPIRV-Tools source.; if (NOT SPIRV_DIS OR NOT SPIRV_VAL); include(ExternalProject). set(BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/SPIRVTools-bin). ExternalProject_Add(SPIRVTools; GIT_REPOSITORY https://github.com/KhronosGroup/SPIRV-Tools.git; GIT_TAG main; BINARY_DIR ${BINARY_DIR}; BUILD_COMMAND ${CMAKE_COMMAND} --build ${BINARY_DIR} --target spirv-dis spirv-val; BUILD_BYPRODUCTS ${BINARY_DIR}/tools/spirv-dis ${BINARY_DIR}/tools/spirv-val; DOWNLOAD_COMMAND git clone https://github.com/KhronosGroup/SPIRV-Tools.git SPIRVTools &&; cd SPIRVTools &&; ${Python3_EXECUTABLE} utils/git-sync-deps; UPDATE_COMMAND git pull origin main &&; ${Python3_EXECUTABLE} utils/git-sync-deps; # Don't auto-update on every build.; UPDATE_DISCONNECTED 1; # Allow manual updating with an explicit SPIRVTools-update target.; STEP_TARGETS update; # Install handled below.; INSTALL_COMMAND """"; ); endif (). if (CMAKE_HOST_UNIX); set(LLVM_LINK_OR_COPY create_symlink); else (); set(LLVM_LINK_OR_COPY copy); endif (). # Link the provided or just built spirv-dis and spirv-val binaries.; if (SPIRV_DIS); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${SPIRV_DIS}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""); else (); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/tools/spirv-dis"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""; DEPENDS SPIRVTools; ); endif (). if (SPIRV_VAL); add_custom_target(spirv-val; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${SPIRV_VAL}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-val""); else (); add_custom_target(spirv-val; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/tools/spirv-val"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-val""; DEPENDS SPIRVTools; ); endif (); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt:368,Modifiability,variab,variables,368,"option(LLVM_INCLUDE_SPIRV_TOOLS_TESTS ""Include tests that use SPIRV-Tools"" Off); mark_as_advanced(LLVM_INCLUDE_SPIRV_TOOLS_TESTS). if (NOT LLVM_INCLUDE_SPIRV_TOOLS_TESTS); return(); endif (). if (NOT ""SPIRV"" IN_LIST LLVM_TARGETS_TO_BUILD); message(FATAL_ERROR ""Building SPIRV-Tools tests is unsupported without the SPIR-V target""); endif (). # SPIRV_DIS and SPIRV_VAL variables can be used to provide paths to existing; # spirv-dis and spirv-val binaries, respectively. Otherwise, build them from; # SPIRV-Tools source.; if (NOT SPIRV_DIS OR NOT SPIRV_VAL); include(ExternalProject). set(BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/SPIRVTools-bin). ExternalProject_Add(SPIRVTools; GIT_REPOSITORY https://github.com/KhronosGroup/SPIRV-Tools.git; GIT_TAG main; BINARY_DIR ${BINARY_DIR}; BUILD_COMMAND ${CMAKE_COMMAND} --build ${BINARY_DIR} --target spirv-dis spirv-val; BUILD_BYPRODUCTS ${BINARY_DIR}/tools/spirv-dis ${BINARY_DIR}/tools/spirv-val; DOWNLOAD_COMMAND git clone https://github.com/KhronosGroup/SPIRV-Tools.git SPIRVTools &&; cd SPIRVTools &&; ${Python3_EXECUTABLE} utils/git-sync-deps; UPDATE_COMMAND git pull origin main &&; ${Python3_EXECUTABLE} utils/git-sync-deps; # Don't auto-update on every build.; UPDATE_DISCONNECTED 1; # Allow manual updating with an explicit SPIRVTools-update target.; STEP_TARGETS update; # Install handled below.; INSTALL_COMMAND """"; ); endif (). if (CMAKE_HOST_UNIX); set(LLVM_LINK_OR_COPY create_symlink); else (); set(LLVM_LINK_OR_COPY copy); endif (). # Link the provided or just built spirv-dis and spirv-val binaries.; if (SPIRV_DIS); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${SPIRV_DIS}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""); else (); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/tools/spirv-dis"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""; DEPENDS SPIRVTools; ); endif (). if (SPIRV_VAL); add_custom_target(spirv-val; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt:47,Testability,test,tests,47,"option(LLVM_INCLUDE_SPIRV_TOOLS_TESTS ""Include tests that use SPIRV-Tools"" Off); mark_as_advanced(LLVM_INCLUDE_SPIRV_TOOLS_TESTS). if (NOT LLVM_INCLUDE_SPIRV_TOOLS_TESTS); return(); endif (). if (NOT ""SPIRV"" IN_LIST LLVM_TARGETS_TO_BUILD); message(FATAL_ERROR ""Building SPIRV-Tools tests is unsupported without the SPIR-V target""); endif (). # SPIRV_DIS and SPIRV_VAL variables can be used to provide paths to existing; # spirv-dis and spirv-val binaries, respectively. Otherwise, build them from; # SPIRV-Tools source.; if (NOT SPIRV_DIS OR NOT SPIRV_VAL); include(ExternalProject). set(BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/SPIRVTools-bin). ExternalProject_Add(SPIRVTools; GIT_REPOSITORY https://github.com/KhronosGroup/SPIRV-Tools.git; GIT_TAG main; BINARY_DIR ${BINARY_DIR}; BUILD_COMMAND ${CMAKE_COMMAND} --build ${BINARY_DIR} --target spirv-dis spirv-val; BUILD_BYPRODUCTS ${BINARY_DIR}/tools/spirv-dis ${BINARY_DIR}/tools/spirv-val; DOWNLOAD_COMMAND git clone https://github.com/KhronosGroup/SPIRV-Tools.git SPIRVTools &&; cd SPIRVTools &&; ${Python3_EXECUTABLE} utils/git-sync-deps; UPDATE_COMMAND git pull origin main &&; ${Python3_EXECUTABLE} utils/git-sync-deps; # Don't auto-update on every build.; UPDATE_DISCONNECTED 1; # Allow manual updating with an explicit SPIRVTools-update target.; STEP_TARGETS update; # Install handled below.; INSTALL_COMMAND """"; ); endif (). if (CMAKE_HOST_UNIX); set(LLVM_LINK_OR_COPY create_symlink); else (); set(LLVM_LINK_OR_COPY copy); endif (). # Link the provided or just built spirv-dis and spirv-val binaries.; if (SPIRV_DIS); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${SPIRV_DIS}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""); else (); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/tools/spirv-dis"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""; DEPENDS SPIRVTools; ); endif (). if (SPIRV_VAL); add_custom_target(spirv-val; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt:282,Testability,test,tests,282,"option(LLVM_INCLUDE_SPIRV_TOOLS_TESTS ""Include tests that use SPIRV-Tools"" Off); mark_as_advanced(LLVM_INCLUDE_SPIRV_TOOLS_TESTS). if (NOT LLVM_INCLUDE_SPIRV_TOOLS_TESTS); return(); endif (). if (NOT ""SPIRV"" IN_LIST LLVM_TARGETS_TO_BUILD); message(FATAL_ERROR ""Building SPIRV-Tools tests is unsupported without the SPIR-V target""); endif (). # SPIRV_DIS and SPIRV_VAL variables can be used to provide paths to existing; # spirv-dis and spirv-val binaries, respectively. Otherwise, build them from; # SPIRV-Tools source.; if (NOT SPIRV_DIS OR NOT SPIRV_VAL); include(ExternalProject). set(BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/SPIRVTools-bin). ExternalProject_Add(SPIRVTools; GIT_REPOSITORY https://github.com/KhronosGroup/SPIRV-Tools.git; GIT_TAG main; BINARY_DIR ${BINARY_DIR}; BUILD_COMMAND ${CMAKE_COMMAND} --build ${BINARY_DIR} --target spirv-dis spirv-val; BUILD_BYPRODUCTS ${BINARY_DIR}/tools/spirv-dis ${BINARY_DIR}/tools/spirv-val; DOWNLOAD_COMMAND git clone https://github.com/KhronosGroup/SPIRV-Tools.git SPIRVTools &&; cd SPIRVTools &&; ${Python3_EXECUTABLE} utils/git-sync-deps; UPDATE_COMMAND git pull origin main &&; ${Python3_EXECUTABLE} utils/git-sync-deps; # Don't auto-update on every build.; UPDATE_DISCONNECTED 1; # Allow manual updating with an explicit SPIRVTools-update target.; STEP_TARGETS update; # Install handled below.; INSTALL_COMMAND """"; ); endif (). if (CMAKE_HOST_UNIX); set(LLVM_LINK_OR_COPY create_symlink); else (); set(LLVM_LINK_OR_COPY copy); endif (). # Link the provided or just built spirv-dis and spirv-val binaries.; if (SPIRV_DIS); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${SPIRV_DIS}"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""); else (); add_custom_target(spirv-dis; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY} ""${BINARY_DIR}/tools/spirv-dis"" ""${LLVM_RUNTIME_OUTPUT_INTDIR}/spirv-dis""; DEPENDS SPIRVTools; ); endif (). if (SPIRV_VAL); add_custom_target(spirv-val; COMMAND ${CMAKE_COMMAND} -E ${LLVM_LINK_OR_COPY",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/spirv-tools/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/verify-uselistorder/CMakeLists.txt:147,Integrability,DEPEND,DEPENDS,147,set(LLVM_LINK_COMPONENTS; AsmParser; BitReader; BitWriter; Core; IRReader; Support; ). add_llvm_tool(verify-uselistorder; verify-uselistorder.cpp. DEPENDS; intrinsics_gen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/verify-uselistorder/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/verify-uselistorder/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:38,Deployability,release,released,38,"# OS X 10.11 El Capitan has just been released. One of the new features, System; # Integrity Protection, prevents modifying the base OS install, even with sudo.; # This prevents LLVM developers on OS X from being able to easily install new; # system compilers. The feature can be disabled, but to make it easier for; # developers to work without disabling SIP, this file can generate an Xcode; # toolchain. Xcode toolchains are a mostly-undocumented feature that allows; # multiple copies of low level tools to be installed to different locations, and; # users can easily switch between them. # Setting an environment variable TOOLCHAINS to the toolchain's identifier will; # result in /usr/bin/<tool> or xcrun <tool> to find the tool in the toolchain. # To make this work with Xcode 7.1 and later you can install the toolchain this; # file generates anywhere on your system and set EXTERNAL_TOOLCHAINS_DIR to the; # path specified by $CMAKE_INSTALL_PREFIX/Toolchains. # This file generates a custom install-xcode-toolchain target which constructs; # and installs a toolchain with the identifier in the pattern:; # org.llvm.${PACKAGE_VERSION}. This toolchain can then be used to override the; # system compiler by setting TOOLCHAINS=org.llvm.${PACKAGE_VERSION} in the; # in the environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -vers",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:136,Deployability,install,install,136,"# OS X 10.11 El Capitan has just been released. One of the new features, System; # Integrity Protection, prevents modifying the base OS install, even with sudo.; # This prevents LLVM developers on OS X from being able to easily install new; # system compilers. The feature can be disabled, but to make it easier for; # developers to work without disabling SIP, this file can generate an Xcode; # toolchain. Xcode toolchains are a mostly-undocumented feature that allows; # multiple copies of low level tools to be installed to different locations, and; # users can easily switch between them. # Setting an environment variable TOOLCHAINS to the toolchain's identifier will; # result in /usr/bin/<tool> or xcrun <tool> to find the tool in the toolchain. # To make this work with Xcode 7.1 and later you can install the toolchain this; # file generates anywhere on your system and set EXTERNAL_TOOLCHAINS_DIR to the; # path specified by $CMAKE_INSTALL_PREFIX/Toolchains. # This file generates a custom install-xcode-toolchain target which constructs; # and installs a toolchain with the identifier in the pattern:; # org.llvm.${PACKAGE_VERSION}. This toolchain can then be used to override the; # system compiler by setting TOOLCHAINS=org.llvm.${PACKAGE_VERSION} in the; # in the environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -vers",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:228,Deployability,install,install,228,"# OS X 10.11 El Capitan has just been released. One of the new features, System; # Integrity Protection, prevents modifying the base OS install, even with sudo.; # This prevents LLVM developers on OS X from being able to easily install new; # system compilers. The feature can be disabled, but to make it easier for; # developers to work without disabling SIP, this file can generate an Xcode; # toolchain. Xcode toolchains are a mostly-undocumented feature that allows; # multiple copies of low level tools to be installed to different locations, and; # users can easily switch between them. # Setting an environment variable TOOLCHAINS to the toolchain's identifier will; # result in /usr/bin/<tool> or xcrun <tool> to find the tool in the toolchain. # To make this work with Xcode 7.1 and later you can install the toolchain this; # file generates anywhere on your system and set EXTERNAL_TOOLCHAINS_DIR to the; # path specified by $CMAKE_INSTALL_PREFIX/Toolchains. # This file generates a custom install-xcode-toolchain target which constructs; # and installs a toolchain with the identifier in the pattern:; # org.llvm.${PACKAGE_VERSION}. This toolchain can then be used to override the; # system compiler by setting TOOLCHAINS=org.llvm.${PACKAGE_VERSION} in the; # in the environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -vers",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:514,Deployability,install,installed,514,"# OS X 10.11 El Capitan has just been released. One of the new features, System; # Integrity Protection, prevents modifying the base OS install, even with sudo.; # This prevents LLVM developers on OS X from being able to easily install new; # system compilers. The feature can be disabled, but to make it easier for; # developers to work without disabling SIP, this file can generate an Xcode; # toolchain. Xcode toolchains are a mostly-undocumented feature that allows; # multiple copies of low level tools to be installed to different locations, and; # users can easily switch between them. # Setting an environment variable TOOLCHAINS to the toolchain's identifier will; # result in /usr/bin/<tool> or xcrun <tool> to find the tool in the toolchain. # To make this work with Xcode 7.1 and later you can install the toolchain this; # file generates anywhere on your system and set EXTERNAL_TOOLCHAINS_DIR to the; # path specified by $CMAKE_INSTALL_PREFIX/Toolchains. # This file generates a custom install-xcode-toolchain target which constructs; # and installs a toolchain with the identifier in the pattern:; # org.llvm.${PACKAGE_VERSION}. This toolchain can then be used to override the; # system compiler by setting TOOLCHAINS=org.llvm.${PACKAGE_VERSION} in the; # in the environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -vers",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:806,Deployability,install,install,806,"# OS X 10.11 El Capitan has just been released. One of the new features, System; # Integrity Protection, prevents modifying the base OS install, even with sudo.; # This prevents LLVM developers on OS X from being able to easily install new; # system compilers. The feature can be disabled, but to make it easier for; # developers to work without disabling SIP, this file can generate an Xcode; # toolchain. Xcode toolchains are a mostly-undocumented feature that allows; # multiple copies of low level tools to be installed to different locations, and; # users can easily switch between them. # Setting an environment variable TOOLCHAINS to the toolchain's identifier will; # result in /usr/bin/<tool> or xcrun <tool> to find the tool in the toolchain. # To make this work with Xcode 7.1 and later you can install the toolchain this; # file generates anywhere on your system and set EXTERNAL_TOOLCHAINS_DIR to the; # path specified by $CMAKE_INSTALL_PREFIX/Toolchains. # This file generates a custom install-xcode-toolchain target which constructs; # and installs a toolchain with the identifier in the pattern:; # org.llvm.${PACKAGE_VERSION}. This toolchain can then be used to override the; # system compiler by setting TOOLCHAINS=org.llvm.${PACKAGE_VERSION} in the; # in the environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -vers",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:1000,Deployability,install,install-xcode-toolchain,1000,"sed. One of the new features, System; # Integrity Protection, prevents modifying the base OS install, even with sudo.; # This prevents LLVM developers on OS X from being able to easily install new; # system compilers. The feature can be disabled, but to make it easier for; # developers to work without disabling SIP, this file can generate an Xcode; # toolchain. Xcode toolchains are a mostly-undocumented feature that allows; # multiple copies of low level tools to be installed to different locations, and; # users can easily switch between them. # Setting an environment variable TOOLCHAINS to the toolchain's identifier will; # result in /usr/bin/<tool> or xcrun <tool> to find the tool in the toolchain. # To make this work with Xcode 7.1 and later you can install the toolchain this; # file generates anywhere on your system and set EXTERNAL_TOOLCHAINS_DIR to the; # path specified by $CMAKE_INSTALL_PREFIX/Toolchains. # This file generates a custom install-xcode-toolchain target which constructs; # and installs a toolchain with the identifier in the pattern:; # org.llvm.${PACKAGE_VERSION}. This toolchain can then be used to override the; # system compiler by setting TOOLCHAINS=org.llvm.${PACKAGE_VERSION} in the; # in the environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -version; OUTPUT_VARIABLE xcodebuild_version; OU",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:1055,Deployability,install,installs,1055,"sed. One of the new features, System; # Integrity Protection, prevents modifying the base OS install, even with sudo.; # This prevents LLVM developers on OS X from being able to easily install new; # system compilers. The feature can be disabled, but to make it easier for; # developers to work without disabling SIP, this file can generate an Xcode; # toolchain. Xcode toolchains are a mostly-undocumented feature that allows; # multiple copies of low level tools to be installed to different locations, and; # users can easily switch between them. # Setting an environment variable TOOLCHAINS to the toolchain's identifier will; # result in /usr/bin/<tool> or xcrun <tool> to find the tool in the toolchain. # To make this work with Xcode 7.1 and later you can install the toolchain this; # file generates anywhere on your system and set EXTERNAL_TOOLCHAINS_DIR to the; # path specified by $CMAKE_INSTALL_PREFIX/Toolchains. # This file generates a custom install-xcode-toolchain target which constructs; # and installs a toolchain with the identifier in the pattern:; # org.llvm.${PACKAGE_VERSION}. This toolchain can then be used to override the; # system compiler by setting TOOLCHAINS=org.llvm.${PACKAGE_VERSION} in the; # in the environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -version; OUTPUT_VARIABLE xcodebuild_version; OU",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:1390,Deployability,install,install,1390,"ain. Xcode toolchains are a mostly-undocumented feature that allows; # multiple copies of low level tools to be installed to different locations, and; # users can easily switch between them. # Setting an environment variable TOOLCHAINS to the toolchain's identifier will; # result in /usr/bin/<tool> or xcrun <tool> to find the tool in the toolchain. # To make this work with Xcode 7.1 and later you can install the toolchain this; # file generates anywhere on your system and set EXTERNAL_TOOLCHAINS_DIR to the; # path specified by $CMAKE_INSTALL_PREFIX/Toolchains. # This file generates a custom install-xcode-toolchain target which constructs; # and installs a toolchain with the identifier in the pattern:; # org.llvm.${PACKAGE_VERSION}. This toolchain can then be used to override the; # system compiler by setting TOOLCHAINS=org.llvm.${PACKAGE_VERSION} in the; # in the environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -version; OUTPUT_VARIABLE xcodebuild_version; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""Xcode ([0-9][0-9]?([.][0-9])+)"" version_match ${xcodebuild_version}); if(version_match); message(STATUS ""Identified Xcode Version: ${CMAKE_MATCH_1}""); set(XCODE_VERSION ${CMAKE_MATCH_1}); else(); # If detecting Xcode version failed, set a crazy high version so we default; # to the",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:1407,Deployability,install,install-xcode-toolchain,1407,"ain. Xcode toolchains are a mostly-undocumented feature that allows; # multiple copies of low level tools to be installed to different locations, and; # users can easily switch between them. # Setting an environment variable TOOLCHAINS to the toolchain's identifier will; # result in /usr/bin/<tool> or xcrun <tool> to find the tool in the toolchain. # To make this work with Xcode 7.1 and later you can install the toolchain this; # file generates anywhere on your system and set EXTERNAL_TOOLCHAINS_DIR to the; # path specified by $CMAKE_INSTALL_PREFIX/Toolchains. # This file generates a custom install-xcode-toolchain target which constructs; # and installs a toolchain with the identifier in the pattern:; # org.llvm.${PACKAGE_VERSION}. This toolchain can then be used to override the; # system compiler by setting TOOLCHAINS=org.llvm.${PACKAGE_VERSION} in the; # in the environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -version; OUTPUT_VARIABLE xcodebuild_version; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""Xcode ([0-9][0-9]?([.][0-9])+)"" version_match ${xcodebuild_version}); if(version_match); message(STATUS ""Identified Xcode Version: ${CMAKE_MATCH_1}""); set(XCODE_VERSION ${CMAKE_MATCH_1}); else(); # If detecting Xcode version failed, set a crazy high version so we default; # to the",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:1470,Deployability,install,install,1470,"ain. Xcode toolchains are a mostly-undocumented feature that allows; # multiple copies of low level tools to be installed to different locations, and; # users can easily switch between them. # Setting an environment variable TOOLCHAINS to the toolchain's identifier will; # result in /usr/bin/<tool> or xcrun <tool> to find the tool in the toolchain. # To make this work with Xcode 7.1 and later you can install the toolchain this; # file generates anywhere on your system and set EXTERNAL_TOOLCHAINS_DIR to the; # path specified by $CMAKE_INSTALL_PREFIX/Toolchains. # This file generates a custom install-xcode-toolchain target which constructs; # and installs a toolchain with the identifier in the pattern:; # org.llvm.${PACKAGE_VERSION}. This toolchain can then be used to override the; # system compiler by setting TOOLCHAINS=org.llvm.${PACKAGE_VERSION} in the; # in the environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -version; OUTPUT_VARIABLE xcodebuild_version; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""Xcode ([0-9][0-9]?([.][0-9])+)"" version_match ${xcodebuild_version}); if(version_match); message(STATUS ""Identified Xcode Version: ${CMAKE_MATCH_1}""); set(XCODE_VERSION ${CMAKE_MATCH_1}); else(); # If detecting Xcode version failed, set a crazy high version so we default; # to the",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:1569,Deployability,install,installed,1569,"ch between them. # Setting an environment variable TOOLCHAINS to the toolchain's identifier will; # result in /usr/bin/<tool> or xcrun <tool> to find the tool in the toolchain. # To make this work with Xcode 7.1 and later you can install the toolchain this; # file generates anywhere on your system and set EXTERNAL_TOOLCHAINS_DIR to the; # path specified by $CMAKE_INSTALL_PREFIX/Toolchains. # This file generates a custom install-xcode-toolchain target which constructs; # and installs a toolchain with the identifier in the pattern:; # org.llvm.${PACKAGE_VERSION}. This toolchain can then be used to override the; # system compiler by setting TOOLCHAINS=org.llvm.${PACKAGE_VERSION} in the; # in the environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -version; OUTPUT_VARIABLE xcodebuild_version; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""Xcode ([0-9][0-9]?([.][0-9])+)"" version_match ${xcodebuild_version}); if(version_match); message(STATUS ""Identified Xcode Version: ${CMAKE_MATCH_1}""); set(XCODE_VERSION ${CMAKE_MATCH_1}); else(); # If detecting Xcode version failed, set a crazy high version so we default; # to the newest.; set(XCODE_VERSION 99); message(WARNING ""Failed to detect the version of an installed copy of Xcode, falling back to highest supported version. Set XCODE_VERSION to o",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:1722,Deployability,install,install,1722,"l> to find the tool in the toolchain. # To make this work with Xcode 7.1 and later you can install the toolchain this; # file generates anywhere on your system and set EXTERNAL_TOOLCHAINS_DIR to the; # path specified by $CMAKE_INSTALL_PREFIX/Toolchains. # This file generates a custom install-xcode-toolchain target which constructs; # and installs a toolchain with the identifier in the pattern:; # org.llvm.${PACKAGE_VERSION}. This toolchain can then be used to override the; # system compiler by setting TOOLCHAINS=org.llvm.${PACKAGE_VERSION} in the; # in the environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -version; OUTPUT_VARIABLE xcodebuild_version; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""Xcode ([0-9][0-9]?([.][0-9])+)"" version_match ${xcodebuild_version}); if(version_match); message(STATUS ""Identified Xcode Version: ${CMAKE_MATCH_1}""); set(XCODE_VERSION ${CMAKE_MATCH_1}); else(); # If detecting Xcode version failed, set a crazy high version so we default; # to the newest.; set(XCODE_VERSION 99); message(WARNING ""Failed to detect the version of an installed copy of Xcode, falling back to highest supported version. Set XCODE_VERSION to override.""); endif(); endif(). # Xcode 8 requires CompatibilityVersion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xco",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:2487,Deployability,install,installed,2487,"chains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -version; OUTPUT_VARIABLE xcodebuild_version; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""Xcode ([0-9][0-9]?([.][0-9])+)"" version_match ${xcodebuild_version}); if(version_match); message(STATUS ""Identified Xcode Version: ${CMAKE_MATCH_1}""); set(XCODE_VERSION ${CMAKE_MATCH_1}); else(); # If detecting Xcode version failed, set a crazy high version so we default; # to the newest.; set(XCODE_VERSION 99); message(WARNING ""Failed to detect the version of an installed copy of Xcode, falling back to highest supported version. Set XCODE_VERSION to override.""); endif(); endif(). # Xcode 8 requires CompatibilityVersion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add_custom_command(OUTPUT ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E make_directory ${LLVMToolchainDir}). add_custom_command(OUTPUT ${LLVMToolchainDir}/Info.plist; DEPENDS ${LLVMToolchainDir}; COMMAND ${",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:3929,Deployability,install,install-xcode-toolchain,3929,"rsion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add_custom_command(OUTPUT ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E make_directory ${LLVMToolchainDir}). add_custom_command(OUTPUT ${LLVMToolchainDir}/Info.plist; DEPENDS ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E remove ${LLVMToolchainDir}/Info.plist; COMMAND /usr/libexec/PlistBuddy -c ""Add:CFBundleIdentifier string org.llvm.${PACKAGE_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; COMMAND /usr/libexec/PlistBuddy -c ""Add:CompatibilityVersion integer ${COMPAT_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; ). add_custom_target(build-xcode-toolchain; COMMAND ""${CMAKE_COMMAND}"" --build ${CMAKE_BINARY_DIR} --target all); add_llvm_install_targets(install-xcode-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist build-xcode-toolchain; PREFIX ${LLVMToolchainDir}/usr/). if(LLVM_DISTRIBUTION_COMPONENTS); if(LLVM_ENABLE_IDE); message(FATAL_ERROR ""LLVM_DISTRIBUTION_COMPONENTS cannot be specified with multi-configuration generators (i.e. Xcode or Visual Studio)""); endif(). add_custom_target(install-distribution-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist distribution). foreach(target ${LLVM_DISTRIBUTION_COMPONENTS}); add_llvm_install_targets(install-distribution-${target}; DEPENDS ${target}; COMPONENT ${target}; PREFIX ${LLVMToolchainDir}/usr/); add_dependencies(install-distribution-toolchain install-distribution-${target}); endforeach(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:4186,Deployability,configurat,configuration,4186,"rsion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add_custom_command(OUTPUT ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E make_directory ${LLVMToolchainDir}). add_custom_command(OUTPUT ${LLVMToolchainDir}/Info.plist; DEPENDS ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E remove ${LLVMToolchainDir}/Info.plist; COMMAND /usr/libexec/PlistBuddy -c ""Add:CFBundleIdentifier string org.llvm.${PACKAGE_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; COMMAND /usr/libexec/PlistBuddy -c ""Add:CompatibilityVersion integer ${COMPAT_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; ). add_custom_target(build-xcode-toolchain; COMMAND ""${CMAKE_COMMAND}"" --build ${CMAKE_BINARY_DIR} --target all); add_llvm_install_targets(install-xcode-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist build-xcode-toolchain; PREFIX ${LLVMToolchainDir}/usr/). if(LLVM_DISTRIBUTION_COMPONENTS); if(LLVM_ENABLE_IDE); message(FATAL_ERROR ""LLVM_DISTRIBUTION_COMPONENTS cannot be specified with multi-configuration generators (i.e. Xcode or Visual Studio)""); endif(). add_custom_target(install-distribution-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist distribution). foreach(target ${LLVM_DISTRIBUTION_COMPONENTS}); add_llvm_install_targets(install-distribution-${target}; DEPENDS ${target}; COMPONENT ${target}; PREFIX ${LLVMToolchainDir}/usr/); add_dependencies(install-distribution-toolchain install-distribution-${target}); endforeach(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:4271,Deployability,install,install-distribution-toolchain,4271,"rsion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add_custom_command(OUTPUT ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E make_directory ${LLVMToolchainDir}). add_custom_command(OUTPUT ${LLVMToolchainDir}/Info.plist; DEPENDS ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E remove ${LLVMToolchainDir}/Info.plist; COMMAND /usr/libexec/PlistBuddy -c ""Add:CFBundleIdentifier string org.llvm.${PACKAGE_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; COMMAND /usr/libexec/PlistBuddy -c ""Add:CompatibilityVersion integer ${COMPAT_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; ). add_custom_target(build-xcode-toolchain; COMMAND ""${CMAKE_COMMAND}"" --build ${CMAKE_BINARY_DIR} --target all); add_llvm_install_targets(install-xcode-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist build-xcode-toolchain; PREFIX ${LLVMToolchainDir}/usr/). if(LLVM_DISTRIBUTION_COMPONENTS); if(LLVM_ENABLE_IDE); message(FATAL_ERROR ""LLVM_DISTRIBUTION_COMPONENTS cannot be specified with multi-configuration generators (i.e. Xcode or Visual Studio)""); endif(). add_custom_target(install-distribution-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist distribution). foreach(target ${LLVM_DISTRIBUTION_COMPONENTS}); add_llvm_install_targets(install-distribution-${target}; DEPENDS ${target}; COMPONENT ${target}; PREFIX ${LLVMToolchainDir}/usr/); add_dependencies(install-distribution-toolchain install-distribution-${target}); endforeach(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:4431,Deployability,install,install-distribution,4431,"rsion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add_custom_command(OUTPUT ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E make_directory ${LLVMToolchainDir}). add_custom_command(OUTPUT ${LLVMToolchainDir}/Info.plist; DEPENDS ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E remove ${LLVMToolchainDir}/Info.plist; COMMAND /usr/libexec/PlistBuddy -c ""Add:CFBundleIdentifier string org.llvm.${PACKAGE_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; COMMAND /usr/libexec/PlistBuddy -c ""Add:CompatibilityVersion integer ${COMPAT_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; ). add_custom_target(build-xcode-toolchain; COMMAND ""${CMAKE_COMMAND}"" --build ${CMAKE_BINARY_DIR} --target all); add_llvm_install_targets(install-xcode-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist build-xcode-toolchain; PREFIX ${LLVMToolchainDir}/usr/). if(LLVM_DISTRIBUTION_COMPONENTS); if(LLVM_ENABLE_IDE); message(FATAL_ERROR ""LLVM_DISTRIBUTION_COMPONENTS cannot be specified with multi-configuration generators (i.e. Xcode or Visual Studio)""); endif(). add_custom_target(install-distribution-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist distribution). foreach(target ${LLVM_DISTRIBUTION_COMPONENTS}); add_llvm_install_targets(install-distribution-${target}; DEPENDS ${target}; COMPONENT ${target}; PREFIX ${LLVMToolchainDir}/usr/); add_dependencies(install-distribution-toolchain install-distribution-${target}); endforeach(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:4554,Deployability,install,install-distribution-toolchain,4554,"rsion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add_custom_command(OUTPUT ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E make_directory ${LLVMToolchainDir}). add_custom_command(OUTPUT ${LLVMToolchainDir}/Info.plist; DEPENDS ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E remove ${LLVMToolchainDir}/Info.plist; COMMAND /usr/libexec/PlistBuddy -c ""Add:CFBundleIdentifier string org.llvm.${PACKAGE_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; COMMAND /usr/libexec/PlistBuddy -c ""Add:CompatibilityVersion integer ${COMPAT_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; ). add_custom_target(build-xcode-toolchain; COMMAND ""${CMAKE_COMMAND}"" --build ${CMAKE_BINARY_DIR} --target all); add_llvm_install_targets(install-xcode-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist build-xcode-toolchain; PREFIX ${LLVMToolchainDir}/usr/). if(LLVM_DISTRIBUTION_COMPONENTS); if(LLVM_ENABLE_IDE); message(FATAL_ERROR ""LLVM_DISTRIBUTION_COMPONENTS cannot be specified with multi-configuration generators (i.e. Xcode or Visual Studio)""); endif(). add_custom_target(install-distribution-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist distribution). foreach(target ${LLVM_DISTRIBUTION_COMPONENTS}); add_llvm_install_targets(install-distribution-${target}; DEPENDS ${target}; COMPONENT ${target}; PREFIX ${LLVMToolchainDir}/usr/); add_dependencies(install-distribution-toolchain install-distribution-${target}); endforeach(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:4585,Deployability,install,install-distribution,4585,"rsion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add_custom_command(OUTPUT ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E make_directory ${LLVMToolchainDir}). add_custom_command(OUTPUT ${LLVMToolchainDir}/Info.plist; DEPENDS ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E remove ${LLVMToolchainDir}/Info.plist; COMMAND /usr/libexec/PlistBuddy -c ""Add:CFBundleIdentifier string org.llvm.${PACKAGE_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; COMMAND /usr/libexec/PlistBuddy -c ""Add:CompatibilityVersion integer ${COMPAT_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; ). add_custom_target(build-xcode-toolchain; COMMAND ""${CMAKE_COMMAND}"" --build ${CMAKE_BINARY_DIR} --target all); add_llvm_install_targets(install-xcode-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist build-xcode-toolchain; PREFIX ${LLVMToolchainDir}/usr/). if(LLVM_DISTRIBUTION_COMPONENTS); if(LLVM_ENABLE_IDE); message(FATAL_ERROR ""LLVM_DISTRIBUTION_COMPONENTS cannot be specified with multi-configuration generators (i.e. Xcode or Visual Studio)""); endif(). add_custom_target(install-distribution-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist distribution). foreach(target ${LLVM_DISTRIBUTION_COMPONENTS}); add_llvm_install_targets(install-distribution-${target}; DEPENDS ${target}; COMPONENT ${target}; PREFIX ${LLVMToolchainDir}/usr/); add_dependencies(install-distribution-toolchain install-distribution-${target}); endforeach(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:2210,Integrability,message,message,2210,"e environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -version; OUTPUT_VARIABLE xcodebuild_version; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""Xcode ([0-9][0-9]?([.][0-9])+)"" version_match ${xcodebuild_version}); if(version_match); message(STATUS ""Identified Xcode Version: ${CMAKE_MATCH_1}""); set(XCODE_VERSION ${CMAKE_MATCH_1}); else(); # If detecting Xcode version failed, set a crazy high version so we default; # to the newest.; set(XCODE_VERSION 99); message(WARNING ""Failed to detect the version of an installed copy of Xcode, falling back to highest supported version. Set XCODE_VERSION to override.""); endif(); endif(). # Xcode 8 requires CompatibilityVersion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:2435,Integrability,message,message,2435,"chains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -version; OUTPUT_VARIABLE xcodebuild_version; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""Xcode ([0-9][0-9]?([.][0-9])+)"" version_match ${xcodebuild_version}); if(version_match); message(STATUS ""Identified Xcode Version: ${CMAKE_MATCH_1}""); set(XCODE_VERSION ${CMAKE_MATCH_1}); else(); # If detecting Xcode version failed, set a crazy high version so we default; # to the newest.; set(XCODE_VERSION 99); message(WARNING ""Failed to detect the version of an installed copy of Xcode, falling back to highest supported version. Set XCODE_VERSION to override.""); endif(); endif(). # Xcode 8 requires CompatibilityVersion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add_custom_command(OUTPUT ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E make_directory ${LLVMToolchainDir}). add_custom_command(OUTPUT ${LLVMToolchainDir}/Info.plist; DEPENDS ${LLVMToolchainDir}; COMMAND ${",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:3073,Integrability,message,message,3073,"ull; ); string(REGEX MATCH ""Xcode ([0-9][0-9]?([.][0-9])+)"" version_match ${xcodebuild_version}); if(version_match); message(STATUS ""Identified Xcode Version: ${CMAKE_MATCH_1}""); set(XCODE_VERSION ${CMAKE_MATCH_1}); else(); # If detecting Xcode version failed, set a crazy high version so we default; # to the newest.; set(XCODE_VERSION 99); message(WARNING ""Failed to detect the version of an installed copy of Xcode, falling back to highest supported version. Set XCODE_VERSION to override.""); endif(); endif(). # Xcode 8 requires CompatibilityVersion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add_custom_command(OUTPUT ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E make_directory ${LLVMToolchainDir}). add_custom_command(OUTPUT ${LLVMToolchainDir}/Info.plist; DEPENDS ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E remove ${LLVMToolchainDir}/Info.plist; COMMAND /usr/libexec/PlistBuddy -c ""Add:CFBundleIdentifier string org.llvm.${PACKAGE_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; COMMAND /usr/libexec/PlistBuddy -c ""Add:CompatibilityVersion integer ${COMPAT_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; ). add_custom_target(build-xcode-toolchain; COMMAND ""${CMAKE_COMMAND}"" --build ${CMAKE_BINARY_DIR} --target all); add_llvm_install_targets(install-xcode-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist build-xcode-toolchain; PREFIX ${LLVMToolchainDir}/usr/). if(LLVM_DISTRIBUTION_COMPONENTS); if(LLVM_EN",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:3443,Integrability,DEPEND,DEPENDS,3443,"an installed copy of Xcode, falling back to highest supported version. Set XCODE_VERSION to override.""); endif(); endif(). # Xcode 8 requires CompatibilityVersion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add_custom_command(OUTPUT ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E make_directory ${LLVMToolchainDir}). add_custom_command(OUTPUT ${LLVMToolchainDir}/Info.plist; DEPENDS ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E remove ${LLVMToolchainDir}/Info.plist; COMMAND /usr/libexec/PlistBuddy -c ""Add:CFBundleIdentifier string org.llvm.${PACKAGE_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; COMMAND /usr/libexec/PlistBuddy -c ""Add:CompatibilityVersion integer ${COMPAT_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; ). add_custom_target(build-xcode-toolchain; COMMAND ""${CMAKE_COMMAND}"" --build ${CMAKE_BINARY_DIR} --target all); add_llvm_install_targets(install-xcode-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist build-xcode-toolchain; PREFIX ${LLVMToolchainDir}/usr/). if(LLVM_DISTRIBUTION_COMPONENTS); if(LLVM_ENABLE_IDE); message(FATAL_ERROR ""LLVM_DISTRIBUTION_COMPONENTS cannot be specified with multi-configuration generators (i.e. Xcode or Visual Studio)""); endif(). add_custom_target(install-distribution-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist distribution). foreach(target ${LLVM_DISTRIBUTION_COMPONENTS}); add_llvm_install_targets(install-distribution-${target}; DEPENDS ${target}; CO",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:3954,Integrability,DEPEND,DEPENDS,3954,"rsion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add_custom_command(OUTPUT ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E make_directory ${LLVMToolchainDir}). add_custom_command(OUTPUT ${LLVMToolchainDir}/Info.plist; DEPENDS ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E remove ${LLVMToolchainDir}/Info.plist; COMMAND /usr/libexec/PlistBuddy -c ""Add:CFBundleIdentifier string org.llvm.${PACKAGE_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; COMMAND /usr/libexec/PlistBuddy -c ""Add:CompatibilityVersion integer ${COMPAT_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; ). add_custom_target(build-xcode-toolchain; COMMAND ""${CMAKE_COMMAND}"" --build ${CMAKE_BINARY_DIR} --target all); add_llvm_install_targets(install-xcode-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist build-xcode-toolchain; PREFIX ${LLVMToolchainDir}/usr/). if(LLVM_DISTRIBUTION_COMPONENTS); if(LLVM_ENABLE_IDE); message(FATAL_ERROR ""LLVM_DISTRIBUTION_COMPONENTS cannot be specified with multi-configuration generators (i.e. Xcode or Visual Studio)""); endif(). add_custom_target(install-distribution-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist distribution). foreach(target ${LLVM_DISTRIBUTION_COMPONENTS}); add_llvm_install_targets(install-distribution-${target}; DEPENDS ${target}; COMPONENT ${target}; PREFIX ${LLVMToolchainDir}/usr/); add_dependencies(install-distribution-toolchain install-distribution-${target}); endforeach(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:4105,Integrability,message,message,4105,"rsion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add_custom_command(OUTPUT ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E make_directory ${LLVMToolchainDir}). add_custom_command(OUTPUT ${LLVMToolchainDir}/Info.plist; DEPENDS ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E remove ${LLVMToolchainDir}/Info.plist; COMMAND /usr/libexec/PlistBuddy -c ""Add:CFBundleIdentifier string org.llvm.${PACKAGE_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; COMMAND /usr/libexec/PlistBuddy -c ""Add:CompatibilityVersion integer ${COMPAT_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; ). add_custom_target(build-xcode-toolchain; COMMAND ""${CMAKE_COMMAND}"" --build ${CMAKE_BINARY_DIR} --target all); add_llvm_install_targets(install-xcode-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist build-xcode-toolchain; PREFIX ${LLVMToolchainDir}/usr/). if(LLVM_DISTRIBUTION_COMPONENTS); if(LLVM_ENABLE_IDE); message(FATAL_ERROR ""LLVM_DISTRIBUTION_COMPONENTS cannot be specified with multi-configuration generators (i.e. Xcode or Visual Studio)""); endif(). add_custom_target(install-distribution-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist distribution). foreach(target ${LLVM_DISTRIBUTION_COMPONENTS}); add_llvm_install_targets(install-distribution-${target}; DEPENDS ${target}; COMPONENT ${target}; PREFIX ${LLVMToolchainDir}/usr/); add_dependencies(install-distribution-toolchain install-distribution-${target}); endforeach(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:4303,Integrability,DEPEND,DEPENDS,4303,"rsion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add_custom_command(OUTPUT ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E make_directory ${LLVMToolchainDir}). add_custom_command(OUTPUT ${LLVMToolchainDir}/Info.plist; DEPENDS ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E remove ${LLVMToolchainDir}/Info.plist; COMMAND /usr/libexec/PlistBuddy -c ""Add:CFBundleIdentifier string org.llvm.${PACKAGE_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; COMMAND /usr/libexec/PlistBuddy -c ""Add:CompatibilityVersion integer ${COMPAT_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; ). add_custom_target(build-xcode-toolchain; COMMAND ""${CMAKE_COMMAND}"" --build ${CMAKE_BINARY_DIR} --target all); add_llvm_install_targets(install-xcode-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist build-xcode-toolchain; PREFIX ${LLVMToolchainDir}/usr/). if(LLVM_DISTRIBUTION_COMPONENTS); if(LLVM_ENABLE_IDE); message(FATAL_ERROR ""LLVM_DISTRIBUTION_COMPONENTS cannot be specified with multi-configuration generators (i.e. Xcode or Visual Studio)""); endif(). add_custom_target(install-distribution-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist distribution). foreach(target ${LLVM_DISTRIBUTION_COMPONENTS}); add_llvm_install_targets(install-distribution-${target}; DEPENDS ${target}; COMPONENT ${target}; PREFIX ${LLVMToolchainDir}/usr/); add_dependencies(install-distribution-toolchain install-distribution-${target}); endforeach(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:4463,Integrability,DEPEND,DEPENDS,4463,"rsion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add_custom_command(OUTPUT ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E make_directory ${LLVMToolchainDir}). add_custom_command(OUTPUT ${LLVMToolchainDir}/Info.plist; DEPENDS ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E remove ${LLVMToolchainDir}/Info.plist; COMMAND /usr/libexec/PlistBuddy -c ""Add:CFBundleIdentifier string org.llvm.${PACKAGE_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; COMMAND /usr/libexec/PlistBuddy -c ""Add:CompatibilityVersion integer ${COMPAT_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; ). add_custom_target(build-xcode-toolchain; COMMAND ""${CMAKE_COMMAND}"" --build ${CMAKE_BINARY_DIR} --target all); add_llvm_install_targets(install-xcode-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist build-xcode-toolchain; PREFIX ${LLVMToolchainDir}/usr/). if(LLVM_DISTRIBUTION_COMPONENTS); if(LLVM_ENABLE_IDE); message(FATAL_ERROR ""LLVM_DISTRIBUTION_COMPONENTS cannot be specified with multi-configuration generators (i.e. Xcode or Visual Studio)""); endif(). add_custom_target(install-distribution-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist distribution). foreach(target ${LLVM_DISTRIBUTION_COMPONENTS}); add_llvm_install_targets(install-distribution-${target}; DEPENDS ${target}; COMPONENT ${target}; PREFIX ${LLVMToolchainDir}/usr/); add_dependencies(install-distribution-toolchain install-distribution-${target}); endforeach(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:618,Modifiability,variab,variable,618,"# OS X 10.11 El Capitan has just been released. One of the new features, System; # Integrity Protection, prevents modifying the base OS install, even with sudo.; # This prevents LLVM developers on OS X from being able to easily install new; # system compilers. The feature can be disabled, but to make it easier for; # developers to work without disabling SIP, this file can generate an Xcode; # toolchain. Xcode toolchains are a mostly-undocumented feature that allows; # multiple copies of low level tools to be installed to different locations, and; # users can easily switch between them. # Setting an environment variable TOOLCHAINS to the toolchain's identifier will; # result in /usr/bin/<tool> or xcrun <tool> to find the tool in the toolchain. # To make this work with Xcode 7.1 and later you can install the toolchain this; # file generates anywhere on your system and set EXTERNAL_TOOLCHAINS_DIR to the; # path specified by $CMAKE_INSTALL_PREFIX/Toolchains. # This file generates a custom install-xcode-toolchain target which constructs; # and installs a toolchain with the identifier in the pattern:; # org.llvm.${PACKAGE_VERSION}. This toolchain can then be used to override the; # system compiler by setting TOOLCHAINS=org.llvm.${PACKAGE_VERSION} in the; # in the environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -vers",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:4186,Modifiability,config,configuration,4186,"rsion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add_custom_command(OUTPUT ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E make_directory ${LLVMToolchainDir}). add_custom_command(OUTPUT ${LLVMToolchainDir}/Info.plist; DEPENDS ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E remove ${LLVMToolchainDir}/Info.plist; COMMAND /usr/libexec/PlistBuddy -c ""Add:CFBundleIdentifier string org.llvm.${PACKAGE_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; COMMAND /usr/libexec/PlistBuddy -c ""Add:CompatibilityVersion integer ${COMPAT_VERSION}"" ""${LLVMToolchainDir}/Info.plist""; ). add_custom_target(build-xcode-toolchain; COMMAND ""${CMAKE_COMMAND}"" --build ${CMAKE_BINARY_DIR} --target all); add_llvm_install_targets(install-xcode-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist build-xcode-toolchain; PREFIX ${LLVMToolchainDir}/usr/). if(LLVM_DISTRIBUTION_COMPONENTS); if(LLVM_ENABLE_IDE); message(FATAL_ERROR ""LLVM_DISTRIBUTION_COMPONENTS cannot be specified with multi-configuration generators (i.e. Xcode or Visual Studio)""); endif(). add_custom_target(install-distribution-toolchain; DEPENDS ${LLVMToolchainDir}/Info.plist distribution). foreach(target ${LLVM_DISTRIBUTION_COMPONENTS}); add_llvm_install_targets(install-distribution-${target}; DEPENDS ${target}; COMPONENT ${target}; PREFIX ${LLVMToolchainDir}/usr/); add_dependencies(install-distribution-toolchain install-distribution-${target}); endforeach(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:1910,Safety,detect,detect,1910,"d set EXTERNAL_TOOLCHAINS_DIR to the; # path specified by $CMAKE_INSTALL_PREFIX/Toolchains. # This file generates a custom install-xcode-toolchain target which constructs; # and installs a toolchain with the identifier in the pattern:; # org.llvm.${PACKAGE_VERSION}. This toolchain can then be used to override the; # system compiler by setting TOOLCHAINS=org.llvm.${PACKAGE_VERSION} in the; # in the environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -version; OUTPUT_VARIABLE xcodebuild_version; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""Xcode ([0-9][0-9]?([.][0-9])+)"" version_match ${xcodebuild_version}); if(version_match); message(STATUS ""Identified Xcode Version: ${CMAKE_MATCH_1}""); set(XCODE_VERSION ${CMAKE_MATCH_1}); else(); # If detecting Xcode version failed, set a crazy high version so we default; # to the newest.; set(XCODE_VERSION 99); message(WARNING ""Failed to detect the version of an installed copy of Xcode, falling back to highest supported version. Set XCODE_VERSION to override.""); endif(); endif(). # Xcode 8 requires CompatibilityVersion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -fin",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:2322,Safety,detect,detecting,2322,"e environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -version; OUTPUT_VARIABLE xcodebuild_version; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""Xcode ([0-9][0-9]?([.][0-9])+)"" version_match ${xcodebuild_version}); if(version_match); message(STATUS ""Identified Xcode Version: ${CMAKE_MATCH_1}""); set(XCODE_VERSION ${CMAKE_MATCH_1}); else(); # If detecting Xcode version failed, set a crazy high version so we default; # to the newest.; set(XCODE_VERSION 99); message(WARNING ""Failed to detect the version of an installed copy of Xcode, falling back to highest supported version. Set XCODE_VERSION to override.""); endif(); endif(). # Xcode 8 requires CompatibilityVersion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:2462,Safety,detect,detect,2462,"chains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -version; OUTPUT_VARIABLE xcodebuild_version; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""Xcode ([0-9][0-9]?([.][0-9])+)"" version_match ${xcodebuild_version}); if(version_match); message(STATUS ""Identified Xcode Version: ${CMAKE_MATCH_1}""); set(XCODE_VERSION ${CMAKE_MATCH_1}); else(); # If detecting Xcode version failed, set a crazy high version so we default; # to the newest.; set(XCODE_VERSION 99); message(WARNING ""Failed to detect the version of an installed copy of Xcode, falling back to highest supported version. Set XCODE_VERSION to override.""); endif(); endif(). # Xcode 8 requires CompatibilityVersion 2; set(COMPAT_VERSION 2); if(XCODE_VERSION VERSION_LESS 8.0.0); # Xcode 7.3 (the first version supporting external toolchains) requires; # CompatibilityVersion 1; set(COMPAT_VERSION 1); endif(). execute_process(; COMMAND xcrun -find otool; OUTPUT_VARIABLE clang_path; OUTPUT_STRIP_TRAILING_WHITESPACE; ERROR_FILE /dev/null; ); string(REGEX MATCH ""(.*/Toolchains)/.*"" toolchains_match ${clang_path}); if(NOT toolchains_match); message(FATAL_ERROR ""Could not identify toolchain dir""); endif(); set(toolchains_dir ${CMAKE_MATCH_1}). set(LLVMToolchainDir ""${CMAKE_INSTALL_PREFIX}/Toolchains/LLVM${PACKAGE_VERSION}.xctoolchain/""). add_custom_command(OUTPUT ${LLVMToolchainDir}; COMMAND ${CMAKE_COMMAND} -E make_directory ${LLVMToolchainDir}). add_custom_command(OUTPUT ${LLVMToolchainDir}/Info.plist; DEPENDS ${LLVMToolchainDir}; COMMAND ${",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:83,Security,Integrity,Integrity,83,"# OS X 10.11 El Capitan has just been released. One of the new features, System; # Integrity Protection, prevents modifying the base OS install, even with sudo.; # This prevents LLVM developers on OS X from being able to easily install new; # system compilers. The feature can be disabled, but to make it easier for; # developers to work without disabling SIP, this file can generate an Xcode; # toolchain. Xcode toolchains are a mostly-undocumented feature that allows; # multiple copies of low level tools to be installed to different locations, and; # users can easily switch between them. # Setting an environment variable TOOLCHAINS to the toolchain's identifier will; # result in /usr/bin/<tool> or xcrun <tool> to find the tool in the toolchain. # To make this work with Xcode 7.1 and later you can install the toolchain this; # file generates anywhere on your system and set EXTERNAL_TOOLCHAINS_DIR to the; # path specified by $CMAKE_INSTALL_PREFIX/Toolchains. # This file generates a custom install-xcode-toolchain target which constructs; # and installs a toolchain with the identifier in the pattern:; # org.llvm.${PACKAGE_VERSION}. This toolchain can then be used to override the; # system compiler by setting TOOLCHAINS=org.llvm.${PACKAGE_VERSION} in the; # in the environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -vers",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt:437,Usability,undo,undocumented,437,"# OS X 10.11 El Capitan has just been released. One of the new features, System; # Integrity Protection, prevents modifying the base OS install, even with sudo.; # This prevents LLVM developers on OS X from being able to easily install new; # system compilers. The feature can be disabled, but to make it easier for; # developers to work without disabling SIP, this file can generate an Xcode; # toolchain. Xcode toolchains are a mostly-undocumented feature that allows; # multiple copies of low level tools to be installed to different locations, and; # users can easily switch between them. # Setting an environment variable TOOLCHAINS to the toolchain's identifier will; # result in /usr/bin/<tool> or xcrun <tool> to find the tool in the toolchain. # To make this work with Xcode 7.1 and later you can install the toolchain this; # file generates anywhere on your system and set EXTERNAL_TOOLCHAINS_DIR to the; # path specified by $CMAKE_INSTALL_PREFIX/Toolchains. # This file generates a custom install-xcode-toolchain target which constructs; # and installs a toolchain with the identifier in the pattern:; # org.llvm.${PACKAGE_VERSION}. This toolchain can then be used to override the; # system compiler by setting TOOLCHAINS=org.llvm.${PACKAGE_VERSION} in the; # in the environment. # Example usage:; # cmake -G Ninja -DLLVM_CREATE_XCODE_TOOLCHAIN=On; # -DCMAKE_INSTALL_PREFIX=$PWD/install; # ninja install-xcode-toolchain; # export EXTERNAL_TOOLCHAINS_DIR=$PWD/install/Toolchains; # export TOOLCHAINS=org.llvm.3.8.0svn. # `xcrun -find clang` should return the installed clang, and `clang --version`; # should show 3.8.0svn. if(NOT APPLE); return(); endif(). option(LLVM_CREATE_XCODE_TOOLCHAIN ""Create a target to install LLVM into an Xcode toolchain"" Off). if(NOT LLVM_CREATE_XCODE_TOOLCHAIN); return(); endif(). # XCODE_VERSION is set by CMake when using the Xcode generator, otherwise we need; # to detect it manually here.; if(NOT XCODE_VERSION); execute_process(; COMMAND xcodebuild -vers",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/xcode-toolchain/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/lli/ChildTarget/CMakeLists.txt:127,Integrability,DEPEND,DEPENDS,127,set(LLVM_LINK_COMPONENTS; OrcJIT; OrcShared; OrcTargetProcess; Support; ). add_llvm_utility(lli-child-target; ChildTarget.cpp. DEPENDS; intrinsics_gen; ). export_executable_symbols(lli-child-target); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/lli/ChildTarget/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/lli/ChildTarget/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt:1085,Availability,Error,Error,1085,"; set(LLVM_EXEGESIS_TARGETS); if (LLVM_TARGETS_TO_BUILD MATCHES ""X86""); list(APPEND LLVM_EXEGESIS_TARGETS ""X86""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""AArch64""); list(APPEND LLVM_EXEGESIS_TARGETS ""AArch64""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""PowerPC""); list(APPEND LLVM_EXEGESIS_TARGETS ""PowerPC""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""Mips""); list(APPEND LLVM_EXEGESIS_TARGETS ""Mips""); endif(). set(LLVM_EXEGESIS_TARGETS ${LLVM_EXEGESIS_TARGETS} PARENT_SCOPE). foreach(t ${LLVM_EXEGESIS_TARGETS}); add_subdirectory(${t}); endforeach(). set(LLVM_LINK_COMPONENTS; Analysis; CodeGen; CodeGenTypes; Core; ExecutionEngine; GlobalISel; MC; MCA; MCDisassembler; MCParser; Object; ObjectYAML; OrcJIT; RuntimeDyld; Support; TargetParser; ). set(libs); if(LLVM_ENABLE_LIBPFM AND HAVE_LIBPFM); list(APPEND libs pfm); endif(); if(HAVE_LIBRT); list(APPEND libs rt); endif(). add_llvm_library(LLVMExegesis; DISABLE_LLVM_LINK_LLVM_DYLIB; STATIC; Analysis.cpp; Assembler.cpp; BenchmarkResult.cpp; BenchmarkRunner.cpp; Clustering.cpp; CodeTemplate.cpp; DisassemblerHelper.cpp; Error.cpp; LatencyBenchmarkRunner.cpp; LlvmState.cpp; MCInstrDescView.cpp; ParallelSnippetGenerator.cpp; PerfHelper.cpp; RegisterAliasing.cpp; RegisterValue.cpp; SchedClassResolution.cpp; SerialSnippetGenerator.cpp; SnippetFile.cpp; SnippetGenerator.cpp; SnippetRepetitor.cpp; SubprocessMemory.cpp; Target.cpp; UopsBenchmarkRunner.cpp. LINK_LIBS ${libs}. DEPENDS; intrinsics_gen; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt:259,Energy Efficiency,Power,PowerPC,259,"; set(LLVM_EXEGESIS_TARGETS); if (LLVM_TARGETS_TO_BUILD MATCHES ""X86""); list(APPEND LLVM_EXEGESIS_TARGETS ""X86""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""AArch64""); list(APPEND LLVM_EXEGESIS_TARGETS ""AArch64""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""PowerPC""); list(APPEND LLVM_EXEGESIS_TARGETS ""PowerPC""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""Mips""); list(APPEND LLVM_EXEGESIS_TARGETS ""Mips""); endif(). set(LLVM_EXEGESIS_TARGETS ${LLVM_EXEGESIS_TARGETS} PARENT_SCOPE). foreach(t ${LLVM_EXEGESIS_TARGETS}); add_subdirectory(${t}); endforeach(). set(LLVM_LINK_COMPONENTS; Analysis; CodeGen; CodeGenTypes; Core; ExecutionEngine; GlobalISel; MC; MCA; MCDisassembler; MCParser; Object; ObjectYAML; OrcJIT; RuntimeDyld; Support; TargetParser; ). set(libs); if(LLVM_ENABLE_LIBPFM AND HAVE_LIBPFM); list(APPEND libs pfm); endif(); if(HAVE_LIBRT); list(APPEND libs rt); endif(). add_llvm_library(LLVMExegesis; DISABLE_LLVM_LINK_LLVM_DYLIB; STATIC; Analysis.cpp; Assembler.cpp; BenchmarkResult.cpp; BenchmarkRunner.cpp; Clustering.cpp; CodeTemplate.cpp; DisassemblerHelper.cpp; Error.cpp; LatencyBenchmarkRunner.cpp; LlvmState.cpp; MCInstrDescView.cpp; ParallelSnippetGenerator.cpp; PerfHelper.cpp; RegisterAliasing.cpp; RegisterValue.cpp; SchedClassResolution.cpp; SerialSnippetGenerator.cpp; SnippetFile.cpp; SnippetGenerator.cpp; SnippetRepetitor.cpp; SubprocessMemory.cpp; Target.cpp; UopsBenchmarkRunner.cpp. LINK_LIBS ${libs}. DEPENDS; intrinsics_gen; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt:305,Energy Efficiency,Power,PowerPC,305,"; set(LLVM_EXEGESIS_TARGETS); if (LLVM_TARGETS_TO_BUILD MATCHES ""X86""); list(APPEND LLVM_EXEGESIS_TARGETS ""X86""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""AArch64""); list(APPEND LLVM_EXEGESIS_TARGETS ""AArch64""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""PowerPC""); list(APPEND LLVM_EXEGESIS_TARGETS ""PowerPC""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""Mips""); list(APPEND LLVM_EXEGESIS_TARGETS ""Mips""); endif(). set(LLVM_EXEGESIS_TARGETS ${LLVM_EXEGESIS_TARGETS} PARENT_SCOPE). foreach(t ${LLVM_EXEGESIS_TARGETS}); add_subdirectory(${t}); endforeach(). set(LLVM_LINK_COMPONENTS; Analysis; CodeGen; CodeGenTypes; Core; ExecutionEngine; GlobalISel; MC; MCA; MCDisassembler; MCParser; Object; ObjectYAML; OrcJIT; RuntimeDyld; Support; TargetParser; ). set(libs); if(LLVM_ENABLE_LIBPFM AND HAVE_LIBPFM); list(APPEND libs pfm); endif(); if(HAVE_LIBRT); list(APPEND libs rt); endif(). add_llvm_library(LLVMExegesis; DISABLE_LLVM_LINK_LLVM_DYLIB; STATIC; Analysis.cpp; Assembler.cpp; BenchmarkResult.cpp; BenchmarkRunner.cpp; Clustering.cpp; CodeTemplate.cpp; DisassemblerHelper.cpp; Error.cpp; LatencyBenchmarkRunner.cpp; LlvmState.cpp; MCInstrDescView.cpp; ParallelSnippetGenerator.cpp; PerfHelper.cpp; RegisterAliasing.cpp; RegisterValue.cpp; SchedClassResolution.cpp; SerialSnippetGenerator.cpp; SnippetFile.cpp; SnippetGenerator.cpp; SnippetRepetitor.cpp; SubprocessMemory.cpp; Target.cpp; UopsBenchmarkRunner.cpp. LINK_LIBS ${libs}. DEPENDS; intrinsics_gen; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt:1440,Integrability,DEPEND,DEPENDS,1440,"; set(LLVM_EXEGESIS_TARGETS); if (LLVM_TARGETS_TO_BUILD MATCHES ""X86""); list(APPEND LLVM_EXEGESIS_TARGETS ""X86""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""AArch64""); list(APPEND LLVM_EXEGESIS_TARGETS ""AArch64""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""PowerPC""); list(APPEND LLVM_EXEGESIS_TARGETS ""PowerPC""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""Mips""); list(APPEND LLVM_EXEGESIS_TARGETS ""Mips""); endif(). set(LLVM_EXEGESIS_TARGETS ${LLVM_EXEGESIS_TARGETS} PARENT_SCOPE). foreach(t ${LLVM_EXEGESIS_TARGETS}); add_subdirectory(${t}); endforeach(). set(LLVM_LINK_COMPONENTS; Analysis; CodeGen; CodeGenTypes; Core; ExecutionEngine; GlobalISel; MC; MCA; MCDisassembler; MCParser; Object; ObjectYAML; OrcJIT; RuntimeDyld; Support; TargetParser; ). set(libs); if(LLVM_ENABLE_LIBPFM AND HAVE_LIBPFM); list(APPEND libs pfm); endif(); if(HAVE_LIBRT); list(APPEND libs rt); endif(). add_llvm_library(LLVMExegesis; DISABLE_LLVM_LINK_LLVM_DYLIB; STATIC; Analysis.cpp; Assembler.cpp; BenchmarkResult.cpp; BenchmarkRunner.cpp; Clustering.cpp; CodeTemplate.cpp; DisassemblerHelper.cpp; Error.cpp; LatencyBenchmarkRunner.cpp; LlvmState.cpp; MCInstrDescView.cpp; ParallelSnippetGenerator.cpp; PerfHelper.cpp; RegisterAliasing.cpp; RegisterValue.cpp; SchedClassResolution.cpp; SerialSnippetGenerator.cpp; SnippetFile.cpp; SnippetGenerator.cpp; SnippetRepetitor.cpp; SubprocessMemory.cpp; Target.cpp; UopsBenchmarkRunner.cpp. LINK_LIBS ${libs}. DEPENDS; intrinsics_gen; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt:1096,Performance,Latency,LatencyBenchmarkRunner,1096,"; set(LLVM_EXEGESIS_TARGETS); if (LLVM_TARGETS_TO_BUILD MATCHES ""X86""); list(APPEND LLVM_EXEGESIS_TARGETS ""X86""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""AArch64""); list(APPEND LLVM_EXEGESIS_TARGETS ""AArch64""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""PowerPC""); list(APPEND LLVM_EXEGESIS_TARGETS ""PowerPC""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""Mips""); list(APPEND LLVM_EXEGESIS_TARGETS ""Mips""); endif(). set(LLVM_EXEGESIS_TARGETS ${LLVM_EXEGESIS_TARGETS} PARENT_SCOPE). foreach(t ${LLVM_EXEGESIS_TARGETS}); add_subdirectory(${t}); endforeach(). set(LLVM_LINK_COMPONENTS; Analysis; CodeGen; CodeGenTypes; Core; ExecutionEngine; GlobalISel; MC; MCA; MCDisassembler; MCParser; Object; ObjectYAML; OrcJIT; RuntimeDyld; Support; TargetParser; ). set(libs); if(LLVM_ENABLE_LIBPFM AND HAVE_LIBPFM); list(APPEND libs pfm); endif(); if(HAVE_LIBRT); list(APPEND libs rt); endif(). add_llvm_library(LLVMExegesis; DISABLE_LLVM_LINK_LLVM_DYLIB; STATIC; Analysis.cpp; Assembler.cpp; BenchmarkResult.cpp; BenchmarkRunner.cpp; Clustering.cpp; CodeTemplate.cpp; DisassemblerHelper.cpp; Error.cpp; LatencyBenchmarkRunner.cpp; LlvmState.cpp; MCInstrDescView.cpp; ParallelSnippetGenerator.cpp; PerfHelper.cpp; RegisterAliasing.cpp; RegisterValue.cpp; SchedClassResolution.cpp; SerialSnippetGenerator.cpp; SnippetFile.cpp; SnippetGenerator.cpp; SnippetRepetitor.cpp; SubprocessMemory.cpp; Target.cpp; UopsBenchmarkRunner.cpp. LINK_LIBS ${libs}. DEPENDS; intrinsics_gen; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt:985,Testability,Benchmark,BenchmarkResult,985,"; set(LLVM_EXEGESIS_TARGETS); if (LLVM_TARGETS_TO_BUILD MATCHES ""X86""); list(APPEND LLVM_EXEGESIS_TARGETS ""X86""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""AArch64""); list(APPEND LLVM_EXEGESIS_TARGETS ""AArch64""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""PowerPC""); list(APPEND LLVM_EXEGESIS_TARGETS ""PowerPC""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""Mips""); list(APPEND LLVM_EXEGESIS_TARGETS ""Mips""); endif(). set(LLVM_EXEGESIS_TARGETS ${LLVM_EXEGESIS_TARGETS} PARENT_SCOPE). foreach(t ${LLVM_EXEGESIS_TARGETS}); add_subdirectory(${t}); endforeach(). set(LLVM_LINK_COMPONENTS; Analysis; CodeGen; CodeGenTypes; Core; ExecutionEngine; GlobalISel; MC; MCA; MCDisassembler; MCParser; Object; ObjectYAML; OrcJIT; RuntimeDyld; Support; TargetParser; ). set(libs); if(LLVM_ENABLE_LIBPFM AND HAVE_LIBPFM); list(APPEND libs pfm); endif(); if(HAVE_LIBRT); list(APPEND libs rt); endif(). add_llvm_library(LLVMExegesis; DISABLE_LLVM_LINK_LLVM_DYLIB; STATIC; Analysis.cpp; Assembler.cpp; BenchmarkResult.cpp; BenchmarkRunner.cpp; Clustering.cpp; CodeTemplate.cpp; DisassemblerHelper.cpp; Error.cpp; LatencyBenchmarkRunner.cpp; LlvmState.cpp; MCInstrDescView.cpp; ParallelSnippetGenerator.cpp; PerfHelper.cpp; RegisterAliasing.cpp; RegisterValue.cpp; SchedClassResolution.cpp; SerialSnippetGenerator.cpp; SnippetFile.cpp; SnippetGenerator.cpp; SnippetRepetitor.cpp; SubprocessMemory.cpp; Target.cpp; UopsBenchmarkRunner.cpp. LINK_LIBS ${libs}. DEPENDS; intrinsics_gen; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt:1006,Testability,Benchmark,BenchmarkRunner,1006,"; set(LLVM_EXEGESIS_TARGETS); if (LLVM_TARGETS_TO_BUILD MATCHES ""X86""); list(APPEND LLVM_EXEGESIS_TARGETS ""X86""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""AArch64""); list(APPEND LLVM_EXEGESIS_TARGETS ""AArch64""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""PowerPC""); list(APPEND LLVM_EXEGESIS_TARGETS ""PowerPC""); endif(); if (LLVM_TARGETS_TO_BUILD MATCHES ""Mips""); list(APPEND LLVM_EXEGESIS_TARGETS ""Mips""); endif(). set(LLVM_EXEGESIS_TARGETS ${LLVM_EXEGESIS_TARGETS} PARENT_SCOPE). foreach(t ${LLVM_EXEGESIS_TARGETS}); add_subdirectory(${t}); endforeach(). set(LLVM_LINK_COMPONENTS; Analysis; CodeGen; CodeGenTypes; Core; ExecutionEngine; GlobalISel; MC; MCA; MCDisassembler; MCParser; Object; ObjectYAML; OrcJIT; RuntimeDyld; Support; TargetParser; ). set(libs); if(LLVM_ENABLE_LIBPFM AND HAVE_LIBPFM); list(APPEND libs pfm); endif(); if(HAVE_LIBRT); list(APPEND libs rt); endif(). add_llvm_library(LLVMExegesis; DISABLE_LLVM_LINK_LLVM_DYLIB; STATIC; Analysis.cpp; Assembler.cpp; BenchmarkResult.cpp; BenchmarkRunner.cpp; Clustering.cpp; CodeTemplate.cpp; DisassemblerHelper.cpp; Error.cpp; LatencyBenchmarkRunner.cpp; LlvmState.cpp; MCInstrDescView.cpp; ParallelSnippetGenerator.cpp; PerfHelper.cpp; RegisterAliasing.cpp; RegisterValue.cpp; SchedClassResolution.cpp; SerialSnippetGenerator.cpp; SnippetFile.cpp; SnippetGenerator.cpp; SnippetRepetitor.cpp; SubprocessMemory.cpp; Target.cpp; UopsBenchmarkRunner.cpp. LINK_LIBS ${libs}. DEPENDS; intrinsics_gen; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/AArch64/CMakeLists.txt:274,Integrability,DEPEND,DEPENDS,274,include_directories(; ${LLVM_MAIN_SRC_DIR}/lib/Target/AArch64; ${LLVM_BINARY_DIR}/lib/Target/AArch64; ). set(LLVM_LINK_COMPONENTS; AArch64; CodeGenTypes; Core; Exegesis; MC; Support; ). add_llvm_library(LLVMExegesisAArch64; DISABLE_LLVM_LINK_LLVM_DYLIB; STATIC; Target.cpp. DEPENDS; intrinsics_gen; AArch64CommonTableGen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/AArch64/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/AArch64/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/Mips/CMakeLists.txt:276,Integrability,DEPEND,DEPENDS,276,include_directories(; ${LLVM_MAIN_SRC_DIR}/lib/Target/Mips; ${LLVM_BINARY_DIR}/lib/Target/Mips; ). set(LLVM_LINK_COMPONENTS; CodeGenTypes; Core; Exegesis; MC; Mips; Support; TargetParser; ). add_llvm_library(LLVMExegesisMips; DISABLE_LLVM_LINK_LLVM_DYLIB; STATIC; Target.cpp. DEPENDS; intrinsics_gen; MipsCommonTableGen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/Mips/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/Mips/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/PowerPC/CMakeLists.txt:54,Energy Efficiency,Power,PowerPC,54,include_directories(; ${LLVM_MAIN_SRC_DIR}/lib/Target/PowerPC; ${LLVM_BINARY_DIR}/lib/Target/PowerPC; ). set(LLVM_LINK_COMPONENTS; CodeGenTypes; Core; Exegesis; MC; PowerPC; Support; TargetParser; ). add_llvm_library(LLVMExegesisPowerPC; DISABLE_LLVM_LINK_LLVM_DYLIB; STATIC; Target.cpp. DEPENDS; intrinsics_gen; PowerPCCommonTableGen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/PowerPC/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/PowerPC/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/PowerPC/CMakeLists.txt:93,Energy Efficiency,Power,PowerPC,93,include_directories(; ${LLVM_MAIN_SRC_DIR}/lib/Target/PowerPC; ${LLVM_BINARY_DIR}/lib/Target/PowerPC; ). set(LLVM_LINK_COMPONENTS; CodeGenTypes; Core; Exegesis; MC; PowerPC; Support; TargetParser; ). add_llvm_library(LLVMExegesisPowerPC; DISABLE_LLVM_LINK_LLVM_DYLIB; STATIC; Target.cpp. DEPENDS; intrinsics_gen; PowerPCCommonTableGen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/PowerPC/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/PowerPC/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/PowerPC/CMakeLists.txt:165,Energy Efficiency,Power,PowerPC,165,include_directories(; ${LLVM_MAIN_SRC_DIR}/lib/Target/PowerPC; ${LLVM_BINARY_DIR}/lib/Target/PowerPC; ). set(LLVM_LINK_COMPONENTS; CodeGenTypes; Core; Exegesis; MC; PowerPC; Support; TargetParser; ). add_llvm_library(LLVMExegesisPowerPC; DISABLE_LLVM_LINK_LLVM_DYLIB; STATIC; Target.cpp. DEPENDS; intrinsics_gen; PowerPCCommonTableGen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/PowerPC/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/PowerPC/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/PowerPC/CMakeLists.txt:313,Energy Efficiency,Power,PowerPCCommonTableGen,313,include_directories(; ${LLVM_MAIN_SRC_DIR}/lib/Target/PowerPC; ${LLVM_BINARY_DIR}/lib/Target/PowerPC; ). set(LLVM_LINK_COMPONENTS; CodeGenTypes; Core; Exegesis; MC; PowerPC; Support; TargetParser; ). add_llvm_library(LLVMExegesisPowerPC; DISABLE_LLVM_LINK_LLVM_DYLIB; STATIC; Target.cpp. DEPENDS; intrinsics_gen; PowerPCCommonTableGen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/PowerPC/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/PowerPC/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/PowerPC/CMakeLists.txt:288,Integrability,DEPEND,DEPENDS,288,include_directories(; ${LLVM_MAIN_SRC_DIR}/lib/Target/PowerPC; ${LLVM_BINARY_DIR}/lib/Target/PowerPC; ). set(LLVM_LINK_COMPONENTS; CodeGenTypes; Core; Exegesis; MC; PowerPC; Support; TargetParser; ). add_llvm_library(LLVMExegesisPowerPC; DISABLE_LLVM_LINK_LLVM_DYLIB; STATIC; Target.cpp. DEPENDS; intrinsics_gen; PowerPCCommonTableGen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/PowerPC/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/PowerPC/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/X86/CMakeLists.txt:297,Integrability,DEPEND,DEPENDS,297,include_directories(; ${LLVM_MAIN_SRC_DIR}/lib/Target/X86; ${LLVM_BINARY_DIR}/lib/Target/X86; ). set(LLVM_LINK_COMPONENTS; CodeGen; CodeGenTypes; Core; Exegesis; MC; Support; TargetParser; X86; ). add_llvm_library(LLVMExegesisX86; DISABLE_LLVM_LINK_LLVM_DYLIB; STATIC; Target.cpp; X86Counter.cpp. DEPENDS; intrinsics_gen; X86CommonTableGen; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/X86/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/X86/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-jitlink/llvm-jitlink-executor/CMakeLists.txt:134,Integrability,DEPEND,DEPENDS,134,set(LLVM_LINK_COMPONENTS; OrcShared; OrcTargetProcess; Support; ). add_llvm_utility(llvm-jitlink-executor; llvm-jitlink-executor.cpp. DEPENDS; intrinsics_gen; ). export_executable_symbols(llvm-jitlink-executor); ,MatchSource.DOCS,interpreter/llvm-project/llvm/tools/llvm-jitlink/llvm-jitlink-executor/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-jitlink/llvm-jitlink-executor/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/requirements.txt:783,Integrability,wrap,wrapt,783,#; # This file is autogenerated by pip-compile with Python 3.11; # by the following command:; #; # pip-compile --output-file=requirements.txt requirements.txt.in; #; certifi==2023.7.22; # via; # -r requirements.txt.in; # requests; cffi==1.15.1; # via; # cryptography; # pynacl; charset-normalizer==2.1.1; # via requests; cryptography==41.0.3; # via pyjwt; deprecated==1.2.13; # via pygithub; gitdb==4.0.9; # via gitpython; gitpython==3.1.32; # via -r requirements.txt.in; idna==3.4; # via requests; pycparser==2.21; # via cffi; pygithub==1.59.1; # via -r requirements.txt.in; pyjwt[crypto]==2.5.0; # via pygithub; pynacl==1.5.0; # via pygithub; requests==2.28.1; # via pygithub; smmap==5.0.0; # via gitdb; types-cryptography==3.3.23.2; # via pyjwt; urllib3==1.26.12; # via requests; wrapt==1.14.1; # via deprecated; ,MatchSource.DOCS,interpreter/llvm-project/llvm/utils/git/requirements.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/requirements.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/requirements_formatting.txt:1023,Integrability,wrap,wrapt,1023,#; # This file is autogenerated by pip-compile with Python 3.11; # by the following command:; #; # pip-compile --output-file=llvm/utils/git/requirements_formatting.txt llvm/utils/git/requirements_formatting.txt.in; #; black==23.9.1; # via; # -r llvm/utils/git/requirements_formatting.txt.in; # darker; certifi==2023.7.22; # via requests; cffi==1.15.1; # via; # cryptography; # pynacl; charset-normalizer==3.2.0; # via requests; click==8.1.7; # via black; cryptography==41.0.3; # via pyjwt; darker==1.7.2; # via -r llvm/utils/git/requirements_formatting.txt.in; deprecated==1.2.14; # via pygithub; idna==3.4; # via requests; mypy-extensions==1.0.0; # via black; packaging==23.1; # via black; pathspec==0.11.2; # via black; platformdirs==3.10.0; # via black; pycparser==2.21; # via cffi; pygithub==1.59.1; # via -r llvm/utils/git/requirements_formatting.txt.in; pyjwt[crypto]==2.8.0; # via pygithub; pynacl==1.5.0; # via pygithub; requests==2.31.0; # via pygithub; toml==0.10.2; # via darker; urllib3==2.0.4; # via requests; wrapt==1.15.0; # via deprecated; ,MatchSource.DOCS,interpreter/llvm-project/llvm/utils/git/requirements_formatting.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/git/requirements_formatting.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:465,Deployability,configurat,configuration,465,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:1178,Integrability,DEPEND,DEPENDS,1178,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:6,Modifiability,config,configured,6,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:465,Modifiability,config,configuration,465,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:585,Modifiability,config,configure,585,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:73,Testability,test,tests,73,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:131,Testability,test,tests,131,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:199,Testability,test,tests,199,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:278,Testability,test,test,278,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:451,Testability,test,tests,451,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:553,Testability,test,tests,553,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:660,Testability,test,tests,660,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:789,Testability,test,tests,789,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:869,Testability,test,tests,869,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:905,Testability,test,tests,905,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:1018,Testability,test,tests,1018,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:1049,Testability,test,tests,1049,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:1086,Testability,test,test,1086,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:1141,Testability,test,tests,1141,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:1291,Testability,Test,Tests,1291,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt:1359,Testability,Test,Tests,1359,"# The configured file is not placed in the correct location; # until the tests are run as we need to copy it into; # a copy of the tests folder; configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). # Lit's test suite creates output files next to the sources which makes the; # source tree dirty. This is undesirable because we do out of source builds.; # To work around this the tests and the configuration file are copied into the; # build directory just before running them. The tests are not copied over at; # configure time (i.e. `file(COPY ...)`) because this could lead to stale; # tests being run.; add_custom_target(prepare-check-lit; COMMAND ${CMAKE_COMMAND} -E remove_directory ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy_directory ""${CMAKE_CURRENT_SOURCE_DIR}/tests"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMAND ${CMAKE_COMMAND} -E copy ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg"" ""${CMAKE_CURRENT_BINARY_DIR}/tests""; COMMENT ""Preparing lit tests""; ). # Add rules for lit's own test suite; add_lit_testsuite(check-lit ""Running lit's tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""prepare-check-lit""; ). # For IDEs; set_target_properties(check-lit PROPERTIES FOLDER ""Tests""); set_target_properties(prepare-check-lit PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:1691,Availability,avail,available,1691,"t control, are controlled by, or are under common; control with that entity. For the purposes of this definition,; ""control"" means (i) the power, direct or indirect, to cause the; direction or management of such entity, whether by contract or; otherwise, or (ii) ownership of fifty percent (50%) or more of the; outstanding shares, or (iii) beneficial ownership of such entity. ""You"" (or ""Your"") shall mean an individual or Legal Entity; exercising permissions granted by this License. ""Source"" form shall mean the preferred form for making modifications,; including but not limited to software source code, documentation; source, and configuration files. ""Object"" form shall mean any form resulting from mechanical; transformation or translation of a Source form, including but; not limited to compiled object code, generated documentation,; and conversions to other media types. ""Work"" shall mean the work of authorship, whether in Source or; Object form, made available under the License, as indicated by a; copyright notice that is included in or attached to the work; (an example is provided in the Appendix below). ""Derivative Works"" shall mean any work, whether in Source or Object; form, that is based on (or derived from) the Work and for which the; editorial revisions, annotations, elaborations, or other modifications; represent, as a whole, an original work of authorship. For the purposes; of this License, Derivative Works shall not include works that remain; separable from, or merely link (or bind by name) to the interfaces of,; the Work and Derivative Works thereof. ""Contribution"" shall mean any work of authorship, including; the original version of the Work and any modifications or additions; to that Work or Derivative Works thereof, that is intentionally; submitted to Licensor for inclusion in the Work by the copyright owner; or by an individual or Legal Entity authorized to submit on behalf of; the copyright owner. For the purposes of this definition, ""submitted""; means",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:8653,Availability,failure,failure,8653,"cing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or; agreed to in writing, Licensor provides the Work (and each; Contributor provides its Contributions) on an ""AS IS"" BASIS,; WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or; implied, including, without limitation, any warranties or conditions; of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A; PARTICULAR PURPOSE. You are solely responsible for determining the; appropriateness of using or redistributing the Work and assume any; risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory,; whether in tort (including negligence), contract, or otherwise,; unless required by applicable law (such as deliberate and grossly; negligent acts) or agreed to in writing, shall any Contributor be; liable to You for damages, including any direct, indirect, special,; incidental, or consequential damages of any character arising as a; result of this License or out of the use or inability to use the; Work (including but not limited to damages for loss of goodwill,; work stoppage, computer failure or malfunction, or any and all; other commercial damages or losses), even if such Contributor; has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing; the Work or Derivative Works thereof, You may choose to offer,; and charge a fee for, acceptance of support, warranty, indemnity,; or other liability obligations and/or rights consistent with this; License. However, in accepting such obligations, You may act only; on Your own behalf and on Your sole responsibility, not on behalf; of any other Contributor, and only if You agree to indemnify,; defend, and hold each Contributor harmless for any liability; incurred by, or claims asserted against, such Contributor by reason; of your accepting any such warranty or additional liability.",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:1363,Deployability,configurat,configuration,1363,"licenses/. TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION. 1. Definitions. ""License"" shall mean the terms and conditions for use, reproduction,; and distribution as defined by Sections 1 through 9 of this document. ""Licensor"" shall mean the copyright owner or entity authorized by; the copyright owner that is granting the License. ""Legal Entity"" shall mean the union of the acting entity and all; other entities that control, are controlled by, or are under common; control with that entity. For the purposes of this definition,; ""control"" means (i) the power, direct or indirect, to cause the; direction or management of such entity, whether by contract or; otherwise, or (ii) ownership of fifty percent (50%) or more of the; outstanding shares, or (iii) beneficial ownership of such entity. ""You"" (or ""Your"") shall mean an individual or Legal Entity; exercising permissions granted by this License. ""Source"" form shall mean the preferred form for making modifications,; including but not limited to software source code, documentation; source, and configuration files. ""Object"" form shall mean any form resulting from mechanical; transformation or translation of a Source form, including but; not limited to compiled object code, generated documentation,; and conversions to other media types. ""Work"" shall mean the work of authorship, whether in Source or; Object form, made available under the License, as indicated by a; copyright notice that is included in or attached to the work; (an example is provided in the Appendix below). ""Derivative Works"" shall mean any work, whether in Source or Object; form, that is based on (or derived from) the Work and for which the; editorial revisions, annotations, elaborations, or other modifications; represent, as a whole, an original work of authorship. For the purposes; of this License, Derivative Works shall not include works that remain; separable from, or merely link (or bind by name) to the interfaces of,; the Work and Derivative",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:867,Energy Efficiency,power,power,867,"==============================================================================; The LLVM Project is under the Apache License v2.0 with LLVM Exceptions:; ==============================================================================. Apache License; Version 2.0, January 2004; http://www.apache.org/licenses/. TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION. 1. Definitions. ""License"" shall mean the terms and conditions for use, reproduction,; and distribution as defined by Sections 1 through 9 of this document. ""Licensor"" shall mean the copyright owner or entity authorized by; the copyright owner that is granting the License. ""Legal Entity"" shall mean the union of the acting entity and all; other entities that control, are controlled by, or are under common; control with that entity. For the purposes of this definition,; ""control"" means (i) the power, direct or indirect, to cause the; direction or management of such entity, whether by contract or; otherwise, or (ii) ownership of fifty percent (50%) or more of the; outstanding shares, or (iii) beneficial ownership of such entity. ""You"" (or ""Your"") shall mean an individual or Legal Entity; exercising permissions granted by this License. ""Source"" form shall mean the preferred form for making modifications,; including but not limited to software source code, documentation; source, and configuration files. ""Object"" form shall mean any form resulting from mechanical; transformation or translation of a Source form, including but; not limited to compiled object code, generated documentation,; and conversions to other media types. ""Work"" shall mean the work of authorship, whether in Source or; Object form, made available under the License, as indicated by a; copyright notice that is included in or attached to the work; (an example is provided in the Appendix below). ""Derivative Works"" shall mean any work, whether in Source or Object; form, that is based on (or derived from) the Work and for which the; editorial revi",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:3558,Energy Efficiency,charge,charge,3558," or Legal Entity authorized to submit on behalf of; the copyright owner. For the purposes of this definition, ""submitted""; means any form of electronic, verbal, or written communication sent; to the Licensor or its representatives, including but not limited to; communication on electronic mailing lists, source code control systems,; and issue tracking systems that are managed by, or on behalf of, the; Licensor for the purpose of discussing and improving the Work, but; excluding communication that is conspicuously marked or otherwise; designated in writing by the copyright owner as ""Not a Contribution."". ""Contributor"" shall mean Licensor and any individual or Legal Entity; on behalf of whom a Contribution has been received by Licensor and; subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of; this License, each Contributor hereby grants to You a perpetual,; worldwide, non-exclusive, no-charge, royalty-free, irrevocable; copyright license to reproduce, prepare Derivative Works of,; publicly display, publicly perform, sublicense, and distribute the; Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of; this License, each Contributor hereby grants to You a perpetual,; worldwide, non-exclusive, no-charge, royalty-free, irrevocable; (except as stated in this section) patent license to make, have made,; use, offer to sell, sell, import, and otherwise transfer the Work,; where such license applies only to those patent claims licensable; by such Contributor that are necessarily infringed by their; Contribution(s) alone or by combination of their Contribution(s); with the Work to which such Contribution(s) was submitted. If You; institute patent litigation against any entity (including a; cross-claim or counterclaim in a lawsuit) alleging that the Work; or a Contribution incorporated within the Work constitutes direct; or contributory patent infrin",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:3943,Energy Efficiency,charge,charge,3943,"nication that is conspicuously marked or otherwise; designated in writing by the copyright owner as ""Not a Contribution."". ""Contributor"" shall mean Licensor and any individual or Legal Entity; on behalf of whom a Contribution has been received by Licensor and; subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of; this License, each Contributor hereby grants to You a perpetual,; worldwide, non-exclusive, no-charge, royalty-free, irrevocable; copyright license to reproduce, prepare Derivative Works of,; publicly display, publicly perform, sublicense, and distribute the; Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of; this License, each Contributor hereby grants to You a perpetual,; worldwide, non-exclusive, no-charge, royalty-free, irrevocable; (except as stated in this section) patent license to make, have made,; use, offer to sell, sell, import, and otherwise transfer the Work,; where such license applies only to those patent claims licensable; by such Contributor that are necessarily infringed by their; Contribution(s) alone or by combination of their Contribution(s); with the Work to which such Contribution(s) was submitted. If You; institute patent litigation against any entity (including a; cross-claim or counterclaim in a lawsuit) alleging that the Work; or a Contribution incorporated within the Work constitutes direct; or contributory patent infringement, then any patent licenses; granted to You under this License for that Work shall terminate; as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the; Work or Derivative Works thereof in any medium, with or without; modifications, and in Source or Object form, provided that You; meet the following conditions:. (a) You must give any other recipients of the Work or; Derivative Works a copy of this License; and. (b) You must cause",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:8946,Energy Efficiency,charge,charge,8946," of using or redistributing the Work and assume any; risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory,; whether in tort (including negligence), contract, or otherwise,; unless required by applicable law (such as deliberate and grossly; negligent acts) or agreed to in writing, shall any Contributor be; liable to You for damages, including any direct, indirect, special,; incidental, or consequential damages of any character arising as a; result of this License or out of the use or inability to use the; Work (including but not limited to damages for loss of goodwill,; work stoppage, computer failure or malfunction, or any and all; other commercial damages or losses), even if such Contributor; has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing; the Work or Derivative Works thereof, You may choose to offer,; and charge a fee for, acceptance of support, warranty, indemnity,; or other liability obligations and/or rights consistent with this; License. However, in accepting such obligations, You may act only; on Your own behalf and on Your sole responsibility, not on behalf; of any other Contributor, and only if You agree to indemnify,; defend, and hold each Contributor harmless for any liability; incurred by, or claims asserted against, such Contributor by reason; of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS. APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following; boilerplate notice, with the fields enclosed by brackets ""[]""; replaced with your own identifying information. (Don't include; the brackets!) The text should be enclosed in the appropriate; comment syntax for the file format. We also recommend that a; file or class name and description of purpose be included on the; same ""printed page"" as the copyright n",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:12650,Energy Efficiency,charge,charge,12650,"ed clearly using at least one of two; mechanisms:; 1) It will be in a separate directory tree with its own `LICENSE.txt` or; `LICENSE` file at the top containing the specific license and restrictions; which apply to that software, or; 2) It will contain specific license and restriction terms at the top of every; file. ==============================================================================; Legacy LLVM License (https://llvm.org/docs/DeveloperPolicy.html#legacy):; ==============================================================================; University of Illinois/NCSA; Open Source License. Copyright (c) 2003-2019 University of Illinois at Urbana-Champaign.; All rights reserved. Developed by:. LLVM Team. University of Illinois at Urbana-Champaign. http://llvm.org. Permission is hereby granted, free of charge, to any person obtaining a copy of; this software and associated documentation files (the ""Software""), to deal with; the Software without restriction, including without limitation the rights to; use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies; of the Software, and to permit persons to whom the Software is furnished to do; so, subject to the following conditions:. * Redistributions of source code must retain the above copyright notice,; this list of conditions and the following disclaimers. * Redistributions in binary form must reproduce the above copyright notice,; this list of conditions and the following disclaimers in the; documentation and/or other materials provided with the distribution. * Neither the names of the LLVM Team, University of Illinois at; Urbana-Champaign, nor the names of its contributors may be used to; endorse or promote products derived from this Software without specific; prior written permission. THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR; IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS; FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:959,Integrability,contract,contract,959,"==============================================================================; The LLVM Project is under the Apache License v2.0 with LLVM Exceptions:; ==============================================================================. Apache License; Version 2.0, January 2004; http://www.apache.org/licenses/. TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION. 1. Definitions. ""License"" shall mean the terms and conditions for use, reproduction,; and distribution as defined by Sections 1 through 9 of this document. ""Licensor"" shall mean the copyright owner or entity authorized by; the copyright owner that is granting the License. ""Legal Entity"" shall mean the union of the acting entity and all; other entities that control, are controlled by, or are under common; control with that entity. For the purposes of this definition,; ""control"" means (i) the power, direct or indirect, to cause the; direction or management of such entity, whether by contract or; otherwise, or (ii) ownership of fifty percent (50%) or more of the; outstanding shares, or (iii) beneficial ownership of such entity. ""You"" (or ""Your"") shall mean an individual or Legal Entity; exercising permissions granted by this License. ""Source"" form shall mean the preferred form for making modifications,; including but not limited to software source code, documentation; source, and configuration files. ""Object"" form shall mean any form resulting from mechanical; transformation or translation of a Source form, including but; not limited to compiled object code, generated documentation,; and conversions to other media types. ""Work"" shall mean the work of authorship, whether in Source or; Object form, made available under the License, as indicated by a; copyright notice that is included in or attached to the work; (an example is provided in the Appendix below). ""Derivative Works"" shall mean any work, whether in Source or Object; form, that is based on (or derived from) the Work and for which the; editorial revi",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:2259,Integrability,interface,interfaces,2259," ""Source"" form shall mean the preferred form for making modifications,; including but not limited to software source code, documentation; source, and configuration files. ""Object"" form shall mean any form resulting from mechanical; transformation or translation of a Source form, including but; not limited to compiled object code, generated documentation,; and conversions to other media types. ""Work"" shall mean the work of authorship, whether in Source or; Object form, made available under the License, as indicated by a; copyright notice that is included in or attached to the work; (an example is provided in the Appendix below). ""Derivative Works"" shall mean any work, whether in Source or Object; form, that is based on (or derived from) the Work and for which the; editorial revisions, annotations, elaborations, or other modifications; represent, as a whole, an original work of authorship. For the purposes; of this License, Derivative Works shall not include works that remain; separable from, or merely link (or bind by name) to the interfaces of,; the Work and Derivative Works thereof. ""Contribution"" shall mean any work of authorship, including; the original version of the Work and any modifications or additions; to that Work or Derivative Works thereof, that is intentionally; submitted to Licensor for inclusion in the Work by the copyright owner; or by an individual or Legal Entity authorized to submit on behalf of; the copyright owner. For the purposes of this definition, ""submitted""; means any form of electronic, verbal, or written communication sent; to the Licensor or its representatives, including but not limited to; communication on electronic mailing lists, source code control systems,; and issue tracking systems that are managed by, or on behalf of, the; Licensor for the purpose of discussing and improving the Work, but; excluding communication that is conspicuously marked or otherwise; designated in writing by the copyright owner as ""Not a Contribution."". ""C",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:8201,Integrability,contract,contract,8201,"cing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or; agreed to in writing, Licensor provides the Work (and each; Contributor provides its Contributions) on an ""AS IS"" BASIS,; WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or; implied, including, without limitation, any warranties or conditions; of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A; PARTICULAR PURPOSE. You are solely responsible for determining the; appropriateness of using or redistributing the Work and assume any; risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory,; whether in tort (including negligence), contract, or otherwise,; unless required by applicable law (such as deliberate and grossly; negligent acts) or agreed to in writing, shall any Contributor be; liable to You for damages, including any direct, indirect, special,; incidental, or consequential damages of any character arising as a; result of this License or out of the use or inability to use the; Work (including but not limited to damages for loss of goodwill,; work stoppage, computer failure or malfunction, or any and all; other commercial damages or losses), even if such Contributor; has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing; the Work or Derivative Works thereof, You may choose to offer,; and charge a fee for, acceptance of support, warranty, indemnity,; or other liability obligations and/or rights consistent with this; License. However, in accepting such obligations, You may act only; on Your own behalf and on Your sole responsibility, not on behalf; of any other Contributor, and only if You agree to indemnify,; defend, and hold each Contributor harmless for any liability; incurred by, or claims asserted against, such Contributor by reason; of your accepting any such warranty or additional liability.",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:13961,Integrability,CONTRACT,CONTRACT,13961,"cense and restriction terms at the top of every; file. ==============================================================================; Legacy LLVM License (https://llvm.org/docs/DeveloperPolicy.html#legacy):; ==============================================================================; University of Illinois/NCSA; Open Source License. Copyright (c) 2003-2019 University of Illinois at Urbana-Champaign.; All rights reserved. Developed by:. LLVM Team. University of Illinois at Urbana-Champaign. http://llvm.org. Permission is hereby granted, free of charge, to any person obtaining a copy of; this software and associated documentation files (the ""Software""), to deal with; the Software without restriction, including without limitation the rights to; use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies; of the Software, and to permit persons to whom the Software is furnished to do; so, subject to the following conditions:. * Redistributions of source code must retain the above copyright notice,; this list of conditions and the following disclaimers. * Redistributions in binary form must reproduce the above copyright notice,; this list of conditions and the following disclaimers in the; documentation and/or other materials provided with the distribution. * Neither the names of the LLVM Team, University of Illinois at; Urbana-Champaign, nor the names of its contributors may be used to; endorse or promote products derived from this Software without specific; prior written permission. THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR; IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS; FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE; CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER; LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,; OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE; SOFTWARE. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:1363,Modifiability,config,configuration,1363,"licenses/. TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION. 1. Definitions. ""License"" shall mean the terms and conditions for use, reproduction,; and distribution as defined by Sections 1 through 9 of this document. ""Licensor"" shall mean the copyright owner or entity authorized by; the copyright owner that is granting the License. ""Legal Entity"" shall mean the union of the acting entity and all; other entities that control, are controlled by, or are under common; control with that entity. For the purposes of this definition,; ""control"" means (i) the power, direct or indirect, to cause the; direction or management of such entity, whether by contract or; otherwise, or (ii) ownership of fifty percent (50%) or more of the; outstanding shares, or (iii) beneficial ownership of such entity. ""You"" (or ""Your"") shall mean an individual or Legal Entity; exercising permissions granted by this License. ""Source"" form shall mean the preferred form for making modifications,; including but not limited to software source code, documentation; source, and configuration files. ""Object"" form shall mean any form resulting from mechanical; transformation or translation of a Source form, including but; not limited to compiled object code, generated documentation,; and conversions to other media types. ""Work"" shall mean the work of authorship, whether in Source or; Object form, made available under the License, as indicated by a; copyright notice that is included in or attached to the work; (an example is provided in the Appendix below). ""Derivative Works"" shall mean any work, whether in Source or Object; form, that is based on (or derived from) the Work and for which the; editorial revisions, annotations, elaborations, or other modifications; represent, as a whole, an original work of authorship. For the purposes; of this License, Derivative Works shall not include works that remain; separable from, or merely link (or bind by name) to the interfaces of,; the Work and Derivative",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:3682,Performance,perform,perform,3682," or Legal Entity authorized to submit on behalf of; the copyright owner. For the purposes of this definition, ""submitted""; means any form of electronic, verbal, or written communication sent; to the Licensor or its representatives, including but not limited to; communication on electronic mailing lists, source code control systems,; and issue tracking systems that are managed by, or on behalf of, the; Licensor for the purpose of discussing and improving the Work, but; excluding communication that is conspicuously marked or otherwise; designated in writing by the copyright owner as ""Not a Contribution."". ""Contributor"" shall mean Licensor and any individual or Legal Entity; on behalf of whom a Contribution has been received by Licensor and; subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of; this License, each Contributor hereby grants to You a perpetual,; worldwide, non-exclusive, no-charge, royalty-free, irrevocable; copyright license to reproduce, prepare Derivative Works of,; publicly display, publicly perform, sublicense, and distribute the; Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of; this License, each Contributor hereby grants to You a perpetual,; worldwide, non-exclusive, no-charge, royalty-free, irrevocable; (except as stated in this section) patent license to make, have made,; use, offer to sell, sell, import, and otherwise transfer the Work,; where such license applies only to those patent claims licensable; by such Contributor that are necessarily infringed by their; Contribution(s) alone or by combination of their Contribution(s); with the Work to which such Contribution(s) was submitted. If You; institute patent litigation against any entity (including a; cross-claim or counterclaim in a lawsuit) alleging that the Work; or a Contribution incorporated within the Work constitutes direct; or contributory patent infrin",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:8022,Safety,risk,risks,8022,"ithout any additional terms or conditions.; Notwithstanding the above, nothing herein shall supersede or modify; the terms of any separate license agreement you may have executed; with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade; names, trademarks, service marks, or product names of the Licensor,; except as required for reasonable and customary use in describing the; origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or; agreed to in writing, Licensor provides the Work (and each; Contributor provides its Contributions) on an ""AS IS"" BASIS,; WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or; implied, including, without limitation, any warranties or conditions; of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A; PARTICULAR PURPOSE. You are solely responsible for determining the; appropriateness of using or redistributing the Work and assume any; risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory,; whether in tort (including negligence), contract, or otherwise,; unless required by applicable law (such as deliberate and grossly; negligent acts) or agreed to in writing, shall any Contributor be; liable to You for damages, including any direct, indirect, special,; incidental, or consequential damages of any character arising as a; result of this License or out of the use or inability to use the; Work (including but not limited to damages for loss of goodwill,; work stoppage, computer failure or malfunction, or any and all; other commercial damages or losses), even if such Contributor; has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing; the Work or Derivative Works thereof, You may choose to offer,; and charge a fee for, acceptance of support, warranty, in",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:579,Security,authoriz,authorized,579,"==============================================================================; The LLVM Project is under the Apache License v2.0 with LLVM Exceptions:; ==============================================================================. Apache License; Version 2.0, January 2004; http://www.apache.org/licenses/. TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION. 1. Definitions. ""License"" shall mean the terms and conditions for use, reproduction,; and distribution as defined by Sections 1 through 9 of this document. ""Licensor"" shall mean the copyright owner or entity authorized by; the copyright owner that is granting the License. ""Legal Entity"" shall mean the union of the acting entity and all; other entities that control, are controlled by, or are under common; control with that entity. For the purposes of this definition,; ""control"" means (i) the power, direct or indirect, to cause the; direction or management of such entity, whether by contract or; otherwise, or (ii) ownership of fifty percent (50%) or more of the; outstanding shares, or (iii) beneficial ownership of such entity. ""You"" (or ""Your"") shall mean an individual or Legal Entity; exercising permissions granted by this License. ""Source"" form shall mean the preferred form for making modifications,; including but not limited to software source code, documentation; source, and configuration files. ""Object"" form shall mean any form resulting from mechanical; transformation or translation of a Source form, including but; not limited to compiled object code, generated documentation,; and conversions to other media types. ""Work"" shall mean the work of authorship, whether in Source or; Object form, made available under the License, as indicated by a; copyright notice that is included in or attached to the work; (an example is provided in the Appendix below). ""Derivative Works"" shall mean any work, whether in Source or Object; form, that is based on (or derived from) the Work and for which the; editorial revi",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:2617,Security,authoriz,authorized,2617," including but; not limited to compiled object code, generated documentation,; and conversions to other media types. ""Work"" shall mean the work of authorship, whether in Source or; Object form, made available under the License, as indicated by a; copyright notice that is included in or attached to the work; (an example is provided in the Appendix below). ""Derivative Works"" shall mean any work, whether in Source or Object; form, that is based on (or derived from) the Work and for which the; editorial revisions, annotations, elaborations, or other modifications; represent, as a whole, an original work of authorship. For the purposes; of this License, Derivative Works shall not include works that remain; separable from, or merely link (or bind by name) to the interfaces of,; the Work and Derivative Works thereof. ""Contribution"" shall mean any work of authorship, including; the original version of the Work and any modifications or additions; to that Work or Derivative Works thereof, that is intentionally; submitted to Licensor for inclusion in the Work by the copyright owner; or by an individual or Legal Entity authorized to submit on behalf of; the copyright owner. For the purposes of this definition, ""submitted""; means any form of electronic, verbal, or written communication sent; to the Licensor or its representatives, including but not limited to; communication on electronic mailing lists, source code control systems,; and issue tracking systems that are managed by, or on behalf of, the; Licensor for the purpose of discussing and improving the Work, but; excluding communication that is conspicuously marked or otherwise; designated in writing by the copyright owner as ""Not a Contribution."". ""Contributor"" shall mean Licensor and any individual or Legal Entity; on behalf of whom a Contribution has been received by Licensor and; subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of; this License, each Contributor",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:9358,Testability,assert,asserted,9358,"erate and grossly; negligent acts) or agreed to in writing, shall any Contributor be; liable to You for damages, including any direct, indirect, special,; incidental, or consequential damages of any character arising as a; result of this License or out of the use or inability to use the; Work (including but not limited to damages for loss of goodwill,; work stoppage, computer failure or malfunction, or any and all; other commercial damages or losses), even if such Contributor; has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing; the Work or Derivative Works thereof, You may choose to offer,; and charge a fee for, acceptance of support, warranty, indemnity,; or other liability obligations and/or rights consistent with this; License. However, in accepting such obligations, You may act only; on Your own behalf and on Your sole responsibility, not on behalf; of any other Contributor, and only if You agree to indemnify,; defend, and hold each Contributor harmless for any liability; incurred by, or claims asserted against, such Contributor by reason; of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS. APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following; boilerplate notice, with the fields enclosed by brackets ""[]""; replaced with your own identifying information. (Don't include; the brackets!) The text should be enclosed in the appropriate; comment syntax for the file format. We also recommend that a; file or class name and description of purpose be included on the; same ""printed page"" as the copyright notice for easier; identification within third-party archives. Copyright [yyyy] [name of copyright owner]. Licensed under the Apache License, Version 2.0 (the ""License"");; you may not use this file except in compliance with the License.; You may obtain a copy of the License at. http://www.apache.org/lice",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT:11834,Usability,clear,clearly,11834,"hout complying; with the conditions of Sections 4(a), 4(b) and 4(d) of the License. In addition, if you combine or link compiled forms of this Software with; software that is licensed under the GPLv2 (""Combined Software"") and if a; court of competent jurisdiction determines that the patent provision (Section; 3), the indemnity provision (Section 9) or other Section of the License; conflicts with the conditions of the GPLv2, you may retroactively and; prospectively choose to deem waived or otherwise exclude such Section(s) of; the License, but only in their entirety and only with respect to the Combined; Software. ==============================================================================; Software from third parties included in the LLVM Project:; ==============================================================================; The LLVM Project contains third party software which is under different license; terms. All such code will be identified clearly using at least one of two; mechanisms:; 1) It will be in a separate directory tree with its own `LICENSE.txt` or; `LICENSE` file at the top containing the specific license and restrictions; which apply to that software, or; 2) It will contain specific license and restriction terms at the top of every; file. ==============================================================================; Legacy LLVM License (https://llvm.org/docs/DeveloperPolicy.html#legacy):; ==============================================================================; University of Illinois/NCSA; Open Source License. Copyright (c) 2003-2019 University of Illinois at Urbana-Champaign.; All rights reserved. Developed by:. LLVM Team. University of Illinois at Urbana-Champaign. http://llvm.org. Permission is hereby granted, free of charge, to any person obtaining a copy of; this software and associated documentation files (the ""Software""), to deal with; the Software without restriction, including without limitation the rights to; use, copy, modify, mer",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/LICENSE.TXT
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/llvm-lit/CMakeLists.txt:300,Modifiability,config,config,300,"get_property(LLVM_LIT_CONFIG_FILES GLOBAL PROPERTY LLVM_LIT_CONFIG_FILES); list(LENGTH LLVM_LIT_CONFIG_FILES file_len); math(EXPR file_last ""${file_len} - 1""). get_llvm_lit_path(LIT_BASE_DIR LIT_FILE_NAME). set(LLVM_SOURCE_DIR ${LLVM_MAIN_SRC_DIR}). # LLVM_LIT_CONFIG_FILES contains interleaved main config (in the source tree); # and site config (in the build tree) pairs. Make them relative to; # llvm-lit and then convert them to map_config() calls.; if(""${CMAKE_CFG_INTDIR}"" STREQUAL "".""); make_paths_relative(; LLVM_LIT_CONFIG_FILES ""${LIT_BASE_DIR}"" ""${LLVM_LIT_CONFIG_FILES}""); make_paths_relative(; LLVM_SOURCE_DIR ""${LIT_BASE_DIR}"" ""${LLVM_SOURCE_DIR}""); endif(). set(LLVM_LIT_CONFIG_MAP ""${LLVM_LIT_PATH_FUNCTION}\n""); if (${file_last} GREATER -1); foreach(i RANGE 0 ${file_last} 2); list(GET LLVM_LIT_CONFIG_FILES ${i} main_config); math(EXPR i1 ""${i} + 1""); list(GET LLVM_LIT_CONFIG_FILES ${i1} site_out); set(map ""map_config(path(r'${main_config}'), path(r'${site_out}'))""); set(LLVM_LIT_CONFIG_MAP ""${LLVM_LIT_CONFIG_MAP}\n${map}""); endforeach(); endif(). if(NOT ""${CMAKE_CFG_INTDIR}"" STREQUAL "".""); foreach(BUILD_MODE ${CMAKE_CONFIGURATION_TYPES}); string(REPLACE ${CMAKE_CFG_INTDIR} ${BUILD_MODE} bi ${LIT_BASE_DIR}); set(bi ""${bi}/${LIT_FILE_NAME}""); configure_file(; llvm-lit.in; ${bi}; ); endforeach(); else(); set(BUILD_MODE .); configure_file(; llvm-lit.in; ${LIT_BASE_DIR}/${LIT_FILE_NAME}; ); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/llvm-lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/llvm-lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/llvm-lit/CMakeLists.txt:340,Modifiability,config,config,340,"get_property(LLVM_LIT_CONFIG_FILES GLOBAL PROPERTY LLVM_LIT_CONFIG_FILES); list(LENGTH LLVM_LIT_CONFIG_FILES file_len); math(EXPR file_last ""${file_len} - 1""). get_llvm_lit_path(LIT_BASE_DIR LIT_FILE_NAME). set(LLVM_SOURCE_DIR ${LLVM_MAIN_SRC_DIR}). # LLVM_LIT_CONFIG_FILES contains interleaved main config (in the source tree); # and site config (in the build tree) pairs. Make them relative to; # llvm-lit and then convert them to map_config() calls.; if(""${CMAKE_CFG_INTDIR}"" STREQUAL "".""); make_paths_relative(; LLVM_LIT_CONFIG_FILES ""${LIT_BASE_DIR}"" ""${LLVM_LIT_CONFIG_FILES}""); make_paths_relative(; LLVM_SOURCE_DIR ""${LIT_BASE_DIR}"" ""${LLVM_SOURCE_DIR}""); endif(). set(LLVM_LIT_CONFIG_MAP ""${LLVM_LIT_PATH_FUNCTION}\n""); if (${file_last} GREATER -1); foreach(i RANGE 0 ${file_last} 2); list(GET LLVM_LIT_CONFIG_FILES ${i} main_config); math(EXPR i1 ""${i} + 1""); list(GET LLVM_LIT_CONFIG_FILES ${i1} site_out); set(map ""map_config(path(r'${main_config}'), path(r'${site_out}'))""); set(LLVM_LIT_CONFIG_MAP ""${LLVM_LIT_CONFIG_MAP}\n${map}""); endforeach(); endif(). if(NOT ""${CMAKE_CFG_INTDIR}"" STREQUAL "".""); foreach(BUILD_MODE ${CMAKE_CONFIGURATION_TYPES}); string(REPLACE ${CMAKE_CFG_INTDIR} ${BUILD_MODE} bi ${LIT_BASE_DIR}); set(bi ""${bi}/${LIT_FILE_NAME}""); configure_file(; llvm-lit.in; ${bi}; ); endforeach(); else(); set(BUILD_MODE .); configure_file(; llvm-lit.in; ${LIT_BASE_DIR}/${LIT_FILE_NAME}; ); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/llvm-lit/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/llvm-lit/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/llvm-locstats/CMakeLists.txt:116,Integrability,DEPEND,DEPENDS,116,"if (LLVM_INCLUDE_UTILS AND LLVM_INCLUDE_TOOLS); add_custom_command(; OUTPUT ${LLVM_TOOLS_BINARY_DIR}/llvm-locstats; DEPENDS ${LLVM_MAIN_SRC_DIR}/utils/llvm-locstats/llvm-locstats.py; DEPENDS llvm-dwarfdump; COMMAND ${CMAKE_COMMAND} -E copy ${LLVM_MAIN_SRC_DIR}/utils/llvm-locstats/llvm-locstats.py ${LLVM_TOOLS_BINARY_DIR}/llvm-locstats; COMMENT ""Copying llvm-locstats into ${LLVM_TOOLS_BINARY_DIR}""; ); add_custom_target(llvm-locstats ALL; DEPENDS ${LLVM_TOOLS_BINARY_DIR}/llvm-locstats; ); if (NOT LLVM_BUILD_TOOLS); set_target_properties(llvm-locstats PROPERTIES EXCLUDE_FROM_ALL ON); endif(); set_target_properties(llvm-locstats PROPERTIES FOLDER ""Tools""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/llvm-locstats/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/llvm-locstats/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/llvm-locstats/CMakeLists.txt:183,Integrability,DEPEND,DEPENDS,183,"if (LLVM_INCLUDE_UTILS AND LLVM_INCLUDE_TOOLS); add_custom_command(; OUTPUT ${LLVM_TOOLS_BINARY_DIR}/llvm-locstats; DEPENDS ${LLVM_MAIN_SRC_DIR}/utils/llvm-locstats/llvm-locstats.py; DEPENDS llvm-dwarfdump; COMMAND ${CMAKE_COMMAND} -E copy ${LLVM_MAIN_SRC_DIR}/utils/llvm-locstats/llvm-locstats.py ${LLVM_TOOLS_BINARY_DIR}/llvm-locstats; COMMENT ""Copying llvm-locstats into ${LLVM_TOOLS_BINARY_DIR}""; ); add_custom_target(llvm-locstats ALL; DEPENDS ${LLVM_TOOLS_BINARY_DIR}/llvm-locstats; ); if (NOT LLVM_BUILD_TOOLS); set_target_properties(llvm-locstats PROPERTIES EXCLUDE_FROM_ALL ON); endif(); set_target_properties(llvm-locstats PROPERTIES FOLDER ""Tools""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/llvm-locstats/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/llvm-locstats/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/llvm-locstats/CMakeLists.txt:441,Integrability,DEPEND,DEPENDS,441,"if (LLVM_INCLUDE_UTILS AND LLVM_INCLUDE_TOOLS); add_custom_command(; OUTPUT ${LLVM_TOOLS_BINARY_DIR}/llvm-locstats; DEPENDS ${LLVM_MAIN_SRC_DIR}/utils/llvm-locstats/llvm-locstats.py; DEPENDS llvm-dwarfdump; COMMAND ${CMAKE_COMMAND} -E copy ${LLVM_MAIN_SRC_DIR}/utils/llvm-locstats/llvm-locstats.py ${LLVM_TOOLS_BINARY_DIR}/llvm-locstats; COMMENT ""Copying llvm-locstats into ${LLVM_TOOLS_BINARY_DIR}""; ); add_custom_target(llvm-locstats ALL; DEPENDS ${LLVM_TOOLS_BINARY_DIR}/llvm-locstats; ); if (NOT LLVM_BUILD_TOOLS); set_target_properties(llvm-locstats PROPERTIES EXCLUDE_FROM_ALL ON); endif(); set_target_properties(llvm-locstats PROPERTIES FOLDER ""Tools""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/llvm-locstats/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/llvm-locstats/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/mlgo-utils/CMakeLists.txt:217,Integrability,DEPEND,DEPENDS,217,"configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). add_lit_testsuite(check-mlgo-utils ""Running mlgo-utils tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""count"" ""split-file"" ""yaml2obj"" ""llvm-objcopy""; ). set_target_properties(check-mlgo-utils PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/mlgo-utils/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/mlgo-utils/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/mlgo-utils/CMakeLists.txt:54,Testability,test,tests,54,"configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). add_lit_testsuite(check-mlgo-utils ""Running mlgo-utils tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""count"" ""split-file"" ""yaml2obj"" ""llvm-objcopy""; ). set_target_properties(check-mlgo-utils PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/mlgo-utils/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/mlgo-utils/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/mlgo-utils/CMakeLists.txt:180,Testability,test,tests,180,"configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). add_lit_testsuite(check-mlgo-utils ""Running mlgo-utils tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""count"" ""split-file"" ""yaml2obj"" ""llvm-objcopy""; ). set_target_properties(check-mlgo-utils PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/mlgo-utils/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/mlgo-utils/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/mlgo-utils/CMakeLists.txt:352,Testability,Test,Tests,352,"configure_lit_site_cfg(; ""${CMAKE_CURRENT_SOURCE_DIR}/tests/lit.site.cfg.in""; ""${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg""; ). add_lit_testsuite(check-mlgo-utils ""Running mlgo-utils tests""; ${CMAKE_CURRENT_BINARY_DIR}; DEPENDS ""FileCheck"" ""not"" ""count"" ""split-file"" ""yaml2obj"" ""llvm-objcopy""; ). set_target_properties(check-mlgo-utils PROPERTIES FOLDER ""Tests""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/mlgo-utils/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/mlgo-utils/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/CMakeLists.txt:2078,Integrability,DEPEND,DEPENDS,2078,"DED. LINK_COMPONENTS; Support; TableGen; ); set_target_properties(LLVMTableGenCommon PROPERTIES FOLDER ""Tablegenning""). set(LLVM_LINK_COMPONENTS Support). add_tablegen(llvm-min-tblgen LLVM_HEADERS; TableGen.cpp; $<TARGET_OBJECTS:obj.LLVMTableGenCommon>; PARTIAL_SOURCES_INTENDED; ); set_target_properties(llvm-min-tblgen PROPERTIES FOLDER ""Tablegenning""). set(LLVM_LINK_COMPONENTS; CodeGenTypes; Support; ). add_tablegen(llvm-tblgen LLVM; DESTINATION ""${LLVM_TOOLS_INSTALL_DIR}""; EXPORT LLVM; AsmMatcherEmitter.cpp; AsmWriterEmitter.cpp; AsmWriterInst.cpp; CTagsEmitter.cpp; CallingConvEmitter.cpp; CodeEmitterGen.cpp; CodeGenDAGPatterns.cpp; CodeGenHwModes.cpp; CodeGenInstAlias.cpp; CodeGenInstruction.cpp; CodeGenMapTable.cpp; CodeGenRegisters.cpp; CodeGenSchedule.cpp; CodeGenTarget.cpp; DAGISelEmitter.cpp; DAGISelMatcherEmitter.cpp; DAGISelMatcherGen.cpp; DAGISelMatcherOpt.cpp; DAGISelMatcher.cpp; DecoderEmitter.cpp; DFAEmitter.cpp; DFAPacketizerEmitter.cpp; DisassemblerEmitter.cpp; DXILEmitter.cpp; ExegesisEmitter.cpp; FastISelEmitter.cpp; GlobalISelCombinerEmitter.cpp; GlobalISelEmitter.cpp; GlobalISelMatchTable.cpp; GlobalISelMatchTableExecutorEmitter.cpp; InfoByHwMode.cpp; InstrInfoEmitter.cpp; InstrDocsEmitter.cpp; OptEmitter.cpp; OptParserEmitter.cpp; OptRSTEmitter.cpp; PredicateExpander.cpp; PseudoLoweringEmitter.cpp; CompressInstEmitter.cpp; MacroFusionPredicatorEmitter.cpp; RegisterBankEmitter.cpp; RegisterInfoEmitter.cpp; SearchableTableEmitter.cpp; SubtargetEmitter.cpp; SubtargetFeatureInfo.cpp; TableGen.cpp; Types.cpp; VarLenCodeEmitterGen.cpp; X86DisassemblerTables.cpp; X86CompressEVEXTablesEmitter.cpp; X86FoldTablesEmitter.cpp; X86MnemonicTables.cpp; X86ModRMFilters.cpp; X86RecognizableInstr.cpp; WebAssemblyDisassemblerEmitter.cpp; $<TARGET_OBJECTS:obj.LLVMTableGenCommon>. DEPENDS; intrinsics_gen # via llvm-min-tablegen; ); target_link_libraries(llvm-tblgen PRIVATE LLVMTableGenGlobalISel); set_target_properties(llvm-tblgen PROPERTIES FOLDER ""Tablegenning""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/TableGen/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/README.txt:99,Deployability,configurat,configurations,99,"==============; lit Examples; ==============. This directory contains examples of 'lit' test suite configurations. The test; suites they define can be run with 'lit examples/example-name', for more details; see the README in each example.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/examples/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/README.txt:99,Modifiability,config,configurations,99,"==============; lit Examples; ==============. This directory contains examples of 'lit' test suite configurations. The test; suites they define can be run with 'lit examples/example-name', for more details; see the README in each example.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/examples/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/README.txt:88,Testability,test,test,88,"==============; lit Examples; ==============. This directory contains examples of 'lit' test suite configurations. The test; suites they define can be run with 'lit examples/example-name', for more details; see the README in each example.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/examples/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/README.txt:119,Testability,test,test,119,"==============; lit Examples; ==============. This directory contains examples of 'lit' test suite configurations. The test; suites they define can be run with 'lit examples/example-name', for more details; see the README in each example.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/examples/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt:125,Deployability,configurat,configuration,125,========================; Many Tests lit Example; ========================. This directory contains a trivial lit test suite configuration that defines a; custom test format which just generates a large (N=10000) number of tests that; do a small amount of work in the Python test execution code. This test suite is useful for testing the performance of lit on large numbers of; tests.; ,MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt:125,Modifiability,config,configuration,125,========================; Many Tests lit Example; ========================. This directory contains a trivial lit test suite configuration that defines a; custom test format which just generates a large (N=10000) number of tests that; do a small amount of work in the Python test execution code. This test suite is useful for testing the performance of lit on large numbers of; tests.; ,MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt:338,Performance,perform,performance,338,========================; Many Tests lit Example; ========================. This directory contains a trivial lit test suite configuration that defines a; custom test format which just generates a large (N=10000) number of tests that; do a small amount of work in the Python test execution code. This test suite is useful for testing the performance of lit on large numbers of; tests.; ,MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt:31,Testability,Test,Tests,31,========================; Many Tests lit Example; ========================. This directory contains a trivial lit test suite configuration that defines a; custom test format which just generates a large (N=10000) number of tests that; do a small amount of work in the Python test execution code. This test suite is useful for testing the performance of lit on large numbers of; tests.; ,MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt:114,Testability,test,test,114,========================; Many Tests lit Example; ========================. This directory contains a trivial lit test suite configuration that defines a; custom test format which just generates a large (N=10000) number of tests that; do a small amount of work in the Python test execution code. This test suite is useful for testing the performance of lit on large numbers of; tests.; ,MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt:162,Testability,test,test,162,========================; Many Tests lit Example; ========================. This directory contains a trivial lit test suite configuration that defines a; custom test format which just generates a large (N=10000) number of tests that; do a small amount of work in the Python test execution code. This test suite is useful for testing the performance of lit on large numbers of; tests.; ,MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt:223,Testability,test,tests,223,========================; Many Tests lit Example; ========================. This directory contains a trivial lit test suite configuration that defines a; custom test format which just generates a large (N=10000) number of tests that; do a small amount of work in the Python test execution code. This test suite is useful for testing the performance of lit on large numbers of; tests.; ,MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt:275,Testability,test,test,275,========================; Many Tests lit Example; ========================. This directory contains a trivial lit test suite configuration that defines a; custom test format which just generates a large (N=10000) number of tests that; do a small amount of work in the Python test execution code. This test suite is useful for testing the performance of lit on large numbers of; tests.; ,MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt:301,Testability,test,test,301,========================; Many Tests lit Example; ========================. This directory contains a trivial lit test suite configuration that defines a; custom test format which just generates a large (N=10000) number of tests that; do a small amount of work in the Python test execution code. This test suite is useful for testing the performance of lit on large numbers of; tests.; ,MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt:326,Testability,test,testing,326,========================; Many Tests lit Example; ========================. This directory contains a trivial lit test suite configuration that defines a; custom test format which just generates a large (N=10000) number of tests that; do a small amount of work in the Python test execution code. This test suite is useful for testing the performance of lit on large numbers of; tests.; ,MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt:378,Testability,test,tests,378,========================; Many Tests lit Example; ========================. This directory contains a trivial lit test suite configuration that defines a; custom test format which just generates a large (N=10000) number of tests that; do a small amount of work in the Python test execution code. This test suite is useful for testing the performance of lit on large numbers of; tests.; ,MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/examples/many-tests/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/GlobalISel/CMakeLists.txt:195,Integrability,DEPEND,DEPENDS,195,"set(LLVM_LINK_COMPONENTS; Support; TableGen; ). add_llvm_library(LLVMTableGenGlobalISel STATIC DISABLE_LLVM_LINK_LLVM_DYLIB; CodeExpander.cpp; CXXPredicates.cpp; MatchDataInfo.cpp; Patterns.cpp. DEPENDS; vt_gen; ). # Users may include its headers as ""GlobalISel/*.h""; target_include_directories(LLVMTableGenGlobalISel; INTERFACE; $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/..>; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/TableGen/GlobalISel/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/GlobalISel/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/GlobalISel/CMakeLists.txt:319,Integrability,INTERFACE,INTERFACE,319,"set(LLVM_LINK_COMPONENTS; Support; TableGen; ). add_llvm_library(LLVMTableGenGlobalISel STATIC DISABLE_LLVM_LINK_LLVM_DYLIB; CodeExpander.cpp; CXXPredicates.cpp; MatchDataInfo.cpp; Patterns.cpp. DEPENDS; vt_gen; ). # Users may include its headers as ""GlobalISel/*.h""; target_include_directories(LLVMTableGenGlobalISel; INTERFACE; $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/..>; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/TableGen/GlobalISel/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/GlobalISel/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/io/dcache/CMakeLists.txt:579,Integrability,DEPEND,DEPENDENCIES,579,"# Copyright (C) 1995-2019, Rene Brun and Fons Rademakers.; # All rights reserved.; #; # For the licensing terms see $ROOTSYS/LICENSE.; # For the list of contributors see $ROOTSYS/README/CREDITS. ############################################################################; # CMakeLists.txt file for building ROOT io/dcache package; ############################################################################. include_directories(SYSTEM ${DCAP_INCLUDE_DIRS}). ROOT_STANDARD_LIBRARY_PACKAGE(DCache; HEADERS TDCacheFile.h; SOURCES src/TDCacheFile.cxx; LIBRARIES ${DCAP_LIBRARIES}; DEPENDENCIES Core Net RIO); ",MatchSource.DOCS,io/dcache/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/dcache/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt:579,Integrability,interface,interface,579,"Manual Data Model Evolution Capabilities - the user documentation. 1. Overview. The automatic data model schema evolution implemented in ROOT makes it possible; to read back the serialized data object in the situation when the definition of; the classes those objects represent changed slightly (some of the data members were; removed or some new ones added). It is also possible to manually specify the rules; for more sophisticated data transformations done while reading to load the serialized; objects into data structures that changed quite significantly. ROOT provides two interface enabling users to specify the conversion rules. The; first way is to define a rule in the dictionary file and the second way is to insert; it to the TClass object using the C++ API. There are two types of conversion rules. The first of them, the normal rules, are; the ones that should be used in the most of the cases. They provide a buffered input; data and an address of the in-memory target object and allow user to specify the; conversion function mapping the data being read to the output format. The second type; of the rules, the raw rules, also provide the pointer to the target object but the; input is a raw TBuffer object containing the input data member declared as an input; to the rule. This type of a rule is provided mainly to handle the file format changes; that couldn't have been handled otherwise and in general should not be used unless there; is no other option. 2. The dictionaries. The most convenient place to specify the conversion rules is a dictionary. One can; do that either in CINT's LinkDef file or in the selection xml file being fed to genreflex.; The syntax of the rules is the following:. * For CINT dictionaries:. #pragma read \; sourceClass=""ClassA"" \; source=""double m_a; double m_b; double m_c"" \; version=""[4-5,7,9,12-]"" \; checksum=""[12345,123456]"" \; targetClass=""ClassB"" \; target=""m_x"" \; embed=""true"" \; include=""iostream,cstdlib"" \; code=""{m_x = onfile.m_a * onfil",MatchSource.DOCS,io/doc/DataModelEvolution.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt
https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt:4904,Integrability,wrap,wrapping,4904,"al to a; * checksum - A list of checksums of the source class that can be an input for this; rule. The list has to be enclosed in a square brackets and is a; comma-separated list of integers.; * targetClass - The field is obligatory and defines the name of the in-memory class that; this rule can be applied to.; * target - A semicolon-separated list of target class data member names that this rule; is capable of calculating.; * embed - This property tells the system if the rule should be written in the output; file is some objects of this class are serialized.; * include - A list of header files that should be included in order to provide the func-; tionality used in the code snippet; the list is comma delimited.; * code - An user specified code snippet. The user can assume that in the provided code snippet the following variables; will be defined:. The user provided code snippets have to consist of valid C++ code. The system can do; some preprocessing before wrapping the code into function calls and declare some variables to; facilitate the rule definitions. The user can expect the following variables being predeclared:. * newObj - variable representing the target in-memory object, it’s type is that of the; target object; * oldObj - in normal conversion rules, an object of TVirtualObject class representing the; input data, guaranteed to hold the data members declared in the source property; of the rule; * buffer - in raw conversion rules, an object of TBuﬀer class holding the data member; declared in source property of the rule; * names of the data members of the target object declared in the target property of the; rule declared to be the appropriate type; * onfile.xxx - in normal conversion rules, names of the variables of basic types declared; in the source property of the rule. 3. The C++ API. The schema evolution C++ API consists of two classes: ROOT::TSchemaRuleSet and ROOT::TSchemaRule.; Objects of the TSchemaRule class represent the rules and their fields hav",MatchSource.DOCS,io/doc/DataModelEvolution.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt
https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt:2819,Modifiability,variab,variables,2819,"7,9,12-]"" \; checksum=""[12345,123456]"" \; targetClass=""ClassB"" \; target=""m_x"" \; embed=""true"" \; include=""iostream,cstdlib"" \; code=""{m_x = onfile.m_a * onfile.m_b * onfile.m_c; }"" \. #pragma readraw \; sourceClass=""TAxis"" \; source=""fXbins"" \; targetClass=""TAxis"" \; target=""fXbins"" \; version=""[-5]"" \; include=""TAxis.h"" \; code=""\; {\; Float_t * xbins=0; \; Int_t n = buffer.ReadArray( xbins ); \; fXbins.Set( xbins ); \; }"". * For REFLEX dictionaries:. <ioread sourceClass=""ClassA""; source=""double m_a; double m_b; double m_c""; version=""[4-5,7,9,12-]""; checksum=""[12345,123456]""; targetClass=""ClassB""; target=""m_x""; embed=""true""; include=""iostream,cstdlib"">; <![CDATA[; m_x = onfile.m_a * onfile.m_b * onfile.m_c;; ]] >; </ioread>. <ioreadraw sourceClass=""TAxis""; source=""fXbins""; targetClass=""TAxis""; target=""fXbins""; version=""[-5]""; include=""TAxis.h"">; <![CDATA[; Float_t *xbins = 0;; Int_t n = buffer.ReadArray( xbins ) ;; fXbins.Set( xbins );; ]] >; </ioreadraw>. The variables in the rules have the following meaning:. * sourceClass - The field defines the on-disk class that is the input for the rule.; * source - A semicolon-separated list of values defining the source class data members; that need to be cached and accessible via object proxy when the rule is; executed. The values are either the names of the data members or the type-name; pairs (separated by a space). If types are specified then the ondisk structure; can be generated and used in the code snippet defined by the user.; * version - A list of versions of the source class that can be an input for this rule.; The list has to be enclosed in a square bracket and be a comma-separated; list of versions or version ranges. The version is an integer number, whereas; the version range is one of the following:; – ""a-b"" - a and b are integers and the expression means all the numbers between; and including a and b; – ""-a"" - a is an integer and the expression means all the version numbers smaller; than or equal to a; – ""a-",MatchSource.DOCS,io/doc/DataModelEvolution.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt
https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt:4763,Modifiability,variab,variables,4763," a is an integer and the expression means all the version numbers smaller; than or equal to a; – ""a-"" - a is an integer and the expression means all the version numbers greater; than or equal to a; * checksum - A list of checksums of the source class that can be an input for this; rule. The list has to be enclosed in a square brackets and is a; comma-separated list of integers.; * targetClass - The field is obligatory and defines the name of the in-memory class that; this rule can be applied to.; * target - A semicolon-separated list of target class data member names that this rule; is capable of calculating.; * embed - This property tells the system if the rule should be written in the output; file is some objects of this class are serialized.; * include - A list of header files that should be included in order to provide the func-; tionality used in the code snippet; the list is comma delimited.; * code - An user specified code snippet. The user can assume that in the provided code snippet the following variables; will be defined:. The user provided code snippets have to consist of valid C++ code. The system can do; some preprocessing before wrapping the code into function calls and declare some variables to; facilitate the rule definitions. The user can expect the following variables being predeclared:. * newObj - variable representing the target in-memory object, it’s type is that of the; target object; * oldObj - in normal conversion rules, an object of TVirtualObject class representing the; input data, guaranteed to hold the data members declared in the source property; of the rule; * buffer - in raw conversion rules, an object of TBuﬀer class holding the data member; declared in source property of the rule; * names of the data members of the target object declared in the target property of the; rule declared to be the appropriate type; * onfile.xxx - in normal conversion rules, names of the variables of basic types declared; in the source property of the rule.",MatchSource.DOCS,io/doc/DataModelEvolution.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt
https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt:4959,Modifiability,variab,variables,4959,"al to a; * checksum - A list of checksums of the source class that can be an input for this; rule. The list has to be enclosed in a square brackets and is a; comma-separated list of integers.; * targetClass - The field is obligatory and defines the name of the in-memory class that; this rule can be applied to.; * target - A semicolon-separated list of target class data member names that this rule; is capable of calculating.; * embed - This property tells the system if the rule should be written in the output; file is some objects of this class are serialized.; * include - A list of header files that should be included in order to provide the func-; tionality used in the code snippet; the list is comma delimited.; * code - An user specified code snippet. The user can assume that in the provided code snippet the following variables; will be defined:. The user provided code snippets have to consist of valid C++ code. The system can do; some preprocessing before wrapping the code into function calls and declare some variables to; facilitate the rule definitions. The user can expect the following variables being predeclared:. * newObj - variable representing the target in-memory object, it’s type is that of the; target object; * oldObj - in normal conversion rules, an object of TVirtualObject class representing the; input data, guaranteed to hold the data members declared in the source property; of the rule; * buffer - in raw conversion rules, an object of TBuﬀer class holding the data member; declared in source property of the rule; * names of the data members of the target object declared in the target property of the; rule declared to be the appropriate type; * onfile.xxx - in normal conversion rules, names of the variables of basic types declared; in the source property of the rule. 3. The C++ API. The schema evolution C++ API consists of two classes: ROOT::TSchemaRuleSet and ROOT::TSchemaRule.; Objects of the TSchemaRule class represent the rules and their fields hav",MatchSource.DOCS,io/doc/DataModelEvolution.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt
https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt:5040,Modifiability,variab,variables,5040,"t has to be enclosed in a square brackets and is a; comma-separated list of integers.; * targetClass - The field is obligatory and defines the name of the in-memory class that; this rule can be applied to.; * target - A semicolon-separated list of target class data member names that this rule; is capable of calculating.; * embed - This property tells the system if the rule should be written in the output; file is some objects of this class are serialized.; * include - A list of header files that should be included in order to provide the func-; tionality used in the code snippet; the list is comma delimited.; * code - An user specified code snippet. The user can assume that in the provided code snippet the following variables; will be defined:. The user provided code snippets have to consist of valid C++ code. The system can do; some preprocessing before wrapping the code into function calls and declare some variables to; facilitate the rule definitions. The user can expect the following variables being predeclared:. * newObj - variable representing the target in-memory object, it’s type is that of the; target object; * oldObj - in normal conversion rules, an object of TVirtualObject class representing the; input data, guaranteed to hold the data members declared in the source property; of the rule; * buffer - in raw conversion rules, an object of TBuﬀer class holding the data member; declared in source property of the rule; * names of the data members of the target object declared in the target property of the; rule declared to be the appropriate type; * onfile.xxx - in normal conversion rules, names of the variables of basic types declared; in the source property of the rule. 3. The C++ API. The schema evolution C++ API consists of two classes: ROOT::TSchemaRuleSet and ROOT::TSchemaRule.; Objects of the TSchemaRule class represent the rules and their fields have exactly the same; meaning as the ones of rules specified in the dictionaries. TSchemaRuleSet objects; m",MatchSource.DOCS,io/doc/DataModelEvolution.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt
https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt:5081,Modifiability,variab,variable,5081,"ber names that this rule; is capable of calculating.; * embed - This property tells the system if the rule should be written in the output; file is some objects of this class are serialized.; * include - A list of header files that should be included in order to provide the func-; tionality used in the code snippet; the list is comma delimited.; * code - An user specified code snippet. The user can assume that in the provided code snippet the following variables; will be defined:. The user provided code snippets have to consist of valid C++ code. The system can do; some preprocessing before wrapping the code into function calls and declare some variables to; facilitate the rule definitions. The user can expect the following variables being predeclared:. * newObj - variable representing the target in-memory object, it’s type is that of the; target object; * oldObj - in normal conversion rules, an object of TVirtualObject class representing the; input data, guaranteed to hold the data members declared in the source property; of the rule; * buffer - in raw conversion rules, an object of TBuﬀer class holding the data member; declared in source property of the rule; * names of the data members of the target object declared in the target property of the; rule declared to be the appropriate type; * onfile.xxx - in normal conversion rules, names of the variables of basic types declared; in the source property of the rule. 3. The C++ API. The schema evolution C++ API consists of two classes: ROOT::TSchemaRuleSet and ROOT::TSchemaRule.; Objects of the TSchemaRule class represent the rules and their fields have exactly the same; meaning as the ones of rules specified in the dictionaries. TSchemaRuleSet objects; manage the sets of rules and ensure their consistency. There can be no conflicting; rules in the rule sets. The rule sets are owned by the TClass objects corresponding to the; target classes defined in the rules and can be accessed using TClass::{Get|Adopt}SchemaRules; ",MatchSource.DOCS,io/doc/DataModelEvolution.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt
https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt:5673,Modifiability,variab,variables,5673,"ber names that this rule; is capable of calculating.; * embed - This property tells the system if the rule should be written in the output; file is some objects of this class are serialized.; * include - A list of header files that should be included in order to provide the func-; tionality used in the code snippet; the list is comma delimited.; * code - An user specified code snippet. The user can assume that in the provided code snippet the following variables; will be defined:. The user provided code snippets have to consist of valid C++ code. The system can do; some preprocessing before wrapping the code into function calls and declare some variables to; facilitate the rule definitions. The user can expect the following variables being predeclared:. * newObj - variable representing the target in-memory object, it’s type is that of the; target object; * oldObj - in normal conversion rules, an object of TVirtualObject class representing the; input data, guaranteed to hold the data members declared in the source property; of the rule; * buffer - in raw conversion rules, an object of TBuﬀer class holding the data member; declared in source property of the rule; * names of the data members of the target object declared in the target property of the; rule declared to be the appropriate type; * onfile.xxx - in normal conversion rules, names of the variables of basic types declared; in the source property of the rule. 3. The C++ API. The schema evolution C++ API consists of two classes: ROOT::TSchemaRuleSet and ROOT::TSchemaRule.; Objects of the TSchemaRule class represent the rules and their fields have exactly the same; meaning as the ones of rules specified in the dictionaries. TSchemaRuleSet objects; manage the sets of rules and ensure their consistency. There can be no conflicting; rules in the rule sets. The rule sets are owned by the TClass objects corresponding to the; target classes defined in the rules and can be accessed using TClass::{Get|Adopt}SchemaRules; ",MatchSource.DOCS,io/doc/DataModelEvolution.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/doc/DataModelEvolution.txt
