id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/gatk/pull/1211:740,Deployability,install,installed,740,"this is a script which can be used after running gradle installDist to run spark jobs; it can be used identically to ths build/install/bin/gatk script, but has extra features for dealing with spark. running a spark tool and supplying the option --sparkTarget with LOCAL, CLUSTER, or GCS has special behavior; LOCAL will run the tool in the in memory spark runner; CLUSTER along with an appropriate --sparkMaster will run on an accessible spark cluster using spark-submit; arguments to spark-submit may be specified before the arguments to GATK by separating them with a --; GCS will submit jobs to google dataproc using gcloud; common arguments for spark submit will be adapted to match the gcloud formating; this will fail if gcloud isn't installed. if GATK_GCS_STAGING is specified, the jar will be uploaded and cached in the specified bucket for rapid re-use. input files will not be autouploaded to the cloud. --dry-run may be specified before the --, this will only print the commands that will be run instead of actually running them. Adding DataProcArgumentReplace simple tool to convert spark-submit args into gcloud args.; This conversion is not guarenteed to translate all spark command line options to matching gcloud ones.; If you find options that are not translated or are miss-translated please file an issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1211
https://github.com/broadinstitute/gatk/pull/1211:670,Energy Efficiency,adapt,adapted,670,"this is a script which can be used after running gradle installDist to run spark jobs; it can be used identically to ths build/install/bin/gatk script, but has extra features for dealing with spark. running a spark tool and supplying the option --sparkTarget with LOCAL, CLUSTER, or GCS has special behavior; LOCAL will run the tool in the in memory spark runner; CLUSTER along with an appropriate --sparkMaster will run on an accessible spark cluster using spark-submit; arguments to spark-submit may be specified before the arguments to GATK by separating them with a --; GCS will submit jobs to google dataproc using gcloud; common arguments for spark submit will be adapted to match the gcloud formating; this will fail if gcloud isn't installed. if GATK_GCS_STAGING is specified, the jar will be uploaded and cached in the specified bucket for rapid re-use. input files will not be autouploaded to the cloud. --dry-run may be specified before the --, this will only print the commands that will be run instead of actually running them. Adding DataProcArgumentReplace simple tool to convert spark-submit args into gcloud args.; This conversion is not guarenteed to translate all spark command line options to matching gcloud ones.; If you find options that are not translated or are miss-translated please file an issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1211
https://github.com/broadinstitute/gatk/pull/1211:670,Modifiability,adapt,adapted,670,"this is a script which can be used after running gradle installDist to run spark jobs; it can be used identically to ths build/install/bin/gatk script, but has extra features for dealing with spark. running a spark tool and supplying the option --sparkTarget with LOCAL, CLUSTER, or GCS has special behavior; LOCAL will run the tool in the in memory spark runner; CLUSTER along with an appropriate --sparkMaster will run on an accessible spark cluster using spark-submit; arguments to spark-submit may be specified before the arguments to GATK by separating them with a --; GCS will submit jobs to google dataproc using gcloud; common arguments for spark submit will be adapted to match the gcloud formating; this will fail if gcloud isn't installed. if GATK_GCS_STAGING is specified, the jar will be uploaded and cached in the specified bucket for rapid re-use. input files will not be autouploaded to the cloud. --dry-run may be specified before the --, this will only print the commands that will be run instead of actually running them. Adding DataProcArgumentReplace simple tool to convert spark-submit args into gcloud args.; This conversion is not guarenteed to translate all spark command line options to matching gcloud ones.; If you find options that are not translated or are miss-translated please file an issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1211
https://github.com/broadinstitute/gatk/pull/1211:814,Performance,cache,cached,814,"this is a script which can be used after running gradle installDist to run spark jobs; it can be used identically to ths build/install/bin/gatk script, but has extra features for dealing with spark. running a spark tool and supplying the option --sparkTarget with LOCAL, CLUSTER, or GCS has special behavior; LOCAL will run the tool in the in memory spark runner; CLUSTER along with an appropriate --sparkMaster will run on an accessible spark cluster using spark-submit; arguments to spark-submit may be specified before the arguments to GATK by separating them with a --; GCS will submit jobs to google dataproc using gcloud; common arguments for spark submit will be adapted to match the gcloud formating; this will fail if gcloud isn't installed. if GATK_GCS_STAGING is specified, the jar will be uploaded and cached in the specified bucket for rapid re-use. input files will not be autouploaded to the cloud. --dry-run may be specified before the --, this will only print the commands that will be run instead of actually running them. Adding DataProcArgumentReplace simple tool to convert spark-submit args into gcloud args.; This conversion is not guarenteed to translate all spark command line options to matching gcloud ones.; If you find options that are not translated or are miss-translated please file an issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1211
https://github.com/broadinstitute/gatk/pull/1211:427,Security,access,accessible,427,"this is a script which can be used after running gradle installDist to run spark jobs; it can be used identically to ths build/install/bin/gatk script, but has extra features for dealing with spark. running a spark tool and supplying the option --sparkTarget with LOCAL, CLUSTER, or GCS has special behavior; LOCAL will run the tool in the in memory spark runner; CLUSTER along with an appropriate --sparkMaster will run on an accessible spark cluster using spark-submit; arguments to spark-submit may be specified before the arguments to GATK by separating them with a --; GCS will submit jobs to google dataproc using gcloud; common arguments for spark submit will be adapted to match the gcloud formating; this will fail if gcloud isn't installed. if GATK_GCS_STAGING is specified, the jar will be uploaded and cached in the specified bucket for rapid re-use. input files will not be autouploaded to the cloud. --dry-run may be specified before the --, this will only print the commands that will be run instead of actually running them. Adding DataProcArgumentReplace simple tool to convert spark-submit args into gcloud args.; This conversion is not guarenteed to translate all spark command line options to matching gcloud ones.; If you find options that are not translated or are miss-translated please file an issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1211
https://github.com/broadinstitute/gatk/pull/1211:1072,Usability,simpl,simple,1072,"this is a script which can be used after running gradle installDist to run spark jobs; it can be used identically to ths build/install/bin/gatk script, but has extra features for dealing with spark. running a spark tool and supplying the option --sparkTarget with LOCAL, CLUSTER, or GCS has special behavior; LOCAL will run the tool in the in memory spark runner; CLUSTER along with an appropriate --sparkMaster will run on an accessible spark cluster using spark-submit; arguments to spark-submit may be specified before the arguments to GATK by separating them with a --; GCS will submit jobs to google dataproc using gcloud; common arguments for spark submit will be adapted to match the gcloud formating; this will fail if gcloud isn't installed. if GATK_GCS_STAGING is specified, the jar will be uploaded and cached in the specified bucket for rapid re-use. input files will not be autouploaded to the cloud. --dry-run may be specified before the --, this will only print the commands that will be run instead of actually running them. Adding DataProcArgumentReplace simple tool to convert spark-submit args into gcloud args.; This conversion is not guarenteed to translate all spark command line options to matching gcloud ones.; If you find options that are not translated or are miss-translated please file an issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1211
https://github.com/broadinstitute/gatk/pull/1213:82,Availability,error,error,82,"The current fatJar gradle task does not properly merge resource files, causing an error when you try to run a Spark tool from the resulting jar. This PR replaces the fatJar task by configuring our shadowJar task to properly merge resource files. I've attempted to share configuration with the sparkJar task, which is also of type ShadowJar. Discussed briefly with @lbergelson.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1213
https://github.com/broadinstitute/gatk/pull/1213:270,Deployability,configurat,configuration,270,"The current fatJar gradle task does not properly merge resource files, causing an error when you try to run a Spark tool from the resulting jar. This PR replaces the fatJar task by configuring our shadowJar task to properly merge resource files. I've attempted to share configuration with the sparkJar task, which is also of type ShadowJar. Discussed briefly with @lbergelson.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1213
https://github.com/broadinstitute/gatk/pull/1213:181,Modifiability,config,configuring,181,"The current fatJar gradle task does not properly merge resource files, causing an error when you try to run a Spark tool from the resulting jar. This PR replaces the fatJar task by configuring our shadowJar task to properly merge resource files. I've attempted to share configuration with the sparkJar task, which is also of type ShadowJar. Discussed briefly with @lbergelson.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1213
https://github.com/broadinstitute/gatk/pull/1213:270,Modifiability,config,configuration,270,"The current fatJar gradle task does not properly merge resource files, causing an error when you try to run a Spark tool from the resulting jar. This PR replaces the fatJar task by configuring our shadowJar task to properly merge resource files. I've attempted to share configuration with the sparkJar task, which is also of type ShadowJar. Discussed briefly with @lbergelson.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1213
https://github.com/broadinstitute/gatk/issues/1214:744,Testability,Assert,AssertionError,744,command . ```; spark-submit \; --master $SPARK_MASTER \; --driver-memory 8G \; --num-executors ${1} \; --executor-cores 4 \; --executor-memory 20G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; $JAR BaseRecalibratorSpark \; --input hdfs:///user/akiezun/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; --output bqsr_out_${1}.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --knownSites hdfs:////user/akiezun/dbsnp_138.b37.excluding_sites_after_129.vcf \; --joinStrategy BROADCAST \; --apiKey $API_KEY \; --sparkMaster $SPARK_MASTER; ```. fails with. ```; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:165); at org.bdgenomics.adam.util.TwoBitFile.extract(TwoBitFile.scala:112); at org.bdgenomics.adam.util.TwoBitFile.extract(TwoBitFile.scala:170); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.getReferenceBases(ReferenceTwoBitSource.java:48); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.getReferenceBases(ReferenceMultiSource.java:99); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithRefBases.lambda$addBases$c54addeb$1(BroadcastJoinReadsWithRefBases.java:53); ...; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1214
https://github.com/broadinstitute/gatk/issues/1214:760,Testability,assert,assertion,760,command . ```; spark-submit \; --master $SPARK_MASTER \; --driver-memory 8G \; --num-executors ${1} \; --executor-cores 4 \; --executor-memory 20G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; $JAR BaseRecalibratorSpark \; --input hdfs:///user/akiezun/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; --output bqsr_out_${1}.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --knownSites hdfs:////user/akiezun/dbsnp_138.b37.excluding_sites_after_129.vcf \; --joinStrategy BROADCAST \; --apiKey $API_KEY \; --sparkMaster $SPARK_MASTER; ```. fails with. ```; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:165); at org.bdgenomics.adam.util.TwoBitFile.extract(TwoBitFile.scala:112); at org.bdgenomics.adam.util.TwoBitFile.extract(TwoBitFile.scala:170); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.getReferenceBases(ReferenceTwoBitSource.java:48); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.getReferenceBases(ReferenceMultiSource.java:99); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithRefBases.lambda$addBases$c54addeb$1(BroadcastJoinReadsWithRefBases.java:53); ...; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1214
https://github.com/broadinstitute/gatk/issues/1214:795,Testability,assert,assert,795,command . ```; spark-submit \; --master $SPARK_MASTER \; --driver-memory 8G \; --num-executors ${1} \; --executor-cores 4 \; --executor-memory 20G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; $JAR BaseRecalibratorSpark \; --input hdfs:///user/akiezun/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; --output bqsr_out_${1}.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --knownSites hdfs:////user/akiezun/dbsnp_138.b37.excluding_sites_after_129.vcf \; --joinStrategy BROADCAST \; --apiKey $API_KEY \; --sparkMaster $SPARK_MASTER; ```. fails with. ```; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:165); at org.bdgenomics.adam.util.TwoBitFile.extract(TwoBitFile.scala:112); at org.bdgenomics.adam.util.TwoBitFile.extract(TwoBitFile.scala:170); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.getReferenceBases(ReferenceTwoBitSource.java:48); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.getReferenceBases(ReferenceMultiSource.java:99); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithRefBases.lambda$addBases$c54addeb$1(BroadcastJoinReadsWithRefBases.java:53); ...; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1214
https://github.com/broadinstitute/gatk/issues/1227:36,Testability,test,test,36,org.broadinstitute.hellbender.utils.test.testers.CleanSamIntegrationTest; org.broadinstitute.hellbender.tools.picard.sam.CleanSamIntegrationTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1227
https://github.com/broadinstitute/gatk/issues/1227:41,Testability,test,testers,41,org.broadinstitute.hellbender.utils.test.testers.CleanSamIntegrationTest; org.broadinstitute.hellbender.tools.picard.sam.CleanSamIntegrationTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1227
https://github.com/broadinstitute/gatk/issues/1230:13,Testability,test,tests,13,include CRAM tests too,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1230
https://github.com/broadinstitute/gatk/issues/1234:66,Deployability,deploy,deploy-mode,66,"```; spark-submit --class org.broadinstitute.hellbender.Main \; --deploy-mode client \; --master yarn-client \; --driver-memory 8G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; --executor-memory ${execMem}g \; --num-executors $execs \; --executor-cores $cores \; bin/cleanHellbender/gatk/build/libs/gatk-all-*-spark.jar \; ReadsPipelineSpark \; --sparkMaster yarn-client \; -I hdfs:///user/akiezun/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --programName ${name} \; -O $bamout \; --knownSites hdfs:////user/akiezun/dbsnp_138.b37.excluding_sites_after_129.vcf \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES; ```. exec=24; cores=5; execMem=25. fails with . ```; java.lang.IllegalArgumentException: SimpleInterval is 1 based, so start must be >= 1, start: 0; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:58); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.baq.BAQ.getReferenceWindowForRead(BAQ.java:525); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:46); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:41); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithRefBases.lambda$addBases$c54addeb$1(BroadcastJoinReadsWithRefBases.java:52); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.coll",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1234
https://github.com/broadinstitute/gatk/issues/1234:2080,Integrability,Wrap,Wrappers,2080,"conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; --executor-memory ${execMem}g \; --num-executors $execs \; --executor-cores $cores \; bin/cleanHellbender/gatk/build/libs/gatk-all-*-spark.jar \; ReadsPipelineSpark \; --sparkMaster yarn-client \; -I hdfs:///user/akiezun/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --programName ${name} \; -O $bamout \; --knownSites hdfs:////user/akiezun/dbsnp_138.b37.excluding_sites_after_129.vcf \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES; ```. exec=24; cores=5; execMem=25. fails with . ```; java.lang.IllegalArgumentException: SimpleInterval is 1 based, so start must be >= 1, start: 0; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:58); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.baq.BAQ.getReferenceWindowForRead(BAQ.java:525); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:46); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:41); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithRefBases.lambda$addBases$c54addeb$1(BroadcastJoinReadsWithRefBases.java:52); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:30). ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1234
https://github.com/broadinstitute/gatk/issues/1234:2110,Integrability,Wrap,Wrappers,2110,"conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; --executor-memory ${execMem}g \; --num-executors $execs \; --executor-cores $cores \; bin/cleanHellbender/gatk/build/libs/gatk-all-*-spark.jar \; ReadsPipelineSpark \; --sparkMaster yarn-client \; -I hdfs:///user/akiezun/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --programName ${name} \; -O $bamout \; --knownSites hdfs:////user/akiezun/dbsnp_138.b37.excluding_sites_after_129.vcf \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES; ```. exec=24; cores=5; execMem=25. fails with . ```; java.lang.IllegalArgumentException: SimpleInterval is 1 based, so start must be >= 1, start: 0; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:58); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.baq.BAQ.getReferenceWindowForRead(BAQ.java:525); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:46); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:41); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithRefBases.lambda$addBases$c54addeb$1(BroadcastJoinReadsWithRefBases.java:52); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:30). ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1234
https://github.com/broadinstitute/gatk/issues/1234:1070,Security,validat,validatePositions,1070,"; --master yarn-client \; --driver-memory 8G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; --executor-memory ${execMem}g \; --num-executors $execs \; --executor-cores $cores \; bin/cleanHellbender/gatk/build/libs/gatk-all-*-spark.jar \; ReadsPipelineSpark \; --sparkMaster yarn-client \; -I hdfs:///user/akiezun/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --programName ${name} \; -O $bamout \; --knownSites hdfs:////user/akiezun/dbsnp_138.b37.excluding_sites_after_129.vcf \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES; ```. exec=24; cores=5; execMem=25. fails with . ```; java.lang.IllegalArgumentException: SimpleInterval is 1 based, so start must be >= 1, start: 0; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:58); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.baq.BAQ.getReferenceWindowForRead(BAQ.java:525); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:46); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:41); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithRefBases.lambda$addBases$c54addeb$1(BroadcastJoinReadsWithRefBases.java:52); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrapper",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1234
https://github.com/broadinstitute/gatk/issues/1234:956,Usability,Simpl,SimpleInterval,956,"```; spark-submit --class org.broadinstitute.hellbender.Main \; --deploy-mode client \; --master yarn-client \; --driver-memory 8G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; --executor-memory ${execMem}g \; --num-executors $execs \; --executor-cores $cores \; bin/cleanHellbender/gatk/build/libs/gatk-all-*-spark.jar \; ReadsPipelineSpark \; --sparkMaster yarn-client \; -I hdfs:///user/akiezun/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --programName ${name} \; -O $bamout \; --knownSites hdfs:////user/akiezun/dbsnp_138.b37.excluding_sites_after_129.vcf \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES; ```. exec=24; cores=5; execMem=25. fails with . ```; java.lang.IllegalArgumentException: SimpleInterval is 1 based, so start must be >= 1, start: 0; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:58); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.baq.BAQ.getReferenceWindowForRead(BAQ.java:525); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:46); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:41); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithRefBases.lambda$addBases$c54addeb$1(BroadcastJoinReadsWithRefBases.java:52); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.coll",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1234
https://github.com/broadinstitute/gatk/issues/1234:1055,Usability,Simpl,SimpleInterval,1055,"; --deploy-mode client \; --master yarn-client \; --driver-memory 8G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; --executor-memory ${execMem}g \; --num-executors $execs \; --executor-cores $cores \; bin/cleanHellbender/gatk/build/libs/gatk-all-*-spark.jar \; ReadsPipelineSpark \; --sparkMaster yarn-client \; -I hdfs:///user/akiezun/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --programName ${name} \; -O $bamout \; --knownSites hdfs:////user/akiezun/dbsnp_138.b37.excluding_sites_after_129.vcf \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES; ```. exec=24; cores=5; execMem=25. fails with . ```; java.lang.IllegalArgumentException: SimpleInterval is 1 based, so start must be >= 1, start: 0; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:58); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.baq.BAQ.getReferenceWindowForRead(BAQ.java:525); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:46); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:41); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithRefBases.lambda$addBases$c54addeb$1(BroadcastJoinReadsWithRefBases.java:52); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1234
https://github.com/broadinstitute/gatk/issues/1234:1088,Usability,Simpl,SimpleInterval,1088,"; --master yarn-client \; --driver-memory 8G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; --executor-memory ${execMem}g \; --num-executors $execs \; --executor-cores $cores \; bin/cleanHellbender/gatk/build/libs/gatk-all-*-spark.jar \; ReadsPipelineSpark \; --sparkMaster yarn-client \; -I hdfs:///user/akiezun/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --programName ${name} \; -O $bamout \; --knownSites hdfs:////user/akiezun/dbsnp_138.b37.excluding_sites_after_129.vcf \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES; ```. exec=24; cores=5; execMem=25. fails with . ```; java.lang.IllegalArgumentException: SimpleInterval is 1 based, so start must be >= 1, start: 0; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:58); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.baq.BAQ.getReferenceWindowForRead(BAQ.java:525); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:46); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:41); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithRefBases.lambda$addBases$c54addeb$1(BroadcastJoinReadsWithRefBases.java:52); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrapper",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1234
https://github.com/broadinstitute/gatk/issues/1234:1152,Usability,Simpl,SimpleInterval,1152,"sultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; --executor-memory ${execMem}g \; --num-executors $execs \; --executor-cores $cores \; bin/cleanHellbender/gatk/build/libs/gatk-all-*-spark.jar \; ReadsPipelineSpark \; --sparkMaster yarn-client \; -I hdfs:///user/akiezun/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --programName ${name} \; -O $bamout \; --knownSites hdfs:////user/akiezun/dbsnp_138.b37.excluding_sites_after_129.vcf \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES; ```. exec=24; cores=5; execMem=25. fails with . ```; java.lang.IllegalArgumentException: SimpleInterval is 1 based, so start must be >= 1, start: 0; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:58); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.baq.BAQ.getReferenceWindowForRead(BAQ.java:525); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:46); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:41); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithRefBases.lambda$addBases$c54addeb$1(BroadcastJoinReadsWithRefBases.java:52); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbend",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1234
https://github.com/broadinstitute/gatk/issues/1234:1174,Usability,Simpl,SimpleInterval,1174,"f spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; --executor-memory ${execMem}g \; --num-executors $execs \; --executor-cores $cores \; bin/cleanHellbender/gatk/build/libs/gatk-all-*-spark.jar \; ReadsPipelineSpark \; --sparkMaster yarn-client \; -I hdfs:///user/akiezun/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --programName ${name} \; -O $bamout \; --knownSites hdfs:////user/akiezun/dbsnp_138.b37.excluding_sites_after_129.vcf \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES; ```. exec=24; cores=5; execMem=25. fails with . ```; java.lang.IllegalArgumentException: SimpleInterval is 1 based, so start must be >= 1, start: 0; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:58); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.baq.BAQ.getReferenceWindowForRead(BAQ.java:525); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:46); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:41); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithRefBases.lambda$addBases$c54addeb$1(BroadcastJoinReadsWithRefBases.java:52); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.tra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1234
https://github.com/broadinstitute/gatk/pull/1236:85,Availability,failure,failure,85,The TwoBit API can't handle queries beyond the ends of contigs (we get; an assertion failure). This changes ReferenceTwoBitSource to truncate query; intervals at contig ends as necessary. Resolves #1214,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1236
https://github.com/broadinstitute/gatk/pull/1236:75,Testability,assert,assertion,75,The TwoBit API can't handle queries beyond the ends of contigs (we get; an assertion failure). This changes ReferenceTwoBitSource to truncate query; intervals at contig ends as necessary. Resolves #1214,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1236
https://github.com/broadinstitute/gatk/issues/1237:24,Deployability,pipeline,pipeline,24,add some metrics to the pipeline to see how well that scales. Metrics should come cheaply because the data RDD is loaded at that point.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1237
https://github.com/broadinstitute/gatk/issues/1237:114,Performance,load,loaded,114,add some metrics to the pipeline to see how well that scales. Metrics should come cheaply because the data RDD is loaded at that point.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1237
https://github.com/broadinstitute/gatk/pull/1241:15,Deployability,integrat,integration,15,The SplitReads integration tests will fail once we upgrade htsjdk without this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1241
https://github.com/broadinstitute/gatk/pull/1241:51,Deployability,upgrade,upgrade,51,The SplitReads integration tests will fail once we upgrade htsjdk without this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1241
https://github.com/broadinstitute/gatk/pull/1241:15,Integrability,integrat,integration,15,The SplitReads integration tests will fail once we upgrade htsjdk without this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1241
https://github.com/broadinstitute/gatk/pull/1241:27,Testability,test,tests,27,The SplitReads integration tests will fail once we upgrade htsjdk without this.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1241
https://github.com/broadinstitute/gatk/pull/1242:42,Availability,error,error,42,fixes #1234 ; i checked that the original error goes away. @droazen can you have a quick look?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1242
https://github.com/broadinstitute/gatk/pull/1243:240,Availability,avail,available,240,This change is dependent on [this recent change](https://github.com/samtools/htsjdk/commit/4f550e1f1afabf21467957fa672ca2a4ad457897#diff-b678735810949d4263df7bd0fffdecb8L42) in htsjdk (and the build will fail without it). Once htsjdk2.0 is available we'll upgrade it in this branch/pr so the two changes can go in together.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1243
https://github.com/broadinstitute/gatk/pull/1243:256,Deployability,upgrade,upgrade,256,This change is dependent on [this recent change](https://github.com/samtools/htsjdk/commit/4f550e1f1afabf21467957fa672ca2a4ad457897#diff-b678735810949d4263df7bd0fffdecb8L42) in htsjdk (and the build will fail without it). Once htsjdk2.0 is available we'll upgrade it in this branch/pr so the two changes can go in together.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1243
https://github.com/broadinstitute/gatk/pull/1243:15,Integrability,depend,dependent,15,This change is dependent on [this recent change](https://github.com/samtools/htsjdk/commit/4f550e1f1afabf21467957fa672ca2a4ad457897#diff-b678735810949d4263df7bd0fffdecb8L42) in htsjdk (and the build will fail without it). Once htsjdk2.0 is available we'll upgrade it in this branch/pr so the two changes can go in together.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1243
https://github.com/broadinstitute/gatk/issues/1245:32,Testability,test,test,32,"right now both Picard and Spark test only the TOTAL_MAPPED_REFERENCE_LENGTH gets tested. GIven that SUM_OF_BASE_QUALITIES is the default, we need tests that use it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1245
https://github.com/broadinstitute/gatk/issues/1245:81,Testability,test,tested,81,"right now both Picard and Spark test only the TOTAL_MAPPED_REFERENCE_LENGTH gets tested. GIven that SUM_OF_BASE_QUALITIES is the default, we need tests that use it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1245
https://github.com/broadinstitute/gatk/issues/1245:146,Testability,test,tests,146,"right now both Picard and Spark test only the TOTAL_MAPPED_REFERENCE_LENGTH gets tested. GIven that SUM_OF_BASE_QUALITIES is the default, we need tests that use it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1245
https://github.com/broadinstitute/gatk/issues/1246:162,Security,access,accessing,162,I currently have a feature driven walker in gatk-protected that is slower than a read-walker equivalent due to the repetitive Read iterator re-instantiation when accessing overlapping reads using the ReadContext. . Ideally the engine (FeatureManager?) would try to reuse open read iterators instead of creating them for each feature/interval.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1246
https://github.com/broadinstitute/gatk/issues/1248:30,Performance,load,loading,30,"Using a larger split size for loading reads via Hadoop-BAM would improve performance -- @tomwhite has shown that the process of reading the input splits is expensive with a small split size like our current default of 10 MB (`ReadsSparkSource.DEFAULT_SPLIT_SIZE`). Let's see if we can use a larger split size for loading the reads, then re-partition back to a smaller split size that is a factor of the larger size before doing any work to avoid out-of-memory issues in map jobs. Ideally the re-partitioning step should NOT trigger a shuffle -- if it does, we may not want to make this change.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1248
https://github.com/broadinstitute/gatk/issues/1248:73,Performance,perform,performance,73,"Using a larger split size for loading reads via Hadoop-BAM would improve performance -- @tomwhite has shown that the process of reading the input splits is expensive with a small split size like our current default of 10 MB (`ReadsSparkSource.DEFAULT_SPLIT_SIZE`). Let's see if we can use a larger split size for loading the reads, then re-partition back to a smaller split size that is a factor of the larger size before doing any work to avoid out-of-memory issues in map jobs. Ideally the re-partitioning step should NOT trigger a shuffle -- if it does, we may not want to make this change.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1248
https://github.com/broadinstitute/gatk/issues/1248:313,Performance,load,loading,313,"Using a larger split size for loading reads via Hadoop-BAM would improve performance -- @tomwhite has shown that the process of reading the input splits is expensive with a small split size like our current default of 10 MB (`ReadsSparkSource.DEFAULT_SPLIT_SIZE`). Let's see if we can use a larger split size for loading the reads, then re-partition back to a smaller split size that is a factor of the larger size before doing any work to avoid out-of-memory issues in map jobs. Ideally the re-partitioning step should NOT trigger a shuffle -- if it does, we may not want to make this change.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1248
https://github.com/broadinstitute/gatk/issues/1248:440,Safety,avoid,avoid,440,"Using a larger split size for loading reads via Hadoop-BAM would improve performance -- @tomwhite has shown that the process of reading the input splits is expensive with a small split size like our current default of 10 MB (`ReadsSparkSource.DEFAULT_SPLIT_SIZE`). Let's see if we can use a larger split size for loading the reads, then re-partition back to a smaller split size that is a factor of the larger size before doing any work to avoid out-of-memory issues in map jobs. Ideally the re-partitioning step should NOT trigger a shuffle -- if it does, we may not want to make this change.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1248
https://github.com/broadinstitute/gatk/issues/1249:40,Testability,test,test,40,${GATK4_JAR} MarkDuplicatesSpark -I src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.bam -O tmp.junk.bam --METRICS_FILE=tmp.metrics -parallelism 1. samtools view tmp.junk.bam | less; and you'll see 1001995 ahead of 1001645; Offending records are at [20:1001995] and [20:1001645],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1249
https://github.com/broadinstitute/gatk/pull/1251:400,Performance,perform,performant,400,"We were not totally consistent with htsjdk BAM ordering in our Spark bam output; previously (particularly with unmapped reads that have the position of their mates).; With this change, we are. -Modified ReadsSparkSink to operate directly on raw, headerless SAMRecords, and use a new; comparator for headerless SAMRecords that calls directly into htsjdk's SAMRecordCoordinateComparator. -To make this performant, added a new serializer for raw SAMRecords that uses the same; approach as the existing SAMRecordToGATKReadAdapterSerializer in using a modified BAM; codec as the encoder. -Added test cases that failed in the previous implementation due to ordering issues, and; pass now. Resolves #1249",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1251
https://github.com/broadinstitute/gatk/pull/1251:590,Testability,test,test,590,"We were not totally consistent with htsjdk BAM ordering in our Spark bam output; previously (particularly with unmapped reads that have the position of their mates).; With this change, we are. -Modified ReadsSparkSink to operate directly on raw, headerless SAMRecords, and use a new; comparator for headerless SAMRecords that calls directly into htsjdk's SAMRecordCoordinateComparator. -To make this performant, added a new serializer for raw SAMRecords that uses the same; approach as the existing SAMRecordToGATKReadAdapterSerializer in using a modified BAM; codec as the encoder. -Added test cases that failed in the previous implementation due to ordering issues, and; pass now. Resolves #1249",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1251
https://github.com/broadinstitute/gatk/issues/1254:214,Usability,clear,clear,214,"`ReadsSparkSinkUnitTest.readsSinkADAMTest()` fails on the bams `CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.bam` and `NA12878.chr17_69k_70k.dictFix.bam` (currently commented-out in its `DataProvider`), and it's not clear what the issue is.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1254
https://github.com/broadinstitute/gatk/pull/1255:26,Deployability,toggle,toggle,26,"This contains a change to toggle the presorted flag used by GatherBAMFiles, and enables some CRAM tests that were previously broken due to htsjdk bugs. Fixes https://github.com/broadinstitute/gatk/issues/1138 and all but one from https://github.com/broadinstitute/gatk/issues/1141.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1255
https://github.com/broadinstitute/gatk/pull/1255:98,Testability,test,tests,98,"This contains a change to toggle the presorted flag used by GatherBAMFiles, and enables some CRAM tests that were previously broken due to htsjdk bugs. Fixes https://github.com/broadinstitute/gatk/issues/1138 and all but one from https://github.com/broadinstitute/gatk/issues/1141.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1255
https://github.com/broadinstitute/gatk/issues/1257:89,Availability,error,error,89,"Running a spark tool with a read input that doesn't exist on hdfs results in a confusing error message. ex: `hdfs://user/local/print_reads.sorted.bam` doesn't exist on the file system ; produces . ```; java.lang.IllegalArgumentException: Wrong FS: hdfs://user/local/print_reads.sorted.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; ```. Full command line to reproduce:. ```; spark-submit --master yarn-client --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.executor.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 build/libs/gatk-all-4.pre-alpha-196-g94b53ee-SNAPSHOT-spark.jar PrintReadsSpark --sparkMaster yarn-client -I hdfs://user/local/print_reads.sorted.bam -O output.bam; ```. ```; java.lang.IllegalArgumentException: Wrong FS: hdfs://user/local/print_reads.sorted.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:474); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:163); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:281); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:261); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:252); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1257
https://github.com/broadinstitute/gatk/issues/1257:2405,Deployability,deploy,deploy,2405,"llegalArgumentException: Wrong FS: hdfs://user/local/print_reads.sorted.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:474); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:163); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:281); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:261); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:252); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:71); at org.broadinstitute.hellbender.Main.main(Main.java:86); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. This `IllegalArgumentException` should be converted to a `UserException` instead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1257
https://github.com/broadinstitute/gatk/issues/1257:2442,Deployability,deploy,deploy,2442,"llegalArgumentException: Wrong FS: hdfs://user/local/print_reads.sorted.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:474); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:163); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:281); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:261); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:252); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:71); at org.broadinstitute.hellbender.Main.main(Main.java:86); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. This `IllegalArgumentException` should be converted to a `UserException` instead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1257
https://github.com/broadinstitute/gatk/issues/1257:2514,Deployability,deploy,deploy,2514,"llegalArgumentException: Wrong FS: hdfs://user/local/print_reads.sorted.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:474); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:163); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:281); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:261); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:252); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:71); at org.broadinstitute.hellbender.Main.main(Main.java:86); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. This `IllegalArgumentException` should be converted to a `UserException` instead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1257
https://github.com/broadinstitute/gatk/issues/1257:2590,Deployability,deploy,deploy,2590,"llegalArgumentException: Wrong FS: hdfs://user/local/print_reads.sorted.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:474); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:163); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:281); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:261); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:252); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:71); at org.broadinstitute.hellbender.Main.main(Main.java:86); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. This `IllegalArgumentException` should be converted to a `UserException` instead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1257
https://github.com/broadinstitute/gatk/issues/1257:2661,Deployability,deploy,deploy,2661,"llegalArgumentException: Wrong FS: hdfs://user/local/print_reads.sorted.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:474); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:163); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:281); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:261); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:252); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:71); at org.broadinstitute.hellbender.Main.main(Main.java:86); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. This `IllegalArgumentException` should be converted to a `UserException` instead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1257
https://github.com/broadinstitute/gatk/issues/1257:2730,Deployability,deploy,deploy,2730,"llegalArgumentException: Wrong FS: hdfs://user/local/print_reads.sorted.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:474); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:163); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:281); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:261); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:252); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:71); at org.broadinstitute.hellbender.Main.main(Main.java:86); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. This `IllegalArgumentException` should be converted to a `UserException` instead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1257
https://github.com/broadinstitute/gatk/issues/1257:95,Integrability,message,message,95,"Running a spark tool with a read input that doesn't exist on hdfs results in a confusing error message. ex: `hdfs://user/local/print_reads.sorted.bam` doesn't exist on the file system ; produces . ```; java.lang.IllegalArgumentException: Wrong FS: hdfs://user/local/print_reads.sorted.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; ```. Full command line to reproduce:. ```; spark-submit --master yarn-client --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.executor.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 build/libs/gatk-all-4.pre-alpha-196-g94b53ee-SNAPSHOT-spark.jar PrintReadsSpark --sparkMaster yarn-client -I hdfs://user/local/print_reads.sorted.bam -O output.bam; ```. ```; java.lang.IllegalArgumentException: Wrong FS: hdfs://user/local/print_reads.sorted.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:474); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:163); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:281); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:261); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:252); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1257
https://github.com/broadinstitute/gatk/issues/1258:483,Performance,cache,caches,483,"`MarkDuplicatesSparkIntegrationTest.testMDOrder()` _always_ passes when run locally, but fails about 50% of the time when run in travis. As a result, it's currently disabled. Some things I've tried to fix the test on travis, without success:; -Run MD with `--parallelism 1` instead of `--parallelism 0` (which doesn't guarantee ordering at all!) -- the original version of the test was running with `--parallelism 0`; -Converted test inputs from sam to bam; -Cleared all of travis's caches. This may be purely a travis issue or a flaw in the test itself, or it may indicate that there remain issues with `MarkDuplicatesSpark` output and ordering despite the recent fixes. Note, however, that the original test from @davidadamsphd that first detected the ordering issue with MD on spark now passes, as do the new `ReadsSparkSink` ordering tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1258
https://github.com/broadinstitute/gatk/issues/1258:741,Safety,detect,detected,741,"`MarkDuplicatesSparkIntegrationTest.testMDOrder()` _always_ passes when run locally, but fails about 50% of the time when run in travis. As a result, it's currently disabled. Some things I've tried to fix the test on travis, without success:; -Run MD with `--parallelism 1` instead of `--parallelism 0` (which doesn't guarantee ordering at all!) -- the original version of the test was running with `--parallelism 0`; -Converted test inputs from sam to bam; -Cleared all of travis's caches. This may be purely a travis issue or a flaw in the test itself, or it may indicate that there remain issues with `MarkDuplicatesSpark` output and ordering despite the recent fixes. Note, however, that the original test from @davidadamsphd that first detected the ordering issue with MD on spark now passes, as do the new `ReadsSparkSink` ordering tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1258
https://github.com/broadinstitute/gatk/issues/1258:36,Testability,test,testMDOrder,36,"`MarkDuplicatesSparkIntegrationTest.testMDOrder()` _always_ passes when run locally, but fails about 50% of the time when run in travis. As a result, it's currently disabled. Some things I've tried to fix the test on travis, without success:; -Run MD with `--parallelism 1` instead of `--parallelism 0` (which doesn't guarantee ordering at all!) -- the original version of the test was running with `--parallelism 0`; -Converted test inputs from sam to bam; -Cleared all of travis's caches. This may be purely a travis issue or a flaw in the test itself, or it may indicate that there remain issues with `MarkDuplicatesSpark` output and ordering despite the recent fixes. Note, however, that the original test from @davidadamsphd that first detected the ordering issue with MD on spark now passes, as do the new `ReadsSparkSink` ordering tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1258
https://github.com/broadinstitute/gatk/issues/1258:209,Testability,test,test,209,"`MarkDuplicatesSparkIntegrationTest.testMDOrder()` _always_ passes when run locally, but fails about 50% of the time when run in travis. As a result, it's currently disabled. Some things I've tried to fix the test on travis, without success:; -Run MD with `--parallelism 1` instead of `--parallelism 0` (which doesn't guarantee ordering at all!) -- the original version of the test was running with `--parallelism 0`; -Converted test inputs from sam to bam; -Cleared all of travis's caches. This may be purely a travis issue or a flaw in the test itself, or it may indicate that there remain issues with `MarkDuplicatesSpark` output and ordering despite the recent fixes. Note, however, that the original test from @davidadamsphd that first detected the ordering issue with MD on spark now passes, as do the new `ReadsSparkSink` ordering tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1258
https://github.com/broadinstitute/gatk/issues/1258:377,Testability,test,test,377,"`MarkDuplicatesSparkIntegrationTest.testMDOrder()` _always_ passes when run locally, but fails about 50% of the time when run in travis. As a result, it's currently disabled. Some things I've tried to fix the test on travis, without success:; -Run MD with `--parallelism 1` instead of `--parallelism 0` (which doesn't guarantee ordering at all!) -- the original version of the test was running with `--parallelism 0`; -Converted test inputs from sam to bam; -Cleared all of travis's caches. This may be purely a travis issue or a flaw in the test itself, or it may indicate that there remain issues with `MarkDuplicatesSpark` output and ordering despite the recent fixes. Note, however, that the original test from @davidadamsphd that first detected the ordering issue with MD on spark now passes, as do the new `ReadsSparkSink` ordering tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1258
https://github.com/broadinstitute/gatk/issues/1258:429,Testability,test,test,429,"`MarkDuplicatesSparkIntegrationTest.testMDOrder()` _always_ passes when run locally, but fails about 50% of the time when run in travis. As a result, it's currently disabled. Some things I've tried to fix the test on travis, without success:; -Run MD with `--parallelism 1` instead of `--parallelism 0` (which doesn't guarantee ordering at all!) -- the original version of the test was running with `--parallelism 0`; -Converted test inputs from sam to bam; -Cleared all of travis's caches. This may be purely a travis issue or a flaw in the test itself, or it may indicate that there remain issues with `MarkDuplicatesSpark` output and ordering despite the recent fixes. Note, however, that the original test from @davidadamsphd that first detected the ordering issue with MD on spark now passes, as do the new `ReadsSparkSink` ordering tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1258
https://github.com/broadinstitute/gatk/issues/1258:542,Testability,test,test,542,"`MarkDuplicatesSparkIntegrationTest.testMDOrder()` _always_ passes when run locally, but fails about 50% of the time when run in travis. As a result, it's currently disabled. Some things I've tried to fix the test on travis, without success:; -Run MD with `--parallelism 1` instead of `--parallelism 0` (which doesn't guarantee ordering at all!) -- the original version of the test was running with `--parallelism 0`; -Converted test inputs from sam to bam; -Cleared all of travis's caches. This may be purely a travis issue or a flaw in the test itself, or it may indicate that there remain issues with `MarkDuplicatesSpark` output and ordering despite the recent fixes. Note, however, that the original test from @davidadamsphd that first detected the ordering issue with MD on spark now passes, as do the new `ReadsSparkSink` ordering tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1258
https://github.com/broadinstitute/gatk/issues/1258:705,Testability,test,test,705,"`MarkDuplicatesSparkIntegrationTest.testMDOrder()` _always_ passes when run locally, but fails about 50% of the time when run in travis. As a result, it's currently disabled. Some things I've tried to fix the test on travis, without success:; -Run MD with `--parallelism 1` instead of `--parallelism 0` (which doesn't guarantee ordering at all!) -- the original version of the test was running with `--parallelism 0`; -Converted test inputs from sam to bam; -Cleared all of travis's caches. This may be purely a travis issue or a flaw in the test itself, or it may indicate that there remain issues with `MarkDuplicatesSpark` output and ordering despite the recent fixes. Note, however, that the original test from @davidadamsphd that first detected the ordering issue with MD on spark now passes, as do the new `ReadsSparkSink` ordering tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1258
https://github.com/broadinstitute/gatk/issues/1258:838,Testability,test,tests,838,"`MarkDuplicatesSparkIntegrationTest.testMDOrder()` _always_ passes when run locally, but fails about 50% of the time when run in travis. As a result, it's currently disabled. Some things I've tried to fix the test on travis, without success:; -Run MD with `--parallelism 1` instead of `--parallelism 0` (which doesn't guarantee ordering at all!) -- the original version of the test was running with `--parallelism 0`; -Converted test inputs from sam to bam; -Cleared all of travis's caches. This may be purely a travis issue or a flaw in the test itself, or it may indicate that there remain issues with `MarkDuplicatesSpark` output and ordering despite the recent fixes. Note, however, that the original test from @davidadamsphd that first detected the ordering issue with MD on spark now passes, as do the new `ReadsSparkSink` ordering tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1258
https://github.com/broadinstitute/gatk/issues/1258:459,Usability,Clear,Cleared,459,"`MarkDuplicatesSparkIntegrationTest.testMDOrder()` _always_ passes when run locally, but fails about 50% of the time when run in travis. As a result, it's currently disabled. Some things I've tried to fix the test on travis, without success:; -Run MD with `--parallelism 1` instead of `--parallelism 0` (which doesn't guarantee ordering at all!) -- the original version of the test was running with `--parallelism 0`; -Converted test inputs from sam to bam; -Cleared all of travis's caches. This may be purely a travis issue or a flaw in the test itself, or it may indicate that there remain issues with `MarkDuplicatesSpark` output and ordering despite the recent fixes. Note, however, that the original test from @davidadamsphd that first detected the ordering issue with MD on spark now passes, as do the new `ReadsSparkSink` ordering tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1258
https://github.com/broadinstitute/gatk/issues/1259:324,Availability,mask,masking,324,"From @cmnbroad:. The write side for non-coordinate sorted CRAMs works now, but there is still a problem with reading queryname sorted input crams with unmapped reads. There is an existing test in GATK for this case that is already enabled and has been passing, but only because there is another bug in the test code that is masking the failure because it uses the value ""outputExtension"", which results in a BAM file being created. When I went to fix that as part of re-enabling the disabled CRAM tests, the verification of the queryname test output now fails. The test itself runs, but the samsEqualStringent verification fails reading the output. So the write-side issues with the presorted flag are fixed with samtools/htsjdk#368, but there is still a limitation in htsjdk for reading crams with unmapped reads that are not coordinate sorted that we need to fix/understand. See also:; https://github.com/broadinstitute/gatk/issues/777; https://github.com/samtools/htsjdk/issues/404",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1259
https://github.com/broadinstitute/gatk/issues/1259:336,Availability,failure,failure,336,"From @cmnbroad:. The write side for non-coordinate sorted CRAMs works now, but there is still a problem with reading queryname sorted input crams with unmapped reads. There is an existing test in GATK for this case that is already enabled and has been passing, but only because there is another bug in the test code that is masking the failure because it uses the value ""outputExtension"", which results in a BAM file being created. When I went to fix that as part of re-enabling the disabled CRAM tests, the verification of the queryname test output now fails. The test itself runs, but the samsEqualStringent verification fails reading the output. So the write-side issues with the presorted flag are fixed with samtools/htsjdk#368, but there is still a limitation in htsjdk for reading crams with unmapped reads that are not coordinate sorted that we need to fix/understand. See also:; https://github.com/broadinstitute/gatk/issues/777; https://github.com/samtools/htsjdk/issues/404",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1259
https://github.com/broadinstitute/gatk/issues/1259:188,Testability,test,test,188,"From @cmnbroad:. The write side for non-coordinate sorted CRAMs works now, but there is still a problem with reading queryname sorted input crams with unmapped reads. There is an existing test in GATK for this case that is already enabled and has been passing, but only because there is another bug in the test code that is masking the failure because it uses the value ""outputExtension"", which results in a BAM file being created. When I went to fix that as part of re-enabling the disabled CRAM tests, the verification of the queryname test output now fails. The test itself runs, but the samsEqualStringent verification fails reading the output. So the write-side issues with the presorted flag are fixed with samtools/htsjdk#368, but there is still a limitation in htsjdk for reading crams with unmapped reads that are not coordinate sorted that we need to fix/understand. See also:; https://github.com/broadinstitute/gatk/issues/777; https://github.com/samtools/htsjdk/issues/404",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1259
https://github.com/broadinstitute/gatk/issues/1259:306,Testability,test,test,306,"From @cmnbroad:. The write side for non-coordinate sorted CRAMs works now, but there is still a problem with reading queryname sorted input crams with unmapped reads. There is an existing test in GATK for this case that is already enabled and has been passing, but only because there is another bug in the test code that is masking the failure because it uses the value ""outputExtension"", which results in a BAM file being created. When I went to fix that as part of re-enabling the disabled CRAM tests, the verification of the queryname test output now fails. The test itself runs, but the samsEqualStringent verification fails reading the output. So the write-side issues with the presorted flag are fixed with samtools/htsjdk#368, but there is still a limitation in htsjdk for reading crams with unmapped reads that are not coordinate sorted that we need to fix/understand. See also:; https://github.com/broadinstitute/gatk/issues/777; https://github.com/samtools/htsjdk/issues/404",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1259
https://github.com/broadinstitute/gatk/issues/1259:497,Testability,test,tests,497,"From @cmnbroad:. The write side for non-coordinate sorted CRAMs works now, but there is still a problem with reading queryname sorted input crams with unmapped reads. There is an existing test in GATK for this case that is already enabled and has been passing, but only because there is another bug in the test code that is masking the failure because it uses the value ""outputExtension"", which results in a BAM file being created. When I went to fix that as part of re-enabling the disabled CRAM tests, the verification of the queryname test output now fails. The test itself runs, but the samsEqualStringent verification fails reading the output. So the write-side issues with the presorted flag are fixed with samtools/htsjdk#368, but there is still a limitation in htsjdk for reading crams with unmapped reads that are not coordinate sorted that we need to fix/understand. See also:; https://github.com/broadinstitute/gatk/issues/777; https://github.com/samtools/htsjdk/issues/404",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1259
https://github.com/broadinstitute/gatk/issues/1259:538,Testability,test,test,538,"From @cmnbroad:. The write side for non-coordinate sorted CRAMs works now, but there is still a problem with reading queryname sorted input crams with unmapped reads. There is an existing test in GATK for this case that is already enabled and has been passing, but only because there is another bug in the test code that is masking the failure because it uses the value ""outputExtension"", which results in a BAM file being created. When I went to fix that as part of re-enabling the disabled CRAM tests, the verification of the queryname test output now fails. The test itself runs, but the samsEqualStringent verification fails reading the output. So the write-side issues with the presorted flag are fixed with samtools/htsjdk#368, but there is still a limitation in htsjdk for reading crams with unmapped reads that are not coordinate sorted that we need to fix/understand. See also:; https://github.com/broadinstitute/gatk/issues/777; https://github.com/samtools/htsjdk/issues/404",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1259
https://github.com/broadinstitute/gatk/issues/1259:565,Testability,test,test,565,"From @cmnbroad:. The write side for non-coordinate sorted CRAMs works now, but there is still a problem with reading queryname sorted input crams with unmapped reads. There is an existing test in GATK for this case that is already enabled and has been passing, but only because there is another bug in the test code that is masking the failure because it uses the value ""outputExtension"", which results in a BAM file being created. When I went to fix that as part of re-enabling the disabled CRAM tests, the verification of the queryname test output now fails. The test itself runs, but the samsEqualStringent verification fails reading the output. So the write-side issues with the presorted flag are fixed with samtools/htsjdk#368, but there is still a limitation in htsjdk for reading crams with unmapped reads that are not coordinate sorted that we need to fix/understand. See also:; https://github.com/broadinstitute/gatk/issues/777; https://github.com/samtools/htsjdk/issues/404",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1259
https://github.com/broadinstitute/gatk/issues/1260:80,Testability,test,tests,80,SortBamSpark does not have a SORT_ORDER parameter. once this is fixed - renable tests in SortBamSparkIntegrationTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1260
https://github.com/broadinstitute/gatk/issues/1261:13,Testability,test,test,13,see disabled test in ReadsSparkSourceUnitTest.readsSparkSourceTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1261
https://github.com/broadinstitute/gatk/issues/1262:475,Availability,ERROR,ERROR,475,"spark complains but non-spark does not. ```; CollectQualityYieldMetricsSpark --output a.metrics --useOriginalQualities true --reference src/test/resources/Homo_sapiens_assembly19_chr1_1M.fasta --input src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectQualityYieldMetrics/collect_quality_yield_metrics.cram --disableSequenceDictionaryValidation false. org.broadinstitute.hellbender.exceptions.UserException$IncompatibleSequenceDictionaries: A USER ERROR has occurred: Input files reference and reads have incompatible contigs: Found contigs with the same name but different lengths:; contig reference = 1 / 1000000; contig reads = 1 / 249250621.; reference contigs = [1]; reads contigs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, X, Y, MT, GL000207.1, GL000226.1, GL000229.1, GL000231.1, GL000210.1, GL000239.1, GL000235.1, GL000201.1, GL000247.1, GL000245.1, GL000197.1, GL000203.1, GL000246.1, GL000249.1, GL000196.1, GL000248.1, GL000244.1, GL000238.1, GL000202.1, GL000234.1, GL000232.1, GL000206.1, GL000240.1, GL000236.1, GL000241.1, GL000243.1, GL000242.1, GL000230.1, GL000237.1, GL000233.1, GL000204.1, GL000198.1, GL000208.1, GL000191.1, GL000227.1, GL000228.1, GL000214.1, GL000221.1, GL000209.1, GL000218.1, GL000220.1, GL000213.1, GL000211.1, GL000199.1, GL000217.1, GL000216.1, GL000215.1, GL000205.1, GL000219.1, GL000224.1, GL000223.1, GL000195.1, GL000212.1, GL000222.1, GL000200.1, GL000193.1, GL000194.1, GL000225.1, GL000192.1, NC_007605]; ```. Once fixed, reenable 2 tests in CollectQualityYieldMetricsSparkIntegrationTest",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1262
https://github.com/broadinstitute/gatk/issues/1262:140,Testability,test,test,140,"spark complains but non-spark does not. ```; CollectQualityYieldMetricsSpark --output a.metrics --useOriginalQualities true --reference src/test/resources/Homo_sapiens_assembly19_chr1_1M.fasta --input src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectQualityYieldMetrics/collect_quality_yield_metrics.cram --disableSequenceDictionaryValidation false. org.broadinstitute.hellbender.exceptions.UserException$IncompatibleSequenceDictionaries: A USER ERROR has occurred: Input files reference and reads have incompatible contigs: Found contigs with the same name but different lengths:; contig reference = 1 / 1000000; contig reads = 1 / 249250621.; reference contigs = [1]; reads contigs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, X, Y, MT, GL000207.1, GL000226.1, GL000229.1, GL000231.1, GL000210.1, GL000239.1, GL000235.1, GL000201.1, GL000247.1, GL000245.1, GL000197.1, GL000203.1, GL000246.1, GL000249.1, GL000196.1, GL000248.1, GL000244.1, GL000238.1, GL000202.1, GL000234.1, GL000232.1, GL000206.1, GL000240.1, GL000236.1, GL000241.1, GL000243.1, GL000242.1, GL000230.1, GL000237.1, GL000233.1, GL000204.1, GL000198.1, GL000208.1, GL000191.1, GL000227.1, GL000228.1, GL000214.1, GL000221.1, GL000209.1, GL000218.1, GL000220.1, GL000213.1, GL000211.1, GL000199.1, GL000217.1, GL000216.1, GL000215.1, GL000205.1, GL000219.1, GL000224.1, GL000223.1, GL000195.1, GL000212.1, GL000222.1, GL000200.1, GL000193.1, GL000194.1, GL000225.1, GL000192.1, NC_007605]; ```. Once fixed, reenable 2 tests in CollectQualityYieldMetricsSparkIntegrationTest",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1262
https://github.com/broadinstitute/gatk/issues/1262:205,Testability,test,test,205,"spark complains but non-spark does not. ```; CollectQualityYieldMetricsSpark --output a.metrics --useOriginalQualities true --reference src/test/resources/Homo_sapiens_assembly19_chr1_1M.fasta --input src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectQualityYieldMetrics/collect_quality_yield_metrics.cram --disableSequenceDictionaryValidation false. org.broadinstitute.hellbender.exceptions.UserException$IncompatibleSequenceDictionaries: A USER ERROR has occurred: Input files reference and reads have incompatible contigs: Found contigs with the same name but different lengths:; contig reference = 1 / 1000000; contig reads = 1 / 249250621.; reference contigs = [1]; reads contigs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, X, Y, MT, GL000207.1, GL000226.1, GL000229.1, GL000231.1, GL000210.1, GL000239.1, GL000235.1, GL000201.1, GL000247.1, GL000245.1, GL000197.1, GL000203.1, GL000246.1, GL000249.1, GL000196.1, GL000248.1, GL000244.1, GL000238.1, GL000202.1, GL000234.1, GL000232.1, GL000206.1, GL000240.1, GL000236.1, GL000241.1, GL000243.1, GL000242.1, GL000230.1, GL000237.1, GL000233.1, GL000204.1, GL000198.1, GL000208.1, GL000191.1, GL000227.1, GL000228.1, GL000214.1, GL000221.1, GL000209.1, GL000218.1, GL000220.1, GL000213.1, GL000211.1, GL000199.1, GL000217.1, GL000216.1, GL000215.1, GL000205.1, GL000219.1, GL000224.1, GL000223.1, GL000195.1, GL000212.1, GL000222.1, GL000200.1, GL000193.1, GL000194.1, GL000225.1, GL000192.1, NC_007605]; ```. Once fixed, reenable 2 tests in CollectQualityYieldMetricsSparkIntegrationTest",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1262
https://github.com/broadinstitute/gatk/issues/1262:1553,Testability,test,tests,1553,"spark complains but non-spark does not. ```; CollectQualityYieldMetricsSpark --output a.metrics --useOriginalQualities true --reference src/test/resources/Homo_sapiens_assembly19_chr1_1M.fasta --input src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectQualityYieldMetrics/collect_quality_yield_metrics.cram --disableSequenceDictionaryValidation false. org.broadinstitute.hellbender.exceptions.UserException$IncompatibleSequenceDictionaries: A USER ERROR has occurred: Input files reference and reads have incompatible contigs: Found contigs with the same name but different lengths:; contig reference = 1 / 1000000; contig reads = 1 / 249250621.; reference contigs = [1]; reads contigs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, X, Y, MT, GL000207.1, GL000226.1, GL000229.1, GL000231.1, GL000210.1, GL000239.1, GL000235.1, GL000201.1, GL000247.1, GL000245.1, GL000197.1, GL000203.1, GL000246.1, GL000249.1, GL000196.1, GL000248.1, GL000244.1, GL000238.1, GL000202.1, GL000234.1, GL000232.1, GL000206.1, GL000240.1, GL000236.1, GL000241.1, GL000243.1, GL000242.1, GL000230.1, GL000237.1, GL000233.1, GL000204.1, GL000198.1, GL000208.1, GL000191.1, GL000227.1, GL000228.1, GL000214.1, GL000221.1, GL000209.1, GL000218.1, GL000220.1, GL000213.1, GL000211.1, GL000199.1, GL000217.1, GL000216.1, GL000215.1, GL000205.1, GL000219.1, GL000224.1, GL000223.1, GL000195.1, GL000212.1, GL000222.1, GL000200.1, GL000193.1, GL000194.1, GL000225.1, GL000192.1, NC_007605]; ```. Once fixed, reenable 2 tests in CollectQualityYieldMetricsSparkIntegrationTest",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1262
https://github.com/broadinstitute/gatk/issues/1266:228,Availability,error,errors,228,"The result of this bug is that if you produce a BAM with Spark, then produce a different BAM with the same name with the walker-framework, then try to read _that_ bam with Spark, it produces checksum (and other more misleading) errors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1266
https://github.com/broadinstitute/gatk/issues/1266:191,Security,checksum,checksum,191,"The result of this bug is that if you produce a BAM with Spark, then produce a different BAM with the same name with the walker-framework, then try to read _that_ bam with Spark, it produces checksum (and other more misleading) errors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1266
https://github.com/broadinstitute/gatk/issues/1267:35,Performance,load,loadReadsADAM,35,see test in ReadsSparkSinkUnitTest.loadReadsADAM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1267
https://github.com/broadinstitute/gatk/issues/1267:4,Testability,test,test,4,see test in ReadsSparkSinkUnitTest.loadReadsADAM,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1267
https://github.com/broadinstitute/gatk/issues/1269:176,Testability,test,tests,176,"came up in review of https://github.com/broadinstitute/gatk/pull/1268; (on spark, we were writing files named '.cram' that were in fact just bam files). We need to add code to tests that write cram files that checks that the right format is written).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1269
https://github.com/broadinstitute/gatk/issues/1270:242,Testability,test,tests,242,writing cram files on spark is not implemented yet and it'll throw when user attempts is (as per review of https://github.com/broadinstitute/gatk/pull/1268). The task for this ticket is to implement/enable writing cram files on spark and add tests for it (do check that the results are in fact cram files - see https://github.com/broadinstitute/gatk/issues/1269),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1270
https://github.com/broadinstitute/gatk/issues/1271:11,Testability,test,test,11,printreads test using `print_reads.sorted.queryname.sam` and printing to `print_reads.sorted.queryname.cram`. it may be a htsjdk bug. ```; htsjdk.samtools.util.RuntimeEOFException: java.lang.RuntimeException: Slice mapped outside of the reference. at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:270); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:568); at htsjdk.samtools.util.PeekIterator.hasNext(PeekIterator.java:45); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.skipAnyNotprimary(SecondaryOrSupplementarySkippingIterator.java:36); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.advance(SecondaryOrSupplementarySkippingIterator.java:31); at org.broadinstitute.hellbender.utils.read.SamComparison.compareQueryNameSortedAlignments(SamComparison.java:218); at org.broadinstitute.hellbender.utils.read.SamComparison.compareAlignments(SamComparison.java:70); at org.broadinstitute.hellbender.utils.read.SamComparison.<init>(SamComparison.java:44); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.samsEqualLenient(SamAssertionUtils.java:144); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.assertSamsEqualLenient(SamAssertionUtils.java:48); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.assertSamsEqualLenient(SamAssertionUtils.java:92); at org.broadinstitute.hellbender.tools.PrintReadsIntegrationTest.testFileToFileCRAM(PrintReadsIntegrationTest.java:50); ...; Caused by: java.lang.RuntimeException: Slice mapped outside of the reference.; at htsjdk.samtools.cram.structure.Slice.alignmentBordersSanityCheck(Slice.java:74); at htsjdk.samtools.cram.structure.Slice.validateRefMD5(Slice.java:84); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:185); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:263); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1271
https://github.com/broadinstitute/gatk/issues/1271:341,Testability,Assert,AssertingIterator,341,printreads test using `print_reads.sorted.queryname.sam` and printing to `print_reads.sorted.queryname.cram`. it may be a htsjdk bug. ```; htsjdk.samtools.util.RuntimeEOFException: java.lang.RuntimeException: Slice mapped outside of the reference. at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:270); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:568); at htsjdk.samtools.util.PeekIterator.hasNext(PeekIterator.java:45); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.skipAnyNotprimary(SecondaryOrSupplementarySkippingIterator.java:36); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.advance(SecondaryOrSupplementarySkippingIterator.java:31); at org.broadinstitute.hellbender.utils.read.SamComparison.compareQueryNameSortedAlignments(SamComparison.java:218); at org.broadinstitute.hellbender.utils.read.SamComparison.compareAlignments(SamComparison.java:70); at org.broadinstitute.hellbender.utils.read.SamComparison.<init>(SamComparison.java:44); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.samsEqualLenient(SamAssertionUtils.java:144); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.assertSamsEqualLenient(SamAssertionUtils.java:48); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.assertSamsEqualLenient(SamAssertionUtils.java:92); at org.broadinstitute.hellbender.tools.PrintReadsIntegrationTest.testFileToFileCRAM(PrintReadsIntegrationTest.java:50); ...; Caused by: java.lang.RuntimeException: Slice mapped outside of the reference.; at htsjdk.samtools.cram.structure.Slice.alignmentBordersSanityCheck(Slice.java:74); at htsjdk.samtools.cram.structure.Slice.validateRefMD5(Slice.java:84); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:185); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:263); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1271
https://github.com/broadinstitute/gatk/issues/1271:1048,Testability,test,test,1048,printreads test using `print_reads.sorted.queryname.sam` and printing to `print_reads.sorted.queryname.cram`. it may be a htsjdk bug. ```; htsjdk.samtools.util.RuntimeEOFException: java.lang.RuntimeException: Slice mapped outside of the reference. at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:270); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:568); at htsjdk.samtools.util.PeekIterator.hasNext(PeekIterator.java:45); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.skipAnyNotprimary(SecondaryOrSupplementarySkippingIterator.java:36); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.advance(SecondaryOrSupplementarySkippingIterator.java:31); at org.broadinstitute.hellbender.utils.read.SamComparison.compareQueryNameSortedAlignments(SamComparison.java:218); at org.broadinstitute.hellbender.utils.read.SamComparison.compareAlignments(SamComparison.java:70); at org.broadinstitute.hellbender.utils.read.SamComparison.<init>(SamComparison.java:44); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.samsEqualLenient(SamAssertionUtils.java:144); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.assertSamsEqualLenient(SamAssertionUtils.java:48); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.assertSamsEqualLenient(SamAssertionUtils.java:92); at org.broadinstitute.hellbender.tools.PrintReadsIntegrationTest.testFileToFileCRAM(PrintReadsIntegrationTest.java:50); ...; Caused by: java.lang.RuntimeException: Slice mapped outside of the reference.; at htsjdk.samtools.cram.structure.Slice.alignmentBordersSanityCheck(Slice.java:74); at htsjdk.samtools.cram.structure.Slice.validateRefMD5(Slice.java:84); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:185); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:263); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1271
https://github.com/broadinstitute/gatk/issues/1271:1156,Testability,test,test,1156,printreads test using `print_reads.sorted.queryname.sam` and printing to `print_reads.sorted.queryname.cram`. it may be a htsjdk bug. ```; htsjdk.samtools.util.RuntimeEOFException: java.lang.RuntimeException: Slice mapped outside of the reference. at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:270); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:568); at htsjdk.samtools.util.PeekIterator.hasNext(PeekIterator.java:45); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.skipAnyNotprimary(SecondaryOrSupplementarySkippingIterator.java:36); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.advance(SecondaryOrSupplementarySkippingIterator.java:31); at org.broadinstitute.hellbender.utils.read.SamComparison.compareQueryNameSortedAlignments(SamComparison.java:218); at org.broadinstitute.hellbender.utils.read.SamComparison.compareAlignments(SamComparison.java:70); at org.broadinstitute.hellbender.utils.read.SamComparison.<init>(SamComparison.java:44); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.samsEqualLenient(SamAssertionUtils.java:144); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.assertSamsEqualLenient(SamAssertionUtils.java:48); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.assertSamsEqualLenient(SamAssertionUtils.java:92); at org.broadinstitute.hellbender.tools.PrintReadsIntegrationTest.testFileToFileCRAM(PrintReadsIntegrationTest.java:50); ...; Caused by: java.lang.RuntimeException: Slice mapped outside of the reference.; at htsjdk.samtools.cram.structure.Slice.alignmentBordersSanityCheck(Slice.java:74); at htsjdk.samtools.cram.structure.Slice.validateRefMD5(Slice.java:84); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:185); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:263); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1271
https://github.com/broadinstitute/gatk/issues/1271:1179,Testability,assert,assertSamsEqualLenient,1179,printreads test using `print_reads.sorted.queryname.sam` and printing to `print_reads.sorted.queryname.cram`. it may be a htsjdk bug. ```; htsjdk.samtools.util.RuntimeEOFException: java.lang.RuntimeException: Slice mapped outside of the reference. at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:270); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:568); at htsjdk.samtools.util.PeekIterator.hasNext(PeekIterator.java:45); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.skipAnyNotprimary(SecondaryOrSupplementarySkippingIterator.java:36); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.advance(SecondaryOrSupplementarySkippingIterator.java:31); at org.broadinstitute.hellbender.utils.read.SamComparison.compareQueryNameSortedAlignments(SamComparison.java:218); at org.broadinstitute.hellbender.utils.read.SamComparison.compareAlignments(SamComparison.java:70); at org.broadinstitute.hellbender.utils.read.SamComparison.<init>(SamComparison.java:44); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.samsEqualLenient(SamAssertionUtils.java:144); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.assertSamsEqualLenient(SamAssertionUtils.java:48); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.assertSamsEqualLenient(SamAssertionUtils.java:92); at org.broadinstitute.hellbender.tools.PrintReadsIntegrationTest.testFileToFileCRAM(PrintReadsIntegrationTest.java:50); ...; Caused by: java.lang.RuntimeException: Slice mapped outside of the reference.; at htsjdk.samtools.cram.structure.Slice.alignmentBordersSanityCheck(Slice.java:74); at htsjdk.samtools.cram.structure.Slice.validateRefMD5(Slice.java:84); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:185); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:263); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1271
https://github.com/broadinstitute/gatk/issues/1271:1269,Testability,test,test,1269,printreads test using `print_reads.sorted.queryname.sam` and printing to `print_reads.sorted.queryname.cram`. it may be a htsjdk bug. ```; htsjdk.samtools.util.RuntimeEOFException: java.lang.RuntimeException: Slice mapped outside of the reference. at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:270); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:568); at htsjdk.samtools.util.PeekIterator.hasNext(PeekIterator.java:45); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.skipAnyNotprimary(SecondaryOrSupplementarySkippingIterator.java:36); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.advance(SecondaryOrSupplementarySkippingIterator.java:31); at org.broadinstitute.hellbender.utils.read.SamComparison.compareQueryNameSortedAlignments(SamComparison.java:218); at org.broadinstitute.hellbender.utils.read.SamComparison.compareAlignments(SamComparison.java:70); at org.broadinstitute.hellbender.utils.read.SamComparison.<init>(SamComparison.java:44); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.samsEqualLenient(SamAssertionUtils.java:144); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.assertSamsEqualLenient(SamAssertionUtils.java:48); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.assertSamsEqualLenient(SamAssertionUtils.java:92); at org.broadinstitute.hellbender.tools.PrintReadsIntegrationTest.testFileToFileCRAM(PrintReadsIntegrationTest.java:50); ...; Caused by: java.lang.RuntimeException: Slice mapped outside of the reference.; at htsjdk.samtools.cram.structure.Slice.alignmentBordersSanityCheck(Slice.java:74); at htsjdk.samtools.cram.structure.Slice.validateRefMD5(Slice.java:84); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:185); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:263); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1271
https://github.com/broadinstitute/gatk/issues/1271:1292,Testability,assert,assertSamsEqualLenient,1292,printreads test using `print_reads.sorted.queryname.sam` and printing to `print_reads.sorted.queryname.cram`. it may be a htsjdk bug. ```; htsjdk.samtools.util.RuntimeEOFException: java.lang.RuntimeException: Slice mapped outside of the reference. at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:270); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:568); at htsjdk.samtools.util.PeekIterator.hasNext(PeekIterator.java:45); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.skipAnyNotprimary(SecondaryOrSupplementarySkippingIterator.java:36); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.advance(SecondaryOrSupplementarySkippingIterator.java:31); at org.broadinstitute.hellbender.utils.read.SamComparison.compareQueryNameSortedAlignments(SamComparison.java:218); at org.broadinstitute.hellbender.utils.read.SamComparison.compareAlignments(SamComparison.java:70); at org.broadinstitute.hellbender.utils.read.SamComparison.<init>(SamComparison.java:44); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.samsEqualLenient(SamAssertionUtils.java:144); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.assertSamsEqualLenient(SamAssertionUtils.java:48); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.assertSamsEqualLenient(SamAssertionUtils.java:92); at org.broadinstitute.hellbender.tools.PrintReadsIntegrationTest.testFileToFileCRAM(PrintReadsIntegrationTest.java:50); ...; Caused by: java.lang.RuntimeException: Slice mapped outside of the reference.; at htsjdk.samtools.cram.structure.Slice.alignmentBordersSanityCheck(Slice.java:74); at htsjdk.samtools.cram.structure.Slice.validateRefMD5(Slice.java:84); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:185); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:263); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1271
https://github.com/broadinstitute/gatk/issues/1271:1408,Testability,test,testFileToFileCRAM,1408,printreads test using `print_reads.sorted.queryname.sam` and printing to `print_reads.sorted.queryname.cram`. it may be a htsjdk bug. ```; htsjdk.samtools.util.RuntimeEOFException: java.lang.RuntimeException: Slice mapped outside of the reference. at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:270); at htsjdk.samtools.SamReader$AssertingIterator.hasNext(SamReader.java:568); at htsjdk.samtools.util.PeekIterator.hasNext(PeekIterator.java:45); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.skipAnyNotprimary(SecondaryOrSupplementarySkippingIterator.java:36); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.advance(SecondaryOrSupplementarySkippingIterator.java:31); at org.broadinstitute.hellbender.utils.read.SamComparison.compareQueryNameSortedAlignments(SamComparison.java:218); at org.broadinstitute.hellbender.utils.read.SamComparison.compareAlignments(SamComparison.java:70); at org.broadinstitute.hellbender.utils.read.SamComparison.<init>(SamComparison.java:44); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.samsEqualLenient(SamAssertionUtils.java:144); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.assertSamsEqualLenient(SamAssertionUtils.java:48); at org.broadinstitute.hellbender.utils.test.SamAssertionUtils.assertSamsEqualLenient(SamAssertionUtils.java:92); at org.broadinstitute.hellbender.tools.PrintReadsIntegrationTest.testFileToFileCRAM(PrintReadsIntegrationTest.java:50); ...; Caused by: java.lang.RuntimeException: Slice mapped outside of the reference.; at htsjdk.samtools.cram.structure.Slice.alignmentBordersSanityCheck(Slice.java:74); at htsjdk.samtools.cram.structure.Slice.validateRefMD5(Slice.java:84); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:185); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:263); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1271
https://github.com/broadinstitute/gatk/issues/1272:97,Availability,error,errors,97,Picard tools don't perform validation of the sequence dictionary which will occasionally lead to errors. They should implement the same checking as the rest of our tools,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1272
https://github.com/broadinstitute/gatk/issues/1272:19,Performance,perform,perform,19,Picard tools don't perform validation of the sequence dictionary which will occasionally lead to errors. They should implement the same checking as the rest of our tools,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1272
https://github.com/broadinstitute/gatk/issues/1272:27,Security,validat,validation,27,Picard tools don't perform validation of the sequence dictionary which will occasionally lead to errors. They should implement the same checking as the rest of our tools,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1272
https://github.com/broadinstitute/gatk/pull/1276:141,Energy Efficiency,adapt,adapt,141,"Instead of calling setHeader() to temporarily give headerless reads; a header and then calling into htsjdk's SAMRecordCoordinateComparator,; adapt the htsjdk code directly to work with headerless reads. This should; be safer (especially in a multithreaded context), as mutating the objects; being compared within a comparator is a violation of contract.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1276
https://github.com/broadinstitute/gatk/pull/1276:344,Integrability,contract,contract,344,"Instead of calling setHeader() to temporarily give headerless reads; a header and then calling into htsjdk's SAMRecordCoordinateComparator,; adapt the htsjdk code directly to work with headerless reads. This should; be safer (especially in a multithreaded context), as mutating the objects; being compared within a comparator is a violation of contract.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1276
https://github.com/broadinstitute/gatk/pull/1276:141,Modifiability,adapt,adapt,141,"Instead of calling setHeader() to temporarily give headerless reads; a header and then calling into htsjdk's SAMRecordCoordinateComparator,; adapt the htsjdk code directly to work with headerless reads. This should; be safer (especially in a multithreaded context), as mutating the objects; being compared within a comparator is a violation of contract.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1276
https://github.com/broadinstitute/gatk/pull/1276:219,Safety,safe,safer,219,"Instead of calling setHeader() to temporarily give headerless reads; a header and then calling into htsjdk's SAMRecordCoordinateComparator,; adapt the htsjdk code directly to work with headerless reads. This should; be safer (especially in a multithreaded context), as mutating the objects; being compared within a comparator is a violation of contract.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1276
https://github.com/broadinstitute/gatk/issues/1280:2684,Deployability,pipeline,pipelines,2684,tute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:264); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunn,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:559,Modifiability,layers,layers,559,"And probably other kinds of files too. The stack below results from handing it a .ADAM file in the MeanQualityByCycleSparkIntegrationTest.test_ADAM test. The ReadSparkSource code is currently delegating to Hadoop-BAM, which is in turn delegating to promiscuous htsjdk code that says anything that doesn't look like known file type [must be a SAM file](https://github.com/samtools/htsjdk/blob/bd92747fd3672635de96473fea6d4b38e8635c8e/src/java/htsjdk/samtools/SAMFileReader.java#L754). It then happily creates a bogus SAMFileHeader from the .ADAM stream. All 3 layers should probably be more discriminating. This currently doesn't break any tests. I discovered it when running the HB tests against a local version of htsjdk with a strict setHeader implementation that attempts to resolve all reference names on every setHeader call. That code caused this test to fail because its using the bogus header. ""main@1"" prio=5 tid=0x1 nid=NA runnable; java.lang.Thread.State: RUNNABLE; at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:113); at htsjdk.samtools.SAMTextReader.readHeader(SAMTextReader.java:200); at htsjdk.samtools.SAMTextReader.<init>(SAMTextReader.java:63); at htsjdk.samtools.SAMTextReader.<init>(SAMTextReader.java:73); at htsjdk.samtools.SAMFileReader.init(SAMFileReader.java:684); at htsjdk.samtools.SAMFileReader.<init>(SAMFileReader.java:148); at org.seqdoop.hadoop_bam.util.SAMHeaderReader.readSAMHeaderFrom(SAMHeaderReader.java:66); at org.seqdoop.hadoop_bam.util.SAMHeaderReader.readSAMHeaderFrom(SAMHeaderReader.java:47); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:195); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:264); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:255); at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:148,Testability,test,test,148,"And probably other kinds of files too. The stack below results from handing it a .ADAM file in the MeanQualityByCycleSparkIntegrationTest.test_ADAM test. The ReadSparkSource code is currently delegating to Hadoop-BAM, which is in turn delegating to promiscuous htsjdk code that says anything that doesn't look like known file type [must be a SAM file](https://github.com/samtools/htsjdk/blob/bd92747fd3672635de96473fea6d4b38e8635c8e/src/java/htsjdk/samtools/SAMFileReader.java#L754). It then happily creates a bogus SAMFileHeader from the .ADAM stream. All 3 layers should probably be more discriminating. This currently doesn't break any tests. I discovered it when running the HB tests against a local version of htsjdk with a strict setHeader implementation that attempts to resolve all reference names on every setHeader call. That code caused this test to fail because its using the bogus header. ""main@1"" prio=5 tid=0x1 nid=NA runnable; java.lang.Thread.State: RUNNABLE; at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:113); at htsjdk.samtools.SAMTextReader.readHeader(SAMTextReader.java:200); at htsjdk.samtools.SAMTextReader.<init>(SAMTextReader.java:63); at htsjdk.samtools.SAMTextReader.<init>(SAMTextReader.java:73); at htsjdk.samtools.SAMFileReader.init(SAMFileReader.java:684); at htsjdk.samtools.SAMFileReader.<init>(SAMFileReader.java:148); at org.seqdoop.hadoop_bam.util.SAMHeaderReader.readSAMHeaderFrom(SAMHeaderReader.java:66); at org.seqdoop.hadoop_bam.util.SAMHeaderReader.readSAMHeaderFrom(SAMHeaderReader.java:47); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:195); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:264); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:255); at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:639,Testability,test,tests,639,"And probably other kinds of files too. The stack below results from handing it a .ADAM file in the MeanQualityByCycleSparkIntegrationTest.test_ADAM test. The ReadSparkSource code is currently delegating to Hadoop-BAM, which is in turn delegating to promiscuous htsjdk code that says anything that doesn't look like known file type [must be a SAM file](https://github.com/samtools/htsjdk/blob/bd92747fd3672635de96473fea6d4b38e8635c8e/src/java/htsjdk/samtools/SAMFileReader.java#L754). It then happily creates a bogus SAMFileHeader from the .ADAM stream. All 3 layers should probably be more discriminating. This currently doesn't break any tests. I discovered it when running the HB tests against a local version of htsjdk with a strict setHeader implementation that attempts to resolve all reference names on every setHeader call. That code caused this test to fail because its using the bogus header. ""main@1"" prio=5 tid=0x1 nid=NA runnable; java.lang.Thread.State: RUNNABLE; at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:113); at htsjdk.samtools.SAMTextReader.readHeader(SAMTextReader.java:200); at htsjdk.samtools.SAMTextReader.<init>(SAMTextReader.java:63); at htsjdk.samtools.SAMTextReader.<init>(SAMTextReader.java:73); at htsjdk.samtools.SAMFileReader.init(SAMFileReader.java:684); at htsjdk.samtools.SAMFileReader.<init>(SAMFileReader.java:148); at org.seqdoop.hadoop_bam.util.SAMHeaderReader.readSAMHeaderFrom(SAMHeaderReader.java:66); at org.seqdoop.hadoop_bam.util.SAMHeaderReader.readSAMHeaderFrom(SAMHeaderReader.java:47); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:195); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:264); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:255); at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:682,Testability,test,tests,682,"And probably other kinds of files too. The stack below results from handing it a .ADAM file in the MeanQualityByCycleSparkIntegrationTest.test_ADAM test. The ReadSparkSource code is currently delegating to Hadoop-BAM, which is in turn delegating to promiscuous htsjdk code that says anything that doesn't look like known file type [must be a SAM file](https://github.com/samtools/htsjdk/blob/bd92747fd3672635de96473fea6d4b38e8635c8e/src/java/htsjdk/samtools/SAMFileReader.java#L754). It then happily creates a bogus SAMFileHeader from the .ADAM stream. All 3 layers should probably be more discriminating. This currently doesn't break any tests. I discovered it when running the HB tests against a local version of htsjdk with a strict setHeader implementation that attempts to resolve all reference names on every setHeader call. That code caused this test to fail because its using the bogus header. ""main@1"" prio=5 tid=0x1 nid=NA runnable; java.lang.Thread.State: RUNNABLE; at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:113); at htsjdk.samtools.SAMTextReader.readHeader(SAMTextReader.java:200); at htsjdk.samtools.SAMTextReader.<init>(SAMTextReader.java:63); at htsjdk.samtools.SAMTextReader.<init>(SAMTextReader.java:73); at htsjdk.samtools.SAMFileReader.init(SAMFileReader.java:684); at htsjdk.samtools.SAMFileReader.<init>(SAMFileReader.java:148); at org.seqdoop.hadoop_bam.util.SAMHeaderReader.readSAMHeaderFrom(SAMHeaderReader.java:66); at org.seqdoop.hadoop_bam.util.SAMHeaderReader.readSAMHeaderFrom(SAMHeaderReader.java:47); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:195); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:264); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:255); at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:853,Testability,test,test,853,"And probably other kinds of files too. The stack below results from handing it a .ADAM file in the MeanQualityByCycleSparkIntegrationTest.test_ADAM test. The ReadSparkSource code is currently delegating to Hadoop-BAM, which is in turn delegating to promiscuous htsjdk code that says anything that doesn't look like known file type [must be a SAM file](https://github.com/samtools/htsjdk/blob/bd92747fd3672635de96473fea6d4b38e8635c8e/src/java/htsjdk/samtools/SAMFileReader.java#L754). It then happily creates a bogus SAMFileHeader from the .ADAM stream. All 3 layers should probably be more discriminating. This currently doesn't break any tests. I discovered it when running the HB tests against a local version of htsjdk with a strict setHeader implementation that attempts to resolve all reference names on every setHeader call. That code caused this test to fail because its using the bogus header. ""main@1"" prio=5 tid=0x1 nid=NA runnable; java.lang.Thread.State: RUNNABLE; at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:113); at htsjdk.samtools.SAMTextReader.readHeader(SAMTextReader.java:200); at htsjdk.samtools.SAMTextReader.<init>(SAMTextReader.java:63); at htsjdk.samtools.SAMTextReader.<init>(SAMTextReader.java:73); at htsjdk.samtools.SAMFileReader.init(SAMFileReader.java:684); at htsjdk.samtools.SAMFileReader.<init>(SAMFileReader.java:148); at org.seqdoop.hadoop_bam.util.SAMHeaderReader.readSAMHeaderFrom(SAMHeaderReader.java:66); at org.seqdoop.hadoop_bam.util.SAMHeaderReader.readSAMHeaderFrom(SAMHeaderReader.java:47); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:195); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:264); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:255); at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3115,Testability,test,testng,3115,r.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.ru,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3207,Testability,test,testng,3207,bender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3270,Testability,test,testng,3270,mandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.R,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3337,Testability,test,testng,3337,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3406,Testability,test,testng,3406,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3422,Testability,Test,TestMethodWorker,3422,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3457,Testability,Test,TestMethodWorker,3457,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3492,Testability,test,testng,3492,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3508,Testability,Test,TestMethodWorker,3508,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3529,Testability,Test,TestMethodWorker,3529,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3564,Testability,test,testng,3564,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3571,Testability,Test,TestRunner,3571,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3593,Testability,Test,TestRunner,3593,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3622,Testability,test,testng,3622,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3629,Testability,Test,TestRunner,3629,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3644,Testability,Test,TestRunner,3644,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3673,Testability,test,testng,3673,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3730,Testability,test,testng,3730,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3795,Testability,test,testng,3795,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3855,Testability,test,testng,3855,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3908,Testability,test,testng,3908,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:3977,Testability,test,testng,3977,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:4041,Testability,test,testng,4041,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:4048,Testability,Test,TestNG,4048,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:4077,Testability,Test,TestNG,4077,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:4103,Testability,test,testng,4103,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:4110,Testability,Test,TestNG,4110,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:4134,Testability,Test,TestNG,4134,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:4160,Testability,test,testng,4160,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:4167,Testability,Test,TestNG,4167,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:4178,Testability,Test,TestNG,4178,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:4204,Testability,test,testng,4204,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1280:4266,Testability,test,testng,4266,ender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280
https://github.com/broadinstitute/gatk/issues/1282:199,Deployability,pipeline,pipelines,199,"The output file (`-O`/`--output`) is optional, but if not set you get a NPE:. ```; java.lang.NullPointerException; at java.io.File.<init>(File.java:277); at org.broadinstitute.hellbender.tools.spark.pipelines.FlagStatSpark.runTool(FlagStatSpark.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); ```. This is the command I ran:. ```; ./gatk-launch FlagStatSpark \; --input hdfs:///user/$USER/bam/CEUTrio.HiSeq.WGS.b37.ch1.1m-65m.NA12878.bam \; -bps 134217728 \; -- \; --sparkRunner SUBMIT --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1282
https://github.com/broadinstitute/gatk/issues/1282:1212,Deployability,deploy,deploy,1212,"The output file (`-O`/`--output`) is optional, but if not set you get a NPE:. ```; java.lang.NullPointerException; at java.io.File.<init>(File.java:277); at org.broadinstitute.hellbender.tools.spark.pipelines.FlagStatSpark.runTool(FlagStatSpark.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); ```. This is the command I ran:. ```; ./gatk-launch FlagStatSpark \; --input hdfs:///user/$USER/bam/CEUTrio.HiSeq.WGS.b37.ch1.1m-65m.NA12878.bam \; -bps 134217728 \; -- \; --sparkRunner SUBMIT --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1282
https://github.com/broadinstitute/gatk/issues/1282:1249,Deployability,deploy,deploy,1249,"The output file (`-O`/`--output`) is optional, but if not set you get a NPE:. ```; java.lang.NullPointerException; at java.io.File.<init>(File.java:277); at org.broadinstitute.hellbender.tools.spark.pipelines.FlagStatSpark.runTool(FlagStatSpark.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); ```. This is the command I ran:. ```; ./gatk-launch FlagStatSpark \; --input hdfs:///user/$USER/bam/CEUTrio.HiSeq.WGS.b37.ch1.1m-65m.NA12878.bam \; -bps 134217728 \; -- \; --sparkRunner SUBMIT --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1282
https://github.com/broadinstitute/gatk/issues/1282:1321,Deployability,deploy,deploy,1321,"The output file (`-O`/`--output`) is optional, but if not set you get a NPE:. ```; java.lang.NullPointerException; at java.io.File.<init>(File.java:277); at org.broadinstitute.hellbender.tools.spark.pipelines.FlagStatSpark.runTool(FlagStatSpark.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); ```. This is the command I ran:. ```; ./gatk-launch FlagStatSpark \; --input hdfs:///user/$USER/bam/CEUTrio.HiSeq.WGS.b37.ch1.1m-65m.NA12878.bam \; -bps 134217728 \; -- \; --sparkRunner SUBMIT --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1282
https://github.com/broadinstitute/gatk/issues/1282:1397,Deployability,deploy,deploy,1397,"The output file (`-O`/`--output`) is optional, but if not set you get a NPE:. ```; java.lang.NullPointerException; at java.io.File.<init>(File.java:277); at org.broadinstitute.hellbender.tools.spark.pipelines.FlagStatSpark.runTool(FlagStatSpark.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); ```. This is the command I ran:. ```; ./gatk-launch FlagStatSpark \; --input hdfs:///user/$USER/bam/CEUTrio.HiSeq.WGS.b37.ch1.1m-65m.NA12878.bam \; -bps 134217728 \; -- \; --sparkRunner SUBMIT --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1282
https://github.com/broadinstitute/gatk/issues/1282:1468,Deployability,deploy,deploy,1468,"The output file (`-O`/`--output`) is optional, but if not set you get a NPE:. ```; java.lang.NullPointerException; at java.io.File.<init>(File.java:277); at org.broadinstitute.hellbender.tools.spark.pipelines.FlagStatSpark.runTool(FlagStatSpark.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); ```. This is the command I ran:. ```; ./gatk-launch FlagStatSpark \; --input hdfs:///user/$USER/bam/CEUTrio.HiSeq.WGS.b37.ch1.1m-65m.NA12878.bam \; -bps 134217728 \; -- \; --sparkRunner SUBMIT --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 14 \; --executor-cores 1 \; --executor-memory 3G; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1282
https://github.com/broadinstitute/gatk/issues/1285:219,Availability,ERROR,ERROR,219,```; ./gatk-launch FlagStat; ```. gives me this. ```; Running:; /local/dev/akiezun/alphaTesting/gatk/build/install/gatk/bin/gatk FlagStat; ***********************************************************************. A USER ERROR has occurred: Invalid command line: Argument input was missing: Argument 'input' must be specified at least once.; ```. it needs to add something about -h to see all arguments,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1285
https://github.com/broadinstitute/gatk/issues/1285:107,Deployability,install,install,107,```; ./gatk-launch FlagStat; ```. gives me this. ```; Running:; /local/dev/akiezun/alphaTesting/gatk/build/install/gatk/bin/gatk FlagStat; ***********************************************************************. A USER ERROR has occurred: Invalid command line: Argument input was missing: Argument 'input' must be specified at least once.; ```. it needs to add something about -h to see all arguments,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1285
https://github.com/broadinstitute/gatk/issues/1286:8,Usability,clear,clear,8,"Its not clear what the distinction is between ""Tool"" and ""GATKTool"", or ""toolsArgs"" vs ""GATK arg 1"".",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1286
https://github.com/broadinstitute/gatk/issues/1288:81,Availability,ERROR,ERROR,81,```; ./gatk-launch FlagStat --input ../HCC1143_BL.cram; ```. prints. ```; A USER ERROR has occurred: A reference file is required when using CRAM files.; ```. two problems:; 1 `USER ERROR` is super unfriendly; 2 it should say what argument to use to pass in a reference,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1288
https://github.com/broadinstitute/gatk/issues/1288:182,Availability,ERROR,ERROR,182,```; ./gatk-launch FlagStat --input ../HCC1143_BL.cram; ```. prints. ```; A USER ERROR has occurred: A reference file is required when using CRAM files.; ```. two problems:; 1 `USER ERROR` is super unfriendly; 2 it should say what argument to use to pass in a reference,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1288
https://github.com/broadinstitute/gatk/issues/1289:31,Availability,ERROR,ERROR,31,"rather than this:. ```; A USER ERROR has occurred: Input files reference and reads have incompatible contigs: Dictionary reference is missing contigs found in dictionary reads.; reference contigs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, X, Y, MT, GL000207.1, GL000226.1, GL000229.1, GL000231.1, GL000210.1, GL000239.1, GL000235.1, GL000201.1, GL000247.1, GL000245.1, GL000197.1, GL000203.1, GL000246.1, GL000249.1, GL000196.1, GL000248.1, GL000244.1, GL000238.1, GL000202.1, GL000234.1, GL000232.1, GL000206.1, GL000240.1, GL000236.1, GL000241.1, GL000243.1, GL000242.1, GL000230.1, GL000237.1, GL000233.1, GL000204.1, GL000198.1, GL000208.1, GL000191.1, GL000227.1, GL000228.1, GL000214.1, GL000221.1, GL000209.1, GL000218.1, GL000220.1, GL000213.1, GL000211.1, GL000199.1, GL000217.1, GL000216.1, GL000215.1, GL000205.1, GL000219.1, GL000224.1, GL000223.1, GL000195.1, GL000212.1, GL000222.1, GL000200.1, GL000193.1, GL000194.1, GL000225.1, GL000192.1]; reads contigs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, X, Y, MT, GL000207.1, GL000226.1, GL000229.1, GL000231.1, GL000210.1, GL000239.1, GL000235.1, GL000201.1, GL000247.1, GL000245.1, GL000197.1, GL000203.1, GL000246.1, GL000249.1, GL000196.1, GL000248.1, GL000244.1, GL000238.1, GL000202.1, GL000234.1, GL000232.1, GL000206.1, GL000240.1, GL000236.1, GL000241.1, GL000243.1, GL000242.1, GL000230.1, GL000237.1, GL000233.1, GL000204.1, GL000198.1, GL000208.1, GL000191.1, GL000227.1, GL000228.1, GL000214.1, GL000221.1, GL000209.1, GL000218.1, GL000220.1, GL000213.1, GL000211.1, GL000199.1, GL000217.1, GL000216.1, GL000215.1, GL000205.1, GL000219.1, GL000224.1, GL000223.1, GL000195.1, GL000212.1, GL000222.1, GL000200.1, GL000193.1, GL000194.1, GL000225.1, GL000192.1, NC_007605]; ```. it would be more friendly to say `contig NC_007605 is missing from the reference`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1289
https://github.com/broadinstitute/gatk/issues/1291:281,Deployability,pipeline,pipelines,281,"Even if you know that only some tools are enabled for Spark, its not obvious how to find them. And the tool list has more than one Spark program group, which I didn't notice at first:. Spark Validation tools: Tools written in Spark to compare aspects of two different files; Spark pipelines: Pipelines that combine tools and use Apache Spark for scaling out (experimental); Spark tools: Tools that use Apache Spark for scaling out (experimental)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1291
https://github.com/broadinstitute/gatk/issues/1291:292,Deployability,Pipeline,Pipelines,292,"Even if you know that only some tools are enabled for Spark, its not obvious how to find them. And the tool list has more than one Spark program group, which I didn't notice at first:. Spark Validation tools: Tools written in Spark to compare aspects of two different files; Spark pipelines: Pipelines that combine tools and use Apache Spark for scaling out (experimental); Spark tools: Tools that use Apache Spark for scaling out (experimental)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1291
https://github.com/broadinstitute/gatk/issues/1291:191,Security,Validat,Validation,191,"Even if you know that only some tools are enabled for Spark, its not obvious how to find them. And the tool list has more than one Spark program group, which I didn't notice at first:. Spark Validation tools: Tools written in Spark to compare aspects of two different files; Spark pipelines: Pipelines that combine tools and use Apache Spark for scaling out (experimental); Spark tools: Tools that use Apache Spark for scaling out (experimental)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1291
https://github.com/broadinstitute/gatk/issues/1293:67,Availability,ERROR,ERROR,67,i get this when adding `--version` to the commandline. ```; A USER ERROR has occurred: Invalid command line: Argument secondsBetweenProgressUpdates has a bad value: --version. Problem constructing Double from the string '--version'.; ```. what does that mean? seems bogus,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1293
https://github.com/broadinstitute/gatk/issues/1294:219,Performance,load,loading,219,```; ./gatk-launch FlagStatSpark --input HCC1143_BL.bam -R -L 21; ```. looks like it's hanging for a long time after it prints. ```; IntervalArgumentCollection - Processing 48129895 bp from intervals; ```. i think it's loading the file. It would be good to see progress,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1294
https://github.com/broadinstitute/gatk/issues/1297:51,Availability,ERROR,ERROR,51,"`./gatk-launch FlagStat --input fred`. ```; A USER ERROR has occurred: Couldn't read file /local/dev/akiezun/alphaTesting/gatk/fred because Cannot read non-existent file: /local/dev/akiezun/alphaTesting/gatk/fred; ```. ""Couldn't read file because cannot read non-existent file"" ? Should be improved",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1297
https://github.com/broadinstitute/gatk/issues/1301:35,Availability,avail,available,35,`./gatk-launch --help`. prints the available tools and ends with . ```; A USER ERROR has occurred: '--help' is not a valid command.; ```. this is weird,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1301
https://github.com/broadinstitute/gatk/issues/1301:79,Availability,ERROR,ERROR,79,`./gatk-launch --help`. prints the available tools and ends with . ```; A USER ERROR has occurred: '--help' is not a valid command.; ```. this is weird,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1301
https://github.com/broadinstitute/gatk/issues/1303:13,Availability,Down,DownsampleSam,13,"`gatk-launch DownsampleSam --help`. ```; /local/dev/akiezun/alphaTesting/gatk/build/install/gatk/bin/gatk DownsampleSam --help |; org.broadinstitute.hellbender.exceptions.GATKException$CommandLineParserInternalException: [R, reference] has already been used. |; at org.broadinstitute.hellbender.cmdline.CommandLineParser.handleArgumentAnnotation(CommandLineParser.java:620) |; at org.broadinstitute.hellbender.cmdline.CommandLineParser.createArgumentDefinitions(CommandLineParser.java:149) |; at org.broadinstitute.hellbender.cmdline.CommandLineParser.<init>(CommandLineParser.java:134) |; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:186) |; at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:41) |; at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66) |; at org.broadinstitute.hellbender.Main.main(Main.java:81); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1303
https://github.com/broadinstitute/gatk/issues/1303:106,Availability,Down,DownsampleSam,106,"`gatk-launch DownsampleSam --help`. ```; /local/dev/akiezun/alphaTesting/gatk/build/install/gatk/bin/gatk DownsampleSam --help |; org.broadinstitute.hellbender.exceptions.GATKException$CommandLineParserInternalException: [R, reference] has already been used. |; at org.broadinstitute.hellbender.cmdline.CommandLineParser.handleArgumentAnnotation(CommandLineParser.java:620) |; at org.broadinstitute.hellbender.cmdline.CommandLineParser.createArgumentDefinitions(CommandLineParser.java:149) |; at org.broadinstitute.hellbender.cmdline.CommandLineParser.<init>(CommandLineParser.java:134) |; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:186) |; at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:41) |; at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66) |; at org.broadinstitute.hellbender.Main.main(Main.java:81); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1303
https://github.com/broadinstitute/gatk/issues/1303:84,Deployability,install,install,84,"`gatk-launch DownsampleSam --help`. ```; /local/dev/akiezun/alphaTesting/gatk/build/install/gatk/bin/gatk DownsampleSam --help |; org.broadinstitute.hellbender.exceptions.GATKException$CommandLineParserInternalException: [R, reference] has already been used. |; at org.broadinstitute.hellbender.cmdline.CommandLineParser.handleArgumentAnnotation(CommandLineParser.java:620) |; at org.broadinstitute.hellbender.cmdline.CommandLineParser.createArgumentDefinitions(CommandLineParser.java:149) |; at org.broadinstitute.hellbender.cmdline.CommandLineParser.<init>(CommandLineParser.java:134) |; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:186) |; at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:41) |; at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66) |; at org.broadinstitute.hellbender.Main.main(Main.java:81); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1303
https://github.com/broadinstitute/gatk/issues/1304:243,Availability,error,error,243,"CountVariants and SelectVariants both read the same file successfully. gatk-launch SortVcf --input /humgen/test.filtered.maf_annotated.vcf --output sortvcf.vcf. htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv4; please use the VCF3 codec for VCFv3.3, for input source: /humgen/test.filtered.maf_annotated.vcf; at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:226); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:92); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:103); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:89); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:66); at htsjdk.variant.vcf.VCFFileReader.<init>(VCFFileReader.java:58); at org.broadinstitute.hellbender.tools.picard.vcf.SortVcf.collectFileReadersAndHeaders(SortVcf.java:92); at org.broadinstitute.hellbender.tools.picard.vcf.SortVcf.doWork(SortVcf.java:76); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:61); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); Caused by: htsjdk.tribble.TribbleException$InvalidHeader: Your input file has a malformed header: This codec is strictly for VCFv4; please use the VCF3 codec for VCFv3.3; at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:100); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:88); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:41); at htsjdk.tribble.TribbleIn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1304
https://github.com/broadinstitute/gatk/issues/1304:107,Testability,test,test,107,"CountVariants and SelectVariants both read the same file successfully. gatk-launch SortVcf --input /humgen/test.filtered.maf_annotated.vcf --output sortvcf.vcf. htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv4; please use the VCF3 codec for VCFv3.3, for input source: /humgen/test.filtered.maf_annotated.vcf; at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:226); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:92); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:103); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:89); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:66); at htsjdk.variant.vcf.VCFFileReader.<init>(VCFFileReader.java:58); at org.broadinstitute.hellbender.tools.picard.vcf.SortVcf.collectFileReadersAndHeaders(SortVcf.java:92); at org.broadinstitute.hellbender.tools.picard.vcf.SortVcf.doWork(SortVcf.java:76); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:61); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); Caused by: htsjdk.tribble.TribbleException$InvalidHeader: Your input file has a malformed header: This codec is strictly for VCFv4; please use the VCF3 codec for VCFv3.3; at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:100); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:88); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:41); at htsjdk.tribble.TribbleIn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1304
https://github.com/broadinstitute/gatk/issues/1304:389,Testability,test,test,389,"CountVariants and SelectVariants both read the same file successfully. gatk-launch SortVcf --input /humgen/test.filtered.maf_annotated.vcf --output sortvcf.vcf. htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: Your input file has a malformed header: This codec is strictly for VCFv4; please use the VCF3 codec for VCFv3.3, for input source: /humgen/test.filtered.maf_annotated.vcf; at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:226); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:92); at htsjdk.tribble.TribbleIndexedFeatureReader.<init>(TribbleIndexedFeatureReader.java:103); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:89); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:66); at htsjdk.variant.vcf.VCFFileReader.<init>(VCFFileReader.java:58); at org.broadinstitute.hellbender.tools.picard.vcf.SortVcf.collectFileReadersAndHeaders(SortVcf.java:92); at org.broadinstitute.hellbender.tools.picard.vcf.SortVcf.doWork(SortVcf.java:76); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:61); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); Caused by: htsjdk.tribble.TribbleException$InvalidHeader: Your input file has a malformed header: This codec is strictly for VCFv4; please use the VCF3 codec for VCFv3.3; at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:100); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:88); at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:41); at htsjdk.tribble.TribbleIn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1304
https://github.com/broadinstitute/gatk/issues/1305:71,Availability,ERROR,ERROR,71,`./gatk-launch CountVariants --variant some.vcf -L 21`. i get. `A USER ERROR has occurred: We currently require a sequence dictionary (from a reference or source of reads) to process intervals. This restriction may be removed in the future.`. the message should tell me what to do. References to the future are less useful than help in what argument to pass,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1305
https://github.com/broadinstitute/gatk/issues/1305:247,Integrability,message,message,247,`./gatk-launch CountVariants --variant some.vcf -L 21`. i get. `A USER ERROR has occurred: We currently require a sequence dictionary (from a reference or source of reads) to process intervals. This restriction may be removed in the future.`. the message should tell me what to do. References to the future are less useful than help in what argument to pass,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1305
https://github.com/broadinstitute/gatk/issues/1306:240,Availability,ERROR,ERROR,240,"in CountVariants -V means 'variant'. in IndexFeatureFile it seems to mean 'verbosity', see message below:. ```; ./gatk-launch IndexFeatureFile -V /xchip/cga_home/akiezun/data/dbsnp_135.b37.excluding_sites_after_129.vcf; ```. ``````; A USER ERROR has occurred: Invalid command line: Argument VERBOSITY has a bad value: /xchip/cga_home/akiezun/data/dbsnp_135.b37.excluding_sites_after_129.vcf. '/xchip/cga_home/akiezun/data/dbsnp_135.b37.excluding_sites_after_129.vcf' is not a valid value for LogLevel. Possible values: {ERROR, WARNING, INFO, DEBUG}```. ``````",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1306
https://github.com/broadinstitute/gatk/issues/1306:520,Availability,ERROR,ERROR,520,"in CountVariants -V means 'variant'. in IndexFeatureFile it seems to mean 'verbosity', see message below:. ```; ./gatk-launch IndexFeatureFile -V /xchip/cga_home/akiezun/data/dbsnp_135.b37.excluding_sites_after_129.vcf; ```. ``````; A USER ERROR has occurred: Invalid command line: Argument VERBOSITY has a bad value: /xchip/cga_home/akiezun/data/dbsnp_135.b37.excluding_sites_after_129.vcf. '/xchip/cga_home/akiezun/data/dbsnp_135.b37.excluding_sites_after_129.vcf' is not a valid value for LogLevel. Possible values: {ERROR, WARNING, INFO, DEBUG}```. ``````",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1306
https://github.com/broadinstitute/gatk/issues/1306:91,Integrability,message,message,91,"in CountVariants -V means 'variant'. in IndexFeatureFile it seems to mean 'verbosity', see message below:. ```; ./gatk-launch IndexFeatureFile -V /xchip/cga_home/akiezun/data/dbsnp_135.b37.excluding_sites_after_129.vcf; ```. ``````; A USER ERROR has occurred: Invalid command line: Argument VERBOSITY has a bad value: /xchip/cga_home/akiezun/data/dbsnp_135.b37.excluding_sites_after_129.vcf. '/xchip/cga_home/akiezun/data/dbsnp_135.b37.excluding_sites_after_129.vcf' is not a valid value for LogLevel. Possible values: {ERROR, WARNING, INFO, DEBUG}```. ``````",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1306
https://github.com/broadinstitute/gatk/issues/1306:492,Testability,Log,LogLevel,492,"in CountVariants -V means 'variant'. in IndexFeatureFile it seems to mean 'verbosity', see message below:. ```; ./gatk-launch IndexFeatureFile -V /xchip/cga_home/akiezun/data/dbsnp_135.b37.excluding_sites_after_129.vcf; ```. ``````; A USER ERROR has occurred: Invalid command line: Argument VERBOSITY has a bad value: /xchip/cga_home/akiezun/data/dbsnp_135.b37.excluding_sites_after_129.vcf. '/xchip/cga_home/akiezun/data/dbsnp_135.b37.excluding_sites_after_129.vcf' is not a valid value for LogLevel. Possible values: {ERROR, WARNING, INFO, DEBUG}```. ``````",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1306
https://github.com/broadinstitute/gatk/issues/1308:65,Safety,timeout,timeout,65,We are timing our intermittently on travis -- let's increase the timeout.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1308
https://github.com/broadinstitute/gatk/issues/1309:8,Availability,ERROR,ERROR,8,"`A USER ERROR has occurred: Invalid command line: Argument -L/-XL has a bad value: unmapped. Currently the only way to view unmapped intervals is to perform a traversal of the entire file without specifying any intervals`. ""unmapped intervals""?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1309
https://github.com/broadinstitute/gatk/issues/1309:149,Performance,perform,perform,149,"`A USER ERROR has occurred: Invalid command line: Argument -L/-XL has a bad value: unmapped. Currently the only way to view unmapped intervals is to perform a traversal of the entire file without specifying any intervals`. ""unmapped intervals""?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1309
https://github.com/broadinstitute/gatk/issues/1310:164,Availability,ERROR,ERROR,164,`/gatk-launch CountVariants -V src/test/resources/large/dbsnp_138.b37.20.21.vcf -L 20`. the input file does have a sequence dictionary yet i get this . ```; A USER ERROR has occurred: We currently require a sequence dictionary (from a reference or source of reads) to process intervals.; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1310
https://github.com/broadinstitute/gatk/issues/1310:35,Testability,test,test,35,`/gatk-launch CountVariants -V src/test/resources/large/dbsnp_138.b37.20.21.vcf -L 20`. the input file does have a sequence dictionary yet i get this . ```; A USER ERROR has occurred: We currently require a sequence dictionary (from a reference or source of reads) to process intervals.; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1310
https://github.com/broadinstitute/gatk/issues/1313:121,Integrability,message,message,121,"run `SelectVariants -select 'dbSNPBuildID=119'`. this blows up . ```; Invalid JEXL expression detected for select-0 with message ![0,18]: 'dbSNPBuildID = 119;' context is readonly; ```. which is a suboptimal message - it should point to a doc to JEXL. (btw the fix seems to be to use `==` not `=`)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1313
https://github.com/broadinstitute/gatk/issues/1313:208,Integrability,message,message,208,"run `SelectVariants -select 'dbSNPBuildID=119'`. this blows up . ```; Invalid JEXL expression detected for select-0 with message ![0,18]: 'dbSNPBuildID = 119;' context is readonly; ```. which is a suboptimal message - it should point to a doc to JEXL. (btw the fix seems to be to use `==` not `=`)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1313
https://github.com/broadinstitute/gatk/issues/1313:94,Safety,detect,detected,94,"run `SelectVariants -select 'dbSNPBuildID=119'`. this blows up . ```; Invalid JEXL expression detected for select-0 with message ![0,18]: 'dbSNPBuildID = 119;' context is readonly; ```. which is a suboptimal message - it should point to a doc to JEXL. (btw the fix seems to be to use `==` not `=`)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1313
https://github.com/broadinstitute/gatk/issues/1314:18,Deployability,install,installSpark,18,```; gradle clean installSpark; ./gatk-launch FlagStatSpark -I src/test/resources/org/broadinstitute/hellbender/tools/count_bases.bam ; Missing GATK wrapper script: /Users/droazen/src/hellbender/build/install/gatk/bin/gatk; To generate the wrapper run:. /Users/droazen/src/hellbender/gradlew installDist; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1314
https://github.com/broadinstitute/gatk/issues/1314:201,Deployability,install,install,201,```; gradle clean installSpark; ./gatk-launch FlagStatSpark -I src/test/resources/org/broadinstitute/hellbender/tools/count_bases.bam ; Missing GATK wrapper script: /Users/droazen/src/hellbender/build/install/gatk/bin/gatk; To generate the wrapper run:. /Users/droazen/src/hellbender/gradlew installDist; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1314
https://github.com/broadinstitute/gatk/issues/1314:292,Deployability,install,installDist,292,```; gradle clean installSpark; ./gatk-launch FlagStatSpark -I src/test/resources/org/broadinstitute/hellbender/tools/count_bases.bam ; Missing GATK wrapper script: /Users/droazen/src/hellbender/build/install/gatk/bin/gatk; To generate the wrapper run:. /Users/droazen/src/hellbender/gradlew installDist; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1314
https://github.com/broadinstitute/gatk/issues/1314:149,Integrability,wrap,wrapper,149,```; gradle clean installSpark; ./gatk-launch FlagStatSpark -I src/test/resources/org/broadinstitute/hellbender/tools/count_bases.bam ; Missing GATK wrapper script: /Users/droazen/src/hellbender/build/install/gatk/bin/gatk; To generate the wrapper run:. /Users/droazen/src/hellbender/gradlew installDist; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1314
https://github.com/broadinstitute/gatk/issues/1314:240,Integrability,wrap,wrapper,240,```; gradle clean installSpark; ./gatk-launch FlagStatSpark -I src/test/resources/org/broadinstitute/hellbender/tools/count_bases.bam ; Missing GATK wrapper script: /Users/droazen/src/hellbender/build/install/gatk/bin/gatk; To generate the wrapper run:. /Users/droazen/src/hellbender/gradlew installDist; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1314
https://github.com/broadinstitute/gatk/issues/1314:67,Testability,test,test,67,```; gradle clean installSpark; ./gatk-launch FlagStatSpark -I src/test/resources/org/broadinstitute/hellbender/tools/count_bases.bam ; Missing GATK wrapper script: /Users/droazen/src/hellbender/build/install/gatk/bin/gatk; To generate the wrapper run:. /Users/droazen/src/hellbender/gradlew installDist; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1314
https://github.com/broadinstitute/gatk/issues/1315:36,Availability,error,error,36,"We blow up with the following scary error:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.io.IOException: unexpected exception type; at java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1538); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1110); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1810); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:102,Availability,failure,failure,102,"We blow up with the following scary error:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.io.IOException: unexpected exception type; at java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1538); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1110); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1810); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:159,Availability,failure,failure,159,"We blow up with the following scary error:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.io.IOException: unexpected exception type; at java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1538); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1110); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1810); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:3744,Deployability,pipeline,pipelines,3744,"ang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230); at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1104); ... 31 more; Caused by: java.lang.IllegalAccessError: no such method: org.broadinstitute.hellbender.tools.FlagStat$FlagStatus.add(GATKRead)FlagStatus/invokeVirtual; at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:448); at org.broadinstitute.hellbender.tools.spark.pipelines.FlagStatSpark.$deserializeLambda$(FlagStatSpark.java:20); ... 40 more; Caused by: java.lang.LinkageError: loader constraint violation: when resolving method ""org.broadinstitute.hellbender.tools.FlagStat$FlagStatus.add(Lorg/broadinstitute/hellbender/utils/read/GATKRead;)Lorg/broadinstitute/hellbender/tools/FlagStat$FlagStatus;"" the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) of the current class, org/broadinstitute/hellbender/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:4225,Deployability,pipeline,pipelines,4225,"rImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1104); ... 31 more; Caused by: java.lang.IllegalAccessError: no such method: org.broadinstitute.hellbender.tools.FlagStat$FlagStatus.add(GATKRead)FlagStatus/invokeVirtual; at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:448); at org.broadinstitute.hellbender.tools.spark.pipelines.FlagStatSpark.$deserializeLambda$(FlagStatSpark.java:20); ... 40 more; Caused by: java.lang.LinkageError: loader constraint violation: when resolving method ""org.broadinstitute.hellbender.tools.FlagStat$FlagStatus.add(Lorg/broadinstitute/hellbender/utils/read/GATKRead;)Lorg/broadinstitute/hellbender/tools/FlagStat$FlagStatus;"" the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) of the current class, org/broadinstitute/hellbender/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:2342,Energy Efficiency,schedul,scheduler,2342,readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:95); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:58); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230); at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Meth,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:2413,Energy Efficiency,schedul,scheduler,2413,Stream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:95); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:58); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230); at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStre,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:5067,Energy Efficiency,schedul,scheduler,5067,"agStatus;"" the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) of the current class, org/broadinstitute/hellbender/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.sch",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:5107,Energy Efficiency,schedul,scheduler,5107,"til/ChildFirstURLClassLoader) of the current class, org/broadinstitute/hellbender/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:5205,Energy Efficiency,schedul,scheduler,5205,"er/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:5302,Energy Efficiency,schedul,scheduler,5302,"er/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:5553,Energy Efficiency,schedul,scheduler,5553,"er/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:5633,Energy Efficiency,schedul,scheduler,5633,"er/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:5738,Energy Efficiency,schedul,scheduler,5738,"er/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:5886,Energy Efficiency,schedul,scheduler,5886,"er/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:5974,Energy Efficiency,schedul,scheduler,5974,"er/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:6069,Energy Efficiency,schedul,scheduler,6069,"er/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:2535,Performance,concurren,concurrent,2535,3); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:95); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:58); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230); at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1104); ... 31 more; Caused by: java.lang.IllegalAccessError: no such method: org.broadinstitute.hellbender.too,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:2619,Performance,concurren,concurrent,2619,a.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:95); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:58); at org.apache.spark.scheduler.Task.run(Task.scala:70); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230); at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1104); ... 31 more; Caused by: java.lang.IllegalAccessError: no such method: org.broadinstitute.hellbender.tools.FlagStat$FlagStatus.add(GATKRead)FlagStatus/invokeVirtual; at java.lang.invoke.Me,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:3860,Performance,load,loader,3860,"cessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230); at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1104); ... 31 more; Caused by: java.lang.IllegalAccessError: no such method: org.broadinstitute.hellbender.tools.FlagStat$FlagStatus.add(GATKRead)FlagStatus/invokeVirtual; at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:448); at org.broadinstitute.hellbender.tools.spark.pipelines.FlagStatSpark.$deserializeLambda$(FlagStatSpark.java:20); ... 40 more; Caused by: java.lang.LinkageError: loader constraint violation: when resolving method ""org.broadinstitute.hellbender.tools.FlagStat$FlagStatus.add(Lorg/broadinstitute/hellbender/utils/read/GATKRead;)Lorg/broadinstitute/hellbender/tools/FlagStat$FlagStatus;"" the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) of the current class, org/broadinstitute/hellbender/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHand",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:4093,Performance,load,loader,4093,"rImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1104); ... 31 more; Caused by: java.lang.IllegalAccessError: no such method: org.broadinstitute.hellbender.tools.FlagStat$FlagStatus.add(GATKRead)FlagStatus/invokeVirtual; at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:448); at org.broadinstitute.hellbender.tools.spark.pipelines.FlagStatSpark.$deserializeLambda$(FlagStatSpark.java:20); ... 40 more; Caused by: java.lang.LinkageError: loader constraint violation: when resolving method ""org.broadinstitute.hellbender.tools.FlagStat$FlagStatus.add(Lorg/broadinstitute/hellbender/utils/read/GATKRead;)Lorg/broadinstitute/hellbender/tools/FlagStat$FlagStatus;"" the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) of the current class, org/broadinstitute/hellbender/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:4264,Performance,load,loader,4264,"rImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1104); ... 31 more; Caused by: java.lang.IllegalAccessError: no such method: org.broadinstitute.hellbender.tools.FlagStat$FlagStatus.add(GATKRead)FlagStatus/invokeVirtual; at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:448); at org.broadinstitute.hellbender.tools.spark.pipelines.FlagStatSpark.$deserializeLambda$(FlagStatSpark.java:20); ... 40 more; Caused by: java.lang.LinkageError: loader constraint violation: when resolving method ""org.broadinstitute.hellbender.tools.FlagStat$FlagStatus.add(Lorg/broadinstitute/hellbender/utils/read/GATKRead;)Lorg/broadinstitute/hellbender/tools/FlagStat$FlagStatus;"" the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) of the current class, org/broadinstitute/hellbender/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:81,Safety,abort,aborted,81,"We blow up with the following scary error:. org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.io.IOException: unexpected exception type; at java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1538); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1110); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1810); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801); at ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:5237,Safety,abort,abortStage,5237,"er/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:5334,Safety,abort,abortStage,5334,"er/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1315:5576,Safety,abort,abortStage,5576,"er/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315
https://github.com/broadinstitute/gatk/issues/1320:58,Availability,error,error,58,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:8,Testability,test,testing,8,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:200,Testability,test,testInitialize,200,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:233,Testability,Assert,AssertionError,233,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:306,Testability,test,testng,306,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:313,Testability,Assert,Assert,313,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:325,Testability,Assert,Assert,325,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:349,Testability,test,testng,349,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:356,Testability,Assert,Assert,356,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:377,Testability,Assert,Assert,377,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:402,Testability,test,testng,402,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:409,Testability,Assert,Assert,409,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:416,Testability,assert,assertEquals,416,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:429,Testability,Assert,Assert,429,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:454,Testability,test,testng,454,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:461,Testability,Assert,Assert,461,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:468,Testability,assert,assertEquals,468,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:481,Testability,Assert,Assert,481,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1320:593,Testability,test,testInitialize,593,Locally testing does ok but when run in Travis you get an error consistently. The stack trace reads:. <pre>; org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize FAILED; java.lang.AssertionError: expected [0.0] but found [-0.023368743794425884]; at org.testng.Assert.fail(Assert.java:94); at org.testng.Assert.failNotEquals(Assert.java:496); at org.testng.Assert.assertEquals(Assert.java:209); at org.testng.Assert.assertEquals(Assert.java:222); at org.broadinstitute.hellbender.tools.exome.allelefraction.AlleleFractionInitializerUnitTest.testInitialize(AlleleFractionInitializerUnitTest.java:41); </pre>,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1320
https://github.com/broadinstitute/gatk/issues/1321:165,Availability,ERROR,ERROR,165,`./gatk-launch CompareBaseQualities -I HCC1143.bqsr.bam -I2 HCC1143.bqsr.cram -R ../human_g1k_v37.fasta --disableSequenceDictionaryValidation`. results in ; `A USER ERROR has occurred: A reference file is required when using CRAM files.`. which is bogus because i passed in a reference,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1321
https://github.com/broadinstitute/gatk/pull/1322:29,Availability,error,errors,29,this should stop the timeout errors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1322
https://github.com/broadinstitute/gatk/pull/1322:21,Safety,timeout,timeout,21,this should stop the timeout errors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1322
https://github.com/broadinstitute/gatk/issues/1329:305,Usability,clear,clear,305,"I find the `--sparkRunner` arg to be a bit confusing:; - `DIRECT` uses the local walker impl, and doesn't run Spark at all, so it probably shouldn't be a ""sparkRunner"". I also initially thought this would use the Spark local runner.; - `SUBMIT` only makes sense to me because I use Spark, but also not as clear as could be. Also it seems a bit non-obvious that this is the option to pick if you want to run Spark with the local runner. I would propose; - `--runner LOCAL` or `--runner WALKER` (or `--runner DIRECT`, though I personally find this less clear) for the actual local impl that doesn't use Spark at all.; - `--runner SPARK` for anything that uses the `spark-submit` script, including running it in `local[*]` mode. This is the only option that should accept `--sparkMaster`, or maybe just `--master` to be more in line with `spark-submit`; - `--runner GCS` for Google; - `--runner AWS` for Amazon in the future. Thoughts?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1329
https://github.com/broadinstitute/gatk/issues/1329:551,Usability,clear,clear,551,"I find the `--sparkRunner` arg to be a bit confusing:; - `DIRECT` uses the local walker impl, and doesn't run Spark at all, so it probably shouldn't be a ""sparkRunner"". I also initially thought this would use the Spark local runner.; - `SUBMIT` only makes sense to me because I use Spark, but also not as clear as could be. Also it seems a bit non-obvious that this is the option to pick if you want to run Spark with the local runner. I would propose; - `--runner LOCAL` or `--runner WALKER` (or `--runner DIRECT`, though I personally find this less clear) for the actual local impl that doesn't use Spark at all.; - `--runner SPARK` for anything that uses the `spark-submit` script, including running it in `local[*]` mode. This is the only option that should accept `--sparkMaster`, or maybe just `--master` to be more in line with `spark-submit`; - `--runner GCS` for Google; - `--runner AWS` for Amazon in the future. Thoughts?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1329
https://github.com/broadinstitute/gatk/issues/1331:342,Deployability,install,install,342,"If you haven't ""built"" the wrapper script, it complains. Strictly speaking, if you're only ever going to use Spark, you don't need the wrapper. The check should only happen if you choose the local walker impl. ```; [ec2-user@ip-10-1-1-71 gatk-4.alpha.rc1]$ ./gatk-launch -h; Missing GATK wrapper script: /home/ec2-user/gatk-4.alpha.rc1/build/install/gatk/bin/gatk; To generate the wrapper run:. /home/ec2-user/gatk-4.alpha.rc1/gradlew installDist; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1331
https://github.com/broadinstitute/gatk/issues/1331:435,Deployability,install,installDist,435,"If you haven't ""built"" the wrapper script, it complains. Strictly speaking, if you're only ever going to use Spark, you don't need the wrapper. The check should only happen if you choose the local walker impl. ```; [ec2-user@ip-10-1-1-71 gatk-4.alpha.rc1]$ ./gatk-launch -h; Missing GATK wrapper script: /home/ec2-user/gatk-4.alpha.rc1/build/install/gatk/bin/gatk; To generate the wrapper run:. /home/ec2-user/gatk-4.alpha.rc1/gradlew installDist; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1331
https://github.com/broadinstitute/gatk/issues/1331:27,Integrability,wrap,wrapper,27,"If you haven't ""built"" the wrapper script, it complains. Strictly speaking, if you're only ever going to use Spark, you don't need the wrapper. The check should only happen if you choose the local walker impl. ```; [ec2-user@ip-10-1-1-71 gatk-4.alpha.rc1]$ ./gatk-launch -h; Missing GATK wrapper script: /home/ec2-user/gatk-4.alpha.rc1/build/install/gatk/bin/gatk; To generate the wrapper run:. /home/ec2-user/gatk-4.alpha.rc1/gradlew installDist; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1331
https://github.com/broadinstitute/gatk/issues/1331:135,Integrability,wrap,wrapper,135,"If you haven't ""built"" the wrapper script, it complains. Strictly speaking, if you're only ever going to use Spark, you don't need the wrapper. The check should only happen if you choose the local walker impl. ```; [ec2-user@ip-10-1-1-71 gatk-4.alpha.rc1]$ ./gatk-launch -h; Missing GATK wrapper script: /home/ec2-user/gatk-4.alpha.rc1/build/install/gatk/bin/gatk; To generate the wrapper run:. /home/ec2-user/gatk-4.alpha.rc1/gradlew installDist; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1331
https://github.com/broadinstitute/gatk/issues/1331:288,Integrability,wrap,wrapper,288,"If you haven't ""built"" the wrapper script, it complains. Strictly speaking, if you're only ever going to use Spark, you don't need the wrapper. The check should only happen if you choose the local walker impl. ```; [ec2-user@ip-10-1-1-71 gatk-4.alpha.rc1]$ ./gatk-launch -h; Missing GATK wrapper script: /home/ec2-user/gatk-4.alpha.rc1/build/install/gatk/bin/gatk; To generate the wrapper run:. /home/ec2-user/gatk-4.alpha.rc1/gradlew installDist; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1331
https://github.com/broadinstitute/gatk/issues/1331:381,Integrability,wrap,wrapper,381,"If you haven't ""built"" the wrapper script, it complains. Strictly speaking, if you're only ever going to use Spark, you don't need the wrapper. The check should only happen if you choose the local walker impl. ```; [ec2-user@ip-10-1-1-71 gatk-4.alpha.rc1]$ ./gatk-launch -h; Missing GATK wrapper script: /home/ec2-user/gatk-4.alpha.rc1/build/install/gatk/bin/gatk; To generate the wrapper run:. /home/ec2-user/gatk-4.alpha.rc1/gradlew installDist; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1331
https://github.com/broadinstitute/gatk/issues/1338:149,Safety,detect,detect,149,Currently GATK_GCS_STAGING requires a full bucket path with a trailing /. i.e. `GATK_GCS_STAGING=gs://hellbender/staging/`. it should be modified to detect the absence of the `gcs://` prefix and the trailing `/` and correctly function with options of the following form. `gs://hellbender/staging`; `hellbender/staging/`; `hellbender/staging`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1338
https://github.com/broadinstitute/gatk/issues/1342:9,Testability,test,test,9,"One unit test (`ReadsSparkSinkUnitTest.readsSinkTest[3]`) takes ~1/4 of our test time at around 4-5 minutes when running on travis. It only takes ~20s on my local machine, so I suspect it may be running out of memory on travis and spending all of it's time thrashing, garbage collecting, and spilling to disk. . We should either remove this tests or figure out how to fix it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1342
https://github.com/broadinstitute/gatk/issues/1342:76,Testability,test,test,76,"One unit test (`ReadsSparkSinkUnitTest.readsSinkTest[3]`) takes ~1/4 of our test time at around 4-5 minutes when running on travis. It only takes ~20s on my local machine, so I suspect it may be running out of memory on travis and spending all of it's time thrashing, garbage collecting, and spilling to disk. . We should either remove this tests or figure out how to fix it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1342
https://github.com/broadinstitute/gatk/issues/1342:341,Testability,test,tests,341,"One unit test (`ReadsSparkSinkUnitTest.readsSinkTest[3]`) takes ~1/4 of our test time at around 4-5 minutes when running on travis. It only takes ~20s on my local machine, so I suspect it may be running out of memory on travis and spending all of it's time thrashing, garbage collecting, and spilling to disk. . We should either remove this tests or figure out how to fix it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1342
https://github.com/broadinstitute/gatk/pull/1343:96,Safety,detect,detect,96,"This test case is overkill, since the test just before it in the; DataProvider is sufficient to detect bugs in ReadsSparkSink such; as the one we encountered last week. We also think it is responsible; for the recent intermittent timeouts in travis. Resolves #1342",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1343
https://github.com/broadinstitute/gatk/pull/1343:230,Safety,timeout,timeouts,230,"This test case is overkill, since the test just before it in the; DataProvider is sufficient to detect bugs in ReadsSparkSink such; as the one we encountered last week. We also think it is responsible; for the recent intermittent timeouts in travis. Resolves #1342",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1343
https://github.com/broadinstitute/gatk/pull/1343:5,Testability,test,test,5,"This test case is overkill, since the test just before it in the; DataProvider is sufficient to detect bugs in ReadsSparkSink such; as the one we encountered last week. We also think it is responsible; for the recent intermittent timeouts in travis. Resolves #1342",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1343
https://github.com/broadinstitute/gatk/pull/1343:38,Testability,test,test,38,"This test case is overkill, since the test just before it in the; DataProvider is sufficient to detect bugs in ReadsSparkSink such; as the one we encountered last week. We also think it is responsible; for the recent intermittent timeouts in travis. Resolves #1342",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1343
https://github.com/broadinstitute/gatk/issues/1346:710,Security,access,access,710,"The current implementation delegates to Hadoop-BAM SAMHeaderReader.readSAMHeaderFrom, which in turn delegates to htsjdk SAMFileReader (which is deprecated). This is fragile and in a couple of cases is succeeding now only because of some existing htsjdk quirks/bugs:. -It succeeds on ADAM files only because the htsjdk code [falls through](https://github.com/broadinstitute/gatk/issues/1280) to using a SAMTextReader on the ADAM stream, which surprisingly doesn't throw but returns a completely bogus header.; -It succeeds on CRAM files even though no reference is passed because the htsjdk code currently creates a default reference with nothing backing it in that case. Since we're never using this reader to access reads, this works now, but this will throw if we take [this htsjdk PR](https://github.com/samtools/htsjdk/pull/400) which enforces passing a valid reference to CRAMReaders. More generally, I think we need to reconcile the need to have a SAMFileHeader with the desire to have alternate read stores, and decide of delegating to Hadoop-BAM is sufficient here.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1346
https://github.com/broadinstitute/gatk/pull/1348:101,Testability,test,test,101,fixes #1300; and a few more cases that got uncovered in MergeBamAlignment and SamToFastq. Added some test to check that arguments are printed twice. @lbergelson please review,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1348
https://github.com/broadinstitute/gatk/pull/1351:170,Availability,avail,available,170,fixes https://github.com/broadinstitute/gatk/issues/1284. adds these lines output of running `./gatk-lauch` with no arguments. ```; gatk-launch --help #Print the list of available tools. gatk-launch Tool -help #Print help on a particular tool; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1351
https://github.com/broadinstitute/gatk/pull/1356:40,Deployability,configurat,configuration,40,"using --conf passes args into the spark configuration. these args will take precendence over spark args specified in any other way. moved --sparkMaster and --conf to their own SparkCommandLineArgumentCollection. passing spark options through to gatk with DIRECT, this will only work with --sparkMaster and --conf. fixes #1339",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1356
https://github.com/broadinstitute/gatk/pull/1356:40,Modifiability,config,configuration,40,"using --conf passes args into the spark configuration. these args will take precendence over spark args specified in any other way. moved --sparkMaster and --conf to their own SparkCommandLineArgumentCollection. passing spark options through to gatk with DIRECT, this will only work with --sparkMaster and --conf. fixes #1339",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1356
https://github.com/broadinstitute/gatk/issues/1364:288,Availability,error,error,288,"I cloned GATK4 into /humgen/gsa-scr1/gauthier/workspaces/gatk/ (from gsa5), then tried `./gatk-launch --list`, which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:386,Availability,Down,Download,386,"I cloned GATK4 into /humgen/gsa-scr1/gauthier/workspaces/gatk/ (from gsa5), then tried `./gatk-launch --list`, which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:468,Availability,Down,Download,468,"I cloned GATK4 into /humgen/gsa-scr1/gauthier/workspaces/gatk/ (from gsa5), then tried `./gatk-launch --list`, which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:579,Availability,FAILURE,FAILURE,579,"I cloned GATK4 into /humgen/gsa-scr1/gauthier/workspaces/gatk/ (from gsa5), then tried `./gatk-launch --list`, which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:778,Availability,down,download,778,"I cloned GATK4 into /humgen/gsa-scr1/gauthier/workspaces/gatk/ (from gsa5), then tried `./gatk-launch --list`, which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:22358,Availability,FAILURE,FAILURE,22358,at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); kStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 64 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 68 more. FAILURE: Build failed with an exception.; - What went wrong:; org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:235,Deployability,install,installDist,235,"I cloned GATK4 into /humgen/gsa-scr1/gauthier/workspaces/gatk/ (from gsa5), then tried `./gatk-launch --list`, which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:737,Deployability,configurat,configuration,737,"I cloned GATK4 into /humgen/gsa-scr1/gauthier/workspaces/gatk/ (from gsa5), then tried `./gatk-launch --list`, which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:2962,Deployability,release,release,2962,rg.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(Compo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:4793,Deployability,Continuous,ContinuousBuildActionExecuter,4793,at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:4831,Deployability,Continuous,ContinuousBuildActionExecuter,4831,aultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:4899,Deployability,Continuous,ContinuousBuildActionExecuter,4899,t org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:4937,Deployability,Continuous,ContinuousBuildActionExecuter,4937,ompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:206); at org.gradle.launcher,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:9787,Deployability,release,release,9787,rg.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(Compo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:11618,Deployability,Continuous,ContinuousBuildActionExecuter,11618,at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:11656,Deployability,Continuous,ContinuousBuildActionExecuter,11656,aultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:11724,Deployability,Continuous,ContinuousBuildActionExecuter,11724,t org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:11762,Deployability,Continuous,ContinuousBuildActionExecuter,11762,ompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:206); at org.gradle.launcher,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:16629,Deployability,release,release,16629,rg.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:19001,Deployability,Continuous,ContinuousBuildActionExecuter,19001,at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:19039,Deployability,Continuous,ContinuousBuildActionExecuter,19039,aultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:19107,Deployability,Continuous,ContinuousBuildActionExecuter,19107,t org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:19145,Deployability,Continuous,ContinuousBuildActionExecuter,19145,ompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:206); at org.gradle.launcher,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:720,Integrability,depend,dependencies,720,"I cloned GATK4 into /humgen/gsa-scr1/gauthier/workspaces/gatk/ (from gsa5), then tried `./gatk-launch --list`, which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:7165,Integrability,wrap,wrapper,7165,rtingAction.execute(ExceptionReportingAction.java:22); at org.gradle.launcher.Main.doAction(Main.java:33); at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockSto,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:7245,Integrability,wrap,wrapper,7245,in.doAction(Main.java:33); at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:7253,Integrability,Wrap,WrapperExecutor,7253,Main.java:33); at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.interna,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:7277,Integrability,Wrap,WrapperExecutor,7277,g.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProces,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:7318,Integrability,wrap,wrapper,7318,n(EntryPoint.java:45); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:13990,Integrability,wrap,wrapper,13990,rtingAction.execute(ExceptionReportingAction.java:22); at org.gradle.launcher.Main.doAction(Main.java:33); at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockSto,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:14070,Integrability,wrap,wrapper,14070,in.doAction(Main.java:33); at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service Plug,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:14078,Integrability,Wrap,WrapperExecutor,14078,Main.java:33); at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolution,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:14102,Integrability,Wrap,WrapperExecutor,14102,g.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at Bui,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:14143,Integrability,wrap,wrapper,14143,n(EntryPoint.java:45); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionSer,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:21373,Integrability,wrap,wrapper,21373,rtingAction.execute(ExceptionReportingAction.java:22); at org.gradle.launcher.Main.doAction(Main.java:33); at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); kStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 64 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 68 more. FAILURE: Build fail,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:21453,Integrability,wrap,wrapper,21453,in.doAction(Main.java:33); at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); kStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 64 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 68 more. FAILURE: Build failed with an exception.; - What went wrong:; org.gradle.api.UncheckedIOException: ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:21461,Integrability,Wrap,WrapperExecutor,21461,Main.java:33); at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); kStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 64 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 68 more. FAILURE: Build failed with an exception.; - What went wrong:; org.gradle.api.UncheckedIOException: java.io.IOEx,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:21485,Integrability,Wrap,WrapperExecutor,21485,g.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); kStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 64 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 68 more. FAILURE: Build failed with an exception.; - What went wrong:; org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:21526,Integrability,wrap,wrapper,21526,n(EntryPoint.java:45); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); kStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 64 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 68 more. FAILURE: Build failed with an exception.; - What went wrong:; org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; - Try:; Run with --stacktrace o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:660,Modifiability,config,configuring,660,"I cloned GATK4 into /humgen/gsa-scr1/gauthier/workspaces/gatk/ (from gsa5), then tried `./gatk-launch --list`, which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:737,Modifiability,config,configuration,737,"I cloned GATK4 into /humgen/gsa-scr1/gauthier/workspaces/gatk/ (from gsa5), then tried `./gatk-launch --list`, which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:15070,Modifiability,Plugin,PluginResolutionServiceClient,15070,rapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:16967,Modifiability,plugin,plugin,16967,tentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.Composit,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:17128,Modifiability,plugin,plugin,17128,oppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.grad,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:17284,Modifiability,plugin,plugin,17284,ternal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(Def,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:1100,Performance,cache,caches,1100,", which didn't work because I hadn't built yet. gatk-launch told me to run `/humgen/gsa-scr1/gauthier/workspaces/gatk/gradlew installDist`, which I did and it threw the following error (sorry for the huge stacktrace, but I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.inter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:1423,Performance,cache,cache,1423,xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:1625,Performance,cache,cache,1625,went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.D,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:1733,Performance,cache,cache,1733, configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirector,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:1870,Performance,cache,cache,1870,ource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:1986,Performance,cache,cache,1986,ailed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:2098,Performance,cache,cache,2098,aches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:2207,Performance,cache,cache,2207,ommons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLocking,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:2353,Performance,concurren,concurrent,2353,BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.Composit,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:2442,Performance,concurren,concurrent,2442,ltiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServic,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:2520,Performance,cache,cache,2520,ception: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:2612,Performance,cache,cache,2612,rg.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:2696,Performance,cache,cache,2696,xedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:2805,Performance,cache,cache,2805,ultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:2909,Performance,cache,cache,2909,ultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:3015,Performance,cache,cache,3015,nager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.inte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:3251,Performance,concurren,concurrent,3251,ndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:3340,Performance,concurren,concurrent,3340,nternal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:3542,Performance,concurren,concurrent,3542,Access.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:3744,Performance,concurren,concurrent,3744,DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:3936,Performance,concurren,concurrent,3936,ry$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(Conti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:4134,Performance,concurren,concurrent,4134,tifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonU,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:4315,Performance,concurren,concurrent,4315,org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:4404,Performance,concurren,concurrent,4404,g.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:7481,Performance,cache,cache,7481,a:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersiste,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:7574,Performance,cache,cache,7574,3); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.Defa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:7661,Performance,cache,cache,7661,ap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:7750,Performance,cache,cache,7750,.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:7843,Performance,cache,cache,7843,eMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManag,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:8130,Performance,cache,cache,8130,Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentI,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:8248,Performance,cache,cache,8248,r.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:8450,Performance,cache,cache,8450,ota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.D,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:8558,Performance,cache,cache,8558,t org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirector,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:8695,Performance,cache,cache,8695,re.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:8811,Performance,cache,cache,8811,tore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:8923,Performance,cache,cache,8923,java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:9032,Performance,cache,cache,9032,e0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLocking,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:9178,Performance,concurren,concurrent,9178,ackedBlockStore.java:56); ... 64 more; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.Composit,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:9267,Performance,concurren,concurrent,9267,ltiProcessSafePersistentIndexedCache@4f4dc135.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServic,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:9345,Performance,cache,cache,9345,ception: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:9437,Performance,cache,cache,9437,rg.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:9521,Performance,cache,cache,9521,xedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:9630,Performance,cache,cache,9630,ultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:9734,Performance,cache,cache,9734,ultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:9840,Performance,cache,cache,9840,nager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.inte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:10076,Performance,concurren,concurrent,10076,ndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:10165,Performance,concurren,concurrent,10165,nternal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:10367,Performance,concurren,concurrent,10367,Access.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:10569,Performance,concurren,concurrent,10569,DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:10761,Performance,concurren,concurrent,10761,ry$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(Conti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:10959,Performance,concurren,concurrent,10959,tifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonU,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:11140,Performance,concurren,concurrent,11140,org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:11229,Performance,concurren,concurrent,11229,g.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:14306,Performance,cache,cache,14306,a:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.bt,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:14399,Performance,cache,cache,14399,3); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.ca,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:14486,Performance,cache,cache,14486,ap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSaf,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:14575,Performance,cache,cache,14575,.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$Def,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:14668,Performance,cache,cache,14668,eMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.Def,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:14955,Performance,cache,cache,14955,Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProce,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:15292,Performance,cache,cache,15292,org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.D,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:15400,Performance,cache,cache,15400,e.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); at org.gradle.cache.internal.btree.FreeListBlockStore.close(FreeListBlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirector,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:15537,Performance,cache,cache,15537,BlockStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:15653,Performance,cache,cache,15653, org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 60 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:15765,Performance,cache,cache,15765, more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:15874,Performance,cache,cache,15874,; at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.grad,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:16020,Performance,concurren,concurrent,16020,ava:56); ... 64 more; Could not stop Service PluginResolutionServiceClient at BuildScopeServices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPlugin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:16109,Performance,concurren,concurrent,16109,ervices.createPluginResolutionServiceClient().; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:16187,Performance,cache,cache,16187,ception: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResol,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:16279,Performance,cache,cache,16279,rg.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:16363,Performance,cache,cache,16363,xedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceCli,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:16472,Performance,cache,cache,16472,ultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.Compo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:16576,Performance,cache,cache,16576,ultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:16682,Performance,cache,cache,16682,nager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvid,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:16800,Performance,concurren,concurrent,16800,WorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:16889,Performance,concurren,concurrent,16889,ltMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:17459,Performance,concurren,concurrent,17459,.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:17548,Performance,concurren,concurrent,17548,va:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:17750,Performance,concurren,concurrent,17750,cheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:17952,Performance,concurren,concurrent,17952,rg.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:18144,Performance,concurren,concurrent,18144,rvice.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(Conti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:18342,Performance,concurren,concurrent,18342,esolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonU,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:18523,Performance,concurren,concurrent,18523,org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:18612,Performance,concurren,concurrent,18612,g.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:21689,Performance,cache,cache,21689,at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); kStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 64 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 68 more. FAILURE: Build failed with an exception.; - What went wrong:; org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:21782,Performance,cache,cache,21782,at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); kStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 64 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 68 more. FAILURE: Build failed with an exception.; - What went wrong:; org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:21886,Performance,cache,cache,21886,at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); kStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 64 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 68 more. FAILURE: Build failed with an exception.; - What went wrong:; org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:21979,Performance,cache,cache,21979,at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); kStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 64 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 68 more. FAILURE: Build failed with an exception.; - What went wrong:; org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:22266,Performance,cache,cache,22266,at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); kStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 64 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 68 more. FAILURE: Build failed with an exception.; - What went wrong:; org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:1346,Testability,log,log,1346,t I didn't want to leave out anything important):. [...]; Download https://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar; Download https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar. FAILURE: Build failed with an exception.; - What went wrong:; A problem occurred configuring root project 'gatk'.; ; > Could not resolve all dependencies for configuration ':classpath'.; > Could not download commons-beanutils.jar (commons-beanutils:commons-beanutils:1.8.0); > Could not get resource 'https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.8.0/commons-beanutils-1.8.0.jar'.; > > Failed to move file '/tmp/gradle_download3865353896539966562bin' into filestore at '/home/unix/gauthier/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.8.0/c651d5103c649c12b20d53731643e5fffceb536/commons-beanutils-1.8.0.jar'; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 22.394 secs; Could not stop org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache@1fc775a3.; org.gradle.api.UncheckedIOException: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:197); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache$4.run(DefaultMultiProcessSafePersistentIndexedCache.java:78); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.doWriteAction(DefaultFileLockManager.java:173); at org.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/issues/1364:22606,Testability,log,log,22606,at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:606); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:58); at org.gradle.cache.internal.btree.CachingBlockStore.close(CachingBlockStore.java:40); kStore.java:60); at org.gradle.cache.internal.btree.StateCheckBlockStore.close(StateCheckBlockStore.java:41); at org.gradle.cache.internal.btree.BTreePersistentIndexedCache.close(BTreePersistentIndexedCache.java:195); ... 64 more; Caused by: java.io.IOException: Disk quota exceeded; at java.io.RandomAccessFile.close0(Native Method); at java.io.RandomAccessFile.close(RandomAccessFile.java:645); at org.gradle.cache.internal.btree.FileBackedBlockStore.close(FileBackedBlockStore.java:56); ... 68 more. FAILURE: Build failed with an exception.; - What went wrong:; org.gradle.api.UncheckedIOException: java.io.IOException: Disk quota exceeded; - Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364
https://github.com/broadinstitute/gatk/pull/1368:21,Testability,test,test,21,"Checked all the CRAM test files to make sure they're actually CRAM format; two of these files are not valid but are unused, and one was a BAM file masquerading as a CRAM file that I think was made with Picard.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1368
https://github.com/broadinstitute/gatk/issues/1370:6,Testability,log,logging,6,Spark logging should be controlled by our verbosity argument. Currently it doesn't seem to be.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1370
https://github.com/broadinstitute/gatk/issues/1376:128,Deployability,update,update,128,"http://www.hammerlab.org/2015/07/25/spree-58-a-live-updating-web-ui-for-spark/. the task is to set this up and, if it's useful, update the readme on how to set it up and use. @laserson do you have experience with Spree?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1376
https://github.com/broadinstitute/gatk/issues/1377:131,Deployability,update,update,131,"http://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/. the task here to set this up and, if it's useful, update the readme on how to set it up and use. @laserson do you have experience with Graphite and Grafana",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1377
https://github.com/broadinstitute/gatk/issues/1377:36,Energy Efficiency,monitor,monitoring-spark-with-graphite-and-grafana,36,"http://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/. the task here to set this up and, if it's useful, update the readme on how to set it up and use. @laserson do you have experience with Graphite and Grafana",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1377
https://github.com/broadinstitute/gatk/pull/1380:261,Performance,cache,cache,261,"part of https://github.com/broadinstitute/gatk/issues/914. we do need some caching because the new array creation would happen on every read. For non-spark, the caching going to be on the walker level (ie no change because we're 1-threaded), for spark we get 1 cache per partition in baseRecalibration phase but then there is 1 per read in applyBQSRSpark). The latter is going to cause problems - see https://github.com/broadinstitute/gatk/issues/1381",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1380
https://github.com/broadinstitute/gatk/issues/1384:125,Availability,avail,available,125,"_From @lbergelson on July 20, 2015 18:22_. `PrintReads` allows `--BQSR` to be specified even if it is compiled with out BQSR available. This means that a public only compilation of gatk will look like it can perform BQSR, but silently fails to do so. A check should be added so that `PrintReads` will throw a `UserException` if BQSR is requested but not available.; - [ ] When implemented (if not already fixed in G4), open an issue in gsa-unstable to have the fix backported to G3. . _Copied from original issue: broadinstitute/gsa-unstable#1058_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1384
https://github.com/broadinstitute/gatk/issues/1384:354,Availability,avail,available,354,"_From @lbergelson on July 20, 2015 18:22_. `PrintReads` allows `--BQSR` to be specified even if it is compiled with out BQSR available. This means that a public only compilation of gatk will look like it can perform BQSR, but silently fails to do so. A check should be added so that `PrintReads` will throw a `UserException` if BQSR is requested but not available.; - [ ] When implemented (if not already fixed in G4), open an issue in gsa-unstable to have the fix backported to G3. . _Copied from original issue: broadinstitute/gsa-unstable#1058_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1384
https://github.com/broadinstitute/gatk/issues/1384:208,Performance,perform,perform,208,"_From @lbergelson on July 20, 2015 18:22_. `PrintReads` allows `--BQSR` to be specified even if it is compiled with out BQSR available. This means that a public only compilation of gatk will look like it can perform BQSR, but silently fails to do so. A check should be added so that `PrintReads` will throw a `UserException` if BQSR is requested but not available.; - [ ] When implemented (if not already fixed in G4), open an issue in gsa-unstable to have the fix backported to G3. . _Copied from original issue: broadinstitute/gsa-unstable#1058_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1384
https://github.com/broadinstitute/gatk/issues/1386:487,Availability,error,error,487,"Here is my command:; spark-submit --driver-memory 8G --master local[1] --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.executor.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 gatk-4.alpha/build/libs/gatk-all-version-unknown-SNAPSHOT-spark.jar BaseRecalibratorSpark -knownSites test.vcf -R hg19_rCRSchrm.2bit -I test.bam -O test.txt. The error msg is:; java.lang.ClassCastException: org.broadinstitute.hellbender.utils.variant.MinimalVariant cannot be cast to htsjdk.samtools.util.Locatable; at org.broadinstitute.hellbender.utils.collections.IntervalsSkipList.<init>(IntervalsSkipList.java:35); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.join(BroadcastJoinReadsWithVariants.java:22); at org.broadinstitute.hellbender.engine.spark.AddContextDataToReadSpark.add(AddContextDataToReadSpark.java:35); at org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark.runTool(BaseRecalibratorSpark.java:91); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257). It seems like BroadCast strategy in BaseRecalibratorSpark has some bugs when trying to join variants with reads. . i can see that Variant is implements from htsjdk.samtools.util.Locatable, but why it still have some problem there?. Is it because that somehow in some java environment, this clause will cause problem?; final IntervalsSkipList<Variant> variantSkipList = new IntervalsSkipList<>(variants.collect());. And it should be this assertion:; final IntervalsSkipList<Variant> variantSkipList = new IntervalsSkipList<Variant>(variants.collect());. I use java 1.8.0_66",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1386
https://github.com/broadinstitute/gatk/issues/1386:427,Testability,test,test,427,"Here is my command:; spark-submit --driver-memory 8G --master local[1] --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.executor.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 gatk-4.alpha/build/libs/gatk-all-version-unknown-SNAPSHOT-spark.jar BaseRecalibratorSpark -knownSites test.vcf -R hg19_rCRSchrm.2bit -I test.bam -O test.txt. The error msg is:; java.lang.ClassCastException: org.broadinstitute.hellbender.utils.variant.MinimalVariant cannot be cast to htsjdk.samtools.util.Locatable; at org.broadinstitute.hellbender.utils.collections.IntervalsSkipList.<init>(IntervalsSkipList.java:35); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.join(BroadcastJoinReadsWithVariants.java:22); at org.broadinstitute.hellbender.engine.spark.AddContextDataToReadSpark.add(AddContextDataToReadSpark.java:35); at org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark.runTool(BaseRecalibratorSpark.java:91); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257). It seems like BroadCast strategy in BaseRecalibratorSpark has some bugs when trying to join variants with reads. . i can see that Variant is implements from htsjdk.samtools.util.Locatable, but why it still have some problem there?. Is it because that somehow in some java environment, this clause will cause problem?; final IntervalsSkipList<Variant> variantSkipList = new IntervalsSkipList<>(variants.collect());. And it should be this assertion:; final IntervalsSkipList<Variant> variantSkipList = new IntervalsSkipList<Variant>(variants.collect());. I use java 1.8.0_66",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1386
https://github.com/broadinstitute/gatk/issues/1386:461,Testability,test,test,461,"Here is my command:; spark-submit --driver-memory 8G --master local[1] --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.executor.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 gatk-4.alpha/build/libs/gatk-all-version-unknown-SNAPSHOT-spark.jar BaseRecalibratorSpark -knownSites test.vcf -R hg19_rCRSchrm.2bit -I test.bam -O test.txt. The error msg is:; java.lang.ClassCastException: org.broadinstitute.hellbender.utils.variant.MinimalVariant cannot be cast to htsjdk.samtools.util.Locatable; at org.broadinstitute.hellbender.utils.collections.IntervalsSkipList.<init>(IntervalsSkipList.java:35); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.join(BroadcastJoinReadsWithVariants.java:22); at org.broadinstitute.hellbender.engine.spark.AddContextDataToReadSpark.add(AddContextDataToReadSpark.java:35); at org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark.runTool(BaseRecalibratorSpark.java:91); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257). It seems like BroadCast strategy in BaseRecalibratorSpark has some bugs when trying to join variants with reads. . i can see that Variant is implements from htsjdk.samtools.util.Locatable, but why it still have some problem there?. Is it because that somehow in some java environment, this clause will cause problem?; final IntervalsSkipList<Variant> variantSkipList = new IntervalsSkipList<>(variants.collect());. And it should be this assertion:; final IntervalsSkipList<Variant> variantSkipList = new IntervalsSkipList<Variant>(variants.collect());. I use java 1.8.0_66",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1386
https://github.com/broadinstitute/gatk/issues/1386:473,Testability,test,test,473,"Here is my command:; spark-submit --driver-memory 8G --master local[1] --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.executor.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 gatk-4.alpha/build/libs/gatk-all-version-unknown-SNAPSHOT-spark.jar BaseRecalibratorSpark -knownSites test.vcf -R hg19_rCRSchrm.2bit -I test.bam -O test.txt. The error msg is:; java.lang.ClassCastException: org.broadinstitute.hellbender.utils.variant.MinimalVariant cannot be cast to htsjdk.samtools.util.Locatable; at org.broadinstitute.hellbender.utils.collections.IntervalsSkipList.<init>(IntervalsSkipList.java:35); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.join(BroadcastJoinReadsWithVariants.java:22); at org.broadinstitute.hellbender.engine.spark.AddContextDataToReadSpark.add(AddContextDataToReadSpark.java:35); at org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark.runTool(BaseRecalibratorSpark.java:91); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257). It seems like BroadCast strategy in BaseRecalibratorSpark has some bugs when trying to join variants with reads. . i can see that Variant is implements from htsjdk.samtools.util.Locatable, but why it still have some problem there?. Is it because that somehow in some java environment, this clause will cause problem?; final IntervalsSkipList<Variant> variantSkipList = new IntervalsSkipList<>(variants.collect());. And it should be this assertion:; final IntervalsSkipList<Variant> variantSkipList = new IntervalsSkipList<Variant>(variants.collect());. I use java 1.8.0_66",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1386
https://github.com/broadinstitute/gatk/issues/1386:1621,Testability,assert,assertion,1621,"Here is my command:; spark-submit --driver-memory 8G --master local[1] --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=true --conf spark.executor.userClassPathFirst=true --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 gatk-4.alpha/build/libs/gatk-all-version-unknown-SNAPSHOT-spark.jar BaseRecalibratorSpark -knownSites test.vcf -R hg19_rCRSchrm.2bit -I test.bam -O test.txt. The error msg is:; java.lang.ClassCastException: org.broadinstitute.hellbender.utils.variant.MinimalVariant cannot be cast to htsjdk.samtools.util.Locatable; at org.broadinstitute.hellbender.utils.collections.IntervalsSkipList.<init>(IntervalsSkipList.java:35); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.join(BroadcastJoinReadsWithVariants.java:22); at org.broadinstitute.hellbender.engine.spark.AddContextDataToReadSpark.add(AddContextDataToReadSpark.java:35); at org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark.runTool(BaseRecalibratorSpark.java:91); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257). It seems like BroadCast strategy in BaseRecalibratorSpark has some bugs when trying to join variants with reads. . i can see that Variant is implements from htsjdk.samtools.util.Locatable, but why it still have some problem there?. Is it because that somehow in some java environment, this clause will cause problem?; final IntervalsSkipList<Variant> variantSkipList = new IntervalsSkipList<>(variants.collect());. And it should be this assertion:; final IntervalsSkipList<Variant> variantSkipList = new IntervalsSkipList<Variant>(variants.collect());. I use java 1.8.0_66",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1386
https://github.com/broadinstitute/gatk/issues/1387:12,Usability,simpl,simply,12,"This method simply writes a list of strings (one per line) to a text file. Does such a thing already exist in gatk? If not, we can move `ParamUtils.writeStringListToFile(...)` to `Utils.writeStringListToFile(...)` (or similar).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1387
https://github.com/broadinstitute/gatk/issues/1389:31,Deployability,upgrade,upgrade,31,(this may be related to recent upgrade of our cluster?). running the simples example blows up:. ```; ./bin/gatk/gatk-launch PrintReadsSpark -I hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.bam -O hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.out.bam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --num-executors 5 --executor-cores 2 --executor-memory 4g \; --conf spark.yarn.executor.memoryOverhead=600; ```. blows up with . ```; java.lang.ClassCastException: org.apache.hadoop.fs.RawLocalFileSystem cannot be cast to org.apache.hadoop.fs.LocalFileSystem; at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:350); at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$getQualifiedLocalPath(Client.scala:1373); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:329); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(Comm,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1389:686,Deployability,deploy,deploy,686,(this may be related to recent upgrade of our cluster?). running the simples example blows up:. ```; ./bin/gatk/gatk-launch PrintReadsSpark -I hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.bam -O hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.out.bam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --num-executors 5 --executor-cores 2 --executor-memory 4g \; --conf spark.yarn.executor.memoryOverhead=600; ```. blows up with . ```; java.lang.ClassCastException: org.apache.hadoop.fs.RawLocalFileSystem cannot be cast to org.apache.hadoop.fs.LocalFileSystem; at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:350); at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$getQualifiedLocalPath(Client.scala:1373); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:329); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(Comm,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1389:723,Deployability,deploy,deploy,723,(this may be related to recent upgrade of our cluster?). running the simples example blows up:. ```; ./bin/gatk/gatk-launch PrintReadsSpark -I hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.bam -O hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.out.bam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --num-executors 5 --executor-cores 2 --executor-memory 4g \; --conf spark.yarn.executor.memoryOverhead=600; ```. blows up with . ```; java.lang.ClassCastException: org.apache.hadoop.fs.RawLocalFileSystem cannot be cast to org.apache.hadoop.fs.LocalFileSystem; at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:350); at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$getQualifiedLocalPath(Client.scala:1373); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:329); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(Comm,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1389:805,Deployability,deploy,deploy,805,(this may be related to recent upgrade of our cluster?). running the simples example blows up:. ```; ./bin/gatk/gatk-launch PrintReadsSpark -I hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.bam -O hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.out.bam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --num-executors 5 --executor-cores 2 --executor-memory 4g \; --conf spark.yarn.executor.memoryOverhead=600; ```. blows up with . ```; java.lang.ClassCastException: org.apache.hadoop.fs.RawLocalFileSystem cannot be cast to org.apache.hadoop.fs.LocalFileSystem; at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:350); at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$getQualifiedLocalPath(Client.scala:1373); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:329); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(Comm,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1389:841,Deployability,deploy,deploy,841,(this may be related to recent upgrade of our cluster?). running the simples example blows up:. ```; ./bin/gatk/gatk-launch PrintReadsSpark -I hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.bam -O hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.out.bam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --num-executors 5 --executor-cores 2 --executor-memory 4g \; --conf spark.yarn.executor.memoryOverhead=600; ```. blows up with . ```; java.lang.ClassCastException: org.apache.hadoop.fs.RawLocalFileSystem cannot be cast to org.apache.hadoop.fs.LocalFileSystem; at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:350); at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$getQualifiedLocalPath(Client.scala:1373); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:329); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(Comm,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1389:913,Deployability,deploy,deploy,913,(this may be related to recent upgrade of our cluster?). running the simples example blows up:. ```; ./bin/gatk/gatk-launch PrintReadsSpark -I hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.bam -O hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.out.bam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --num-executors 5 --executor-cores 2 --executor-memory 4g \; --conf spark.yarn.executor.memoryOverhead=600; ```. blows up with . ```; java.lang.ClassCastException: org.apache.hadoop.fs.RawLocalFileSystem cannot be cast to org.apache.hadoop.fs.LocalFileSystem; at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:350); at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$getQualifiedLocalPath(Client.scala:1373); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:329); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(Comm,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1389:993,Deployability,deploy,deploy,993,(this may be related to recent upgrade of our cluster?). running the simples example blows up:. ```; ./bin/gatk/gatk-launch PrintReadsSpark -I hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.bam -O hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.out.bam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --num-executors 5 --executor-cores 2 --executor-memory 4g \; --conf spark.yarn.executor.memoryOverhead=600; ```. blows up with . ```; java.lang.ClassCastException: org.apache.hadoop.fs.RawLocalFileSystem cannot be cast to org.apache.hadoop.fs.LocalFileSystem; at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:350); at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$getQualifiedLocalPath(Client.scala:1373); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:329); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(Comm,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1389:1080,Deployability,deploy,deploy,1080,e blows up:. ```; ./bin/gatk/gatk-launch PrintReadsSpark -I hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.bam -O hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.out.bam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --num-executors 5 --executor-cores 2 --executor-memory 4g \; --conf spark.yarn.executor.memoryOverhead=600; ```. blows up with . ```; java.lang.ClassCastException: org.apache.hadoop.fs.RawLocalFileSystem cannot be cast to org.apache.hadoop.fs.LocalFileSystem; at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:350); at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$getQualifiedLocalPath(Client.scala:1373); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:329); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1389:2563,Deployability,deploy,deploy,2563,pareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1389:2600,Deployability,deploy,deploy,2600,pareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1389:2672,Deployability,deploy,deploy,2672,pareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1389:2748,Deployability,deploy,deploy,2748,pareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1389:2819,Deployability,deploy,deploy,2819,pareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1389:2888,Deployability,deploy,deploy,2888,pareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1389:1156,Energy Efficiency,schedul,scheduler,1156,un/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.bam -O hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.out.bam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --num-executors 5 --executor-cores 2 --executor-memory 4g \; --conf spark.yarn.executor.memoryOverhead=600; ```. blows up with . ```; java.lang.ClassCastException: org.apache.hadoop.fs.RawLocalFileSystem cannot be cast to org.apache.hadoop.fs.LocalFileSystem; at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:350); at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$getQualifiedLocalPath(Client.scala:1373); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:329); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbende,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1389:1265,Energy Efficiency,schedul,scheduler,1265,all.out.bam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --num-executors 5 --executor-cores 2 --executor-memory 4g \; --conf spark.yarn.executor.memoryOverhead=600; ```. blows up with . ```; java.lang.ClassCastException: org.apache.hadoop.fs.RawLocalFileSystem cannot be cast to org.apache.hadoop.fs.LocalFileSystem; at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:350); at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$getQualifiedLocalPath(Client.scala:1373); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:329); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.N,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1389:69,Usability,simpl,simples,69,(this may be related to recent upgrade of our cluster?). running the simples example blows up:. ```; ./bin/gatk/gatk-launch PrintReadsSpark -I hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.bam -O hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.out.bam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --num-executors 5 --executor-cores 2 --executor-memory 4g \; --conf spark.yarn.executor.memoryOverhead=600; ```. blows up with . ```; java.lang.ClassCastException: org.apache.hadoop.fs.RawLocalFileSystem cannot be cast to org.apache.hadoop.fs.LocalFileSystem; at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:350); at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$getQualifiedLocalPath(Client.scala:1373); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:329); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(Comm,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389
https://github.com/broadinstitute/gatk/issues/1391:8,Testability,test,tests,8,"we need tests that will protect us from regressions like #1390 - we must test on hdfs, on travis or elsewhere",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1391
https://github.com/broadinstitute/gatk/issues/1391:73,Testability,test,test,73,"we need tests that will protect us from regressions like #1390 - we must test on hdfs, on travis or elsewhere",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1391
https://github.com/broadinstitute/gatk/issues/1392:200,Availability,error,error,200,"we need a canonical set of tests that we run when we upgrade the cluster. We've been running terasort but it's not enough: 1) it does not run our code and 2) it does not even run java8 (recent config error when 2 nodes were running java7 was undetected). The task here is to write, in readme or in scripts directory, a script or set of scripts that must be run after every change to the cluster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1392
https://github.com/broadinstitute/gatk/issues/1392:53,Deployability,upgrade,upgrade,53,"we need a canonical set of tests that we run when we upgrade the cluster. We've been running terasort but it's not enough: 1) it does not run our code and 2) it does not even run java8 (recent config error when 2 nodes were running java7 was undetected). The task here is to write, in readme or in scripts directory, a script or set of scripts that must be run after every change to the cluster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1392
https://github.com/broadinstitute/gatk/issues/1392:193,Modifiability,config,config,193,"we need a canonical set of tests that we run when we upgrade the cluster. We've been running terasort but it's not enough: 1) it does not run our code and 2) it does not even run java8 (recent config error when 2 nodes were running java7 was undetected). The task here is to write, in readme or in scripts directory, a script or set of scripts that must be run after every change to the cluster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1392
https://github.com/broadinstitute/gatk/issues/1392:27,Testability,test,tests,27,"we need a canonical set of tests that we run when we upgrade the cluster. We've been running terasort but it's not enough: 1) it does not run our code and 2) it does not even run java8 (recent config error when 2 nodes were running java7 was undetected). The task here is to write, in readme or in scripts directory, a script or set of scripts that must be run after every change to the cluster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1392
https://github.com/broadinstitute/gatk/issues/1396:110,Modifiability,extend,extend,110,"the Main class is nominally extensible and our doc claims "" If you want your own single command line program, extend this class and give instanceMain a new list of java packages in which to search for classes that extend CommandLineProgram."". we need a test + example of this (test can serve as and example tool)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1396
https://github.com/broadinstitute/gatk/issues/1396:214,Modifiability,extend,extend,214,"the Main class is nominally extensible and our doc claims "" If you want your own single command line program, extend this class and give instanceMain a new list of java packages in which to search for classes that extend CommandLineProgram."". we need a test + example of this (test can serve as and example tool)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1396
https://github.com/broadinstitute/gatk/issues/1396:253,Testability,test,test,253,"the Main class is nominally extensible and our doc claims "" If you want your own single command line program, extend this class and give instanceMain a new list of java packages in which to search for classes that extend CommandLineProgram."". we need a test + example of this (test can serve as and example tool)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1396
https://github.com/broadinstitute/gatk/issues/1396:277,Testability,test,test,277,"the Main class is nominally extensible and our doc claims "" If you want your own single command line program, extend this class and give instanceMain a new list of java packages in which to search for classes that extend CommandLineProgram."". we need a test + example of this (test can serve as and example tool)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1396
https://github.com/broadinstitute/gatk/issues/1397:211,Modifiability,extend,extend,211,"scenario: someone wants to use gatk4 as a framework and add new tools. They need show on the list of tools etc. They package names are the user's, ie not org.broadinstitute.hellbender*. The way to do this is to extend Main but we have no example and not documentation of this (other that in the Main class, which is not the right place - I think it should be in README or some such).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1397
https://github.com/broadinstitute/gatk/issues/1398:221,Usability,simpl,simplest,221,CompareBaseQualities requires a shuffle because it does not assume identical read sorting. It will be unusably slow for large files - and unnecessarily so because we aim to have the read order exactly the same. It may be simplest to implement as a walker like CompareSamFiles,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1398
https://github.com/broadinstitute/gatk/issues/1400:150,Security,validat,validation,150,"We want something like a hosted jenkins server to run parts of the test suite that can't or shouldn't be run in travis. This includes:. -Long-running validation tests, like those designed by @davidadamsphd for the `ReadsPipelineSpark`; -Tests involving reading data from HDFS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1400
https://github.com/broadinstitute/gatk/issues/1400:67,Testability,test,test,67,"We want something like a hosted jenkins server to run parts of the test suite that can't or shouldn't be run in travis. This includes:. -Long-running validation tests, like those designed by @davidadamsphd for the `ReadsPipelineSpark`; -Tests involving reading data from HDFS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1400
https://github.com/broadinstitute/gatk/issues/1400:161,Testability,test,tests,161,"We want something like a hosted jenkins server to run parts of the test suite that can't or shouldn't be run in travis. This includes:. -Long-running validation tests, like those designed by @davidadamsphd for the `ReadsPipelineSpark`; -Tests involving reading data from HDFS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1400
https://github.com/broadinstitute/gatk/issues/1400:237,Testability,Test,Tests,237,"We want something like a hosted jenkins server to run parts of the test suite that can't or shouldn't be run in travis. This includes:. -Long-running validation tests, like those designed by @davidadamsphd for the `ReadsPipelineSpark`; -Tests involving reading data from HDFS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1400
https://github.com/broadinstitute/gatk/issues/1401:130,Energy Efficiency,schedul,schedule,130,The new validation tests for `ReadsPipelineSpark` should be easily runnable in either a push-button fashion or on a set automatic schedule (nightly or weekly) via a jenkins server. Depends on https://github.com/broadinstitute/gatk/issues/1400,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1401
https://github.com/broadinstitute/gatk/issues/1401:181,Integrability,Depend,Depends,181,The new validation tests for `ReadsPipelineSpark` should be easily runnable in either a push-button fashion or on a set automatic schedule (nightly or weekly) via a jenkins server. Depends on https://github.com/broadinstitute/gatk/issues/1400,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1401
https://github.com/broadinstitute/gatk/issues/1401:8,Security,validat,validation,8,The new validation tests for `ReadsPipelineSpark` should be easily runnable in either a push-button fashion or on a set automatic schedule (nightly or weekly) via a jenkins server. Depends on https://github.com/broadinstitute/gatk/issues/1400,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1401
https://github.com/broadinstitute/gatk/issues/1401:19,Testability,test,tests,19,The new validation tests for `ReadsPipelineSpark` should be easily runnable in either a push-button fashion or on a set automatic schedule (nightly or weekly) via a jenkins server. Depends on https://github.com/broadinstitute/gatk/issues/1400,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1401
https://github.com/broadinstitute/gatk/issues/1402:67,Testability,test,tests,67,"We've recently had some bugs that would have been caught if we had tests that ran on HDFS. Since these probably can't run in travis, let's run them via the public jenkins server we're going to set up for non-travis testing (https://github.com/broadinstitute/gatk/issues/1400).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1402
https://github.com/broadinstitute/gatk/issues/1402:215,Testability,test,testing,215,"We've recently had some bugs that would have been caught if we had tests that ran on HDFS. Since these probably can't run in travis, let's run them via the public jenkins server we're going to set up for non-travis testing (https://github.com/broadinstitute/gatk/issues/1400).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1402
https://github.com/broadinstitute/gatk/issues/1403:651,Energy Efficiency,reduce,reducer,651,"From @tomwhite:. What we can do though is take another look at why we need such a small split size and see if we can increase it (by better tuning etc). Even if we can't we know that some for some classes of jobs (e.g. metrics) we can use the default split size without an issue, so we could add logic to choose an appropriate split size based on the type of job. Thinking about this a bit more, we're actually quite close to a solution. In the existing Spark Mark Duplicates implementation, for example, you can set the parallelism (with -P). By default it is the same as the number of input partitions, but if the partitions are too large then each reducer has too much data to deal with and the job fails. This is why we made the number of partitions large by reducing the split size to 10MB. However, by increasing the number of reducers (by setting -P to be input data size/10MB), the input split size can be increased to its default of 128MB without causing a problem. So I think the work here is to work out a good set of defaults and the smallest number of knobs to override them. For example, I think we could always use the default split size, have some heuristics to choose a good value for the parallelism, and allow it to be overridden with -P. We should also decouple the parallelism and whether the output is a single file. For ReadsPipelineSpark we have -shardedOutput, but not for MarkDuplicatesSpark (where it's signalled by -P 1), so we should make this consistent.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1403
https://github.com/broadinstitute/gatk/issues/1403:833,Energy Efficiency,reduce,reducers,833,"From @tomwhite:. What we can do though is take another look at why we need such a small split size and see if we can increase it (by better tuning etc). Even if we can't we know that some for some classes of jobs (e.g. metrics) we can use the default split size without an issue, so we could add logic to choose an appropriate split size based on the type of job. Thinking about this a bit more, we're actually quite close to a solution. In the existing Spark Mark Duplicates implementation, for example, you can set the parallelism (with -P). By default it is the same as the number of input partitions, but if the partitions are too large then each reducer has too much data to deal with and the job fails. This is why we made the number of partitions large by reducing the split size to 10MB. However, by increasing the number of reducers (by setting -P to be input data size/10MB), the input split size can be increased to its default of 128MB without causing a problem. So I think the work here is to work out a good set of defaults and the smallest number of knobs to override them. For example, I think we could always use the default split size, have some heuristics to choose a good value for the parallelism, and allow it to be overridden with -P. We should also decouple the parallelism and whether the output is a single file. For ReadsPipelineSpark we have -shardedOutput, but not for MarkDuplicatesSpark (where it's signalled by -P 1), so we should make this consistent.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1403
https://github.com/broadinstitute/gatk/issues/1403:296,Testability,log,logic,296,"From @tomwhite:. What we can do though is take another look at why we need such a small split size and see if we can increase it (by better tuning etc). Even if we can't we know that some for some classes of jobs (e.g. metrics) we can use the default split size without an issue, so we could add logic to choose an appropriate split size based on the type of job. Thinking about this a bit more, we're actually quite close to a solution. In the existing Spark Mark Duplicates implementation, for example, you can set the parallelism (with -P). By default it is the same as the number of input partitions, but if the partitions are too large then each reducer has too much data to deal with and the job fails. This is why we made the number of partitions large by reducing the split size to 10MB. However, by increasing the number of reducers (by setting -P to be input data size/10MB), the input split size can be increased to its default of 128MB without causing a problem. So I think the work here is to work out a good set of defaults and the smallest number of knobs to override them. For example, I think we could always use the default split size, have some heuristics to choose a good value for the parallelism, and allow it to be overridden with -P. We should also decouple the parallelism and whether the output is a single file. For ReadsPipelineSpark we have -shardedOutput, but not for MarkDuplicatesSpark (where it's signalled by -P 1), so we should make this consistent.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1403
https://github.com/broadinstitute/gatk/issues/1404:18,Availability,error,error,18,"We encountered an error with git-lfs in #1378. This was worked around by simply pulling twice in `scripts/install_git_lfs.sh`, but it shouldn't have to do that. . Remove this extra pull once a better solution is found in https://github.com/github/git-lfs/issues/904",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1404
https://github.com/broadinstitute/gatk/issues/1404:73,Usability,simpl,simply,73,"We encountered an error with git-lfs in #1378. This was worked around by simply pulling twice in `scripts/install_git_lfs.sh`, but it shouldn't have to do that. . Remove this extra pull once a better solution is found in https://github.com/github/git-lfs/issues/904",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1404
https://github.com/broadinstitute/gatk/issues/1407:630,Availability,failure,failure,630,"using full (even sites-only) VCFs for BQSR is heavy, especially in spark, when we broadcast them. They can go > 10GB in size. Really, it's just a few million positions, so we could compress it hugely: say we have 4 million variants to consider (common sites) - that's just 4M*32bits = 16 MB. . This would require creating a special format for this (or finding an existing one that works for this case). note that need to represent indel positions (with start and end, which complicates things) too but they are much less common (1 in 10 compared to snps). Or maybe Using a BloomFilter is the way to go. For a 10^-3 probability of failure and 4 million entries we only need ~7MB of size with 10 hash functions",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1407
https://github.com/broadinstitute/gatk/issues/1407:694,Security,hash,hash,694,"using full (even sites-only) VCFs for BQSR is heavy, especially in spark, when we broadcast them. They can go > 10GB in size. Really, it's just a few million positions, so we could compress it hugely: say we have 4 million variants to consider (common sites) - that's just 4M*32bits = 16 MB. . This would require creating a special format for this (or finding an existing one that works for this case). note that need to represent indel positions (with start and end, which complicates things) too but they are much less common (1 in 10 compared to snps). Or maybe Using a BloomFilter is the way to go. For a 10^-3 probability of failure and 4 million entries we only need ~7MB of size with 10 hash functions",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1407
https://github.com/broadinstitute/gatk/issues/1408:83,Usability,clear,clear,83,"The situation in `ReadsSparkSource` with unmapped reads is very confused. It's not clear which (if any) methods return unmapped reads. It appears that queries by interval will return some unmapped reads (those that have the position of their mapped mates), but not others (those that are unmapped and not mated to a mapped read). We need to clarify this situation by providing a clear API for retrieving unmapped reads, documenting under precisely what circumstances unmapped reads may be returned even when they're not explicitly requested (eg., unmapped reads that are assigned the position of their mates).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1408
https://github.com/broadinstitute/gatk/issues/1408:379,Usability,clear,clear,379,"The situation in `ReadsSparkSource` with unmapped reads is very confused. It's not clear which (if any) methods return unmapped reads. It appears that queries by interval will return some unmapped reads (those that have the position of their mapped mates), but not others (those that are unmapped and not mated to a mapped read). We need to clarify this situation by providing a clear API for retrieving unmapped reads, documenting under precisely what circumstances unmapped reads may be returned even when they're not explicitly requested (eg., unmapped reads that are assigned the position of their mates).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1408
https://github.com/broadinstitute/gatk/pull/1415:128,Security,validat,validation,128,fixes #1398 . @yfarjoun can you review? it's a super simple picard-style CLP for comparing quals between bams (needed for GATK4 validation of BQSR - may be useful for GoTC too (?)),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1415
https://github.com/broadinstitute/gatk/pull/1415:53,Usability,simpl,simple,53,fixes #1398 . @yfarjoun can you review? it's a super simple picard-style CLP for comparing quals between bams (needed for GATK4 validation of BQSR - may be useful for GoTC too (?)),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1415
https://github.com/broadinstitute/gatk/issues/1417:242,Availability,error,error,242,"I'm almost certain this used to work. ```; ./bin/gatk/gatk-launch FlagStatSpark -I file:///local/dev/akiezun/bin/gatk/src/test/resources/org/broadinstitute/hellbender/tools/valid.bam -- --sparkRunner SPARK --sparkMaster yarn-client; ```. the error is . ```; java.lang.IllegalArgumentException: Wrong FS: file:/local/dev/akiezun/bin/gatk/src/test/resources/org/broadinstitute/hellbender/tools/valid.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:474); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:181); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); ```. It's fine when running a LOCAL runner, or when the file is on HDFS. . When resolving the ticket, make sure to devise a way (or at least enter a ticket) to prevent this from happening again - ie some way to discover this kind of problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1417
https://github.com/broadinstitute/gatk/issues/1417:122,Testability,test,test,122,"I'm almost certain this used to work. ```; ./bin/gatk/gatk-launch FlagStatSpark -I file:///local/dev/akiezun/bin/gatk/src/test/resources/org/broadinstitute/hellbender/tools/valid.bam -- --sparkRunner SPARK --sparkMaster yarn-client; ```. the error is . ```; java.lang.IllegalArgumentException: Wrong FS: file:/local/dev/akiezun/bin/gatk/src/test/resources/org/broadinstitute/hellbender/tools/valid.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:474); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:181); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); ```. It's fine when running a LOCAL runner, or when the file is on HDFS. . When resolving the ticket, make sure to devise a way (or at least enter a ticket) to prevent this from happening again - ie some way to discover this kind of problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1417
https://github.com/broadinstitute/gatk/issues/1417:341,Testability,test,test,341,"I'm almost certain this used to work. ```; ./bin/gatk/gatk-launch FlagStatSpark -I file:///local/dev/akiezun/bin/gatk/src/test/resources/org/broadinstitute/hellbender/tools/valid.bam -- --sparkRunner SPARK --sparkMaster yarn-client; ```. the error is . ```; java.lang.IllegalArgumentException: Wrong FS: file:/local/dev/akiezun/bin/gatk/src/test/resources/org/broadinstitute/hellbender/tools/valid.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:474); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:181); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); ```. It's fine when running a LOCAL runner, or when the file is on HDFS. . When resolving the ticket, make sure to devise a way (or at least enter a ticket) to prevent this from happening again - ie some way to discover this kind of problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1417
https://github.com/broadinstitute/gatk/pull/1421:33,Testability,test,tests,33,I had another go at this. Passes tests and doesn't cause the problem on a cluster that Adam encountered (https://github.com/broadinstitute/gatk/issues/1389). Fixes #1266.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1421
https://github.com/broadinstitute/gatk/pull/1423:38,Testability,test,test,38,fixes #1164. the problem was that the test spec was incorrect - no placeholders.; I fixed the spec and added a check for specs in general and added a test to show that the check does work. for @droazen,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1423
https://github.com/broadinstitute/gatk/pull/1423:150,Testability,test,test,150,fixes #1164. the problem was that the test spec was incorrect - no placeholders.; I fixed the spec and added a check for specs in general and added a test to show that the check does work. for @droazen,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1423
https://github.com/broadinstitute/gatk/issues/1426:341,Availability,avail,available,341,"We want to add an overload of `SamReaderFactory.open()` in htsjdk that accepts a `java.nio.file.Path`, and uses `Files.newInputStream(path)` or similar to get an `InputStream` from it and return a SAM/BAM/CRAM reader. This should enable us to transparently load reads from any input source for which there is a Java NIO file system provider available. Such a provider is already implemented for HDFS (https://github.com/damiencarol/jsr203-hadoop). There may be one for GCS as well (and if there isn't, it might be simple to implement one). Note that this feature needs to handle the companion index as well, if present. Also note that we should not add any new dependencies to htsjdk as part of this change. This change should be modeled on the equivalent change @tomwhite recently made for the reference classes in htsjdk (https://github.com/samtools/htsjdk/pull/308). The unit tests in htsjdk for this feature can be very simple -- just take existing `File` arguments to test cases and call `toPath()` on them, and make sure the existing test cases pass. . On the GATK side, we'd want tests to make sure that we can use the new `SamReaderFactory.open()` overload to open BAM/SAM/CRAM files locally, on HDFS, and on GCS. If it turns out that there isn't already a Java NIO provider for GCS and it's non-trivial to implement, that could become a separate ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1426
https://github.com/broadinstitute/gatk/issues/1426:661,Integrability,depend,dependencies,661,"We want to add an overload of `SamReaderFactory.open()` in htsjdk that accepts a `java.nio.file.Path`, and uses `Files.newInputStream(path)` or similar to get an `InputStream` from it and return a SAM/BAM/CRAM reader. This should enable us to transparently load reads from any input source for which there is a Java NIO file system provider available. Such a provider is already implemented for HDFS (https://github.com/damiencarol/jsr203-hadoop). There may be one for GCS as well (and if there isn't, it might be simple to implement one). Note that this feature needs to handle the companion index as well, if present. Also note that we should not add any new dependencies to htsjdk as part of this change. This change should be modeled on the equivalent change @tomwhite recently made for the reference classes in htsjdk (https://github.com/samtools/htsjdk/pull/308). The unit tests in htsjdk for this feature can be very simple -- just take existing `File` arguments to test cases and call `toPath()` on them, and make sure the existing test cases pass. . On the GATK side, we'd want tests to make sure that we can use the new `SamReaderFactory.open()` overload to open BAM/SAM/CRAM files locally, on HDFS, and on GCS. If it turns out that there isn't already a Java NIO provider for GCS and it's non-trivial to implement, that could become a separate ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1426
https://github.com/broadinstitute/gatk/issues/1426:257,Performance,load,load,257,"We want to add an overload of `SamReaderFactory.open()` in htsjdk that accepts a `java.nio.file.Path`, and uses `Files.newInputStream(path)` or similar to get an `InputStream` from it and return a SAM/BAM/CRAM reader. This should enable us to transparently load reads from any input source for which there is a Java NIO file system provider available. Such a provider is already implemented for HDFS (https://github.com/damiencarol/jsr203-hadoop). There may be one for GCS as well (and if there isn't, it might be simple to implement one). Note that this feature needs to handle the companion index as well, if present. Also note that we should not add any new dependencies to htsjdk as part of this change. This change should be modeled on the equivalent change @tomwhite recently made for the reference classes in htsjdk (https://github.com/samtools/htsjdk/pull/308). The unit tests in htsjdk for this feature can be very simple -- just take existing `File` arguments to test cases and call `toPath()` on them, and make sure the existing test cases pass. . On the GATK side, we'd want tests to make sure that we can use the new `SamReaderFactory.open()` overload to open BAM/SAM/CRAM files locally, on HDFS, and on GCS. If it turns out that there isn't already a Java NIO provider for GCS and it's non-trivial to implement, that could become a separate ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1426
https://github.com/broadinstitute/gatk/issues/1426:879,Testability,test,tests,879,"We want to add an overload of `SamReaderFactory.open()` in htsjdk that accepts a `java.nio.file.Path`, and uses `Files.newInputStream(path)` or similar to get an `InputStream` from it and return a SAM/BAM/CRAM reader. This should enable us to transparently load reads from any input source for which there is a Java NIO file system provider available. Such a provider is already implemented for HDFS (https://github.com/damiencarol/jsr203-hadoop). There may be one for GCS as well (and if there isn't, it might be simple to implement one). Note that this feature needs to handle the companion index as well, if present. Also note that we should not add any new dependencies to htsjdk as part of this change. This change should be modeled on the equivalent change @tomwhite recently made for the reference classes in htsjdk (https://github.com/samtools/htsjdk/pull/308). The unit tests in htsjdk for this feature can be very simple -- just take existing `File` arguments to test cases and call `toPath()` on them, and make sure the existing test cases pass. . On the GATK side, we'd want tests to make sure that we can use the new `SamReaderFactory.open()` overload to open BAM/SAM/CRAM files locally, on HDFS, and on GCS. If it turns out that there isn't already a Java NIO provider for GCS and it's non-trivial to implement, that could become a separate ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1426
https://github.com/broadinstitute/gatk/issues/1426:973,Testability,test,test,973,"We want to add an overload of `SamReaderFactory.open()` in htsjdk that accepts a `java.nio.file.Path`, and uses `Files.newInputStream(path)` or similar to get an `InputStream` from it and return a SAM/BAM/CRAM reader. This should enable us to transparently load reads from any input source for which there is a Java NIO file system provider available. Such a provider is already implemented for HDFS (https://github.com/damiencarol/jsr203-hadoop). There may be one for GCS as well (and if there isn't, it might be simple to implement one). Note that this feature needs to handle the companion index as well, if present. Also note that we should not add any new dependencies to htsjdk as part of this change. This change should be modeled on the equivalent change @tomwhite recently made for the reference classes in htsjdk (https://github.com/samtools/htsjdk/pull/308). The unit tests in htsjdk for this feature can be very simple -- just take existing `File` arguments to test cases and call `toPath()` on them, and make sure the existing test cases pass. . On the GATK side, we'd want tests to make sure that we can use the new `SamReaderFactory.open()` overload to open BAM/SAM/CRAM files locally, on HDFS, and on GCS. If it turns out that there isn't already a Java NIO provider for GCS and it's non-trivial to implement, that could become a separate ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1426
https://github.com/broadinstitute/gatk/issues/1426:1040,Testability,test,test,1040,"We want to add an overload of `SamReaderFactory.open()` in htsjdk that accepts a `java.nio.file.Path`, and uses `Files.newInputStream(path)` or similar to get an `InputStream` from it and return a SAM/BAM/CRAM reader. This should enable us to transparently load reads from any input source for which there is a Java NIO file system provider available. Such a provider is already implemented for HDFS (https://github.com/damiencarol/jsr203-hadoop). There may be one for GCS as well (and if there isn't, it might be simple to implement one). Note that this feature needs to handle the companion index as well, if present. Also note that we should not add any new dependencies to htsjdk as part of this change. This change should be modeled on the equivalent change @tomwhite recently made for the reference classes in htsjdk (https://github.com/samtools/htsjdk/pull/308). The unit tests in htsjdk for this feature can be very simple -- just take existing `File` arguments to test cases and call `toPath()` on them, and make sure the existing test cases pass. . On the GATK side, we'd want tests to make sure that we can use the new `SamReaderFactory.open()` overload to open BAM/SAM/CRAM files locally, on HDFS, and on GCS. If it turns out that there isn't already a Java NIO provider for GCS and it's non-trivial to implement, that could become a separate ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1426
https://github.com/broadinstitute/gatk/issues/1426:1087,Testability,test,tests,1087,"We want to add an overload of `SamReaderFactory.open()` in htsjdk that accepts a `java.nio.file.Path`, and uses `Files.newInputStream(path)` or similar to get an `InputStream` from it and return a SAM/BAM/CRAM reader. This should enable us to transparently load reads from any input source for which there is a Java NIO file system provider available. Such a provider is already implemented for HDFS (https://github.com/damiencarol/jsr203-hadoop). There may be one for GCS as well (and if there isn't, it might be simple to implement one). Note that this feature needs to handle the companion index as well, if present. Also note that we should not add any new dependencies to htsjdk as part of this change. This change should be modeled on the equivalent change @tomwhite recently made for the reference classes in htsjdk (https://github.com/samtools/htsjdk/pull/308). The unit tests in htsjdk for this feature can be very simple -- just take existing `File` arguments to test cases and call `toPath()` on them, and make sure the existing test cases pass. . On the GATK side, we'd want tests to make sure that we can use the new `SamReaderFactory.open()` overload to open BAM/SAM/CRAM files locally, on HDFS, and on GCS. If it turns out that there isn't already a Java NIO provider for GCS and it's non-trivial to implement, that could become a separate ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1426
https://github.com/broadinstitute/gatk/issues/1426:514,Usability,simpl,simple,514,"We want to add an overload of `SamReaderFactory.open()` in htsjdk that accepts a `java.nio.file.Path`, and uses `Files.newInputStream(path)` or similar to get an `InputStream` from it and return a SAM/BAM/CRAM reader. This should enable us to transparently load reads from any input source for which there is a Java NIO file system provider available. Such a provider is already implemented for HDFS (https://github.com/damiencarol/jsr203-hadoop). There may be one for GCS as well (and if there isn't, it might be simple to implement one). Note that this feature needs to handle the companion index as well, if present. Also note that we should not add any new dependencies to htsjdk as part of this change. This change should be modeled on the equivalent change @tomwhite recently made for the reference classes in htsjdk (https://github.com/samtools/htsjdk/pull/308). The unit tests in htsjdk for this feature can be very simple -- just take existing `File` arguments to test cases and call `toPath()` on them, and make sure the existing test cases pass. . On the GATK side, we'd want tests to make sure that we can use the new `SamReaderFactory.open()` overload to open BAM/SAM/CRAM files locally, on HDFS, and on GCS. If it turns out that there isn't already a Java NIO provider for GCS and it's non-trivial to implement, that could become a separate ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1426
https://github.com/broadinstitute/gatk/issues/1426:924,Usability,simpl,simple,924,"We want to add an overload of `SamReaderFactory.open()` in htsjdk that accepts a `java.nio.file.Path`, and uses `Files.newInputStream(path)` or similar to get an `InputStream` from it and return a SAM/BAM/CRAM reader. This should enable us to transparently load reads from any input source for which there is a Java NIO file system provider available. Such a provider is already implemented for HDFS (https://github.com/damiencarol/jsr203-hadoop). There may be one for GCS as well (and if there isn't, it might be simple to implement one). Note that this feature needs to handle the companion index as well, if present. Also note that we should not add any new dependencies to htsjdk as part of this change. This change should be modeled on the equivalent change @tomwhite recently made for the reference classes in htsjdk (https://github.com/samtools/htsjdk/pull/308). The unit tests in htsjdk for this feature can be very simple -- just take existing `File` arguments to test cases and call `toPath()` on them, and make sure the existing test cases pass. . On the GATK side, we'd want tests to make sure that we can use the new `SamReaderFactory.open()` overload to open BAM/SAM/CRAM files locally, on HDFS, and on GCS. If it turns out that there isn't already a Java NIO provider for GCS and it's non-trivial to implement, that could become a separate ticket.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1426
https://github.com/broadinstitute/gatk/issues/1427:78,Performance,load,loading,78,"We want the public GATK framework to contain all the generic capabilities for loading and traversing data in various formats -- all new walker types, general-purpose utilities for working with different formats, etc. should ideally live in the framework rather than in protected. Moving these classes to public should also improve code quality by preventing general-purpose code from getting embedded within particular tools, and would encourage the development of clear APIs for using this code. @akiezun since you love to rip code out, you get this one :) You should consult with @LeeTL1220 and @vruano to identify which classes should be moved. If the classes to be moved have already been adequately reviewed and are covered by tests, they shouldn't require a re-review (though we can do one if @LeeTL1220 requests it).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1427
https://github.com/broadinstitute/gatk/issues/1427:732,Testability,test,tests,732,"We want the public GATK framework to contain all the generic capabilities for loading and traversing data in various formats -- all new walker types, general-purpose utilities for working with different formats, etc. should ideally live in the framework rather than in protected. Moving these classes to public should also improve code quality by preventing general-purpose code from getting embedded within particular tools, and would encourage the development of clear APIs for using this code. @akiezun since you love to rip code out, you get this one :) You should consult with @LeeTL1220 and @vruano to identify which classes should be moved. If the classes to be moved have already been adequately reviewed and are covered by tests, they shouldn't require a re-review (though we can do one if @LeeTL1220 requests it).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1427
https://github.com/broadinstitute/gatk/issues/1427:465,Usability,clear,clear,465,"We want the public GATK framework to contain all the generic capabilities for loading and traversing data in various formats -- all new walker types, general-purpose utilities for working with different formats, etc. should ideally live in the framework rather than in protected. Moving these classes to public should also improve code quality by preventing general-purpose code from getting embedded within particular tools, and would encourage the development of clear APIs for using this code. @akiezun since you love to rip code out, you get this one :) You should consult with @LeeTL1220 and @vruano to identify which classes should be moved. If the classes to be moved have already been adequately reviewed and are covered by tests, they shouldn't require a re-review (though we can do one if @LeeTL1220 requests it).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1427
https://github.com/broadinstitute/gatk/pull/1432:227,Availability,down,down,227,"…m for shuffle jobs. This is a first attempt at #1403 to get feedback on the approach. For aggregating tools that don’t have a shuffle (like CountReadsSpark), the existing 10MB per split is an issue since it dramatically slows down processing. Increasing the split size can be done via -bps, but that is not at all obvious and shouldn’t be necessary. The change I’ve made here uses the default split size for Hadoop (which is 128MB on HDFS). For tools that do have a shuffle, I’ve added a -P argument for all of them, which sets the level of parallelism to use for the shuffle. If not set it defaults to one partition per 10MB of input, which is the existing default. Note that for tools that write an output BAM, the level of parallelism set by -P is used for writing a single BAM (the default, since shardedOutput is false), since the reads are first sorted and written to multiple BAM files before finally being merged. Question: there’s a lot of duplicated code here. Would it be a good idea to have a ParallelismArgumentCollection and a ShardedOutputParallel collection? Note that some tools need a -P but not -shardedOutput (e.g. CompareDuplicatesSpark).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1432
https://github.com/broadinstitute/gatk/pull/1432:61,Usability,feedback,feedback,61,"…m for shuffle jobs. This is a first attempt at #1403 to get feedback on the approach. For aggregating tools that don’t have a shuffle (like CountReadsSpark), the existing 10MB per split is an issue since it dramatically slows down processing. Increasing the split size can be done via -bps, but that is not at all obvious and shouldn’t be necessary. The change I’ve made here uses the default split size for Hadoop (which is 128MB on HDFS). For tools that do have a shuffle, I’ve added a -P argument for all of them, which sets the level of parallelism to use for the shuffle. If not set it defaults to one partition per 10MB of input, which is the existing default. Note that for tools that write an output BAM, the level of parallelism set by -P is used for writing a single BAM (the default, since shardedOutput is false), since the reads are first sorted and written to multiple BAM files before finally being merged. Question: there’s a lot of duplicated code here. Would it be a good idea to have a ParallelismArgumentCollection and a ShardedOutputParallel collection? Note that some tools need a -P but not -shardedOutput (e.g. CompareDuplicatesSpark).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1432
https://github.com/broadinstitute/gatk/pull/1433:8,Deployability,configurat,configuration,8,default configuration rather than the path. Fixes #1324.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1433
https://github.com/broadinstitute/gatk/pull/1433:8,Modifiability,config,configuration,8,default configuration rather than the path. Fixes #1324.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1433
https://github.com/broadinstitute/gatk/pull/1434:504,Testability,test,tests,504,"1) reads a kmer kill list (prepared by FindBadGenomicKmers),; 2) finds reads that support a notion of a breakpoint (split or discordant),; 3) kmerizes them for each breakpoint ignoring those kmers that are on the kill list, and then; 4) passes over all the reads again, pulling out those reads containing the breakpoint-supporting kmers.; 5) Finally, the reads to be assembled for each breakpoint are dumped into a FASTQ file.; FindBadGenomicKmers cleanup. More comments. Fewer magic values.; Added unit tests for Kmer class.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1434
https://github.com/broadinstitute/gatk/issues/1436:292,Deployability,pipeline,pipelines,292,Trying to write bam to dev null using PrintReadsSpark results in:. ```; org.broadinstitute.hellbender.exceptions.GATKException: unable to write bam: org.apache.hadoop.fs.ParentNotDirectoryException: Parent path is not a directory: file:/dev/null; at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:50); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1436
https://github.com/broadinstitute/gatk/issues/1438:414,Deployability,pipeline,pipeline,414,"Charlotte recently stepped on a bug in GATK: It interprets -L argument that ends in the regex ':[0-9]*' as indicating a single site in the contig that precedes it and then barfs if it cannot find that contig in the dictionary. In hg38 we have contig names like 'HLA:01:01:01' and when used on the command-line (as in CreateRealignerTargets) it barfs as in the following workflow: https://picard.broadinstitute.org/pipeline/workflows/viewWorkflow/8536444. given that the SAM spec allows any printed character ! through ~ in the ending of contig names (yikes!!) https://github.com/samtools/hts-specs/issues/124 it seems that some more ""smarts"" needs to be put into the parsing of this argument.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1438
https://github.com/broadinstitute/gatk/pull/1439:310,Deployability,upgrade,upgrade,310,"Fixes https://github.com/broadinstitute/gatk/issues/611. Uses ""validationStringency"" as the argument; PicardCommandLineProgram currently uses ""VALIDATION_STRINGENCY""; should we align all of these to use the same name?. I've done the same work for ReadSparkSource and GATKSparkTool but it requires a Hadoop-BAM upgrade so its in a separate PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1439
https://github.com/broadinstitute/gatk/pull/1439:63,Security,validat,validationStringency,63,"Fixes https://github.com/broadinstitute/gatk/issues/611. Uses ""validationStringency"" as the argument; PicardCommandLineProgram currently uses ""VALIDATION_STRINGENCY""; should we align all of these to use the same name?. I've done the same work for ReadSparkSource and GATKSparkTool but it requires a Hadoop-BAM upgrade so its in a separate PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1439
https://github.com/broadinstitute/gatk/issues/1440:0,Testability,Test,Test,0,Test coverage on the Hadoop-BAM project is currently an abysmal 26% (https://github.com/HadoopGenomics/Hadoop-BAM/issues/51). Let's contribute some tests to both bring this number up and build our own expertise in that codebase.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1440
https://github.com/broadinstitute/gatk/issues/1440:148,Testability,test,tests,148,Test coverage on the Hadoop-BAM project is currently an abysmal 26% (https://github.com/HadoopGenomics/Hadoop-BAM/issues/51). Let's contribute some tests to both bring this number up and build our own expertise in that codebase.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1440
https://github.com/broadinstitute/gatk/issues/1441:505,Integrability,depend,dependency,505,"Investigate whether there's a way to use java.nio.file.Path transparently with files in GCS buckets. We already know we can do this for files in HDFS, as someone has implemented a Java NIO file system provider for HDFS (https://github.com/damiencarol/jsr203-hadoop) -- as long as we have this provider in our classpath, any method that accepts a java.nio.file.Path argument will automatically work with files in HDFS, even if that method lives in a project (like htsjdk) that does not have a Spark/Hadoop dependency or any direct awareness of HDFS. Does such a thing exist for GCS as well? If not, would it be easy to implement? If we could use java.nio.file.Path as a universal resource handle it would help a great deal to resolve the current problems in hellbender with inconsistent support across tools for the various kinds of input. Relates to https://github.com/broadinstitute/gatk/issues/1426 and https://github.com/broadinstitute/gatk/issues/958",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1441
https://github.com/broadinstitute/gatk/issues/1443:156,Integrability,interface,interface,156,"htsjdk doesn't accept 2bit reference files, so we can't use them as CRAM references. One possible fix is to change the htsjdk CRAM reference class to be an interface, and then provide a 2bit implementation from GATK. For Spark, Hadoop-BAM would need to know how to instantiate these as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1443
https://github.com/broadinstitute/gatk/issues/1444:769,Availability,Error,Errors,769,"I am running into an issue with MarkDuplicatesSpark where it runs successfully in local mode, but if I run it in standalone mode with files on the file system or hdfs I get unexpected results. It could be that I'm doing something very basic wrong here, so I'm pasting a bunch of contextual information in case it helps. Thanks in advance!. ```; # Output BAM is correct in local mode:; $GATK_DIR/gatk-launch MarkDuplicatesSpark -I NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam -O test.local.bam. # Outputs empty BAM (i.e. header but no alignments) in standalone mode:; $GATK_DIR/gatk-launch MarkDuplicatesSpark -I NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam -O test.standalone.bam -- --sparkRunner SPARK --sparkMaster spark://localhost:7077. # Errors out with a wrong FS type (stacktrace below):; $GATK_DIR/gatk-launch MarkDuplicatesSpark -I hdfs://bam/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam -O hdfs://bam/test.hdfs.bam -- --sparkRunner SPARK --sparkMaster spark://localhost:7077; ```. And the stacktrace for the HDFS case (I'm running HDFS in Pseudo-Distributed mode on the same host as I a running Spark standalone mode on): . ```; org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=1029177344; java.lang.IllegalArgumentException: Wrong FS: hdfs://bam/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam, expected: file:///; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:465); at org.apache.hadoop.fs.FilterFileSystem.makeQualified(FilterFileSystem.java:119); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:181); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:3467,Availability,error,error,3467,"ommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:3876,Availability,echo,echo,3876,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:3974,Availability,echo,echo,3974,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:4066,Availability,echo,echo,4066,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:4223,Availability,echo,echo,4223,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:4369,Availability,echo,echo,4369,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:4375,Availability,Down,Downloading,4375,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:4566,Availability,echo,echo,4566,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:4721,Availability,echo,echo,4721,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:4727,Availability,Down,Downloading,4727,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:4882,Availability,echo,echo,4882,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:2969,Deployability,deploy,deploy,2969,"lizeToolInputs(GATKSparkTool.java:264); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:3006,Deployability,deploy,deploy,3006,"g.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:3078,Deployability,deploy,deploy,3078,"ne(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf sha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:3154,Deployability,deploy,deploy,3154,"arkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; su",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:3225,Deployability,deploy,deploy,3225,"roadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:3294,Deployability,deploy,deploy,3294,"neProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; su",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:3859,Deployability,install,installed,3859,"mpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:3882,Deployability,Install,Installing,3882,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:3966,Deployability,update,update,3966,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:4173,Deployability,install,install,4173,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:4197,Deployability,install,installer,4197,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:4229,Deployability,Install,Installing,4229,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:4313,Deployability,update,update,4313,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:4350,Deployability,install,install,4350,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:4926,Deployability,install,installAll,4926,"; at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar.gz; export GATK_DIR=gatk-4.alpha. echo ""Building GATK4""; cd $GATK_DIR; gradle installAll; cd .. ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:491,Testability,test,test,491,"I am running into an issue with MarkDuplicatesSpark where it runs successfully in local mode, but if I run it in standalone mode with files on the file system or hdfs I get unexpected results. It could be that I'm doing something very basic wrong here, so I'm pasting a bunch of contextual information in case it helps. Thanks in advance!. ```; # Output BAM is correct in local mode:; $GATK_DIR/gatk-launch MarkDuplicatesSpark -I NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam -O test.local.bam. # Outputs empty BAM (i.e. header but no alignments) in standalone mode:; $GATK_DIR/gatk-launch MarkDuplicatesSpark -I NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam -O test.standalone.bam -- --sparkRunner SPARK --sparkMaster spark://localhost:7077. # Errors out with a wrong FS type (stacktrace below):; $GATK_DIR/gatk-launch MarkDuplicatesSpark -I hdfs://bam/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam -O hdfs://bam/test.hdfs.bam -- --sparkRunner SPARK --sparkMaster spark://localhost:7077; ```. And the stacktrace for the HDFS case (I'm running HDFS in Pseudo-Distributed mode on the same host as I a running Spark standalone mode on): . ```; org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=1029177344; java.lang.IllegalArgumentException: Wrong FS: hdfs://bam/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam, expected: file:///; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:465); at org.apache.hadoop.fs.FilterFileSystem.makeQualified(FilterFileSystem.java:119); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:181); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:686,Testability,test,test,686,"I am running into an issue with MarkDuplicatesSpark where it runs successfully in local mode, but if I run it in standalone mode with files on the file system or hdfs I get unexpected results. It could be that I'm doing something very basic wrong here, so I'm pasting a bunch of contextual information in case it helps. Thanks in advance!. ```; # Output BAM is correct in local mode:; $GATK_DIR/gatk-launch MarkDuplicatesSpark -I NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam -O test.local.bam. # Outputs empty BAM (i.e. header but no alignments) in standalone mode:; $GATK_DIR/gatk-launch MarkDuplicatesSpark -I NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam -O test.standalone.bam -- --sparkRunner SPARK --sparkMaster spark://localhost:7077. # Errors out with a wrong FS type (stacktrace below):; $GATK_DIR/gatk-launch MarkDuplicatesSpark -I hdfs://bam/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam -O hdfs://bam/test.hdfs.bam -- --sparkRunner SPARK --sparkMaster spark://localhost:7077; ```. And the stacktrace for the HDFS case (I'm running HDFS in Pseudo-Distributed mode on the same host as I a running Spark standalone mode on): . ```; org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=1029177344; java.lang.IllegalArgumentException: Wrong FS: hdfs://bam/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam, expected: file:///; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:465); at org.apache.hadoop.fs.FilterFileSystem.makeQualified(FilterFileSystem.java:119); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:181); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1444:950,Testability,test,test,950,"I am running into an issue with MarkDuplicatesSpark where it runs successfully in local mode, but if I run it in standalone mode with files on the file system or hdfs I get unexpected results. It could be that I'm doing something very basic wrong here, so I'm pasting a bunch of contextual information in case it helps. Thanks in advance!. ```; # Output BAM is correct in local mode:; $GATK_DIR/gatk-launch MarkDuplicatesSpark -I NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam -O test.local.bam. # Outputs empty BAM (i.e. header but no alignments) in standalone mode:; $GATK_DIR/gatk-launch MarkDuplicatesSpark -I NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam -O test.standalone.bam -- --sparkRunner SPARK --sparkMaster spark://localhost:7077. # Errors out with a wrong FS type (stacktrace below):; $GATK_DIR/gatk-launch MarkDuplicatesSpark -I hdfs://bam/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam -O hdfs://bam/test.hdfs.bam -- --sparkRunner SPARK --sparkMaster spark://localhost:7077; ```. And the stacktrace for the HDFS case (I'm running HDFS in Pseudo-Distributed mode on the same host as I a running Spark standalone mode on): . ```; org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=1029177344; java.lang.IllegalArgumentException: Wrong FS: hdfs://bam/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam, expected: file:///; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:465); at org.apache.hadoop.fs.FilterFileSystem.makeQualified(FilterFileSystem.java:119); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:181); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444
https://github.com/broadinstitute/gatk/issues/1445:1003,Security,validat,validating,1003,"Currently one can indicate if a program argument is optional by setting the Attribute annotation property `optional` to `true`. . Then, it is up to the programmer to indicate a default value for the parameter explicitly initializing the corresponding field with such a default value. It seems to me that we would gain versatility/expressibility if we use java.util.Optional as the type of the field:; - The code can then know wether the user actually gave a value to the parameter using `isPresent()`; currently there is no way to do so as the value provided by the user happens to be the default value.; - The default value may be defined dynamically based on other argument values or input data using `orElse(dynamicDefault)` or `orElseGet(dynamicDefaultLamda)`.; - The current solution prompts to use arbitrary marker constant value to deactivate the functionality; behind the user argument. eg.`maximumDepth = -1`. This constant may in fact be outside the valid ; range for the argument which makes validating it a bit more difficult.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1445
https://github.com/broadinstitute/gatk/issues/1447:91,Availability,error,error,91,```; // Suggested by the akka devs to make sure that we do not get the spark configuration error.; // http://doc.akka.io/docs/akka/snapshot/general/configuration.html#When_using_JarJar__OneJar__Assembly_or_any_jar-bundler; transform(com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer) {; resource = 'reference.conf'; }; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1447
https://github.com/broadinstitute/gatk/issues/1447:77,Deployability,configurat,configuration,77,```; // Suggested by the akka devs to make sure that we do not get the spark configuration error.; // http://doc.akka.io/docs/akka/snapshot/general/configuration.html#When_using_JarJar__OneJar__Assembly_or_any_jar-bundler; transform(com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer) {; resource = 'reference.conf'; }; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1447
https://github.com/broadinstitute/gatk/issues/1447:148,Deployability,configurat,configuration,148,```; // Suggested by the akka devs to make sure that we do not get the spark configuration error.; // http://doc.akka.io/docs/akka/snapshot/general/configuration.html#When_using_JarJar__OneJar__Assembly_or_any_jar-bundler; transform(com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer) {; resource = 'reference.conf'; }; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1447
https://github.com/broadinstitute/gatk/issues/1447:77,Modifiability,config,configuration,77,```; // Suggested by the akka devs to make sure that we do not get the spark configuration error.; // http://doc.akka.io/docs/akka/snapshot/general/configuration.html#When_using_JarJar__OneJar__Assembly_or_any_jar-bundler; transform(com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer) {; resource = 'reference.conf'; }; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1447
https://github.com/broadinstitute/gatk/issues/1447:148,Modifiability,config,configuration,148,```; // Suggested by the akka devs to make sure that we do not get the spark configuration error.; // http://doc.akka.io/docs/akka/snapshot/general/configuration.html#When_using_JarJar__OneJar__Assembly_or_any_jar-bundler; transform(com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer) {; resource = 'reference.conf'; }; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1447
https://github.com/broadinstitute/gatk/issues/1447:261,Modifiability,plugin,plugins,261,```; // Suggested by the akka devs to make sure that we do not get the spark configuration error.; // http://doc.akka.io/docs/akka/snapshot/general/configuration.html#When_using_JarJar__OneJar__Assembly_or_any_jar-bundler; transform(com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer) {; resource = 'reference.conf'; }; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1447
https://github.com/broadinstitute/gatk/issues/1448:242,Testability,test,test,242,"This was reported by @geetduggal in #1444. . If you run MarkDuplicatesSpark using a local standalone spark cluster it can run to completion, but produces an empty bam file that has only a header. ```; ./gatk-launch MarkDuplicatesSpark -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O testoutput.bam -- --sparkRunner SPARK --sparkMaster spark://wm1b0-8ab:7077; ```. produces an empty bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1448
https://github.com/broadinstitute/gatk/issues/1448:306,Testability,test,testoutput,306,"This was reported by @geetduggal in #1444. . If you run MarkDuplicatesSpark using a local standalone spark cluster it can run to completion, but produces an empty bam file that has only a header. ```; ./gatk-launch MarkDuplicatesSpark -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O testoutput.bam -- --sparkRunner SPARK --sparkMaster spark://wm1b0-8ab:7077; ```. produces an empty bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1448
https://github.com/broadinstitute/gatk/pull/1450:299,Availability,error,errors,299,fixes #1448. MarkDuplicates spark was not writing an output bam when running with a standalone spark instance if the output path was a relative file path.; This fixes the problem by making any relative file paths into absolute file paths.; Added a check to see if no part files can be found so that errors like this will crash in the future instead of silently failing. No tests are added because it's still unclear how to test errors that only occur in certain spark configurations. `makeFilePathAbsolute()` should be redundant once #958 is finished,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1450
https://github.com/broadinstitute/gatk/pull/1450:428,Availability,error,errors,428,fixes #1448. MarkDuplicates spark was not writing an output bam when running with a standalone spark instance if the output path was a relative file path.; This fixes the problem by making any relative file paths into absolute file paths.; Added a check to see if no part files can be found so that errors like this will crash in the future instead of silently failing. No tests are added because it's still unclear how to test errors that only occur in certain spark configurations. `makeFilePathAbsolute()` should be redundant once #958 is finished,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1450
https://github.com/broadinstitute/gatk/pull/1450:519,Availability,redundant,redundant,519,fixes #1448. MarkDuplicates spark was not writing an output bam when running with a standalone spark instance if the output path was a relative file path.; This fixes the problem by making any relative file paths into absolute file paths.; Added a check to see if no part files can be found so that errors like this will crash in the future instead of silently failing. No tests are added because it's still unclear how to test errors that only occur in certain spark configurations. `makeFilePathAbsolute()` should be redundant once #958 is finished,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1450
https://github.com/broadinstitute/gatk/pull/1450:468,Deployability,configurat,configurations,468,fixes #1448. MarkDuplicates spark was not writing an output bam when running with a standalone spark instance if the output path was a relative file path.; This fixes the problem by making any relative file paths into absolute file paths.; Added a check to see if no part files can be found so that errors like this will crash in the future instead of silently failing. No tests are added because it's still unclear how to test errors that only occur in certain spark configurations. `makeFilePathAbsolute()` should be redundant once #958 is finished,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1450
https://github.com/broadinstitute/gatk/pull/1450:468,Modifiability,config,configurations,468,fixes #1448. MarkDuplicates spark was not writing an output bam when running with a standalone spark instance if the output path was a relative file path.; This fixes the problem by making any relative file paths into absolute file paths.; Added a check to see if no part files can be found so that errors like this will crash in the future instead of silently failing. No tests are added because it's still unclear how to test errors that only occur in certain spark configurations. `makeFilePathAbsolute()` should be redundant once #958 is finished,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1450
https://github.com/broadinstitute/gatk/pull/1450:519,Safety,redund,redundant,519,fixes #1448. MarkDuplicates spark was not writing an output bam when running with a standalone spark instance if the output path was a relative file path.; This fixes the problem by making any relative file paths into absolute file paths.; Added a check to see if no part files can be found so that errors like this will crash in the future instead of silently failing. No tests are added because it's still unclear how to test errors that only occur in certain spark configurations. `makeFilePathAbsolute()` should be redundant once #958 is finished,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1450
https://github.com/broadinstitute/gatk/pull/1450:373,Testability,test,tests,373,fixes #1448. MarkDuplicates spark was not writing an output bam when running with a standalone spark instance if the output path was a relative file path.; This fixes the problem by making any relative file paths into absolute file paths.; Added a check to see if no part files can be found so that errors like this will crash in the future instead of silently failing. No tests are added because it's still unclear how to test errors that only occur in certain spark configurations. `makeFilePathAbsolute()` should be redundant once #958 is finished,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1450
https://github.com/broadinstitute/gatk/pull/1450:423,Testability,test,test,423,fixes #1448. MarkDuplicates spark was not writing an output bam when running with a standalone spark instance if the output path was a relative file path.; This fixes the problem by making any relative file paths into absolute file paths.; Added a check to see if no part files can be found so that errors like this will crash in the future instead of silently failing. No tests are added because it's still unclear how to test errors that only occur in certain spark configurations. `makeFilePathAbsolute()` should be redundant once #958 is finished,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1450
https://github.com/broadinstitute/gatk/issues/1451:2411,Deployability,deploy,deploy,2411,edFileSystem.java:641); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:641); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.deleteHadoopFile(ReadsSparkSink.java:200); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:191); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:106); at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:94); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1451
https://github.com/broadinstitute/gatk/issues/1451:2448,Deployability,deploy,deploy,2448,edFileSystem.java:641); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:641); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.deleteHadoopFile(ReadsSparkSink.java:200); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:191); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:106); at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:94); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1451
https://github.com/broadinstitute/gatk/issues/1451:2520,Deployability,deploy,deploy,2520,edFileSystem.java:641); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:641); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.deleteHadoopFile(ReadsSparkSink.java:200); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:191); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:106); at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:94); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1451
https://github.com/broadinstitute/gatk/issues/1451:2596,Deployability,deploy,deploy,2596,edFileSystem.java:641); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:641); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.deleteHadoopFile(ReadsSparkSink.java:200); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:191); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:106); at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:94); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1451
https://github.com/broadinstitute/gatk/issues/1451:2667,Deployability,deploy,deploy,2667,edFileSystem.java:641); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:641); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.deleteHadoopFile(ReadsSparkSink.java:200); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:191); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:106); at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:94); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1451
https://github.com/broadinstitute/gatk/issues/1451:2736,Deployability,deploy,deploy,2736,edFileSystem.java:641); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:641); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.deleteHadoopFile(ReadsSparkSink.java:200); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:191); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:106); at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:94); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1451
https://github.com/broadinstitute/gatk/issues/1451:581,Security,access,access,581,"Likely an analogous problem to #1417. ```; ./gatk-launch MarkDuplicatesSpark -I file:///home/unix/louisb/flag_stat.bam -O file:///home/unix/louisb/testoutput.bam -- --sparkRunner SPARK --sparkMaster yarn-client; ```. results in:. ```; java.lang.IllegalArgumentException: Wrong FS: file:/home/unix/louisb/testoutput.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193); at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105); at org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:645); at org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:641); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:641); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.deleteHadoopFile(ReadsSparkSink.java:200); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:191); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:106); at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:94); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1451
https://github.com/broadinstitute/gatk/issues/1451:147,Testability,test,testoutput,147,"Likely an analogous problem to #1417. ```; ./gatk-launch MarkDuplicatesSpark -I file:///home/unix/louisb/flag_stat.bam -O file:///home/unix/louisb/testoutput.bam -- --sparkRunner SPARK --sparkMaster yarn-client; ```. results in:. ```; java.lang.IllegalArgumentException: Wrong FS: file:/home/unix/louisb/testoutput.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193); at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105); at org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:645); at org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:641); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:641); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.deleteHadoopFile(ReadsSparkSink.java:200); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:191); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:106); at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:94); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1451
https://github.com/broadinstitute/gatk/issues/1451:304,Testability,test,testoutput,304,"Likely an analogous problem to #1417. ```; ./gatk-launch MarkDuplicatesSpark -I file:///home/unix/louisb/flag_stat.bam -O file:///home/unix/louisb/testoutput.bam -- --sparkRunner SPARK --sparkMaster yarn-client; ```. results in:. ```; java.lang.IllegalArgumentException: Wrong FS: file:/home/unix/louisb/testoutput.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193); at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105); at org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:645); at org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:641); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:641); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.deleteHadoopFile(ReadsSparkSink.java:200); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:191); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:106); at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:94); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1451
https://github.com/broadinstitute/gatk/issues/1452:242,Availability,error,error,242,"Hadoop bam `AnySAMInputFormat.createRecordReader` throws `IllegalArgumentException` if it can't determine the format of a sam/bam file. If this is caused by a nonexistant file it hides the `FileNotFoundException` which would be a more useful error message. . This cropped up in a case where a file local to one machine was being specified as an input a spark cluster with multiple nodes. The file existed for the master node, but the workers failed because it was unavailable to them. . It would be helpful if the error message mentioned this possibility. . see #1417 for the original issue",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1452
https://github.com/broadinstitute/gatk/issues/1452:514,Availability,error,error,514,"Hadoop bam `AnySAMInputFormat.createRecordReader` throws `IllegalArgumentException` if it can't determine the format of a sam/bam file. If this is caused by a nonexistant file it hides the `FileNotFoundException` which would be a more useful error message. . This cropped up in a case where a file local to one machine was being specified as an input a spark cluster with multiple nodes. The file existed for the master node, but the workers failed because it was unavailable to them. . It would be helpful if the error message mentioned this possibility. . see #1417 for the original issue",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1452
https://github.com/broadinstitute/gatk/issues/1452:248,Integrability,message,message,248,"Hadoop bam `AnySAMInputFormat.createRecordReader` throws `IllegalArgumentException` if it can't determine the format of a sam/bam file. If this is caused by a nonexistant file it hides the `FileNotFoundException` which would be a more useful error message. . This cropped up in a case where a file local to one machine was being specified as an input a spark cluster with multiple nodes. The file existed for the master node, but the workers failed because it was unavailable to them. . It would be helpful if the error message mentioned this possibility. . see #1417 for the original issue",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1452
https://github.com/broadinstitute/gatk/issues/1452:520,Integrability,message,message,520,"Hadoop bam `AnySAMInputFormat.createRecordReader` throws `IllegalArgumentException` if it can't determine the format of a sam/bam file. If this is caused by a nonexistant file it hides the `FileNotFoundException` which would be a more useful error message. . This cropped up in a case where a file local to one machine was being specified as an input a spark cluster with multiple nodes. The file existed for the master node, but the workers failed because it was unavailable to them. . It would be helpful if the error message mentioned this possibility. . see #1417 for the original issue",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1452
https://github.com/broadinstitute/gatk/issues/1453:147,Deployability,configurat,configuration,147,"A gradle plugin may be able to solve the problem of how people building extensions to gatk can create sparkJar's without a lot of confusing custom configuration. People building projects on top of gatk would apply the plugin to their own build script, and it would automatically configure a sparkJar target.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1453
https://github.com/broadinstitute/gatk/issues/1453:9,Modifiability,plugin,plugin,9,"A gradle plugin may be able to solve the problem of how people building extensions to gatk can create sparkJar's without a lot of confusing custom configuration. People building projects on top of gatk would apply the plugin to their own build script, and it would automatically configure a sparkJar target.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1453
https://github.com/broadinstitute/gatk/issues/1453:147,Modifiability,config,configuration,147,"A gradle plugin may be able to solve the problem of how people building extensions to gatk can create sparkJar's without a lot of confusing custom configuration. People building projects on top of gatk would apply the plugin to their own build script, and it would automatically configure a sparkJar target.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1453
https://github.com/broadinstitute/gatk/issues/1453:218,Modifiability,plugin,plugin,218,"A gradle plugin may be able to solve the problem of how people building extensions to gatk can create sparkJar's without a lot of confusing custom configuration. People building projects on top of gatk would apply the plugin to their own build script, and it would automatically configure a sparkJar target.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1453
https://github.com/broadinstitute/gatk/issues/1453:279,Modifiability,config,configure,279,"A gradle plugin may be able to solve the problem of how people building extensions to gatk can create sparkJar's without a lot of confusing custom configuration. People building projects on top of gatk would apply the plugin to their own build script, and it would automatically configure a sparkJar target.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1453
https://github.com/broadinstitute/gatk/issues/1454:4,Deployability,continuous,continuously,4,I'm continuously sad that my shell can't autocomplete GATK commands. It would be great if we could generate completion files for bash and zsh automatically. This could piggyback on the upcoming documentation generation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1454
https://github.com/broadinstitute/gatk/issues/1457:28,Security,validat,validation,28,"Currently our default reads validation stringency as defined in `GATKTool` is SILENT -- should it be STRICT, or is STRICT too impractical/annoying for read-world data?. Prompted by a discussion in https://github.com/broadinstitute/gatk/pull/1439",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1457
https://github.com/broadinstitute/gatk/issues/1460:206,Availability,down,down,206,There's been a request from production (https://broadinstitute.atlassian.net/browse/DSDEEPB-2789) to try to ensure that the walker BQSR tools in hellbender can run with less than 3.5 GB of RAM to get costs down.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1460
https://github.com/broadinstitute/gatk/pull/1461:605,Availability,down,downstream,605,- fixup for problem with fully specified `file:///` names that I introduced in #1450 ; - adding test for fully specified `file:///` url; - adding additional tests to `ReadSparkSink` for HDFS; - tests for writing to HDFS using `MiniDFSCluster`; - tests for overwriting existing HDFS paths; - fixed instance of Wrong FileSystem exception in `ReadSparkSink`; - refactored `ReadSparkSink` to remove duplication; - adding `MiniClusterUtils`; - revising existing code using `MiniDFSCluster` to go through `MiniClusterUtils`; - had to make the minicluster dependency a compile time instead of test dependency so downstream projects can make use of MiniClusterUtils.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1461
https://github.com/broadinstitute/gatk/pull/1461:549,Integrability,depend,dependency,549,- fixup for problem with fully specified `file:///` names that I introduced in #1450 ; - adding test for fully specified `file:///` url; - adding additional tests to `ReadSparkSink` for HDFS; - tests for writing to HDFS using `MiniDFSCluster`; - tests for overwriting existing HDFS paths; - fixed instance of Wrong FileSystem exception in `ReadSparkSink`; - refactored `ReadSparkSink` to remove duplication; - adding `MiniClusterUtils`; - revising existing code using `MiniDFSCluster` to go through `MiniClusterUtils`; - had to make the minicluster dependency a compile time instead of test dependency so downstream projects can make use of MiniClusterUtils.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1461
https://github.com/broadinstitute/gatk/pull/1461:591,Integrability,depend,dependency,591,- fixup for problem with fully specified `file:///` names that I introduced in #1450 ; - adding test for fully specified `file:///` url; - adding additional tests to `ReadSparkSink` for HDFS; - tests for writing to HDFS using `MiniDFSCluster`; - tests for overwriting existing HDFS paths; - fixed instance of Wrong FileSystem exception in `ReadSparkSink`; - refactored `ReadSparkSink` to remove duplication; - adding `MiniClusterUtils`; - revising existing code using `MiniDFSCluster` to go through `MiniClusterUtils`; - had to make the minicluster dependency a compile time instead of test dependency so downstream projects can make use of MiniClusterUtils.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1461
https://github.com/broadinstitute/gatk/pull/1461:358,Modifiability,refactor,refactored,358,- fixup for problem with fully specified `file:///` names that I introduced in #1450 ; - adding test for fully specified `file:///` url; - adding additional tests to `ReadSparkSink` for HDFS; - tests for writing to HDFS using `MiniDFSCluster`; - tests for overwriting existing HDFS paths; - fixed instance of Wrong FileSystem exception in `ReadSparkSink`; - refactored `ReadSparkSink` to remove duplication; - adding `MiniClusterUtils`; - revising existing code using `MiniDFSCluster` to go through `MiniClusterUtils`; - had to make the minicluster dependency a compile time instead of test dependency so downstream projects can make use of MiniClusterUtils.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1461
https://github.com/broadinstitute/gatk/pull/1461:96,Testability,test,test,96,- fixup for problem with fully specified `file:///` names that I introduced in #1450 ; - adding test for fully specified `file:///` url; - adding additional tests to `ReadSparkSink` for HDFS; - tests for writing to HDFS using `MiniDFSCluster`; - tests for overwriting existing HDFS paths; - fixed instance of Wrong FileSystem exception in `ReadSparkSink`; - refactored `ReadSparkSink` to remove duplication; - adding `MiniClusterUtils`; - revising existing code using `MiniDFSCluster` to go through `MiniClusterUtils`; - had to make the minicluster dependency a compile time instead of test dependency so downstream projects can make use of MiniClusterUtils.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1461
https://github.com/broadinstitute/gatk/pull/1461:157,Testability,test,tests,157,- fixup for problem with fully specified `file:///` names that I introduced in #1450 ; - adding test for fully specified `file:///` url; - adding additional tests to `ReadSparkSink` for HDFS; - tests for writing to HDFS using `MiniDFSCluster`; - tests for overwriting existing HDFS paths; - fixed instance of Wrong FileSystem exception in `ReadSparkSink`; - refactored `ReadSparkSink` to remove duplication; - adding `MiniClusterUtils`; - revising existing code using `MiniDFSCluster` to go through `MiniClusterUtils`; - had to make the minicluster dependency a compile time instead of test dependency so downstream projects can make use of MiniClusterUtils.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1461
https://github.com/broadinstitute/gatk/pull/1461:194,Testability,test,tests,194,- fixup for problem with fully specified `file:///` names that I introduced in #1450 ; - adding test for fully specified `file:///` url; - adding additional tests to `ReadSparkSink` for HDFS; - tests for writing to HDFS using `MiniDFSCluster`; - tests for overwriting existing HDFS paths; - fixed instance of Wrong FileSystem exception in `ReadSparkSink`; - refactored `ReadSparkSink` to remove duplication; - adding `MiniClusterUtils`; - revising existing code using `MiniDFSCluster` to go through `MiniClusterUtils`; - had to make the minicluster dependency a compile time instead of test dependency so downstream projects can make use of MiniClusterUtils.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1461
https://github.com/broadinstitute/gatk/pull/1461:246,Testability,test,tests,246,- fixup for problem with fully specified `file:///` names that I introduced in #1450 ; - adding test for fully specified `file:///` url; - adding additional tests to `ReadSparkSink` for HDFS; - tests for writing to HDFS using `MiniDFSCluster`; - tests for overwriting existing HDFS paths; - fixed instance of Wrong FileSystem exception in `ReadSparkSink`; - refactored `ReadSparkSink` to remove duplication; - adding `MiniClusterUtils`; - revising existing code using `MiniDFSCluster` to go through `MiniClusterUtils`; - had to make the minicluster dependency a compile time instead of test dependency so downstream projects can make use of MiniClusterUtils.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1461
https://github.com/broadinstitute/gatk/pull/1461:586,Testability,test,test,586,- fixup for problem with fully specified `file:///` names that I introduced in #1450 ; - adding test for fully specified `file:///` url; - adding additional tests to `ReadSparkSink` for HDFS; - tests for writing to HDFS using `MiniDFSCluster`; - tests for overwriting existing HDFS paths; - fixed instance of Wrong FileSystem exception in `ReadSparkSink`; - refactored `ReadSparkSink` to remove duplication; - adding `MiniClusterUtils`; - revising existing code using `MiniDFSCluster` to go through `MiniClusterUtils`; - had to make the minicluster dependency a compile time instead of test dependency so downstream projects can make use of MiniClusterUtils.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1461
https://github.com/broadinstitute/gatk/issues/1465:138,Availability,mask,masks,138,"The test `testSortingByColumn` doesn't actually test anything and likely never has. . It throws and silently swallows an exception, which masks the fact that it's creating an empty table to test, and doesn't work when the table isn't empty. This has been inherited unchanged from Gatk3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1465
https://github.com/broadinstitute/gatk/issues/1465:255,Modifiability,inherit,inherited,255,"The test `testSortingByColumn` doesn't actually test anything and likely never has. . It throws and silently swallows an exception, which masks the fact that it's creating an empty table to test, and doesn't work when the table isn't empty. This has been inherited unchanged from Gatk3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1465
https://github.com/broadinstitute/gatk/issues/1465:4,Testability,test,test,4,"The test `testSortingByColumn` doesn't actually test anything and likely never has. . It throws and silently swallows an exception, which masks the fact that it's creating an empty table to test, and doesn't work when the table isn't empty. This has been inherited unchanged from Gatk3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1465
https://github.com/broadinstitute/gatk/issues/1465:10,Testability,test,testSortingByColumn,10,"The test `testSortingByColumn` doesn't actually test anything and likely never has. . It throws and silently swallows an exception, which masks the fact that it's creating an empty table to test, and doesn't work when the table isn't empty. This has been inherited unchanged from Gatk3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1465
https://github.com/broadinstitute/gatk/issues/1465:48,Testability,test,test,48,"The test `testSortingByColumn` doesn't actually test anything and likely never has. . It throws and silently swallows an exception, which masks the fact that it's creating an empty table to test, and doesn't work when the table isn't empty. This has been inherited unchanged from Gatk3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1465
https://github.com/broadinstitute/gatk/issues/1465:190,Testability,test,test,190,"The test `testSortingByColumn` doesn't actually test anything and likely never has. . It throws and silently swallows an exception, which masks the fact that it's creating an empty table to test, and doesn't work when the table isn't empty. This has been inherited unchanged from Gatk3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1465
https://github.com/broadinstitute/gatk/pull/1466:15,Testability,test,test,15,fixing a bogus test that never worked (fixes #1465); changing functions that generate simple GATKReports to require a sort order; renaming GATKReportTable.TableSortingWay -> Sorting. not directly to master this time...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1466
https://github.com/broadinstitute/gatk/pull/1466:86,Usability,simpl,simple,86,fixing a bogus test that never worked (fixes #1465); changing functions that generate simple GATKReports to require a sort order; renaming GATKReportTable.TableSortingWay -> Sorting. not directly to master this time...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1466
https://github.com/broadinstitute/gatk/pull/1469:9,Deployability,upgrade,upgrades,9,"Requires upgrades to Hadoop-BAM (not yet released) and htsjdk. Fixes https://github.com/broadinstitute/gatk/issues/1346, https://github.com/broadinstitute/gatk/issues/1261, https://github.com/broadinstitute/gatk/issues/1175, https://github.com/broadinstitute/gatk/issues/1326, https://github.com/broadinstitute/gatk/issues/1259, and much of the underlying code for https://github.com/broadinstitute/gatk/issues/1270, which will be enabled in a separate PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1469
https://github.com/broadinstitute/gatk/pull/1469:41,Deployability,release,released,41,"Requires upgrades to Hadoop-BAM (not yet released) and htsjdk. Fixes https://github.com/broadinstitute/gatk/issues/1346, https://github.com/broadinstitute/gatk/issues/1261, https://github.com/broadinstitute/gatk/issues/1175, https://github.com/broadinstitute/gatk/issues/1326, https://github.com/broadinstitute/gatk/issues/1259, and much of the underlying code for https://github.com/broadinstitute/gatk/issues/1270, which will be enabled in a separate PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1469
https://github.com/broadinstitute/gatk/pull/1471:56,Testability,test,test,56,for @droazen. a lot of legacy code here with suboptimal test coverage. I only did a very shallow pass over the style issues. I'm mostyl looking for feedback on whether some of those pieces are not needed. I can deal with style later in the review,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1471
https://github.com/broadinstitute/gatk/pull/1471:148,Usability,feedback,feedback,148,for @droazen. a lot of legacy code here with suboptimal test coverage. I only did a very shallow pass over the style issues. I'm mostyl looking for feedback on whether some of those pieces are not needed. I can deal with style later in the review,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1471
https://github.com/broadinstitute/gatk/issues/1473:1093,Integrability,wrap,wrapAndCopyInto,1093,running RevertBaseQualityScores from Version:4.alpha-70-g10d9ec1-SNAPSHOT on /seq/picard_aggregation/G77386/NA12878/v1/NA12878.bam. ```; java.lang.IllegalArgumentException: end must be >= start. start:13984870 end:13984869; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:45); at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$18(ReadWalker.java:79); at org.broadinstitute.hellbender.engine.ReadWalker$$Lambda$45/1492875057.accept(Unknown Source); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:78); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:448); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1473
https://github.com/broadinstitute/gatk/issues/1473:278,Security,validat,validatePositions,278,running RevertBaseQualityScores from Version:4.alpha-70-g10d9ec1-SNAPSHOT on /seq/picard_aggregation/G77386/NA12878/v1/NA12878.bam. ```; java.lang.IllegalArgumentException: end must be >= start. start:13984870 end:13984869; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:45); at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$18(ReadWalker.java:79); at org.broadinstitute.hellbender.engine.ReadWalker$$Lambda$45/1492875057.accept(Unknown Source); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:78); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:448); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1473
https://github.com/broadinstitute/gatk/issues/1473:263,Usability,Simpl,SimpleInterval,263,running RevertBaseQualityScores from Version:4.alpha-70-g10d9ec1-SNAPSHOT on /seq/picard_aggregation/G77386/NA12878/v1/NA12878.bam. ```; java.lang.IllegalArgumentException: end must be >= start. start:13984870 end:13984869; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:45); at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$18(ReadWalker.java:79); at org.broadinstitute.hellbender.engine.ReadWalker$$Lambda$45/1492875057.accept(Unknown Source); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:78); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:448); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1473
https://github.com/broadinstitute/gatk/issues/1473:296,Usability,Simpl,SimpleInterval,296,running RevertBaseQualityScores from Version:4.alpha-70-g10d9ec1-SNAPSHOT on /seq/picard_aggregation/G77386/NA12878/v1/NA12878.bam. ```; java.lang.IllegalArgumentException: end must be >= start. start:13984870 end:13984869; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:45); at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$18(ReadWalker.java:79); at org.broadinstitute.hellbender.engine.ReadWalker$$Lambda$45/1492875057.accept(Unknown Source); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:78); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:448); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1473
https://github.com/broadinstitute/gatk/issues/1473:360,Usability,Simpl,SimpleInterval,360,running RevertBaseQualityScores from Version:4.alpha-70-g10d9ec1-SNAPSHOT on /seq/picard_aggregation/G77386/NA12878/v1/NA12878.bam. ```; java.lang.IllegalArgumentException: end must be >= start. start:13984870 end:13984869; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:45); at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$18(ReadWalker.java:79); at org.broadinstitute.hellbender.engine.ReadWalker$$Lambda$45/1492875057.accept(Unknown Source); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:78); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:448); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1473
https://github.com/broadinstitute/gatk/issues/1473:382,Usability,Simpl,SimpleInterval,382,running RevertBaseQualityScores from Version:4.alpha-70-g10d9ec1-SNAPSHOT on /seq/picard_aggregation/G77386/NA12878/v1/NA12878.bam. ```; java.lang.IllegalArgumentException: end must be >= start. start:13984870 end:13984869; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:45); at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$18(ReadWalker.java:79); at org.broadinstitute.hellbender.engine.ReadWalker$$Lambda$45/1492875057.accept(Unknown Source); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:78); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:448); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1473
https://github.com/broadinstitute/gatk/issues/1473:446,Usability,Simpl,SimpleInterval,446,running RevertBaseQualityScores from Version:4.alpha-70-g10d9ec1-SNAPSHOT on /seq/picard_aggregation/G77386/NA12878/v1/NA12878.bam. ```; java.lang.IllegalArgumentException: end must be >= start. start:13984870 end:13984869; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:45); at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$18(ReadWalker.java:79); at org.broadinstitute.hellbender.engine.ReadWalker$$Lambda$45/1492875057.accept(Unknown Source); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:78); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:448); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1473
https://github.com/broadinstitute/gatk/issues/1473:468,Usability,Simpl,SimpleInterval,468,running RevertBaseQualityScores from Version:4.alpha-70-g10d9ec1-SNAPSHOT on /seq/picard_aggregation/G77386/NA12878/v1/NA12878.bam. ```; java.lang.IllegalArgumentException: end must be >= start. start:13984870 end:13984869; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:45); at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$18(ReadWalker.java:79); at org.broadinstitute.hellbender.engine.ReadWalker$$Lambda$45/1492875057.accept(Unknown Source); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:78); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:448); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1473
https://github.com/broadinstitute/gatk/pull/1475:41,Energy Efficiency,allocate,allocates,41,getBaseQualities is expensive because it allocates a new byte array every time; getSoftStart etc are also costly because they create many objects; getCigar creates a new Cigar object every time. all of those clearly show on the profiler when running BaseRecalibrator. this PR lifts these operations out of the loops they're in. ; @lbergelson please review -,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1475
https://github.com/broadinstitute/gatk/pull/1475:208,Usability,clear,clearly,208,getBaseQualities is expensive because it allocates a new byte array every time; getSoftStart etc are also costly because they create many objects; getCigar creates a new Cigar object every time. all of those clearly show on the profiler when running BaseRecalibrator. this PR lifts these operations out of the loops they're in. ; @lbergelson please review -,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1475
https://github.com/broadinstitute/gatk/pull/1478:0,Deployability,update,updated,0,"updated shadowJar to 1.2.3 since version 1.2.2 of the shadowJar plugin had some issues with gradle ; 2.11 which just released. some `build.gradle` cleanup; - removed dependency on `lib/tools.java` since it doesn't seem to be used and should be provided by the system anyway; - removed individual excludes of `guava-jdk5` since we exclude them globally; - changed our plugin application to use the newer style; - updated jacoco, coverals, and versions plugin versions; - added group and description to sparkJar task so it shows up in `gradle tasks`; - updated gradle wrapper version to 2.11; - readme now states 2.11 as minimum version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1478
https://github.com/broadinstitute/gatk/pull/1478:117,Deployability,release,released,117,"updated shadowJar to 1.2.3 since version 1.2.2 of the shadowJar plugin had some issues with gradle ; 2.11 which just released. some `build.gradle` cleanup; - removed dependency on `lib/tools.java` since it doesn't seem to be used and should be provided by the system anyway; - removed individual excludes of `guava-jdk5` since we exclude them globally; - changed our plugin application to use the newer style; - updated jacoco, coverals, and versions plugin versions; - added group and description to sparkJar task so it shows up in `gradle tasks`; - updated gradle wrapper version to 2.11; - readme now states 2.11 as minimum version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1478
https://github.com/broadinstitute/gatk/pull/1478:412,Deployability,update,updated,412,"updated shadowJar to 1.2.3 since version 1.2.2 of the shadowJar plugin had some issues with gradle ; 2.11 which just released. some `build.gradle` cleanup; - removed dependency on `lib/tools.java` since it doesn't seem to be used and should be provided by the system anyway; - removed individual excludes of `guava-jdk5` since we exclude them globally; - changed our plugin application to use the newer style; - updated jacoco, coverals, and versions plugin versions; - added group and description to sparkJar task so it shows up in `gradle tasks`; - updated gradle wrapper version to 2.11; - readme now states 2.11 as minimum version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1478
https://github.com/broadinstitute/gatk/pull/1478:551,Deployability,update,updated,551,"updated shadowJar to 1.2.3 since version 1.2.2 of the shadowJar plugin had some issues with gradle ; 2.11 which just released. some `build.gradle` cleanup; - removed dependency on `lib/tools.java` since it doesn't seem to be used and should be provided by the system anyway; - removed individual excludes of `guava-jdk5` since we exclude them globally; - changed our plugin application to use the newer style; - updated jacoco, coverals, and versions plugin versions; - added group and description to sparkJar task so it shows up in `gradle tasks`; - updated gradle wrapper version to 2.11; - readme now states 2.11 as minimum version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1478
https://github.com/broadinstitute/gatk/pull/1478:166,Integrability,depend,dependency,166,"updated shadowJar to 1.2.3 since version 1.2.2 of the shadowJar plugin had some issues with gradle ; 2.11 which just released. some `build.gradle` cleanup; - removed dependency on `lib/tools.java` since it doesn't seem to be used and should be provided by the system anyway; - removed individual excludes of `guava-jdk5` since we exclude them globally; - changed our plugin application to use the newer style; - updated jacoco, coverals, and versions plugin versions; - added group and description to sparkJar task so it shows up in `gradle tasks`; - updated gradle wrapper version to 2.11; - readme now states 2.11 as minimum version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1478
https://github.com/broadinstitute/gatk/pull/1478:566,Integrability,wrap,wrapper,566,"updated shadowJar to 1.2.3 since version 1.2.2 of the shadowJar plugin had some issues with gradle ; 2.11 which just released. some `build.gradle` cleanup; - removed dependency on `lib/tools.java` since it doesn't seem to be used and should be provided by the system anyway; - removed individual excludes of `guava-jdk5` since we exclude them globally; - changed our plugin application to use the newer style; - updated jacoco, coverals, and versions plugin versions; - added group and description to sparkJar task so it shows up in `gradle tasks`; - updated gradle wrapper version to 2.11; - readme now states 2.11 as minimum version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1478
https://github.com/broadinstitute/gatk/pull/1478:64,Modifiability,plugin,plugin,64,"updated shadowJar to 1.2.3 since version 1.2.2 of the shadowJar plugin had some issues with gradle ; 2.11 which just released. some `build.gradle` cleanup; - removed dependency on `lib/tools.java` since it doesn't seem to be used and should be provided by the system anyway; - removed individual excludes of `guava-jdk5` since we exclude them globally; - changed our plugin application to use the newer style; - updated jacoco, coverals, and versions plugin versions; - added group and description to sparkJar task so it shows up in `gradle tasks`; - updated gradle wrapper version to 2.11; - readme now states 2.11 as minimum version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1478
https://github.com/broadinstitute/gatk/pull/1478:367,Modifiability,plugin,plugin,367,"updated shadowJar to 1.2.3 since version 1.2.2 of the shadowJar plugin had some issues with gradle ; 2.11 which just released. some `build.gradle` cleanup; - removed dependency on `lib/tools.java` since it doesn't seem to be used and should be provided by the system anyway; - removed individual excludes of `guava-jdk5` since we exclude them globally; - changed our plugin application to use the newer style; - updated jacoco, coverals, and versions plugin versions; - added group and description to sparkJar task so it shows up in `gradle tasks`; - updated gradle wrapper version to 2.11; - readme now states 2.11 as minimum version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1478
https://github.com/broadinstitute/gatk/pull/1478:451,Modifiability,plugin,plugin,451,"updated shadowJar to 1.2.3 since version 1.2.2 of the shadowJar plugin had some issues with gradle ; 2.11 which just released. some `build.gradle` cleanup; - removed dependency on `lib/tools.java` since it doesn't seem to be used and should be provided by the system anyway; - removed individual excludes of `guava-jdk5` since we exclude them globally; - changed our plugin application to use the newer style; - updated jacoco, coverals, and versions plugin versions; - added group and description to sparkJar task so it shows up in `gradle tasks`; - updated gradle wrapper version to 2.11; - readme now states 2.11 as minimum version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1478
https://github.com/broadinstitute/gatk/issues/1480:165,Modifiability,refactor,refactorings,165,"Per discussion with @droazen, we'll do the Spark tool equivalent of https://github.com/broadinstitute/gatk/issues/365 (once the various branches with ReadsSparkSink refactorings are merged in).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1480
https://github.com/broadinstitute/gatk/issues/1481:291,Availability,down,downstream,291,"Right now we publish test utils as part of the gatk artifact. Since these are part of our main compilation unit it means we have several test libraries as compile dependencies instead of as test compile dependencies. . If we separate our test utils into a separate group we can avoid having downstream tools gain various test dependencies if they don't want them. (i.e. TestNG, MiniDFSCluster).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1481
https://github.com/broadinstitute/gatk/issues/1481:163,Integrability,depend,dependencies,163,"Right now we publish test utils as part of the gatk artifact. Since these are part of our main compilation unit it means we have several test libraries as compile dependencies instead of as test compile dependencies. . If we separate our test utils into a separate group we can avoid having downstream tools gain various test dependencies if they don't want them. (i.e. TestNG, MiniDFSCluster).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1481
https://github.com/broadinstitute/gatk/issues/1481:203,Integrability,depend,dependencies,203,"Right now we publish test utils as part of the gatk artifact. Since these are part of our main compilation unit it means we have several test libraries as compile dependencies instead of as test compile dependencies. . If we separate our test utils into a separate group we can avoid having downstream tools gain various test dependencies if they don't want them. (i.e. TestNG, MiniDFSCluster).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1481
https://github.com/broadinstitute/gatk/issues/1481:326,Integrability,depend,dependencies,326,"Right now we publish test utils as part of the gatk artifact. Since these are part of our main compilation unit it means we have several test libraries as compile dependencies instead of as test compile dependencies. . If we separate our test utils into a separate group we can avoid having downstream tools gain various test dependencies if they don't want them. (i.e. TestNG, MiniDFSCluster).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1481
https://github.com/broadinstitute/gatk/issues/1481:278,Safety,avoid,avoid,278,"Right now we publish test utils as part of the gatk artifact. Since these are part of our main compilation unit it means we have several test libraries as compile dependencies instead of as test compile dependencies. . If we separate our test utils into a separate group we can avoid having downstream tools gain various test dependencies if they don't want them. (i.e. TestNG, MiniDFSCluster).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1481
https://github.com/broadinstitute/gatk/issues/1481:21,Testability,test,test,21,"Right now we publish test utils as part of the gatk artifact. Since these are part of our main compilation unit it means we have several test libraries as compile dependencies instead of as test compile dependencies. . If we separate our test utils into a separate group we can avoid having downstream tools gain various test dependencies if they don't want them. (i.e. TestNG, MiniDFSCluster).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1481
https://github.com/broadinstitute/gatk/issues/1481:137,Testability,test,test,137,"Right now we publish test utils as part of the gatk artifact. Since these are part of our main compilation unit it means we have several test libraries as compile dependencies instead of as test compile dependencies. . If we separate our test utils into a separate group we can avoid having downstream tools gain various test dependencies if they don't want them. (i.e. TestNG, MiniDFSCluster).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1481
https://github.com/broadinstitute/gatk/issues/1481:190,Testability,test,test,190,"Right now we publish test utils as part of the gatk artifact. Since these are part of our main compilation unit it means we have several test libraries as compile dependencies instead of as test compile dependencies. . If we separate our test utils into a separate group we can avoid having downstream tools gain various test dependencies if they don't want them. (i.e. TestNG, MiniDFSCluster).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1481
https://github.com/broadinstitute/gatk/issues/1481:238,Testability,test,test,238,"Right now we publish test utils as part of the gatk artifact. Since these are part of our main compilation unit it means we have several test libraries as compile dependencies instead of as test compile dependencies. . If we separate our test utils into a separate group we can avoid having downstream tools gain various test dependencies if they don't want them. (i.e. TestNG, MiniDFSCluster).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1481
https://github.com/broadinstitute/gatk/issues/1481:321,Testability,test,test,321,"Right now we publish test utils as part of the gatk artifact. Since these are part of our main compilation unit it means we have several test libraries as compile dependencies instead of as test compile dependencies. . If we separate our test utils into a separate group we can avoid having downstream tools gain various test dependencies if they don't want them. (i.e. TestNG, MiniDFSCluster).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1481
https://github.com/broadinstitute/gatk/issues/1481:370,Testability,Test,TestNG,370,"Right now we publish test utils as part of the gatk artifact. Since these are part of our main compilation unit it means we have several test libraries as compile dependencies instead of as test compile dependencies. . If we separate our test utils into a separate group we can avoid having downstream tools gain various test dependencies if they don't want them. (i.e. TestNG, MiniDFSCluster).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1481
https://github.com/broadinstitute/gatk/issues/1483:680,Performance,perform,performance,680,"Requested by support team. Follow-up to https://github.com/broadinstitute/dsde-docs/issues/526. `--forceValidOutput` is an argument to SelectVariants (in GATK3) that forces its walker to completely unpack the information in the VariantContext (which holds all the annotations) then re-pack it using the underlying HTSJDK functions, which are usually fully spec-compliant. This is useful when dealing with older VCFs that have some annotations formatted in a way that violates later versions of the spec, since running the VCF through the tool with this flag effectively converts the VCF to spec compliance. This unpacking and repacking can't be done by default because it impacts performance, which is why we need an option to make it happen. . This issue comprises two points to address:; 1. Make the argument usable by all walkers that read and write VCFs (SelectVariants, CombineVariants, VariantFiltration, any others I may have omitted), if possible by promoting it to an engine-level argument.; 2. Ensure that it can be applied by SelectVariants even when there is no subsetting happening. When I checked the (GATK3) code, this comment (which comes before where that work gets done) suggested it might only do the job when you are subsetting samples out of a VCF:; ; `//If no subsetting happened, exit now`; ; This is not actually the desired behavior since you should be able to use this to convert an old VCF to be spec-compliant without having to subset anything. But I could have misread the code. Once this is done, please let me know and make a note in gsa-unstable to have this backported to GATK3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1483
https://github.com/broadinstitute/gatk/issues/1483:811,Usability,usab,usable,811,"Requested by support team. Follow-up to https://github.com/broadinstitute/dsde-docs/issues/526. `--forceValidOutput` is an argument to SelectVariants (in GATK3) that forces its walker to completely unpack the information in the VariantContext (which holds all the annotations) then re-pack it using the underlying HTSJDK functions, which are usually fully spec-compliant. This is useful when dealing with older VCFs that have some annotations formatted in a way that violates later versions of the spec, since running the VCF through the tool with this flag effectively converts the VCF to spec compliance. This unpacking and repacking can't be done by default because it impacts performance, which is why we need an option to make it happen. . This issue comprises two points to address:; 1. Make the argument usable by all walkers that read and write VCFs (SelectVariants, CombineVariants, VariantFiltration, any others I may have omitted), if possible by promoting it to an engine-level argument.; 2. Ensure that it can be applied by SelectVariants even when there is no subsetting happening. When I checked the (GATK3) code, this comment (which comes before where that work gets done) suggested it might only do the job when you are subsetting samples out of a VCF:; ; `//If no subsetting happened, exit now`; ; This is not actually the desired behavior since you should be able to use this to convert an old VCF to be spec-compliant without having to subset anything. But I could have misread the code. Once this is done, please let me know and make a note in gsa-unstable to have this backported to GATK3.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1483
https://github.com/broadinstitute/gatk/issues/1485:87,Safety,detect,detection,87,"came up in review of https://github.com/broadinstitute/gatk/pull/890 - we dont support detection of overlap with arbitrary feature source, only DBSNP. This needs to be enabled and tested.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1485
https://github.com/broadinstitute/gatk/issues/1485:180,Testability,test,tested,180,"came up in review of https://github.com/broadinstitute/gatk/pull/890 - we dont support detection of overlap with arbitrary feature source, only DBSNP. This needs to be enabled and tested.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1485
https://github.com/broadinstitute/gatk/issues/1486:174,Performance,load,load,174,"We can use `SparkContext.textFile` ""https://spark.apache.org/docs/1.5.0/api/java/org/apache/spark/SparkContext.html#textFile(java.lang.String,%20int)"" and VCFCodec.decode to load the file (can be compressed) to an JavaRDD of VariantContexts. The codec needs to be set up with a header but that's easy - just read the first lines.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1486
https://github.com/broadinstitute/gatk/pull/1487:21,Modifiability,refactor,refactored,21,fixes #1484 ; I also refactored the code duplication between ExcessHet and InbreedingCoef . @droazen please have a look,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1487
https://github.com/broadinstitute/gatk/pull/1488:52,Integrability,Depend,Depends,52,"https://github.com/broadinstitute/gatk/issues/1270. Depends on https://github.com/broadinstitute/gatk/pull/1469, which should be reviewed first and then I can remove the first commit from this branch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1488
https://github.com/broadinstitute/gatk/pull/1489:204,Availability,error,error,204,"This PR fixes an exception that was thrown when trying to serialize a ReferenceMultisource object. I was attempting to broadcast a 2bit reference in a Spark tool and found that I received a serialization error of the form:; ...; Caused by: com.esotericsoftware.kryo.KryoException: Unable to find class: ReferenceWindowFunctions$$Lambda$1/1599771323. Along with some related error reports (https://github.com/npgall/mobility-rpc/issues/12), this allowed me to track this down to the use of a Lambda function in the ReferenceWindowFunctions class. Replacing it with an explicitly defined function class solves the problem. Added a unit test which used to fail but now passes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1489
https://github.com/broadinstitute/gatk/pull/1489:374,Availability,error,error,374,"This PR fixes an exception that was thrown when trying to serialize a ReferenceMultisource object. I was attempting to broadcast a 2bit reference in a Spark tool and found that I received a serialization error of the form:; ...; Caused by: com.esotericsoftware.kryo.KryoException: Unable to find class: ReferenceWindowFunctions$$Lambda$1/1599771323. Along with some related error reports (https://github.com/npgall/mobility-rpc/issues/12), this allowed me to track this down to the use of a Lambda function in the ReferenceWindowFunctions class. Replacing it with an explicitly defined function class solves the problem. Added a unit test which used to fail but now passes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1489
https://github.com/broadinstitute/gatk/pull/1489:470,Availability,down,down,470,"This PR fixes an exception that was thrown when trying to serialize a ReferenceMultisource object. I was attempting to broadcast a 2bit reference in a Spark tool and found that I received a serialization error of the form:; ...; Caused by: com.esotericsoftware.kryo.KryoException: Unable to find class: ReferenceWindowFunctions$$Lambda$1/1599771323. Along with some related error reports (https://github.com/npgall/mobility-rpc/issues/12), this allowed me to track this down to the use of a Lambda function in the ReferenceWindowFunctions class. Replacing it with an explicitly defined function class solves the problem. Added a unit test which used to fail but now passes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1489
https://github.com/broadinstitute/gatk/pull/1489:634,Testability,test,test,634,"This PR fixes an exception that was thrown when trying to serialize a ReferenceMultisource object. I was attempting to broadcast a 2bit reference in a Spark tool and found that I received a serialization error of the form:; ...; Caused by: com.esotericsoftware.kryo.KryoException: Unable to find class: ReferenceWindowFunctions$$Lambda$1/1599771323. Along with some related error reports (https://github.com/npgall/mobility-rpc/issues/12), this allowed me to track this down to the use of a Lambda function in the ReferenceWindowFunctions class. Replacing it with an explicitly defined function class solves the problem. Added a unit test which used to fail but now passes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1489
https://github.com/broadinstitute/gatk/issues/1491:54,Availability,error,error,54,"When running jobs on our Spark cluster I start seeing error messages in the logs midway through the job, of the form:. ```; 16/02/16 11:45:10 ERROR TransportRequestHandler: Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=1974353486066, chunkIndex=0}, buffer=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=715964266 cap=715964266]}} to /69.173.65.228:49341; closing connection; ```. java.nio.channels.ClosedChannelException. These are often followed by stacktraces like this:. ```; java.io.IOException: Broken pipe; at sun.nio.ch.FileDispatcherImpl.write0(Native Method); at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47); at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93); at sun.nio.ch.IOUtil.write(IOUtil.java:65); at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:466); at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:105); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:91); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:254); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:281); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:761); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:311); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:729); at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1127); at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663); at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644); at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115); at io.netty.channel.AbstractChannelHandler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:142,Availability,ERROR,ERROR,142,"When running jobs on our Spark cluster I start seeing error messages in the logs midway through the job, of the form:. ```; 16/02/16 11:45:10 ERROR TransportRequestHandler: Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=1974353486066, chunkIndex=0}, buffer=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=715964266 cap=715964266]}} to /69.173.65.228:49341; closing connection; ```. java.nio.channels.ClosedChannelException. These are often followed by stacktraces like this:. ```; java.io.IOException: Broken pipe; at sun.nio.ch.FileDispatcherImpl.write0(Native Method); at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47); at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93); at sun.nio.ch.IOUtil.write(IOUtil.java:65); at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:466); at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:105); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:91); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:254); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:281); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:761); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:311); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:729); at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1127); at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663); at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644); at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115); at io.netty.channel.AbstractChannelHandler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:173,Availability,Error,Error,173,"When running jobs on our Spark cluster I start seeing error messages in the logs midway through the job, of the form:. ```; 16/02/16 11:45:10 ERROR TransportRequestHandler: Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=1974353486066, chunkIndex=0}, buffer=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=715964266 cap=715964266]}} to /69.173.65.228:49341; closing connection; ```. java.nio.channels.ClosedChannelException. These are often followed by stacktraces like this:. ```; java.io.IOException: Broken pipe; at sun.nio.ch.FileDispatcherImpl.write0(Native Method); at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47); at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93); at sun.nio.ch.IOUtil.write(IOUtil.java:65); at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:466); at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:105); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:91); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:254); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:281); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:761); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:311); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:729); at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1127); at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663); at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644); at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115); at io.netty.channel.AbstractChannelHandler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:5461,Availability,error,errors,5461,":308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846); at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111); at java.lang.Thread.run(Thread.java:744); ```. And then warnings about lost tasks:. ```; 16/02/16 11:45:59 WARN TaskSetManager: Lost task 42.1 in stage 0.0 (TID 364, dataflow03.broadinstitute.org): java.io.IOException: Connection from /69.173.65.227:56014 closed; ```. Then errors like this:. ```; 16/02/16 11:47:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@69.173.65.227:47043] -> [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]: Error [Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]] [; ```. akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]; Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: dataflow05.broadinstitute.org/69.173.65.230:36695; ]; akka.event.Logging$Error$NoCause$. ```; 16/02/16 11:47:39 ERROR YarnScheduler: Lost executor 37 on dataflow02.broadinstitute.org: remote Rpc client disassociated; ```. This seems to be causing tasks to be re-queued and executed, which hurts performance. The command line I'm using is:. ```; gatk-launch FindBadGenomicKmersSpark --reference hdfs:///user/cwhelan/reference/Homo_sapiens_assembly19.2bit --output bad_kmers_v5_cl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:5503,Availability,ERROR,ERROR,5503,"dlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846); at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111); at java.lang.Thread.run(Thread.java:744); ```. And then warnings about lost tasks:. ```; 16/02/16 11:45:59 WARN TaskSetManager: Lost task 42.1 in stage 0.0 (TID 364, dataflow03.broadinstitute.org): java.io.IOException: Connection from /69.173.65.227:56014 closed; ```. Then errors like this:. ```; 16/02/16 11:47:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@69.173.65.227:47043] -> [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]: Error [Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]] [; ```. akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]; Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: dataflow05.broadinstitute.org/69.173.65.230:36695; ]; akka.event.Logging$Error$NoCause$. ```; 16/02/16 11:47:39 ERROR YarnScheduler: Lost executor 37 on dataflow02.broadinstitute.org: remote Rpc client disassociated; ```. This seems to be causing tasks to be re-queued and executed, which hurts performance. The command line I'm using is:. ```; gatk-launch FindBadGenomicKmersSpark --reference hdfs:///user/cwhelan/reference/Homo_sapiens_assembly19.2bit --output bad_kmers_v5_cluster.txt -- --sparkRunner SPARK --sparkMaster",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:5509,Availability,Error,ErrorMonitor,5509,"dlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846); at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111); at java.lang.Thread.run(Thread.java:744); ```. And then warnings about lost tasks:. ```; 16/02/16 11:45:59 WARN TaskSetManager: Lost task 42.1 in stage 0.0 (TID 364, dataflow03.broadinstitute.org): java.io.IOException: Connection from /69.173.65.227:56014 closed; ```. Then errors like this:. ```; 16/02/16 11:47:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@69.173.65.227:47043] -> [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]: Error [Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]] [; ```. akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]; Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: dataflow05.broadinstitute.org/69.173.65.230:36695; ]; akka.event.Logging$Error$NoCause$. ```; 16/02/16 11:47:39 ERROR YarnScheduler: Lost executor 37 on dataflow02.broadinstitute.org: remote Rpc client disassociated; ```. This seems to be causing tasks to be re-queued and executed, which hurts performance. The command line I'm using is:. ```; gatk-launch FindBadGenomicKmersSpark --reference hdfs:///user/cwhelan/reference/Homo_sapiens_assembly19.2bit --output bad_kmers_v5_cluster.txt -- --sparkRunner SPARK --sparkMaster",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:5652,Availability,Error,Error,5652,"ava:846); at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111); at java.lang.Thread.run(Thread.java:744); ```. And then warnings about lost tasks:. ```; 16/02/16 11:45:59 WARN TaskSetManager: Lost task 42.1 in stage 0.0 (TID 364, dataflow03.broadinstitute.org): java.io.IOException: Connection from /69.173.65.227:56014 closed; ```. Then errors like this:. ```; 16/02/16 11:47:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@69.173.65.227:47043] -> [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]: Error [Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]] [; ```. akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]; Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: dataflow05.broadinstitute.org/69.173.65.230:36695; ]; akka.event.Logging$Error$NoCause$. ```; 16/02/16 11:47:39 ERROR YarnScheduler: Lost executor 37 on dataflow02.broadinstitute.org: remote Rpc client disassociated; ```. This seems to be causing tasks to be re-queued and executed, which hurts performance. The command line I'm using is:. ```; gatk-launch FindBadGenomicKmersSpark --reference hdfs:///user/cwhelan/reference/Homo_sapiens_assembly19.2bit --output bad_kmers_v5_cluster.txt -- --sparkRunner SPARK --sparkMaster yarn-client --executor-memory 8g --driver-memory 8g --conf spark.broadcast.blockSize=1g; ```. Running against commit f2b3bae of branch https://github.c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:6062,Availability,Error,Error,6062,"NioByteUnsafe.read(AbstractNioByteChannel.java:131); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111); at java.lang.Thread.run(Thread.java:744); ```. And then warnings about lost tasks:. ```; 16/02/16 11:45:59 WARN TaskSetManager: Lost task 42.1 in stage 0.0 (TID 364, dataflow03.broadinstitute.org): java.io.IOException: Connection from /69.173.65.227:56014 closed; ```. Then errors like this:. ```; 16/02/16 11:47:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@69.173.65.227:47043] -> [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]: Error [Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]] [; ```. akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]; Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: dataflow05.broadinstitute.org/69.173.65.230:36695; ]; akka.event.Logging$Error$NoCause$. ```; 16/02/16 11:47:39 ERROR YarnScheduler: Lost executor 37 on dataflow02.broadinstitute.org: remote Rpc client disassociated; ```. This seems to be causing tasks to be re-queued and executed, which hurts performance. The command line I'm using is:. ```; gatk-launch FindBadGenomicKmersSpark --reference hdfs:///user/cwhelan/reference/Homo_sapiens_assembly19.2bit --output bad_kmers_v5_cluster.txt -- --sparkRunner SPARK --sparkMaster yarn-client --executor-memory 8g --driver-memory 8g --conf spark.broadcast.blockSize=1g; ```. Running against commit f2b3bae of branch https://github.com/broadinstitute/gatk/tree/cw_clusterize_sv_spark_tools",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:6101,Availability,ERROR,ERROR,6101,"NioByteUnsafe.read(AbstractNioByteChannel.java:131); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111); at java.lang.Thread.run(Thread.java:744); ```. And then warnings about lost tasks:. ```; 16/02/16 11:45:59 WARN TaskSetManager: Lost task 42.1 in stage 0.0 (TID 364, dataflow03.broadinstitute.org): java.io.IOException: Connection from /69.173.65.227:56014 closed; ```. Then errors like this:. ```; 16/02/16 11:47:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@69.173.65.227:47043] -> [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]: Error [Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]] [; ```. akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]; Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: dataflow05.broadinstitute.org/69.173.65.230:36695; ]; akka.event.Logging$Error$NoCause$. ```; 16/02/16 11:47:39 ERROR YarnScheduler: Lost executor 37 on dataflow02.broadinstitute.org: remote Rpc client disassociated; ```. This seems to be causing tasks to be re-queued and executed, which hurts performance. The command line I'm using is:. ```; gatk-launch FindBadGenomicKmersSpark --reference hdfs:///user/cwhelan/reference/Homo_sapiens_assembly19.2bit --output bad_kmers_v5_cluster.txt -- --sparkRunner SPARK --sparkMaster yarn-client --executor-memory 8g --driver-memory 8g --conf spark.broadcast.blockSize=1g; ```. Running against commit f2b3bae of branch https://github.com/broadinstitute/gatk/tree/cw_clusterize_sv_spark_tools",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:60,Integrability,message,messages,60,"When running jobs on our Spark cluster I start seeing error messages in the logs midway through the job, of the form:. ```; 16/02/16 11:45:10 ERROR TransportRequestHandler: Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=1974353486066, chunkIndex=0}, buffer=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=715964266 cap=715964266]}} to /69.173.65.228:49341; closing connection; ```. java.nio.channels.ClosedChannelException. These are often followed by stacktraces like this:. ```; java.io.IOException: Broken pipe; at sun.nio.ch.FileDispatcherImpl.write0(Native Method); at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47); at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93); at sun.nio.ch.IOUtil.write(IOUtil.java:65); at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:466); at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:105); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:91); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:254); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:281); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:761); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:311); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:729); at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1127); at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663); at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644); at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115); at io.netty.channel.AbstractChannelHandler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:870,Integrability,protocol,protocol,870,"When running jobs on our Spark cluster I start seeing error messages in the logs midway through the job, of the form:. ```; 16/02/16 11:45:10 ERROR TransportRequestHandler: Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=1974353486066, chunkIndex=0}, buffer=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=715964266 cap=715964266]}} to /69.173.65.228:49341; closing connection; ```. java.nio.channels.ClosedChannelException. These are often followed by stacktraces like this:. ```; java.io.IOException: Broken pipe; at sun.nio.ch.FileDispatcherImpl.write0(Native Method); at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47); at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93); at sun.nio.ch.IOUtil.write(IOUtil.java:65); at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:466); at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:105); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:91); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:254); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:281); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:761); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:311); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:729); at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1127); at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663); at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644); at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115); at io.netty.channel.AbstractChannelHandler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:879,Integrability,Message,MessageWithHeader,879,"When running jobs on our Spark cluster I start seeing error messages in the logs midway through the job, of the form:. ```; 16/02/16 11:45:10 ERROR TransportRequestHandler: Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=1974353486066, chunkIndex=0}, buffer=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=715964266 cap=715964266]}} to /69.173.65.228:49341; closing connection; ```. java.nio.channels.ClosedChannelException. These are often followed by stacktraces like this:. ```; java.io.IOException: Broken pipe; at sun.nio.ch.FileDispatcherImpl.write0(Native Method); at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47); at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93); at sun.nio.ch.IOUtil.write(IOUtil.java:65); at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:466); at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:105); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:91); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:254); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:281); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:761); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:311); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:729); at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1127); at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663); at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644); at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115); at io.netty.channel.AbstractChannelHandler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:909,Integrability,Message,MessageWithHeader,909,"When running jobs on our Spark cluster I start seeing error messages in the logs midway through the job, of the form:. ```; 16/02/16 11:45:10 ERROR TransportRequestHandler: Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=1974353486066, chunkIndex=0}, buffer=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=715964266 cap=715964266]}} to /69.173.65.228:49341; closing connection; ```. java.nio.channels.ClosedChannelException. These are often followed by stacktraces like this:. ```; java.io.IOException: Broken pipe; at sun.nio.ch.FileDispatcherImpl.write0(Native Method); at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47); at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93); at sun.nio.ch.IOUtil.write(IOUtil.java:65); at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:466); at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:105); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:91); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:254); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:281); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:761); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:311); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:729); at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1127); at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663); at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644); at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115); at io.netty.channel.AbstractChannelHandler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:966,Integrability,protocol,protocol,966,"When running jobs on our Spark cluster I start seeing error messages in the logs midway through the job, of the form:. ```; 16/02/16 11:45:10 ERROR TransportRequestHandler: Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=1974353486066, chunkIndex=0}, buffer=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=715964266 cap=715964266]}} to /69.173.65.228:49341; closing connection; ```. java.nio.channels.ClosedChannelException. These are often followed by stacktraces like this:. ```; java.io.IOException: Broken pipe; at sun.nio.ch.FileDispatcherImpl.write0(Native Method); at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47); at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93); at sun.nio.ch.IOUtil.write(IOUtil.java:65); at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:466); at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:105); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:91); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:254); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:281); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:761); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:311); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:729); at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1127); at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663); at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644); at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115); at io.netty.channel.AbstractChannelHandler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:975,Integrability,Message,MessageWithHeader,975,"When running jobs on our Spark cluster I start seeing error messages in the logs midway through the job, of the form:. ```; 16/02/16 11:45:10 ERROR TransportRequestHandler: Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=1974353486066, chunkIndex=0}, buffer=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=715964266 cap=715964266]}} to /69.173.65.228:49341; closing connection; ```. java.nio.channels.ClosedChannelException. These are often followed by stacktraces like this:. ```; java.io.IOException: Broken pipe; at sun.nio.ch.FileDispatcherImpl.write0(Native Method); at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47); at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93); at sun.nio.ch.IOUtil.write(IOUtil.java:65); at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:466); at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:105); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:91); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:254); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:281); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:761); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:311); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:729); at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1127); at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663); at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644); at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115); at io.netty.channel.AbstractChannelHandler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:1004,Integrability,Message,MessageWithHeader,1004,"nning jobs on our Spark cluster I start seeing error messages in the logs midway through the job, of the form:. ```; 16/02/16 11:45:10 ERROR TransportRequestHandler: Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=1974353486066, chunkIndex=0}, buffer=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=715964266 cap=715964266]}} to /69.173.65.228:49341; closing connection; ```. java.nio.channels.ClosedChannelException. These are often followed by stacktraces like this:. ```; java.io.IOException: Broken pipe; at sun.nio.ch.FileDispatcherImpl.write0(Native Method); at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47); at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93); at sun.nio.ch.IOUtil.write(IOUtil.java:65); at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:466); at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:105); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:91); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:254); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:281); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:761); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:311); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:729); at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1127); at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663); at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644); at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115); at io.netty.channel.AbstractChannelHandlerContext",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:3987,Integrability,Message,MessageToMessageDecoder,3987,ansportRequestHandler.java:110); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:85); at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:101); at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51); at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846); at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468); at io.netty.channel.nio.NioEventLoop.processSelectedKey,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:4023,Integrability,Message,MessageToMessageDecoder,4023,); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:85); at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:101); at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51); at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846); at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382); at i,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:5107,Performance,concurren,concurrent,5107,"keChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846); at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111); at java.lang.Thread.run(Thread.java:744); ```. And then warnings about lost tasks:. ```; 16/02/16 11:45:59 WARN TaskSetManager: Lost task 42.1 in stage 0.0 (TID 364, dataflow03.broadinstitute.org): java.io.IOException: Connection from /69.173.65.227:56014 closed; ```. Then errors like this:. ```; 16/02/16 11:47:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@69.173.65.227:47043] -> [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]: Error [Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]] [; ```. akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]; Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: dataflow05.broadinstitute.org/69.173.65.230:36695; ]; akka.event.Logging$Error$NoCause$. ```; 16/02/16 11:47:39 ERROR YarnSc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:6251,Performance,queue,queued,6251,"NioByteUnsafe.read(AbstractNioByteChannel.java:131); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111); at java.lang.Thread.run(Thread.java:744); ```. And then warnings about lost tasks:. ```; 16/02/16 11:45:59 WARN TaskSetManager: Lost task 42.1 in stage 0.0 (TID 364, dataflow03.broadinstitute.org): java.io.IOException: Connection from /69.173.65.227:56014 closed; ```. Then errors like this:. ```; 16/02/16 11:47:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@69.173.65.227:47043] -> [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]: Error [Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]] [; ```. akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]; Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: dataflow05.broadinstitute.org/69.173.65.230:36695; ]; akka.event.Logging$Error$NoCause$. ```; 16/02/16 11:47:39 ERROR YarnScheduler: Lost executor 37 on dataflow02.broadinstitute.org: remote Rpc client disassociated; ```. This seems to be causing tasks to be re-queued and executed, which hurts performance. The command line I'm using is:. ```; gatk-launch FindBadGenomicKmersSpark --reference hdfs:///user/cwhelan/reference/Homo_sapiens_assembly19.2bit --output bad_kmers_v5_cluster.txt -- --sparkRunner SPARK --sparkMaster yarn-client --executor-memory 8g --driver-memory 8g --conf spark.broadcast.blockSize=1g; ```. Running against commit f2b3bae of branch https://github.com/broadinstitute/gatk/tree/cw_clusterize_sv_spark_tools",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:6284,Performance,perform,performance,6284,"NioByteUnsafe.read(AbstractNioByteChannel.java:131); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111); at java.lang.Thread.run(Thread.java:744); ```. And then warnings about lost tasks:. ```; 16/02/16 11:45:59 WARN TaskSetManager: Lost task 42.1 in stage 0.0 (TID 364, dataflow03.broadinstitute.org): java.io.IOException: Connection from /69.173.65.227:56014 closed; ```. Then errors like this:. ```; 16/02/16 11:47:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@69.173.65.227:47043] -> [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]: Error [Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]] [; ```. akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]; Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: dataflow05.broadinstitute.org/69.173.65.230:36695; ]; akka.event.Logging$Error$NoCause$. ```; 16/02/16 11:47:39 ERROR YarnScheduler: Lost executor 37 on dataflow02.broadinstitute.org: remote Rpc client disassociated; ```. This seems to be causing tasks to be re-queued and executed, which hurts performance. The command line I'm using is:. ```; gatk-launch FindBadGenomicKmersSpark --reference hdfs:///user/cwhelan/reference/Homo_sapiens_assembly19.2bit --output bad_kmers_v5_cluster.txt -- --sparkRunner SPARK --sparkMaster yarn-client --executor-memory 8g --driver-memory 8g --conf spark.broadcast.blockSize=1g; ```. Running against commit f2b3bae of branch https://github.com/broadinstitute/gatk/tree/cw_clusterize_sv_spark_tools",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:3680,Safety,timeout,timeout,3680,peline.writeAndFlush(DefaultChannelPipeline.java:954); at io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:244); at org.apache.spark.network.server.TransportRequestHandler.respond(TransportRequestHandler.java:138); at org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:110); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:85); at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:101); at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51); at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846); at io.nett,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:76,Testability,log,logs,76,"When running jobs on our Spark cluster I start seeing error messages in the logs midway through the job, of the form:. ```; 16/02/16 11:45:10 ERROR TransportRequestHandler: Error sending result ChunkFetchSuccess{streamChunkId=StreamChunkId{streamId=1974353486066, chunkIndex=0}, buffer=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=715964266 cap=715964266]}} to /69.173.65.228:49341; closing connection; ```. java.nio.channels.ClosedChannelException. These are often followed by stacktraces like this:. ```; java.io.IOException: Broken pipe; at sun.nio.ch.FileDispatcherImpl.write0(Native Method); at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47); at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93); at sun.nio.ch.IOUtil.write(IOUtil.java:65); at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:466); at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:105); at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:91); at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:254); at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237); at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:281); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:761); at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:311); at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:729); at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1127); at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663); at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644); at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115); at io.netty.channel.AbstractChannelHandler",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:6054,Testability,Log,Logging,6054,"NioByteUnsafe.read(AbstractNioByteChannel.java:131); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111); at java.lang.Thread.run(Thread.java:744); ```. And then warnings about lost tasks:. ```; 16/02/16 11:45:59 WARN TaskSetManager: Lost task 42.1 in stage 0.0 (TID 364, dataflow03.broadinstitute.org): java.io.IOException: Connection from /69.173.65.227:56014 closed; ```. Then errors like this:. ```; 16/02/16 11:47:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@69.173.65.227:47043] -> [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]: Error [Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]] [; ```. akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@dataflow05.broadinstitute.org:36695]; Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: dataflow05.broadinstitute.org/69.173.65.230:36695; ]; akka.event.Logging$Error$NoCause$. ```; 16/02/16 11:47:39 ERROR YarnScheduler: Lost executor 37 on dataflow02.broadinstitute.org: remote Rpc client disassociated; ```. This seems to be causing tasks to be re-queued and executed, which hurts performance. The command line I'm using is:. ```; gatk-launch FindBadGenomicKmersSpark --reference hdfs:///user/cwhelan/reference/Homo_sapiens_assembly19.2bit --output bad_kmers_v5_cluster.txt -- --sparkRunner SPARK --sparkMaster yarn-client --executor-memory 8g --driver-memory 8g --conf spark.broadcast.blockSize=1g; ```. Running against commit f2b3bae of branch https://github.com/broadinstitute/gatk/tree/cw_clusterize_sv_spark_tools",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:3365,Usability,Simpl,SimpleChannelInboundHandler,3365,dlerContext.write(AbstractChannelHandlerContext.java:693); at io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:681); at io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:716); at io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:954); at io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:244); at org.apache.spark.network.server.TransportRequestHandler.respond(TransportRequestHandler.java:138); at org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:110); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:85); at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:101); at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51); at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244); at io.netty.ch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1491:3405,Usability,Simpl,SimpleChannelInboundHandler,3405,andlerContext.java:693); at io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:681); at io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:716); at io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:954); at io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:244); at org.apache.spark.network.server.TransportRequestHandler.respond(TransportRequestHandler.java:138); at org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:110); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:85); at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:101); at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51); at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294); at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244); at io.netty.channel.AbstractChannelHandlerContex,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1491
https://github.com/broadinstitute/gatk/issues/1492:687,Integrability,depend,dependsOn,687,"The build.gradle code below builds the native shared library for AVX PairHMM using gcc and copies the .so file to the desired location. The jar task will archive the .so file in the GATK jar file. ``` gradle; apply plugin: 'cpp'; model {; components {; VectorLoglessPairHMM(NativeLibrarySpec) {; binaries.withType(SharedLibraryBinarySpec) { binary ->; cppCompiler.args ""-I"", ""${System.properties['java.home']}/../include""; cppCompiler.args ""-I"", ""${System.properties['java.home']}/../include/linux""; cppCompiler.args ""-mavx""; linker.args ""-static-libgcc"". task copySharedLib(type: Copy) {; from binary.tasks; into ""build/classes/main/org/broadinstitute/hellbender/utils/pairhmm""; }; jar.dependsOn copySharedLib; }; // skip static library build; binaries.withType(StaticLibraryBinarySpec) { binary ->; buildable = false; }; }; }; }; ```. The gradle gcc plugin expects to find the C++ source code in the default location shown below. We can use a different directory structure, if desired. ```; src/; |-- main; |-- test; `-- VectorLoglessPairHMM; |-- cpp; `-- headers; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1492
https://github.com/broadinstitute/gatk/issues/1492:215,Modifiability,plugin,plugin,215,"The build.gradle code below builds the native shared library for AVX PairHMM using gcc and copies the .so file to the desired location. The jar task will archive the .so file in the GATK jar file. ``` gradle; apply plugin: 'cpp'; model {; components {; VectorLoglessPairHMM(NativeLibrarySpec) {; binaries.withType(SharedLibraryBinarySpec) { binary ->; cppCompiler.args ""-I"", ""${System.properties['java.home']}/../include""; cppCompiler.args ""-I"", ""${System.properties['java.home']}/../include/linux""; cppCompiler.args ""-mavx""; linker.args ""-static-libgcc"". task copySharedLib(type: Copy) {; from binary.tasks; into ""build/classes/main/org/broadinstitute/hellbender/utils/pairhmm""; }; jar.dependsOn copySharedLib; }; // skip static library build; binaries.withType(StaticLibraryBinarySpec) { binary ->; buildable = false; }; }; }; }; ```. The gradle gcc plugin expects to find the C++ source code in the default location shown below. We can use a different directory structure, if desired. ```; src/; |-- main; |-- test; `-- VectorLoglessPairHMM; |-- cpp; `-- headers; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1492
https://github.com/broadinstitute/gatk/issues/1492:852,Modifiability,plugin,plugin,852,"The build.gradle code below builds the native shared library for AVX PairHMM using gcc and copies the .so file to the desired location. The jar task will archive the .so file in the GATK jar file. ``` gradle; apply plugin: 'cpp'; model {; components {; VectorLoglessPairHMM(NativeLibrarySpec) {; binaries.withType(SharedLibraryBinarySpec) { binary ->; cppCompiler.args ""-I"", ""${System.properties['java.home']}/../include""; cppCompiler.args ""-I"", ""${System.properties['java.home']}/../include/linux""; cppCompiler.args ""-mavx""; linker.args ""-static-libgcc"". task copySharedLib(type: Copy) {; from binary.tasks; into ""build/classes/main/org/broadinstitute/hellbender/utils/pairhmm""; }; jar.dependsOn copySharedLib; }; // skip static library build; binaries.withType(StaticLibraryBinarySpec) { binary ->; buildable = false; }; }; }; }; ```. The gradle gcc plugin expects to find the C++ source code in the default location shown below. We can use a different directory structure, if desired. ```; src/; |-- main; |-- test; `-- VectorLoglessPairHMM; |-- cpp; `-- headers; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1492
https://github.com/broadinstitute/gatk/issues/1492:1013,Testability,test,test,1013,"The build.gradle code below builds the native shared library for AVX PairHMM using gcc and copies the .so file to the desired location. The jar task will archive the .so file in the GATK jar file. ``` gradle; apply plugin: 'cpp'; model {; components {; VectorLoglessPairHMM(NativeLibrarySpec) {; binaries.withType(SharedLibraryBinarySpec) { binary ->; cppCompiler.args ""-I"", ""${System.properties['java.home']}/../include""; cppCompiler.args ""-I"", ""${System.properties['java.home']}/../include/linux""; cppCompiler.args ""-mavx""; linker.args ""-static-libgcc"". task copySharedLib(type: Copy) {; from binary.tasks; into ""build/classes/main/org/broadinstitute/hellbender/utils/pairhmm""; }; jar.dependsOn copySharedLib; }; // skip static library build; binaries.withType(StaticLibraryBinarySpec) { binary ->; buildable = false; }; }; }; }; ```. The gradle gcc plugin expects to find the C++ source code in the default location shown below. We can use a different directory structure, if desired. ```; src/; |-- main; |-- test; `-- VectorLoglessPairHMM; |-- cpp; `-- headers; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1492
https://github.com/broadinstitute/gatk/issues/1494:84,Testability,test,test,84,come up in review of https://github.com/broadinstitute/gatk/pull/1442; see disabled test testUnmappedAndAllIReadsPassThrough in LocusIteratorByStateUnitTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1494
https://github.com/broadinstitute/gatk/issues/1494:89,Testability,test,testUnmappedAndAllIReadsPassThrough,89,come up in review of https://github.com/broadinstitute/gatk/pull/1442; see disabled test testUnmappedAndAllIReadsPassThrough in LocusIteratorByStateUnitTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1494
https://github.com/broadinstitute/gatk/pull/1496:53,Security,expose,exposed,53,"fixes #1486 (code already existed, just needed to be exposed). I found a limitation in Hadoop-BAM (https://github.com/HadoopGenomics/Hadoop-BAM/issues/68) regarding reading blocked vcfs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1496
https://github.com/broadinstitute/gatk/pull/1502:49,Integrability,depend,depends,49,fixed #358 ; for @droazen ; I don't see why #358 depends on @121 btw.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1502
https://github.com/broadinstitute/gatk/pull/1504:216,Deployability,integrat,integrated,216,"Initial port of native PairHMM AVX code from GATK3. (@akiezun #1492). Includes gradle code to build the shared library and package it in the GATK jar file. . Added a new unit test VectorPairHMMUnitTest, which can be integrated into PairHMMUnitTest later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504
https://github.com/broadinstitute/gatk/pull/1504:216,Integrability,integrat,integrated,216,"Initial port of native PairHMM AVX code from GATK3. (@akiezun #1492). Includes gradle code to build the shared library and package it in the GATK jar file. . Added a new unit test VectorPairHMMUnitTest, which can be integrated into PairHMMUnitTest later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504
https://github.com/broadinstitute/gatk/pull/1504:175,Testability,test,test,175,"Initial port of native PairHMM AVX code from GATK3. (@akiezun #1492). Includes gradle code to build the shared library and package it in the GATK jar file. . Added a new unit test VectorPairHMMUnitTest, which can be integrated into PairHMMUnitTest later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1504
https://github.com/broadinstitute/gatk/issues/1506:178,Testability,test,test,178,came up in review of https://github.com/broadinstitute/gatk/pull/1496; when HadoopGenomics/Hadoop-BAM#68 (block gzipped VCF not recognized as VCF) is fixed we need to reenable a test in CountVariantsSparkIntegrationTest,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1506
https://github.com/broadinstitute/gatk/issues/1508:371,Deployability,pipeline,pipeline,371,"From @tomwhite . ""Isilon exposes a Hadoop filesystem interface which makes it possible; to use it as a source or sink for Spark. (There are some notes here:; http://www.cloudera.com/documentation/enterprise/latest/topics/cm_mc_isilon_service.html). Note however that you lose the benefits of locality, so it won't be as; fast as HDFS. Definitely worth a try. Also, for a pipeline with; multiple steps, you could use HDFS to store intermediate data, only; reading from Isilon for the source files and writing to Isilon with; the final result.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1508
https://github.com/broadinstitute/gatk/issues/1508:53,Integrability,interface,interface,53,"From @tomwhite . ""Isilon exposes a Hadoop filesystem interface which makes it possible; to use it as a source or sink for Spark. (There are some notes here:; http://www.cloudera.com/documentation/enterprise/latest/topics/cm_mc_isilon_service.html). Note however that you lose the benefits of locality, so it won't be as; fast as HDFS. Definitely worth a try. Also, for a pipeline with; multiple steps, you could use HDFS to store intermediate data, only; reading from Isilon for the source files and writing to Isilon with; the final result.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1508
https://github.com/broadinstitute/gatk/issues/1508:25,Security,expose,exposes,25,"From @tomwhite . ""Isilon exposes a Hadoop filesystem interface which makes it possible; to use it as a source or sink for Spark. (There are some notes here:; http://www.cloudera.com/documentation/enterprise/latest/topics/cm_mc_isilon_service.html). Note however that you lose the benefits of locality, so it won't be as; fast as HDFS. Definitely worth a try. Also, for a pipeline with; multiple steps, you could use HDFS to store intermediate data, only; reading from Isilon for the source files and writing to Isilon with; the final result.""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1508
https://github.com/broadinstitute/gatk/issues/1509:98,Performance,bottleneck,bottleneck,98,A lot of data we have lives on NFS (or underlying IFS - Isilon FS). Copying files in and out is a bottleneck and a pain. This ticket for an implementation of a parallel copy of a BAM/CRAM file to HDFS (sharded or unsharded),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1509
https://github.com/broadinstitute/gatk/issues/1510:325,Deployability,upgrade,upgrade,325,"Versions of Kryo less than 3.0 can't serialize Java 8 lambda functions, forcing us in some cases to replace lambdas with full function class objects even when a lambda would be simpler and easier to read. See for example https://github.com/broadinstitute/gatk/pull/1489 . This ticket is to look through the codebase after an upgrade to Kryo 3.0+ takes place and to replace unnecessary function classes with lambdas where they'd be more appropriate.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1510
https://github.com/broadinstitute/gatk/issues/1510:177,Usability,simpl,simpler,177,"Versions of Kryo less than 3.0 can't serialize Java 8 lambda functions, forcing us in some cases to replace lambdas with full function class objects even when a lambda would be simpler and easier to read. See for example https://github.com/broadinstitute/gatk/pull/1489 . This ticket is to look through the codebase after an upgrade to Kryo 3.0+ takes place and to replace unnecessary function classes with lambdas where they'd be more appropriate.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1510
https://github.com/broadinstitute/gatk/pull/1511:29,Usability,simpl,simple,29,We found (the hard way) that simple `gradle` sometimes fails (eg when a dotkit is broken) and `gradlew` should be the uniformly recommended way. @lbergelson please have a look,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1511
https://github.com/broadinstitute/gatk/issues/1512:798,Availability,error,error,798,"this bam file has no reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.bam`. the cram file also has no reads ``src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.cram`. but the corresponding sam file has 52 reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.sam`. However `CollectInsertSizeMetricsTest` does not catch this and incorrectly reports that the test successfully passed. The issue comes from using only a for loop for asserts (the for loop executes 0 times on this file and so it thinks everything is great). The task here is to:; - fix the test to catch this error, then; - fix the bam file to not have this problem, then; - check and fix all other tests that use the same testing pattern (using the forloop). I suggest switching to using actual files for such tests (as done in `MeanQualityByCycleIntegrationTest`) to avoid such problems in the future",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1512
https://github.com/broadinstitute/gatk/issues/1512:1058,Safety,avoid,avoid,1058,"this bam file has no reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.bam`. the cram file also has no reads ``src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.cram`. but the corresponding sam file has 52 reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.sam`. However `CollectInsertSizeMetricsTest` does not catch this and incorrectly reports that the test successfully passed. The issue comes from using only a for loop for asserts (the for loop executes 0 times on this file and so it thinks everything is great). The task here is to:; - fix the test to catch this error, then; - fix the bam file to not have this problem, then; - check and fix all other tests that use the same testing pattern (using the forloop). I suggest switching to using actual files for such tests (as done in `MeanQualityByCycleIntegrationTest`) to avoid such problems in the future",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1512
https://github.com/broadinstitute/gatk/issues/1512:33,Testability,test,test,33,"this bam file has no reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.bam`. the cram file also has no reads ``src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.cram`. but the corresponding sam file has 52 reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.sam`. However `CollectInsertSizeMetricsTest` does not catch this and incorrectly reports that the test successfully passed. The issue comes from using only a for loop for asserts (the for loop executes 0 times on this file and so it thinks everything is great). The task here is to:; - fix the test to catch this error, then; - fix the bam file to not have this problem, then; - check and fix all other tests that use the same testing pattern (using the forloop). I suggest switching to using actual files for such tests (as done in `MeanQualityByCycleIntegrationTest`) to avoid such problems in the future",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1512
https://github.com/broadinstitute/gatk/issues/1512:194,Testability,test,test,194,"this bam file has no reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.bam`. the cram file also has no reads ``src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.cram`. but the corresponding sam file has 52 reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.sam`. However `CollectInsertSizeMetricsTest` does not catch this and incorrectly reports that the test successfully passed. The issue comes from using only a for loop for asserts (the for loop executes 0 times on this file and so it thinks everything is great). The task here is to:; - fix the test to catch this error, then; - fix the bam file to not have this problem, then; - check and fix all other tests that use the same testing pattern (using the forloop). I suggest switching to using actual files for such tests (as done in `MeanQualityByCycleIntegrationTest`) to avoid such problems in the future",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1512
https://github.com/broadinstitute/gatk/issues/1512:368,Testability,test,test,368,"this bam file has no reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.bam`. the cram file also has no reads ``src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.cram`. but the corresponding sam file has 52 reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.sam`. However `CollectInsertSizeMetricsTest` does not catch this and incorrectly reports that the test successfully passed. The issue comes from using only a for loop for asserts (the for loop executes 0 times on this file and so it thinks everything is great). The task here is to:; - fix the test to catch this error, then; - fix the bam file to not have this problem, then; - check and fix all other tests that use the same testing pattern (using the forloop). I suggest switching to using actual files for such tests (as done in `MeanQualityByCycleIntegrationTest`) to avoid such problems in the future",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1512
https://github.com/broadinstitute/gatk/issues/1512:583,Testability,test,test,583,"this bam file has no reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.bam`. the cram file also has no reads ``src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.cram`. but the corresponding sam file has 52 reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.sam`. However `CollectInsertSizeMetricsTest` does not catch this and incorrectly reports that the test successfully passed. The issue comes from using only a for loop for asserts (the for loop executes 0 times on this file and so it thinks everything is great). The task here is to:; - fix the test to catch this error, then; - fix the bam file to not have this problem, then; - check and fix all other tests that use the same testing pattern (using the forloop). I suggest switching to using actual files for such tests (as done in `MeanQualityByCycleIntegrationTest`) to avoid such problems in the future",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1512
https://github.com/broadinstitute/gatk/issues/1512:656,Testability,assert,asserts,656,"this bam file has no reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.bam`. the cram file also has no reads ``src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.cram`. but the corresponding sam file has 52 reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.sam`. However `CollectInsertSizeMetricsTest` does not catch this and incorrectly reports that the test successfully passed. The issue comes from using only a for loop for asserts (the for loop executes 0 times on this file and so it thinks everything is great). The task here is to:; - fix the test to catch this error, then; - fix the bam file to not have this problem, then; - check and fix all other tests that use the same testing pattern (using the forloop). I suggest switching to using actual files for such tests (as done in `MeanQualityByCycleIntegrationTest`) to avoid such problems in the future",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1512
https://github.com/broadinstitute/gatk/issues/1512:779,Testability,test,test,779,"this bam file has no reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.bam`. the cram file also has no reads ``src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.cram`. but the corresponding sam file has 52 reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.sam`. However `CollectInsertSizeMetricsTest` does not catch this and incorrectly reports that the test successfully passed. The issue comes from using only a for loop for asserts (the for loop executes 0 times on this file and so it thinks everything is great). The task here is to:; - fix the test to catch this error, then; - fix the bam file to not have this problem, then; - check and fix all other tests that use the same testing pattern (using the forloop). I suggest switching to using actual files for such tests (as done in `MeanQualityByCycleIntegrationTest`) to avoid such problems in the future",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1512
https://github.com/broadinstitute/gatk/issues/1512:888,Testability,test,tests,888,"this bam file has no reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.bam`. the cram file also has no reads ``src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.cram`. but the corresponding sam file has 52 reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.sam`. However `CollectInsertSizeMetricsTest` does not catch this and incorrectly reports that the test successfully passed. The issue comes from using only a for loop for asserts (the for loop executes 0 times on this file and so it thinks everything is great). The task here is to:; - fix the test to catch this error, then; - fix the bam file to not have this problem, then; - check and fix all other tests that use the same testing pattern (using the forloop). I suggest switching to using actual files for such tests (as done in `MeanQualityByCycleIntegrationTest`) to avoid such problems in the future",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1512
https://github.com/broadinstitute/gatk/issues/1512:912,Testability,test,testing,912,"this bam file has no reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.bam`. the cram file also has no reads ``src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.cram`. but the corresponding sam file has 52 reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.sam`. However `CollectInsertSizeMetricsTest` does not catch this and incorrectly reports that the test successfully passed. The issue comes from using only a for loop for asserts (the for loop executes 0 times on this file and so it thinks everything is great). The task here is to:; - fix the test to catch this error, then; - fix the bam file to not have this problem, then; - check and fix all other tests that use the same testing pattern (using the forloop). I suggest switching to using actual files for such tests (as done in `MeanQualityByCycleIntegrationTest`) to avoid such problems in the future",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1512
https://github.com/broadinstitute/gatk/issues/1512:1000,Testability,test,tests,1000,"this bam file has no reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.bam`. the cram file also has no reads ``src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.cram`. but the corresponding sam file has 52 reads: `src/test/resources/org/broadinstitute/hellbender/tools/picard/analysis/CollectInsertSizeMetrics/insert_size_metrics_test.sam`. However `CollectInsertSizeMetricsTest` does not catch this and incorrectly reports that the test successfully passed. The issue comes from using only a for loop for asserts (the for loop executes 0 times on this file and so it thinks everything is great). The task here is to:; - fix the test to catch this error, then; - fix the bam file to not have this problem, then; - check and fix all other tests that use the same testing pattern (using the forloop). I suggest switching to using actual files for such tests (as done in `MeanQualityByCycleIntegrationTest`) to avoid such problems in the future",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1512
https://github.com/broadinstitute/gatk/pull/1513:33,Testability,Test,TestMath,33,1) removed a few dead classes: ; TestMath (subsumed by code in MathUtilsTests); AlignmentContext (had nothing more than what ReadPileup has); CsvInputParser (not used in picard not used in gatk). 2) created simple tests for the 'constants' classes - just checking that you can't make instances of them ; (fixed the code accordningly). 3) fixed ClassUtils + added tests to check that you cant make instances of classes without public constructors. 4) switched from JUnit assertArrayEquals to TestNG assertEquals for consistency with the rest of the code (we use TestNG everywhere). @lbergelson can you look at this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1513
https://github.com/broadinstitute/gatk/pull/1513:214,Testability,test,tests,214,1) removed a few dead classes: ; TestMath (subsumed by code in MathUtilsTests); AlignmentContext (had nothing more than what ReadPileup has); CsvInputParser (not used in picard not used in gatk). 2) created simple tests for the 'constants' classes - just checking that you can't make instances of them ; (fixed the code accordningly). 3) fixed ClassUtils + added tests to check that you cant make instances of classes without public constructors. 4) switched from JUnit assertArrayEquals to TestNG assertEquals for consistency with the rest of the code (we use TestNG everywhere). @lbergelson can you look at this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1513
https://github.com/broadinstitute/gatk/pull/1513:363,Testability,test,tests,363,1) removed a few dead classes: ; TestMath (subsumed by code in MathUtilsTests); AlignmentContext (had nothing more than what ReadPileup has); CsvInputParser (not used in picard not used in gatk). 2) created simple tests for the 'constants' classes - just checking that you can't make instances of them ; (fixed the code accordningly). 3) fixed ClassUtils + added tests to check that you cant make instances of classes without public constructors. 4) switched from JUnit assertArrayEquals to TestNG assertEquals for consistency with the rest of the code (we use TestNG everywhere). @lbergelson can you look at this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1513
https://github.com/broadinstitute/gatk/pull/1513:470,Testability,assert,assertArrayEquals,470,1) removed a few dead classes: ; TestMath (subsumed by code in MathUtilsTests); AlignmentContext (had nothing more than what ReadPileup has); CsvInputParser (not used in picard not used in gatk). 2) created simple tests for the 'constants' classes - just checking that you can't make instances of them ; (fixed the code accordningly). 3) fixed ClassUtils + added tests to check that you cant make instances of classes without public constructors. 4) switched from JUnit assertArrayEquals to TestNG assertEquals for consistency with the rest of the code (we use TestNG everywhere). @lbergelson can you look at this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1513
https://github.com/broadinstitute/gatk/pull/1513:491,Testability,Test,TestNG,491,1) removed a few dead classes: ; TestMath (subsumed by code in MathUtilsTests); AlignmentContext (had nothing more than what ReadPileup has); CsvInputParser (not used in picard not used in gatk). 2) created simple tests for the 'constants' classes - just checking that you can't make instances of them ; (fixed the code accordningly). 3) fixed ClassUtils + added tests to check that you cant make instances of classes without public constructors. 4) switched from JUnit assertArrayEquals to TestNG assertEquals for consistency with the rest of the code (we use TestNG everywhere). @lbergelson can you look at this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1513
https://github.com/broadinstitute/gatk/pull/1513:498,Testability,assert,assertEquals,498,1) removed a few dead classes: ; TestMath (subsumed by code in MathUtilsTests); AlignmentContext (had nothing more than what ReadPileup has); CsvInputParser (not used in picard not used in gatk). 2) created simple tests for the 'constants' classes - just checking that you can't make instances of them ; (fixed the code accordningly). 3) fixed ClassUtils + added tests to check that you cant make instances of classes without public constructors. 4) switched from JUnit assertArrayEquals to TestNG assertEquals for consistency with the rest of the code (we use TestNG everywhere). @lbergelson can you look at this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1513
https://github.com/broadinstitute/gatk/pull/1513:561,Testability,Test,TestNG,561,1) removed a few dead classes: ; TestMath (subsumed by code in MathUtilsTests); AlignmentContext (had nothing more than what ReadPileup has); CsvInputParser (not used in picard not used in gatk). 2) created simple tests for the 'constants' classes - just checking that you can't make instances of them ; (fixed the code accordningly). 3) fixed ClassUtils + added tests to check that you cant make instances of classes without public constructors. 4) switched from JUnit assertArrayEquals to TestNG assertEquals for consistency with the rest of the code (we use TestNG everywhere). @lbergelson can you look at this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1513
https://github.com/broadinstitute/gatk/pull/1513:207,Usability,simpl,simple,207,1) removed a few dead classes: ; TestMath (subsumed by code in MathUtilsTests); AlignmentContext (had nothing more than what ReadPileup has); CsvInputParser (not used in picard not used in gatk). 2) created simple tests for the 'constants' classes - just checking that you can't make instances of them ; (fixed the code accordningly). 3) fixed ClassUtils + added tests to check that you cant make instances of classes without public constructors. 4) switched from JUnit assertArrayEquals to TestNG assertEquals for consistency with the rest of the code (we use TestNG everywhere). @lbergelson can you look at this?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1513
https://github.com/broadinstitute/gatk/pull/1514:285,Availability,avail,available,285,"Hi Adam,. This is based on your feedback from Friday afternoon. ; Lots of issues are still to be addressed (multilevel collection, more test coverage, performance, etc).; If you are too busy to review, let me know and I'll bug @cwhelan or @tedsharpe or someone else. Branch travis log available [here](https://travis-ci.org/broadinstitute/gatk/builds/110702136). Thank you.; Steve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1514
https://github.com/broadinstitute/gatk/pull/1514:151,Performance,perform,performance,151,"Hi Adam,. This is based on your feedback from Friday afternoon. ; Lots of issues are still to be addressed (multilevel collection, more test coverage, performance, etc).; If you are too busy to review, let me know and I'll bug @cwhelan or @tedsharpe or someone else. Branch travis log available [here](https://travis-ci.org/broadinstitute/gatk/builds/110702136). Thank you.; Steve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1514
https://github.com/broadinstitute/gatk/pull/1514:136,Testability,test,test,136,"Hi Adam,. This is based on your feedback from Friday afternoon. ; Lots of issues are still to be addressed (multilevel collection, more test coverage, performance, etc).; If you are too busy to review, let me know and I'll bug @cwhelan or @tedsharpe or someone else. Branch travis log available [here](https://travis-ci.org/broadinstitute/gatk/builds/110702136). Thank you.; Steve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1514
https://github.com/broadinstitute/gatk/pull/1514:281,Testability,log,log,281,"Hi Adam,. This is based on your feedback from Friday afternoon. ; Lots of issues are still to be addressed (multilevel collection, more test coverage, performance, etc).; If you are too busy to review, let me know and I'll bug @cwhelan or @tedsharpe or someone else. Branch travis log available [here](https://travis-ci.org/broadinstitute/gatk/builds/110702136). Thank you.; Steve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1514
https://github.com/broadinstitute/gatk/pull/1514:32,Usability,feedback,feedback,32,"Hi Adam,. This is based on your feedback from Friday afternoon. ; Lots of issues are still to be addressed (multilevel collection, more test coverage, performance, etc).; If you are too busy to review, let me know and I'll bug @cwhelan or @tedsharpe or someone else. Branch travis log available [here](https://travis-ci.org/broadinstitute/gatk/builds/110702136). Thank you.; Steve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1514
https://github.com/broadinstitute/gatk/issues/1517:143,Deployability,pipeline,pipeline,143,we need a way to bind to a BWA-MEM library to align reads programmatically from within the GATK. . SVs need it and it would simplify the reads pipeline to have it start with a fasta. Commandline behavior we need: `bwa mem -K 100000000 -p` (we may not care about multithreading); - `-K` is undocumented - reading code shows it refers to `fixed_chunk_size` which then sets `actual_chunk_size` @lh3 can you help us understand what it `-K` does?; - `-p` is for interleaved input,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1517
https://github.com/broadinstitute/gatk/issues/1517:124,Usability,simpl,simplify,124,we need a way to bind to a BWA-MEM library to align reads programmatically from within the GATK. . SVs need it and it would simplify the reads pipeline to have it start with a fasta. Commandline behavior we need: `bwa mem -K 100000000 -p` (we may not care about multithreading); - `-K` is undocumented - reading code shows it refers to `fixed_chunk_size` which then sets `actual_chunk_size` @lh3 can you help us understand what it `-K` does?; - `-p` is for interleaved input,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1517
https://github.com/broadinstitute/gatk/issues/1517:289,Usability,undo,undocumented,289,we need a way to bind to a BWA-MEM library to align reads programmatically from within the GATK. . SVs need it and it would simplify the reads pipeline to have it start with a fasta. Commandline behavior we need: `bwa mem -K 100000000 -p` (we may not care about multithreading); - `-K` is undocumented - reading code shows it refers to `fixed_chunk_size` which then sets `actual_chunk_size` @lh3 can you help us understand what it `-K` does?; - `-p` is for interleaved input,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1517
https://github.com/broadinstitute/gatk/pull/1518:269,Testability,test,tested,269,"removing spark.executor.userClassPathFirst=true seems to fix the problems encountered when running using spark-submit and sparkMaster local; this fixes #1315 and also fixes #1386. Removing it doesn't seem to cause any problems when running under yarn on the cluster. I tested with a reasonable size file using the ReadSparkPipeline so that should cover most of our bases. I'm not certain why we added spark.executor.userClassPathFirst=true in the first place. I assumed it was necessary, like spark.driver.userClassPathFirst=true is. ; It seems plausible that it was only added for symmetry and it was never actually needed in the first place.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1518
https://github.com/broadinstitute/gatk/issues/1519:860,Modifiability,variab,variables,860,"Below is probably too specific but I am not sure if this is desired (unlikely IMO) or actually another edge-case bug. Summary: `InsertSizeMetics.WIDTH_OF_99_PERCENT` potentially get larger than `InsertSizeMetrics.MAX_INSERT_SIZE - InserSizeMetrics.MIN_INSERT_SIZE`. Details:; Using the non-empty bam file, fragment length values of valid reads (13 reads) are; {36,36,36,38,38,40,41,41,41,41,44,44,45}, with median value 41.; However, the `CollectInsertSizeMetricsTest.java` unit test is yielding 11 as the value for `InsertSizeMetics.WIDTH_OF_99_PERCENT`, which is greater than 10 (that is, max-min+1).; Debugging step-by-step shows that the problem lies in function . `public void InsertSizeMetricsCollector::addMetricsToFile(final MetricsFile<InsertSizeMetrics,Integer>)` . In short, the logic in the implementation for finding the bin widths is to have two variables, `low` and `high`, that starts from `InsertSizeMetics.MEDIAN_INSERT_SIZE` and incremented and decremented by 1 gradually. The actual widths are calculated by `distance = (max - min)+1`.; In this particular edge case, during the last iteration, `max` was increased from the previous iteration to an out of bound value--46 (that is greater than 45, the actual max in the data). The increment and decrement of `low` and `high` are done at the bottom of the `while` block spanning form line 138 to line 162 in the function. This causes the culprit. In hypothetical scenario, when the distribution under investigation is extremely biased (max-median >> median-min, or the other way around), the `WIDTH_OF_99_PERCENT` could end up much higher than max-min. @akiezun @cwhelan please check. If this is not the intended result, fix should be easy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1519
https://github.com/broadinstitute/gatk/issues/1519:479,Testability,test,test,479,"Below is probably too specific but I am not sure if this is desired (unlikely IMO) or actually another edge-case bug. Summary: `InsertSizeMetics.WIDTH_OF_99_PERCENT` potentially get larger than `InsertSizeMetrics.MAX_INSERT_SIZE - InserSizeMetrics.MIN_INSERT_SIZE`. Details:; Using the non-empty bam file, fragment length values of valid reads (13 reads) are; {36,36,36,38,38,40,41,41,41,41,44,44,45}, with median value 41.; However, the `CollectInsertSizeMetricsTest.java` unit test is yielding 11 as the value for `InsertSizeMetics.WIDTH_OF_99_PERCENT`, which is greater than 10 (that is, max-min+1).; Debugging step-by-step shows that the problem lies in function . `public void InsertSizeMetricsCollector::addMetricsToFile(final MetricsFile<InsertSizeMetrics,Integer>)` . In short, the logic in the implementation for finding the bin widths is to have two variables, `low` and `high`, that starts from `InsertSizeMetics.MEDIAN_INSERT_SIZE` and incremented and decremented by 1 gradually. The actual widths are calculated by `distance = (max - min)+1`.; In this particular edge case, during the last iteration, `max` was increased from the previous iteration to an out of bound value--46 (that is greater than 45, the actual max in the data). The increment and decrement of `low` and `high` are done at the bottom of the `while` block spanning form line 138 to line 162 in the function. This causes the culprit. In hypothetical scenario, when the distribution under investigation is extremely biased (max-median >> median-min, or the other way around), the `WIDTH_OF_99_PERCENT` could end up much higher than max-min. @akiezun @cwhelan please check. If this is not the intended result, fix should be easy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1519
https://github.com/broadinstitute/gatk/issues/1519:790,Testability,log,logic,790,"Below is probably too specific but I am not sure if this is desired (unlikely IMO) or actually another edge-case bug. Summary: `InsertSizeMetics.WIDTH_OF_99_PERCENT` potentially get larger than `InsertSizeMetrics.MAX_INSERT_SIZE - InserSizeMetrics.MIN_INSERT_SIZE`. Details:; Using the non-empty bam file, fragment length values of valid reads (13 reads) are; {36,36,36,38,38,40,41,41,41,41,44,44,45}, with median value 41.; However, the `CollectInsertSizeMetricsTest.java` unit test is yielding 11 as the value for `InsertSizeMetics.WIDTH_OF_99_PERCENT`, which is greater than 10 (that is, max-min+1).; Debugging step-by-step shows that the problem lies in function . `public void InsertSizeMetricsCollector::addMetricsToFile(final MetricsFile<InsertSizeMetrics,Integer>)` . In short, the logic in the implementation for finding the bin widths is to have two variables, `low` and `high`, that starts from `InsertSizeMetics.MEDIAN_INSERT_SIZE` and incremented and decremented by 1 gradually. The actual widths are calculated by `distance = (max - min)+1`.; In this particular edge case, during the last iteration, `max` was increased from the previous iteration to an out of bound value--46 (that is greater than 45, the actual max in the data). The increment and decrement of `low` and `high` are done at the bottom of the `while` block spanning form line 138 to line 162 in the function. This causes the culprit. In hypothetical scenario, when the distribution under investigation is extremely biased (max-median >> median-min, or the other way around), the `WIDTH_OF_99_PERCENT` could end up much higher than max-min. @akiezun @cwhelan please check. If this is not the intended result, fix should be easy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1519
https://github.com/broadinstitute/gatk/pull/1521:18,Safety,detect,detected,18,addresses problem detected when working on #1413,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1521
https://github.com/broadinstitute/gatk/issues/1523:454,Deployability,pipeline,pipelines,454,Running without an output file causes an NPR. Needs a null check. ```; ./gatk-launch CountReadsSpark -I src/test/resources/org/broadinstitute/hellbender/tools/count_reads_sorted.bam; ```. ```; java.lang.NullPointerException; at org.broadinstitute.hellbender.utils.gcs.BucketUtils.isCloudStorageUrl(BucketUtils.java:44); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.createFile(BucketUtils.java:105); at org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark.runTool(CountReadsSpark.java:37); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:310); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1523
https://github.com/broadinstitute/gatk/issues/1523:108,Testability,test,test,108,Running without an output file causes an NPR. Needs a null check. ```; ./gatk-launch CountReadsSpark -I src/test/resources/org/broadinstitute/hellbender/tools/count_reads_sorted.bam; ```. ```; java.lang.NullPointerException; at org.broadinstitute.hellbender.utils.gcs.BucketUtils.isCloudStorageUrl(BucketUtils.java:44); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.createFile(BucketUtils.java:105); at org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark.runTool(CountReadsSpark.java:37); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:310); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1523
https://github.com/broadinstitute/gatk/pull/1526:143,Availability,down,downsampling,143,"As described in #1464, I ported a `LocusWalker` class as the GATK3 one using `LocusIteratorByState` (LIBS). The default implementation uses no downsampling (probably it should be change once #64 is addressed), includes reads with deletions and does not track the previous reads in the LIBS. One important think is that the `intervalsForTraversal` is not used at all, so it is up to the author discard regions out of this ones. I was thinking to check every position for overlap in any of the interval in the list, but I'm not sure if the `intervalsForTraversal` is sorted or not; and an exhaustive checking could reduce performance. I don't know if it could be possible to implement some query in LIBS, to return only the positions that overlaps some intervals, but that will be easier.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1526
https://github.com/broadinstitute/gatk/pull/1526:613,Energy Efficiency,reduce,reduce,613,"As described in #1464, I ported a `LocusWalker` class as the GATK3 one using `LocusIteratorByState` (LIBS). The default implementation uses no downsampling (probably it should be change once #64 is addressed), includes reads with deletions and does not track the previous reads in the LIBS. One important think is that the `intervalsForTraversal` is not used at all, so it is up to the author discard regions out of this ones. I was thinking to check every position for overlap in any of the interval in the list, but I'm not sure if the `intervalsForTraversal` is sorted or not; and an exhaustive checking could reduce performance. I don't know if it could be possible to implement some query in LIBS, to return only the positions that overlaps some intervals, but that will be easier.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1526
https://github.com/broadinstitute/gatk/pull/1526:620,Performance,perform,performance,620,"As described in #1464, I ported a `LocusWalker` class as the GATK3 one using `LocusIteratorByState` (LIBS). The default implementation uses no downsampling (probably it should be change once #64 is addressed), includes reads with deletions and does not track the previous reads in the LIBS. One important think is that the `intervalsForTraversal` is not used at all, so it is up to the author discard regions out of this ones. I was thinking to check every position for overlap in any of the interval in the list, but I'm not sure if the `intervalsForTraversal` is sorted or not; and an exhaustive checking could reduce performance. I don't know if it could be possible to implement some query in LIBS, to return only the positions that overlaps some intervals, but that will be easier.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1526
https://github.com/broadinstitute/gatk/pull/1528:474,Modifiability,extend,extends,474,"As requested in #1198, I would like to have a walker to iterate over data sources with a set of overlapping windows based on a reference genome (that is, a `SAMSequenceDictionary`). This first implementation is a very naive one, because it only construct the overlapping windows from the reference (keeping only the ones that overlaps with the provided `intervalsForTraversal`) and then just query in the different sources each of the windows generated. The next plan is to extends this class and `VariantWalker` to create a `SlidingWindowVariantWalker`, which traverse variants using this sliding-window approach.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1528
https://github.com/broadinstitute/gatk/pull/1529:631,Testability,test,test,631,"Hi Adam,. I've reformatted the code and fixed some issues you raised in the first review.; Specifically:; 1. Default values for tool's boolean arguments are changed to false.; 2. Created an (ugly) SV program group.; 3. Uses Apache library to calculate statistics. Somethings are still to be changed:; 1. A lot of todo's.; 2. Histogram plot is created by Picard's R script, which was copied to SV. Of particular concern is that Picard's collection of the insert size distribution is potentially biased and has edge case bugs, see issue #1519. So I've used the following R script (can't attach on github) for collecting stats on the test bam file. ```; # Serves to collect basic metrics on insert sizes of test bam file and use for ; # unit testing assertions.; testBamFile <- ""insert_size_metrics_test.bam"". library(Rsamtools); # take only first end of read pairs; filter <- ScanBamParam(what=scanBamWhat(),flag=scanBamFlag(isFirstMateRead=T)); dd <- scanBam(testBamFile, param = filter); dd <- dd[[1]]; sizes <- abs(dd$isize[!is.na(dd$isize)]); sizes <- sort(sizes). metrics <- list(); metrics[[""count""]] <- length(sizes); metrics[[""median""]] <- median(sizes); metrics[[""mean""]] <- mean(sizes); metrics[[""min""]] <- min(sizes); metrics[[""max""]] <- max(sizes); metrics[[""MAD""]] <- mad(sizes, constant = 1); metrics[[""SD""]] <- sd(sizes). q <- quantile(sizes, probs = c(0.005, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45,; 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.995)); metrics[[""99_percent_width""]] <- as.integer(round(q[20] - q[1])); metrics[[""90_percent_width""]] <- as.integer(round(q[19] - q[2])); metrics[[""80_percent_width""]] <- as.integer(round(q[18] - q[3])); metrics[[""70_percent_width""]] <- as.integer(round(q[17] - q[4])); metrics[[""60_percent_width""]] <- as.integer(round(q[16] - q[5])); metrics[[""50_percent_width""]] <- as.integer(round(q[15] - q[6])); metrics[[""40_percent_width""]] <- as.integer(round(q[14] - q[7])); metrics[[""30_percent_width""]] <- as.integer(round(q[13] ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1529
https://github.com/broadinstitute/gatk/pull/1529:704,Testability,test,test,704,"Hi Adam,. I've reformatted the code and fixed some issues you raised in the first review.; Specifically:; 1. Default values for tool's boolean arguments are changed to false.; 2. Created an (ugly) SV program group.; 3. Uses Apache library to calculate statistics. Somethings are still to be changed:; 1. A lot of todo's.; 2. Histogram plot is created by Picard's R script, which was copied to SV. Of particular concern is that Picard's collection of the insert size distribution is potentially biased and has edge case bugs, see issue #1519. So I've used the following R script (can't attach on github) for collecting stats on the test bam file. ```; # Serves to collect basic metrics on insert sizes of test bam file and use for ; # unit testing assertions.; testBamFile <- ""insert_size_metrics_test.bam"". library(Rsamtools); # take only first end of read pairs; filter <- ScanBamParam(what=scanBamWhat(),flag=scanBamFlag(isFirstMateRead=T)); dd <- scanBam(testBamFile, param = filter); dd <- dd[[1]]; sizes <- abs(dd$isize[!is.na(dd$isize)]); sizes <- sort(sizes). metrics <- list(); metrics[[""count""]] <- length(sizes); metrics[[""median""]] <- median(sizes); metrics[[""mean""]] <- mean(sizes); metrics[[""min""]] <- min(sizes); metrics[[""max""]] <- max(sizes); metrics[[""MAD""]] <- mad(sizes, constant = 1); metrics[[""SD""]] <- sd(sizes). q <- quantile(sizes, probs = c(0.005, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45,; 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.995)); metrics[[""99_percent_width""]] <- as.integer(round(q[20] - q[1])); metrics[[""90_percent_width""]] <- as.integer(round(q[19] - q[2])); metrics[[""80_percent_width""]] <- as.integer(round(q[18] - q[3])); metrics[[""70_percent_width""]] <- as.integer(round(q[17] - q[4])); metrics[[""60_percent_width""]] <- as.integer(round(q[16] - q[5])); metrics[[""50_percent_width""]] <- as.integer(round(q[15] - q[6])); metrics[[""40_percent_width""]] <- as.integer(round(q[14] - q[7])); metrics[[""30_percent_width""]] <- as.integer(round(q[13] ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1529
https://github.com/broadinstitute/gatk/pull/1529:739,Testability,test,testing,739,"Hi Adam,. I've reformatted the code and fixed some issues you raised in the first review.; Specifically:; 1. Default values for tool's boolean arguments are changed to false.; 2. Created an (ugly) SV program group.; 3. Uses Apache library to calculate statistics. Somethings are still to be changed:; 1. A lot of todo's.; 2. Histogram plot is created by Picard's R script, which was copied to SV. Of particular concern is that Picard's collection of the insert size distribution is potentially biased and has edge case bugs, see issue #1519. So I've used the following R script (can't attach on github) for collecting stats on the test bam file. ```; # Serves to collect basic metrics on insert sizes of test bam file and use for ; # unit testing assertions.; testBamFile <- ""insert_size_metrics_test.bam"". library(Rsamtools); # take only first end of read pairs; filter <- ScanBamParam(what=scanBamWhat(),flag=scanBamFlag(isFirstMateRead=T)); dd <- scanBam(testBamFile, param = filter); dd <- dd[[1]]; sizes <- abs(dd$isize[!is.na(dd$isize)]); sizes <- sort(sizes). metrics <- list(); metrics[[""count""]] <- length(sizes); metrics[[""median""]] <- median(sizes); metrics[[""mean""]] <- mean(sizes); metrics[[""min""]] <- min(sizes); metrics[[""max""]] <- max(sizes); metrics[[""MAD""]] <- mad(sizes, constant = 1); metrics[[""SD""]] <- sd(sizes). q <- quantile(sizes, probs = c(0.005, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45,; 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.995)); metrics[[""99_percent_width""]] <- as.integer(round(q[20] - q[1])); metrics[[""90_percent_width""]] <- as.integer(round(q[19] - q[2])); metrics[[""80_percent_width""]] <- as.integer(round(q[18] - q[3])); metrics[[""70_percent_width""]] <- as.integer(round(q[17] - q[4])); metrics[[""60_percent_width""]] <- as.integer(round(q[16] - q[5])); metrics[[""50_percent_width""]] <- as.integer(round(q[15] - q[6])); metrics[[""40_percent_width""]] <- as.integer(round(q[14] - q[7])); metrics[[""30_percent_width""]] <- as.integer(round(q[13] ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1529
https://github.com/broadinstitute/gatk/pull/1529:747,Testability,assert,assertions,747,"Hi Adam,. I've reformatted the code and fixed some issues you raised in the first review.; Specifically:; 1. Default values for tool's boolean arguments are changed to false.; 2. Created an (ugly) SV program group.; 3. Uses Apache library to calculate statistics. Somethings are still to be changed:; 1. A lot of todo's.; 2. Histogram plot is created by Picard's R script, which was copied to SV. Of particular concern is that Picard's collection of the insert size distribution is potentially biased and has edge case bugs, see issue #1519. So I've used the following R script (can't attach on github) for collecting stats on the test bam file. ```; # Serves to collect basic metrics on insert sizes of test bam file and use for ; # unit testing assertions.; testBamFile <- ""insert_size_metrics_test.bam"". library(Rsamtools); # take only first end of read pairs; filter <- ScanBamParam(what=scanBamWhat(),flag=scanBamFlag(isFirstMateRead=T)); dd <- scanBam(testBamFile, param = filter); dd <- dd[[1]]; sizes <- abs(dd$isize[!is.na(dd$isize)]); sizes <- sort(sizes). metrics <- list(); metrics[[""count""]] <- length(sizes); metrics[[""median""]] <- median(sizes); metrics[[""mean""]] <- mean(sizes); metrics[[""min""]] <- min(sizes); metrics[[""max""]] <- max(sizes); metrics[[""MAD""]] <- mad(sizes, constant = 1); metrics[[""SD""]] <- sd(sizes). q <- quantile(sizes, probs = c(0.005, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45,; 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.995)); metrics[[""99_percent_width""]] <- as.integer(round(q[20] - q[1])); metrics[[""90_percent_width""]] <- as.integer(round(q[19] - q[2])); metrics[[""80_percent_width""]] <- as.integer(round(q[18] - q[3])); metrics[[""70_percent_width""]] <- as.integer(round(q[17] - q[4])); metrics[[""60_percent_width""]] <- as.integer(round(q[16] - q[5])); metrics[[""50_percent_width""]] <- as.integer(round(q[15] - q[6])); metrics[[""40_percent_width""]] <- as.integer(round(q[14] - q[7])); metrics[[""30_percent_width""]] <- as.integer(round(q[13] ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1529
https://github.com/broadinstitute/gatk/pull/1529:760,Testability,test,testBamFile,760,"Hi Adam,. I've reformatted the code and fixed some issues you raised in the first review.; Specifically:; 1. Default values for tool's boolean arguments are changed to false.; 2. Created an (ugly) SV program group.; 3. Uses Apache library to calculate statistics. Somethings are still to be changed:; 1. A lot of todo's.; 2. Histogram plot is created by Picard's R script, which was copied to SV. Of particular concern is that Picard's collection of the insert size distribution is potentially biased and has edge case bugs, see issue #1519. So I've used the following R script (can't attach on github) for collecting stats on the test bam file. ```; # Serves to collect basic metrics on insert sizes of test bam file and use for ; # unit testing assertions.; testBamFile <- ""insert_size_metrics_test.bam"". library(Rsamtools); # take only first end of read pairs; filter <- ScanBamParam(what=scanBamWhat(),flag=scanBamFlag(isFirstMateRead=T)); dd <- scanBam(testBamFile, param = filter); dd <- dd[[1]]; sizes <- abs(dd$isize[!is.na(dd$isize)]); sizes <- sort(sizes). metrics <- list(); metrics[[""count""]] <- length(sizes); metrics[[""median""]] <- median(sizes); metrics[[""mean""]] <- mean(sizes); metrics[[""min""]] <- min(sizes); metrics[[""max""]] <- max(sizes); metrics[[""MAD""]] <- mad(sizes, constant = 1); metrics[[""SD""]] <- sd(sizes). q <- quantile(sizes, probs = c(0.005, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45,; 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.995)); metrics[[""99_percent_width""]] <- as.integer(round(q[20] - q[1])); metrics[[""90_percent_width""]] <- as.integer(round(q[19] - q[2])); metrics[[""80_percent_width""]] <- as.integer(round(q[18] - q[3])); metrics[[""70_percent_width""]] <- as.integer(round(q[17] - q[4])); metrics[[""60_percent_width""]] <- as.integer(round(q[16] - q[5])); metrics[[""50_percent_width""]] <- as.integer(round(q[15] - q[6])); metrics[[""40_percent_width""]] <- as.integer(round(q[14] - q[7])); metrics[[""30_percent_width""]] <- as.integer(round(q[13] ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1529
https://github.com/broadinstitute/gatk/pull/1529:958,Testability,test,testBamFile,958,"Hi Adam,. I've reformatted the code and fixed some issues you raised in the first review.; Specifically:; 1. Default values for tool's boolean arguments are changed to false.; 2. Created an (ugly) SV program group.; 3. Uses Apache library to calculate statistics. Somethings are still to be changed:; 1. A lot of todo's.; 2. Histogram plot is created by Picard's R script, which was copied to SV. Of particular concern is that Picard's collection of the insert size distribution is potentially biased and has edge case bugs, see issue #1519. So I've used the following R script (can't attach on github) for collecting stats on the test bam file. ```; # Serves to collect basic metrics on insert sizes of test bam file and use for ; # unit testing assertions.; testBamFile <- ""insert_size_metrics_test.bam"". library(Rsamtools); # take only first end of read pairs; filter <- ScanBamParam(what=scanBamWhat(),flag=scanBamFlag(isFirstMateRead=T)); dd <- scanBam(testBamFile, param = filter); dd <- dd[[1]]; sizes <- abs(dd$isize[!is.na(dd$isize)]); sizes <- sort(sizes). metrics <- list(); metrics[[""count""]] <- length(sizes); metrics[[""median""]] <- median(sizes); metrics[[""mean""]] <- mean(sizes); metrics[[""min""]] <- min(sizes); metrics[[""max""]] <- max(sizes); metrics[[""MAD""]] <- mad(sizes, constant = 1); metrics[[""SD""]] <- sd(sizes). q <- quantile(sizes, probs = c(0.005, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45,; 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.995)); metrics[[""99_percent_width""]] <- as.integer(round(q[20] - q[1])); metrics[[""90_percent_width""]] <- as.integer(round(q[19] - q[2])); metrics[[""80_percent_width""]] <- as.integer(round(q[18] - q[3])); metrics[[""70_percent_width""]] <- as.integer(round(q[17] - q[4])); metrics[[""60_percent_width""]] <- as.integer(round(q[16] - q[5])); metrics[[""50_percent_width""]] <- as.integer(round(q[15] - q[6])); metrics[[""40_percent_width""]] <- as.integer(round(q[14] - q[7])); metrics[[""30_percent_width""]] <- as.integer(round(q[13] ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1529
https://github.com/broadinstitute/gatk/issues/1534:106,Availability,Error,Errors,106,"@SHuang-Broad reports having problems running spark tests when his machine is connected to the broad vpn. Errors seem to occur with any tests that start a spark context. They all seem to be caused by `java.net.BindException`. ```; 23:59:49.200 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.200 WARN Utils:71 - Service 'sparkDriver' could not bind on port 0. Attempting port 1.; 23:59:49.200 ERROR Remoting:65 - Remoting system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:244,Availability,ERROR,ERROR,244,"@SHuang-Broad reports having problems running spark tests when his machine is connected to the broad vpn. Errors seem to occur with any tests that start a spark context. They all seem to be caused by `java.net.BindException`. ```; 23:59:49.200 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.200 WARN Utils:71 - Service 'sparkDriver' could not bind on port 0. Attempting port 1.; 23:59:49.200 ERROR Remoting:65 - Remoting system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:312,Availability,down,down,312,"@SHuang-Broad reports having problems running spark tests when his machine is connected to the broad vpn. Errors seem to occur with any tests that start a spark context. They all seem to be caused by `java.net.BindException`. ```; 23:59:49.200 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.200 WARN Utils:71 - Service 'sparkDriver' could not bind on port 0. Attempting port 1.; 23:59:49.200 ERROR Remoting:65 - Remoting system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:444,Availability,ERROR,ERROR,444,"@SHuang-Broad reports having problems running spark tests when his machine is connected to the broad vpn. Errors seem to occur with any tests that start a spark context. They all seem to be caused by `java.net.BindException`. ```; 23:59:49.200 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.200 WARN Utils:71 - Service 'sparkDriver' could not bind on port 0. Attempting port 1.; 23:59:49.200 ERROR Remoting:65 - Remoting system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:529,Availability,down,down,529,"@SHuang-Broad reports having problems running spark tests when his machine is connected to the broad vpn. Errors seem to occur with any tests that start a spark context. They all seem to be caused by `java.net.BindException`. ```; 23:59:49.200 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.200 WARN Utils:71 - Service 'sparkDriver' could not bind on port 0. Attempting port 1.; 23:59:49.200 ERROR Remoting:65 - Remoting system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:559,Availability,ERROR,ERROR,559,"@SHuang-Broad reports having problems running spark tests when his machine is connected to the broad vpn. Errors seem to occur with any tests that start a spark context. They all seem to be caused by `java.net.BindException`. ```; 23:59:49.200 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.200 WARN Utils:71 - Service 'sparkDriver' could not bind on port 0. Attempting port 1.; 23:59:49.200 ERROR Remoting:65 - Remoting system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:627,Availability,down,down,627,"@SHuang-Broad reports having problems running spark tests when his machine is connected to the broad vpn. Errors seem to occur with any tests that start a spark context. They all seem to be caused by `java.net.BindException`. ```; 23:59:49.200 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.200 WARN Utils:71 - Service 'sparkDriver' could not bind on port 0. Attempting port 1.; 23:59:49.200 ERROR Remoting:65 - Remoting system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:662,Availability,ERROR,ERROR,662,"@SHuang-Broad reports having problems running spark tests when his machine is connected to the broad vpn. Errors seem to occur with any tests that start a spark context. They all seem to be caused by `java.net.BindException`. ```; 23:59:49.200 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.200 WARN Utils:71 - Service 'sparkDriver' could not bind on port 0. Attempting port 1.; 23:59:49.200 ERROR Remoting:65 - Remoting system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:686,Availability,Error,Error,686,"@SHuang-Broad reports having problems running spark tests when his machine is connected to the broad vpn. Errors seem to occur with any tests that start a spark context. They all seem to be caused by `java.net.BindException`. ```; 23:59:49.200 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.200 WARN Utils:71 - Service 'sparkDriver' could not bind on port 0. Attempting port 1.; 23:59:49.200 ERROR Remoting:65 - Remoting system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:1248,Performance,concurren,concurrent,1248,"tyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.200 WARN Utils:71 - Service 'sparkDriver' could not bind on port 0. Attempting port 1.; 23:59:49.200 ERROR Remoting:65 - Remoting system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.For",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:1315,Performance,concurren,concurrent,1315,"ty transport; 23:59:49.200 WARN Utils:71 - Service 'sparkDriver' could not bind on port 0. Attempting port 1.; 23:59:49.200 ERROR Remoting:65 - Remoting system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:1382,Performance,concurren,concurrent,1382,"uld not bind on port 0. Attempting port 1.; 23:59:49.200 ERROR Remoting:65 - Remoting system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:1837,Performance,concurren,concurrent,1837,"system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:2158,Performance,concurren,concurrent,2158,"system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:2231,Performance,concurren,concurrent,2231,"system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:2316,Performance,concurren,concurrent,2316,"system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:2393,Performance,concurren,concurrent,2393,"system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:52,Testability,test,tests,52,"@SHuang-Broad reports having problems running spark tests when his machine is connected to the broad vpn. Errors seem to occur with any tests that start a spark context. They all seem to be caused by `java.net.BindException`. ```; 23:59:49.200 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.200 WARN Utils:71 - Service 'sparkDriver' could not bind on port 0. Attempting port 1.; 23:59:49.200 ERROR Remoting:65 - Remoting system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/issues/1534:136,Testability,test,tests,136,"@SHuang-Broad reports having problems running spark tests when his machine is connected to the broad vpn. Errors seem to occur with any tests that start a spark context. They all seem to be caused by `java.net.BindException`. ```; 23:59:49.200 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.200 WARN Utils:71 - Service 'sparkDriver' could not bind on port 0. Attempting port 1.; 23:59:49.200 ERROR Remoting:65 - Remoting system has been terminated abrubtly. Attempting to shut down transports; 23:59:49.206 ERROR NettyTransport:65 - failed to bind to /10.1.2.144:0, shutting down Netty transport; 23:59:49.206 ERROR SparkContext:96 - Error initializing SparkContext.; java.net.BindException: Failed to bind to: /10.1.2.144:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1534
https://github.com/broadinstitute/gatk/pull/1535:164,Availability,error,error,164,"adding tests for NPR when there's no output specified; this effects CountBasesSpark, CountReadsSpark, FlagStatSpark, and CountVariantsSpark; fixes #1523. fixing an error in CountVariantsSpark where the vcf input was optional. adding some convinience functions to ArgumentsBuilder",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1535
https://github.com/broadinstitute/gatk/pull/1535:7,Testability,test,tests,7,"adding tests for NPR when there's no output specified; this effects CountBasesSpark, CountReadsSpark, FlagStatSpark, and CountVariantsSpark; fixes #1523. fixing an error in CountVariantsSpark where the vcf input was optional. adding some convinience functions to ArgumentsBuilder",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1535
https://github.com/broadinstitute/gatk/issues/1536:115,Availability,TOLER,TOLERANCE,115,"AFPriorProviderUnitTest.java:37 compared doubles by strict equality which is not the right way. It should use the `TOLERANCE` (equal to `0.0001`) eg change . ```; Assert.assertEquals(priors[j], Math.log10(het) - Math.log10(j)); ```. to. ```; Assert.assertEquals(priors[j], Math.log10(het) - Math.log10(j), TOLERANCE); ```. @frank-y-liu can you make this change and see it if fixed your problem? If yes, can you contribute a fix?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1536
https://github.com/broadinstitute/gatk/issues/1536:306,Availability,TOLER,TOLERANCE,306,"AFPriorProviderUnitTest.java:37 compared doubles by strict equality which is not the right way. It should use the `TOLERANCE` (equal to `0.0001`) eg change . ```; Assert.assertEquals(priors[j], Math.log10(het) - Math.log10(j)); ```. to. ```; Assert.assertEquals(priors[j], Math.log10(het) - Math.log10(j), TOLERANCE); ```. @frank-y-liu can you make this change and see it if fixed your problem? If yes, can you contribute a fix?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1536
https://github.com/broadinstitute/gatk/issues/1536:163,Testability,Assert,Assert,163,"AFPriorProviderUnitTest.java:37 compared doubles by strict equality which is not the right way. It should use the `TOLERANCE` (equal to `0.0001`) eg change . ```; Assert.assertEquals(priors[j], Math.log10(het) - Math.log10(j)); ```. to. ```; Assert.assertEquals(priors[j], Math.log10(het) - Math.log10(j), TOLERANCE); ```. @frank-y-liu can you make this change and see it if fixed your problem? If yes, can you contribute a fix?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1536
https://github.com/broadinstitute/gatk/issues/1536:170,Testability,assert,assertEquals,170,"AFPriorProviderUnitTest.java:37 compared doubles by strict equality which is not the right way. It should use the `TOLERANCE` (equal to `0.0001`) eg change . ```; Assert.assertEquals(priors[j], Math.log10(het) - Math.log10(j)); ```. to. ```; Assert.assertEquals(priors[j], Math.log10(het) - Math.log10(j), TOLERANCE); ```. @frank-y-liu can you make this change and see it if fixed your problem? If yes, can you contribute a fix?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1536
https://github.com/broadinstitute/gatk/issues/1536:242,Testability,Assert,Assert,242,"AFPriorProviderUnitTest.java:37 compared doubles by strict equality which is not the right way. It should use the `TOLERANCE` (equal to `0.0001`) eg change . ```; Assert.assertEquals(priors[j], Math.log10(het) - Math.log10(j)); ```. to. ```; Assert.assertEquals(priors[j], Math.log10(het) - Math.log10(j), TOLERANCE); ```. @frank-y-liu can you make this change and see it if fixed your problem? If yes, can you contribute a fix?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1536
https://github.com/broadinstitute/gatk/issues/1536:249,Testability,assert,assertEquals,249,"AFPriorProviderUnitTest.java:37 compared doubles by strict equality which is not the right way. It should use the `TOLERANCE` (equal to `0.0001`) eg change . ```; Assert.assertEquals(priors[j], Math.log10(het) - Math.log10(j)); ```. to. ```; Assert.assertEquals(priors[j], Math.log10(het) - Math.log10(j), TOLERANCE); ```. @frank-y-liu can you make this change and see it if fixed your problem? If yes, can you contribute a fix?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1536
https://github.com/broadinstitute/gatk/issues/1537:422,Performance,load,loaded,422,"This bug was posted for [gsa-unstable](https://github.com/broadinstitute/gsa-unstable/issues/1302) also, and is being posted here again at David R's request. . In my opinion, this bug is a high priority. Also, it impedes me in my documentation work. So if there is a workaround that I can use in the meanwhile, please share with me so I can continue writing my tutorial for indel realignment.; #### How I found the bug. I loaded both the input and output VCFs into IGV and compared the variants in each. Using IGV's tooltip feature and hovering over each variant, I can see the variant type.; #### Bug description. It appears for those sites I checked, MIXED site INDELs are missing from the VCF with INDELs only. I created the INDELs only VCF using `SelectVariants` and `-selectType INDEL`. ; #### Command to recapitulate what I did. Here is the command I used, where $GATK refers to `../GATK/private_unstable_builds/GenomeAnalysisTK_latest_unstable.jar`:. ```; java -jar $GATK -R $BUNDLE_B37 -T SelectVariants -V chr10_1Mb_b37_dbsnp_138.vcf -o INDEL_chr10_1Mb_b37_dbsnp_138.vcf -selectType INDEL; ```. The input file, `/humgen/gsa-hpprojects/GATK/tutorial/solexa/indel_realignment/population_snippet/chr10_1Mb_b37_dbsnp_138.vcf` is subset for variants in the interval 10:96000000-97000000. Let me know if you want my output file. I won't provide it since I assume this bug will be recapitulated with the input and command above.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1537
https://github.com/broadinstitute/gatk/issues/1538:946,Integrability,interface,interface,946,"This class was created to provided for a need to keep a sorted (by location) set of targets. However there is nothing in it that could not really be applied to any locatable in general. . As a matter of fact now I find myself in a situation in where I need the same functionality for a different subclass of Locatables, TargetCollection (and its implementations) have the functionality I need but using TargetCollection looks ugly due to its name and its methods names. The task is the to rename TargetCollection<T> to LocatableCollection<L> and accordingly replace 'target' in methods names for something else (either locatable or a generic name such 'elements'). . Also I recently noticed the existence of IntervalsSkipList which could be an additional implementation for TargetCollection (or rather the new LocatableCollection). So perhaps as part of this task we could unified the skip-list and the hash based solutions under a single common interface.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1538
https://github.com/broadinstitute/gatk/issues/1538:903,Security,hash,hash,903,"This class was created to provided for a need to keep a sorted (by location) set of targets. However there is nothing in it that could not really be applied to any locatable in general. . As a matter of fact now I find myself in a situation in where I need the same functionality for a different subclass of Locatables, TargetCollection (and its implementations) have the functionality I need but using TargetCollection looks ugly due to its name and its methods names. The task is the to rename TargetCollection<T> to LocatableCollection<L> and accordingly replace 'target' in methods names for something else (either locatable or a generic name such 'elements'). . Also I recently noticed the existence of IntervalsSkipList which could be an additional implementation for TargetCollection (or rather the new LocatableCollection). So perhaps as part of this task we could unified the skip-list and the hash based solutions under a single common interface.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1538
https://github.com/broadinstitute/gatk/issues/1540:287,Testability,test,testADAM,287,"ReadsSparkSource uses SAMHeaderReader.readSAMHeaderFrom to get the header from the input file, but it returns a bogus header object when called on an ADAM file (its actually htsjdk which is happily returning the header). Subsequent setHeaderStrict calls fail when using this header. The testADAM method should be re-enabled when there is a resolution for this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1540
https://github.com/broadinstitute/gatk/pull/1541:27,Security,validat,validates,27,"Use setHeaderStrict, which validates the record's reference and mate reference against the new header. Requires disabling the ADAM test in MeanQualityByCycleSparkIntegrationTest due to https://github.com/broadinstitute/gatk/issues/1540.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1541
https://github.com/broadinstitute/gatk/pull/1541:131,Testability,test,test,131,"Use setHeaderStrict, which validates the record's reference and mate reference against the new header. Requires disabling the ADAM test in MeanQualityByCycleSparkIntegrationTest due to https://github.com/broadinstitute/gatk/issues/1540.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1541
https://github.com/broadinstitute/gatk/pull/1549:164,Deployability,patch,patched,164,"previously CommandLineParser would accept non-ambiguous abbreviations for longopts; this caused issues where -v would have different meanings in different tools. I patched joptSimple to add an option to disallow abbreviations, and updated to 5.0-beta-1 which incorporates the necessary changes.; fixes #1347",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1549
https://github.com/broadinstitute/gatk/pull/1549:231,Deployability,update,updated,231,"previously CommandLineParser would accept non-ambiguous abbreviations for longopts; this caused issues where -v would have different meanings in different tools. I patched joptSimple to add an option to disallow abbreviations, and updated to 5.0-beta-1 which incorporates the necessary changes.; fixes #1347",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1549
https://github.com/broadinstitute/gatk/issues/1550:116,Performance,perform,performance,116,"Currently such information can only be extracted indirectly via a conversion to SamRecord, which incurs unnecessary performance penalty:. ```; GATKRead read = ... // proper initialization; SAMFileHeader header = ...; SamPairUtil.PairOrientation readOrientation = SamPairUtil.getPairOrientation(read.convertToSAMRecord(header)); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1550
https://github.com/broadinstitute/gatk/pull/1551:470,Deployability,update,updated,470,"htsjdk versions older than 2.1.1 would remove NM and MD tags on bam->cram compression, and then automatically regenerate NM tags when reading cram. Starting with 2.1.1, in order to ensure lossless round-tripping, it no longer does either, and restores only the tags present in the compressed file . As a result, any cramfile read with 2.1.1+ that was generated with older htsjdk versions (or samtools) will fail validation due to missing NM tags. So this PR contains an updated cram file that contains NM tags for the SAMFileValidation tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1551
https://github.com/broadinstitute/gatk/pull/1551:412,Security,validat,validation,412,"htsjdk versions older than 2.1.1 would remove NM and MD tags on bam->cram compression, and then automatically regenerate NM tags when reading cram. Starting with 2.1.1, in order to ensure lossless round-tripping, it no longer does either, and restores only the tags present in the compressed file . As a result, any cramfile read with 2.1.1+ that was generated with older htsjdk versions (or samtools) will fail validation due to missing NM tags. So this PR contains an updated cram file that contains NM tags for the SAMFileValidation tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1551
https://github.com/broadinstitute/gatk/pull/1551:536,Testability,test,tests,536,"htsjdk versions older than 2.1.1 would remove NM and MD tags on bam->cram compression, and then automatically regenerate NM tags when reading cram. Starting with 2.1.1, in order to ensure lossless round-tripping, it no longer does either, and restores only the tags present in the compressed file . As a result, any cramfile read with 2.1.1+ that was generated with older htsjdk versions (or samtools) will fail validation due to missing NM tags. So this PR contains an updated cram file that contains NM tags for the SAMFileValidation tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1551
https://github.com/broadinstitute/gatk/issues/1554:30,Availability,error,error,30,The build should give a clear error message explaining how to skip building the native code if it fails to build.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1554
https://github.com/broadinstitute/gatk/issues/1554:36,Integrability,message,message,36,The build should give a clear error message explaining how to skip building the native code if it fails to build.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1554
https://github.com/broadinstitute/gatk/issues/1554:24,Usability,clear,clear,24,The build should give a clear error message explaining how to skip building the native code if it fails to build.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1554
https://github.com/broadinstitute/gatk/pull/1555:64,Testability,Test,Test,64,"relatively straightforward port of CGP. I made minimal changes. Test data is sharable and copied from GATK3. Note: this is going to protected for merging before it's usable,. @lbergelson can you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1555
https://github.com/broadinstitute/gatk/pull/1555:166,Usability,usab,usable,166,"relatively straightforward port of CGP. I made minimal changes. Test data is sharable and copied from GATK3. Note: this is going to protected for merging before it's usable,. @lbergelson can you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1555
https://github.com/broadinstitute/gatk/issues/1556:158,Performance,perform,performance-deflate-compressor-with-optimizations-for-genomic-data,158,this may go to htsjdk (maybe hadoop-bam too) and this issue is a placeholder. Intel's compression code https://software.intel.com/en-us/articles/igzip-a-high-performance-deflate-compressor-with-optimizations-for-genomic-data shows good speedup on reading/writing BAMs. We should investigate using it. . @paolonarvaez assigning to you - let me know if that's ok.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1556
https://github.com/broadinstitute/gatk/issues/1559:56,Deployability,pipeline,pipeline,56,"https://github.com/broadinstitute/gatk/issues/1411. The pipeline takes an readname sorted BAM and does MarkDuplicates and 2 steps of BQSR and saves sharded output. ; - legacy: SortSam + MarkDuplicates + BaseRecalibrator + PrintReads; - new: ReadsPipelineSpark. The test file is 35GB, readname sorted.; - [x] make a querynamesorted file and copy to cluster; `hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam`; - [x] run new code on it, measure time; - [x] run legacy code (picard + gatk3) on coordinate sorted bam, pinned to 1 CPU, 7GB ram",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1559
https://github.com/broadinstitute/gatk/issues/1559:265,Testability,test,test,265,"https://github.com/broadinstitute/gatk/issues/1411. The pipeline takes an readname sorted BAM and does MarkDuplicates and 2 steps of BQSR and saves sharded output. ; - legacy: SortSam + MarkDuplicates + BaseRecalibrator + PrintReads; - new: ReadsPipelineSpark. The test file is 35GB, readname sorted.; - [x] make a querynamesorted file and copy to cluster; `hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam`; - [x] run new code on it, measure time; - [x] run legacy code (picard + gatk3) on coordinate sorted bam, pinned to 1 CPU, 7GB ram",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1559
https://github.com/broadinstitute/gatk/issues/1560:210,Availability,failure,failure,210,"the invalid reads strikes back - i got this when running the ReadsPipelineSpark on qurynamesorted file `hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam`:. ```; Job aborted due to stage failure: Task 47 in stage 2.0 failed 4 times, most recent failure: Lost task 47.3 in stage 2.0 (TID 680, dataflow05.broadinstitute.org): java.lang.IllegalArgumentException: ; Invalid interval. Contig:20 start:62720124 end:62720123; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:34); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:46); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.lambda$join$3d1c3858$1(BroadcastJoinReadsWithVariants.java:27); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1560:268,Availability,failure,failure,268,"the invalid reads strikes back - i got this when running the ReadsPipelineSpark on qurynamesorted file `hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam`:. ```; Job aborted due to stage failure: Task 47 in stage 2.0 failed 4 times, most recent failure: Lost task 47.3 in stage 2.0 (TID 680, dataflow05.broadinstitute.org): java.lang.IllegalArgumentException: ; Invalid interval. Contig:20 start:62720124 end:62720123; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:34); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:46); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.lambda$join$3d1c3858$1(BroadcastJoinReadsWithVariants.java:27); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1560:2436,Energy Efficiency,schedul,scheduler,2436,Fun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1560:2515,Energy Efficiency,schedul,scheduler,2515,Fun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1560:2594,Energy Efficiency,schedul,scheduler,2594,Fun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1560:1274,Integrability,Wrap,Wrappers,1274," 47.3 in stage 2.0 (TID 680, dataflow05.broadinstitute.org): java.lang.IllegalArgumentException: ; Invalid interval. Contig:20 start:62720124 end:62720123; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:34); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:46); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.lambda$join$3d1c3858$1(BroadcastJoinReadsWithVariants.java:27); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.sca",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1560:1304,Integrability,Wrap,Wrappers,1304,"TID 680, dataflow05.broadinstitute.org): java.lang.IllegalArgumentException: ; Invalid interval. Contig:20 start:62720124 end:62720123; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:34); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:46); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.lambda$join$3d1c3858$1(BroadcastJoinReadsWithVariants.java:27); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1560:2716,Performance,concurren,concurrent,2716,Fun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1560:2800,Performance,concurren,concurrent,2800,Fun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1560:189,Safety,abort,aborted,189,"the invalid reads strikes back - i got this when running the ReadsPipelineSpark on qurynamesorted file `hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam`:. ```; Job aborted due to stage failure: Task 47 in stage 2.0 failed 4 times, most recent failure: Lost task 47.3 in stage 2.0 (TID 680, dataflow05.broadinstitute.org): java.lang.IllegalArgumentException: ; Invalid interval. Contig:20 start:62720124 end:62720123; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:34); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:46); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.lambda$join$3d1c3858$1(BroadcastJoinReadsWithVariants.java:27); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1560:496,Security,validat,validatePositions,496,"the invalid reads strikes back - i got this when running the ReadsPipelineSpark on qurynamesorted file `hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam`:. ```; Job aborted due to stage failure: Task 47 in stage 2.0 failed 4 times, most recent failure: Lost task 47.3 in stage 2.0 (TID 680, dataflow05.broadinstitute.org): java.lang.IllegalArgumentException: ; Invalid interval. Contig:20 start:62720124 end:62720123; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:34); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:46); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.lambda$join$3d1c3858$1(BroadcastJoinReadsWithVariants.java:27); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1560:481,Usability,Simpl,SimpleInterval,481,"the invalid reads strikes back - i got this when running the ReadsPipelineSpark on qurynamesorted file `hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam`:. ```; Job aborted due to stage failure: Task 47 in stage 2.0 failed 4 times, most recent failure: Lost task 47.3 in stage 2.0 (TID 680, dataflow05.broadinstitute.org): java.lang.IllegalArgumentException: ; Invalid interval. Contig:20 start:62720124 end:62720123; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:34); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:46); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.lambda$join$3d1c3858$1(BroadcastJoinReadsWithVariants.java:27); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1560:514,Usability,Simpl,SimpleInterval,514,"the invalid reads strikes back - i got this when running the ReadsPipelineSpark on qurynamesorted file `hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam`:. ```; Job aborted due to stage failure: Task 47 in stage 2.0 failed 4 times, most recent failure: Lost task 47.3 in stage 2.0 (TID 680, dataflow05.broadinstitute.org): java.lang.IllegalArgumentException: ; Invalid interval. Contig:20 start:62720124 end:62720123; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:34); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:46); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.lambda$join$3d1c3858$1(BroadcastJoinReadsWithVariants.java:27); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1560:578,Usability,Simpl,SimpleInterval,578,"the invalid reads strikes back - i got this when running the ReadsPipelineSpark on qurynamesorted file `hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam`:. ```; Job aborted due to stage failure: Task 47 in stage 2.0 failed 4 times, most recent failure: Lost task 47.3 in stage 2.0 (TID 680, dataflow05.broadinstitute.org): java.lang.IllegalArgumentException: ; Invalid interval. Contig:20 start:62720124 end:62720123; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:34); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:46); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.lambda$join$3d1c3858$1(BroadcastJoinReadsWithVariants.java:27); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1560:600,Usability,Simpl,SimpleInterval,600,"the invalid reads strikes back - i got this when running the ReadsPipelineSpark on qurynamesorted file `hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam`:. ```; Job aborted due to stage failure: Task 47 in stage 2.0 failed 4 times, most recent failure: Lost task 47.3 in stage 2.0 (TID 680, dataflow05.broadinstitute.org): java.lang.IllegalArgumentException: ; Invalid interval. Contig:20 start:62720124 end:62720123; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:34); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:46); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.lambda$join$3d1c3858$1(BroadcastJoinReadsWithVariants.java:27); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1560:664,Usability,Simpl,SimpleInterval,664,"the invalid reads strikes back - i got this when running the ReadsPipelineSpark on qurynamesorted file `hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam`:. ```; Job aborted due to stage failure: Task 47 in stage 2.0 failed 4 times, most recent failure: Lost task 47.3 in stage 2.0 (TID 680, dataflow05.broadinstitute.org): java.lang.IllegalArgumentException: ; Invalid interval. Contig:20 start:62720124 end:62720123; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:34); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:46); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.lambda$join$3d1c3858$1(BroadcastJoinReadsWithVariants.java:27); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1560:686,Usability,Simpl,SimpleInterval,686,"the invalid reads strikes back - i got this when running the ReadsPipelineSpark on qurynamesorted file `hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam`:. ```; Job aborted due to stage failure: Task 47 in stage 2.0 failed 4 times, most recent failure: Lost task 47.3 in stage 2.0 (TID 680, dataflow05.broadinstitute.org): java.lang.IllegalArgumentException: ; Invalid interval. Contig:20 start:62720124 end:62720123; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:59); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:34); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:46); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.lambda$join$3d1c3858$1(BroadcastJoinReadsWithVariants.java:27); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560
https://github.com/broadinstitute/gatk/issues/1562:30,Deployability,Integrat,IntegrationTestSpec,30,"There are a few problems with IntegrationTestSpec that surface when adding CRAM tests to the Reads/BQSRSparkPipelines:; - generated output filenames contain no sam extension, so outputs always are treated as .bam; - it doesn't have explicit knowledge of the reference file, which is needed to do proper file comparisons through the SamAssertionUtils assertSamsEqual methods; - there is code that assumes that any expectedFile that doesn't end in "".bam"" should use text comparison. We could fix these, but I'm not sure what the incremental value-added of this class is when we can just use TestNG for expected exceptions, etc.; it might make more sense to just eliminate this style of test completely.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1562
https://github.com/broadinstitute/gatk/issues/1562:30,Integrability,Integrat,IntegrationTestSpec,30,"There are a few problems with IntegrationTestSpec that surface when adding CRAM tests to the Reads/BQSRSparkPipelines:; - generated output filenames contain no sam extension, so outputs always are treated as .bam; - it doesn't have explicit knowledge of the reference file, which is needed to do proper file comparisons through the SamAssertionUtils assertSamsEqual methods; - there is code that assumes that any expectedFile that doesn't end in "".bam"" should use text comparison. We could fix these, but I'm not sure what the incremental value-added of this class is when we can just use TestNG for expected exceptions, etc.; it might make more sense to just eliminate this style of test completely.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1562
https://github.com/broadinstitute/gatk/issues/1562:80,Testability,test,tests,80,"There are a few problems with IntegrationTestSpec that surface when adding CRAM tests to the Reads/BQSRSparkPipelines:; - generated output filenames contain no sam extension, so outputs always are treated as .bam; - it doesn't have explicit knowledge of the reference file, which is needed to do proper file comparisons through the SamAssertionUtils assertSamsEqual methods; - there is code that assumes that any expectedFile that doesn't end in "".bam"" should use text comparison. We could fix these, but I'm not sure what the incremental value-added of this class is when we can just use TestNG for expected exceptions, etc.; it might make more sense to just eliminate this style of test completely.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1562
https://github.com/broadinstitute/gatk/issues/1562:350,Testability,assert,assertSamsEqual,350,"There are a few problems with IntegrationTestSpec that surface when adding CRAM tests to the Reads/BQSRSparkPipelines:; - generated output filenames contain no sam extension, so outputs always are treated as .bam; - it doesn't have explicit knowledge of the reference file, which is needed to do proper file comparisons through the SamAssertionUtils assertSamsEqual methods; - there is code that assumes that any expectedFile that doesn't end in "".bam"" should use text comparison. We could fix these, but I'm not sure what the incremental value-added of this class is when we can just use TestNG for expected exceptions, etc.; it might make more sense to just eliminate this style of test completely.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1562
https://github.com/broadinstitute/gatk/issues/1562:589,Testability,Test,TestNG,589,"There are a few problems with IntegrationTestSpec that surface when adding CRAM tests to the Reads/BQSRSparkPipelines:; - generated output filenames contain no sam extension, so outputs always are treated as .bam; - it doesn't have explicit knowledge of the reference file, which is needed to do proper file comparisons through the SamAssertionUtils assertSamsEqual methods; - there is code that assumes that any expectedFile that doesn't end in "".bam"" should use text comparison. We could fix these, but I'm not sure what the incremental value-added of this class is when we can just use TestNG for expected exceptions, etc.; it might make more sense to just eliminate this style of test completely.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1562
https://github.com/broadinstitute/gatk/issues/1562:684,Testability,test,test,684,"There are a few problems with IntegrationTestSpec that surface when adding CRAM tests to the Reads/BQSRSparkPipelines:; - generated output filenames contain no sam extension, so outputs always are treated as .bam; - it doesn't have explicit knowledge of the reference file, which is needed to do proper file comparisons through the SamAssertionUtils assertSamsEqual methods; - there is code that assumes that any expectedFile that doesn't end in "".bam"" should use text comparison. We could fix these, but I'm not sure what the incremental value-added of this class is when we can just use TestNG for expected exceptions, etc.; it might make more sense to just eliminate this style of test completely.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1562
https://github.com/broadinstitute/gatk/pull/1564:135,Deployability,Integrat,IntegrationTestSpec,135,"Fixes https://github.com/broadinstitute/gatk/issues/1226. We should resolve whether or not we want to invest in fixing the issues with IntegrationTestSpec (see https://github.com/broadinstitute/gatk/issues/1562) before we do any more of these, since now that we can write CRAMs on SPARK we can also add CRAM tests to ApplyBQSRSpark as well, we just need to decide which way to write the tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1564
https://github.com/broadinstitute/gatk/pull/1564:135,Integrability,Integrat,IntegrationTestSpec,135,"Fixes https://github.com/broadinstitute/gatk/issues/1226. We should resolve whether or not we want to invest in fixing the issues with IntegrationTestSpec (see https://github.com/broadinstitute/gatk/issues/1562) before we do any more of these, since now that we can write CRAMs on SPARK we can also add CRAM tests to ApplyBQSRSpark as well, we just need to decide which way to write the tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1564
https://github.com/broadinstitute/gatk/pull/1564:308,Testability,test,tests,308,"Fixes https://github.com/broadinstitute/gatk/issues/1226. We should resolve whether or not we want to invest in fixing the issues with IntegrationTestSpec (see https://github.com/broadinstitute/gatk/issues/1562) before we do any more of these, since now that we can write CRAMs on SPARK we can also add CRAM tests to ApplyBQSRSpark as well, we just need to decide which way to write the tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1564
https://github.com/broadinstitute/gatk/pull/1564:387,Testability,test,tests,387,"Fixes https://github.com/broadinstitute/gatk/issues/1226. We should resolve whether or not we want to invest in fixing the issues with IntegrationTestSpec (see https://github.com/broadinstitute/gatk/issues/1562) before we do any more of these, since now that we can write CRAMs on SPARK we can also add CRAM tests to ApplyBQSRSpark as well, we just need to decide which way to write the tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1564
https://github.com/broadinstitute/gatk/pull/1566:158,Security,access,access,158,"Hi Chris,. Would you please review this?; And @tedsharpe, feel free to look at the getters in `InsertSizeMetricsCollectorSpark.java` and see if you need more access functions. Thank you.; Steve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1566
https://github.com/broadinstitute/gatk/pull/1567:491,Performance,perform,performance,491,"-Ported the remaining pieces of the HaplotypeCaller and assembled them; into a runnable tool. Fixed many bugs in our ported code in the process; of doing so. -Extracted a separate ""engine"" that does all the work of the HC and is; separate from the runnable walker. -Added a new walker class, ReadWindowWalker, as a prospective replacement; for active region traversal. -Hooked up the native VECTOR_LOGLESS_CACHING PairHmm to the HaplotypeCaller,; and activated it by default (this speeds up performance by ~3x in my tests).; Also added a fallback mode when AVX is not present.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1567
https://github.com/broadinstitute/gatk/pull/1567:516,Testability,test,tests,516,"-Ported the remaining pieces of the HaplotypeCaller and assembled them; into a runnable tool. Fixed many bugs in our ported code in the process; of doing so. -Extracted a separate ""engine"" that does all the work of the HC and is; separate from the runnable walker. -Added a new walker class, ReadWindowWalker, as a prospective replacement; for active region traversal. -Hooked up the native VECTOR_LOGLESS_CACHING PairHmm to the HaplotypeCaller,; and activated it by default (this speeds up performance by ~3x in my tests).; Also added a fallback mode when AVX is not present.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1567
https://github.com/broadinstitute/gatk/issues/1569:0,Testability,test,test,0,test to see if creating a splitting index for a bam file makes reading it in hadoop bam faster than not having one,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1569
https://github.com/broadinstitute/gatk/pull/1574:0,Testability,Test,Tests,0,Tests developed for https://github.com/broadinstitute/gatk/issues/1321.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1574
https://github.com/broadinstitute/gatk/issues/1575:146,Availability,avail,available,146,"`VectorPairHMMUnitTest` checks `isAVXSupported()` and skips tests if it's not. However, if AVX is supported on the machine, but the library isn't available it fails with . ```; Caused by: java.lang.IllegalArgumentException: Resource not found relative to class org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM: /lib/libVectorLoglessPairHMM.so; at org.broadinstitute.hellbender.utils.io.Resource.getResourceContentsAsStream(Resource.java:60); at org.broadinstitute.hellbender.utils.io.IOUtils.writeResource(IOUtils.java:234); at org.broadinstitute.hellbender.utils.io.IOUtils.writeTempResource(IOUtils.java:222); at org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM.<init>(VectorLoglessPairHMM.java:106); at org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest.<init>(VectorPairHMMUnitTest.java:22); ... 46 more; ```. This should be fixed so that the tests are skipped instead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1575
https://github.com/broadinstitute/gatk/issues/1575:60,Testability,test,tests,60,"`VectorPairHMMUnitTest` checks `isAVXSupported()` and skips tests if it's not. However, if AVX is supported on the machine, but the library isn't available it fails with . ```; Caused by: java.lang.IllegalArgumentException: Resource not found relative to class org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM: /lib/libVectorLoglessPairHMM.so; at org.broadinstitute.hellbender.utils.io.Resource.getResourceContentsAsStream(Resource.java:60); at org.broadinstitute.hellbender.utils.io.IOUtils.writeResource(IOUtils.java:234); at org.broadinstitute.hellbender.utils.io.IOUtils.writeTempResource(IOUtils.java:222); at org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM.<init>(VectorLoglessPairHMM.java:106); at org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest.<init>(VectorPairHMMUnitTest.java:22); ... 46 more; ```. This should be fixed so that the tests are skipped instead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1575
https://github.com/broadinstitute/gatk/issues/1575:894,Testability,test,tests,894,"`VectorPairHMMUnitTest` checks `isAVXSupported()` and skips tests if it's not. However, if AVX is supported on the machine, but the library isn't available it fails with . ```; Caused by: java.lang.IllegalArgumentException: Resource not found relative to class org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM: /lib/libVectorLoglessPairHMM.so; at org.broadinstitute.hellbender.utils.io.Resource.getResourceContentsAsStream(Resource.java:60); at org.broadinstitute.hellbender.utils.io.IOUtils.writeResource(IOUtils.java:234); at org.broadinstitute.hellbender.utils.io.IOUtils.writeTempResource(IOUtils.java:222); at org.broadinstitute.hellbender.utils.pairhmm.VectorLoglessPairHMM.<init>(VectorLoglessPairHMM.java:106); at org.broadinstitute.hellbender.utils.pairhmm.VectorPairHMMUnitTest.<init>(VectorPairHMMUnitTest.java:22); ... 46 more; ```. This should be fixed so that the tests are skipped instead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1575
https://github.com/broadinstitute/gatk/pull/1577:26,Testability,test,tests,26,Fixing an issue where AVX tests would fail if AVX was supported by the computer but the library hadn't been built for some reason. . Fixes #1575,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1577
https://github.com/broadinstitute/gatk/issues/1578:17,Deployability,release,released,17,"Gradle 2.12 just released which includes some improvements we've been waiting for. It includes a ""compileOnly"" scope which should make some of our spark configuration unnecessary. We should investigate if we can simplify the sparkJar setup using the new scope, and possible improve things for gatk-protected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1578
https://github.com/broadinstitute/gatk/issues/1578:153,Deployability,configurat,configuration,153,"Gradle 2.12 just released which includes some improvements we've been waiting for. It includes a ""compileOnly"" scope which should make some of our spark configuration unnecessary. We should investigate if we can simplify the sparkJar setup using the new scope, and possible improve things for gatk-protected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1578
https://github.com/broadinstitute/gatk/issues/1578:153,Modifiability,config,configuration,153,"Gradle 2.12 just released which includes some improvements we've been waiting for. It includes a ""compileOnly"" scope which should make some of our spark configuration unnecessary. We should investigate if we can simplify the sparkJar setup using the new scope, and possible improve things for gatk-protected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1578
https://github.com/broadinstitute/gatk/issues/1578:212,Usability,simpl,simplify,212,"Gradle 2.12 just released which includes some improvements we've been waiting for. It includes a ""compileOnly"" scope which should make some of our spark configuration unnecessary. We should investigate if we can simplify the sparkJar setup using the new scope, and possible improve things for gatk-protected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1578
https://github.com/broadinstitute/gatk/issues/1579:48,Modifiability,flexible,flexible,48,"Gradle is gradually transitioning to a new more flexible software model. We use this new style model for our native code, but not for java. We should investigate switching our java build to the new model. Some details are here https://docs.gradle.org/current/userguide/java_software.html. It includes some interesting new features like a mechanism for explicitly declaring a public api for a project.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1579
https://github.com/broadinstitute/gatk/issues/1581:12,Usability,simpl,simple,12,it's a nice simple example of a locus walker. blocked by #1464,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1581
https://github.com/broadinstitute/gatk/issues/1585:224,Availability,avail,available,224,"using IntelDeflater speeds up writing of bams by at least 15% (latest igzip is even faster, though only at compression level 1). We must have a way to use it. . note: the htsjdk jar does not contain the .so file - it's only available in the zip file",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1585
https://github.com/broadinstitute/gatk/pull/1586:21,Deployability,update,update,21,is this how a gradle update is done?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1586
https://github.com/broadinstitute/gatk/pull/1587:320,Deployability,install,installDist,320,"fixes #1585 - getting IntelDeflater from htsjdk zipfile (it's not part of the jar!) and then putting the path to it as a default JVM argument. @lbergelson can you review? this is a bit of a hack, improvements welcome; Do check that, out of the box (on linux) you get the IntelDeflater. I run this:. ```; ./gradlew clean installDist; ./gatk-launch CountReads -I src/test/resources/large/NA12878.RNAseq.bam; ```. and check output for `IntelDeflater`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1587
https://github.com/broadinstitute/gatk/pull/1587:365,Testability,test,test,365,"fixes #1585 - getting IntelDeflater from htsjdk zipfile (it's not part of the jar!) and then putting the path to it as a default JVM argument. @lbergelson can you review? this is a bit of a hack, improvements welcome; Do check that, out of the box (on linux) you get the IntelDeflater. I run this:. ```; ./gradlew clean installDist; ./gatk-launch CountReads -I src/test/resources/large/NA12878.RNAseq.bam; ```. and check output for `IntelDeflater`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1587
https://github.com/broadinstitute/gatk/pull/1588:465,Testability,test,test,465,"Switch to using async IO by default. It's an out-of-the-box win for almost everybody because almost everyone has more than 1 CPU. Difference is write speed is at least 20%. . Also added a line to the output that indicates what IO we're using (a'la deflater). On single processors it is slower than syncIO, so you can turn it off by running like this:. ```; JAVA_OPTS=""-Dsamjdk.use_async_io=false"" ./gatk-launch PrintReads .....; ```. @lbergelson can you review and test?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1588
https://github.com/broadinstitute/gatk/pull/1590:204,Modifiability,refactor,refactoring,204,One of the non-cloud tests in BQSRSparkIntegrationTest tries to use the API key when its not required; as a result the test fails when the key isn't set even though it should pass. Introduced in the test refactoring that was part of https://github.com/broadinstitute/gatk/pull/1533.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1590
https://github.com/broadinstitute/gatk/pull/1590:21,Testability,test,tests,21,One of the non-cloud tests in BQSRSparkIntegrationTest tries to use the API key when its not required; as a result the test fails when the key isn't set even though it should pass. Introduced in the test refactoring that was part of https://github.com/broadinstitute/gatk/pull/1533.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1590
https://github.com/broadinstitute/gatk/pull/1590:119,Testability,test,test,119,One of the non-cloud tests in BQSRSparkIntegrationTest tries to use the API key when its not required; as a result the test fails when the key isn't set even though it should pass. Introduced in the test refactoring that was part of https://github.com/broadinstitute/gatk/pull/1533.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1590
https://github.com/broadinstitute/gatk/pull/1590:199,Testability,test,test,199,One of the non-cloud tests in BQSRSparkIntegrationTest tries to use the API key when its not required; as a result the test fails when the key isn't set even though it should pass. Introduced in the test refactoring that was part of https://github.com/broadinstitute/gatk/pull/1533.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1590
https://github.com/broadinstitute/gatk/pull/1591:109,Safety,avoid,avoid,109,we don't test on windows and I doubt gatk would work on windows so let's not generate the windows startup to avoid confusing people. @lbergelson can you look at it?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1591
https://github.com/broadinstitute/gatk/pull/1591:9,Testability,test,test,9,we don't test on windows and I doubt gatk would work on windows so let's not generate the windows startup to avoid confusing people. @lbergelson can you look at it?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1591
https://github.com/broadinstitute/gatk/issues/1592:137,Testability,test,test,137,sharded writing feels slow - ~6 minutes for 30GB on 60 disks. I would hope for at least 20 MB/s so > 1GB/s on. @akiezun to investigate - test with writing big rdds as sharded text files should be a good benchmark.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1592
https://github.com/broadinstitute/gatk/issues/1592:203,Testability,benchmark,benchmark,203,sharded writing feels slow - ~6 minutes for 30GB on 60 disks. I would hope for at least 20 MB/s so > 1GB/s on. @akiezun to investigate - test with writing big rdds as sharded text files should be a good benchmark.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1592
https://github.com/broadinstitute/gatk/issues/1593:207,Availability,Error,Error,207,```; ./gatk-launch CompareBaseQualitiesSpark -I HCC1143.bqsr.bam -I2 HCC1143.bqsr.cram -R ../human_g1k_v37.fasta --disableSequenceDictionaryValidation; ```. blows up with. ```; htsjdk.samtools.SAMException: Error opening file: ../human_g1k_v37.fasta; at htsjdk.samtools.util.IOUtil.openFileForReading(IOUtil.java:523); at htsjdk.samtools.reference.FastaSequenceFile.<init>(FastaSequenceFile.java:59); at htsjdk.samtools.reference.ReferenceSequenceFileFactory.getReferenceSequenceFile(ReferenceSequenceFileFactory.java:129); at htsjdk.samtools.reference.ReferenceSequenceFileFactory.getReferenceSequenceFile(ReferenceSequenceFileFactory.java:82); at htsjdk.samtools.reference.ReferenceSequenceFileFactory.getReferenceSequenceFile(ReferenceSequenceFileFactory.java:70); at htsjdk.samtools.reference.ReferenceSequenceFileFactory.getReferenceSequenceFile(ReferenceSequenceFileFactory.java:59); at org.broadinstitute.hellbender.engine.datasources.ReferenceFileSource.getReferenceSequenceDictionary(ReferenceFileSource.java:52); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.getReferenceSequenceDictionary(ReferenceMultiSource.java:110); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:354); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:151); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:170); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.ma,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1593
https://github.com/broadinstitute/gatk/issues/1593:2770,Integrability,message,message,2770,File(ReferenceSequenceFileFactory.java:59); at org.broadinstitute.hellbender.engine.datasources.ReferenceFileSource.getReferenceSequenceDictionary(ReferenceFileSource.java:52); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.getReferenceSequenceDictionary(ReferenceMultiSource.java:110); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:354); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:151); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:170); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); Caused by: java.nio.file.NoSuchFileException: ../human_g1k_v37.fasta; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); at java.nio.file.Files.newInputStream(Files.java:152); at htsjdk.samtools.util.IOUtil.openFileForReading(IOUtil.java:519); ... 16 more; ```. we should present those as a simple message that the reference file is missing: `UserException.MissingReference`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1593
https://github.com/broadinstitute/gatk/issues/1593:2763,Usability,simpl,simple,2763,File(ReferenceSequenceFileFactory.java:59); at org.broadinstitute.hellbender.engine.datasources.ReferenceFileSource.getReferenceSequenceDictionary(ReferenceFileSource.java:52); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.getReferenceSequenceDictionary(ReferenceMultiSource.java:110); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:354); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:151); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:170); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); Caused by: java.nio.file.NoSuchFileException: ../human_g1k_v37.fasta; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); at java.nio.file.Files.newInputStream(Files.java:152); at htsjdk.samtools.util.IOUtil.openFileForReading(IOUtil.java:519); ... 16 more; ```. we should present those as a simple message that the reference file is missing: `UserException.MissingReference`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1593
https://github.com/broadinstitute/gatk/pull/1595:79,Testability,test,tests,79,Fixes https://github.com/broadinstitute/gatk/issues/1321. Also added some CRAM tests. I started to fix https://github.com/broadinstitute/gatk/issues/1593 as part of this but there are several code paths that need to change so I'll do a separate PR.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1595
https://github.com/broadinstitute/gatk/issues/1597:6,Performance,perform,performance,6,write performance is good on asyncIO but read is bad and I'm not sure why. Need to investigate. The test I'm using is BaseRecalibrator - i see a 3x speed difference which is terrible,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1597
https://github.com/broadinstitute/gatk/issues/1597:100,Testability,test,test,100,write performance is good on asyncIO but read is bad and I'm not sure why. Need to investigate. The test I'm using is BaseRecalibrator - i see a 3x speed difference which is terrible,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1597
https://github.com/broadinstitute/gatk/issues/1598:153,Availability,ERROR,ERROR,153,"run this. ```; ./gatk-launch CountReads -I fred; ```. and get this. ```; ***********************************************************************; A USER ERROR has occurred: Couldn't read file /Users/akiezun/IdeaProjects/gatk/fred. Error was: Cannot read non-existent file: /Users/akiezun/IdeaProjects/gatk/fred. ***********************************************************************; Exception in thread ""main"" java.lang.NullPointerException; at org.broadinstitute.hellbender.Main.main(Main.java:93); ```. this is bogus - I don't want any NPEs here",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1598
https://github.com/broadinstitute/gatk/issues/1598:231,Availability,Error,Error,231,"run this. ```; ./gatk-launch CountReads -I fred; ```. and get this. ```; ***********************************************************************; A USER ERROR has occurred: Couldn't read file /Users/akiezun/IdeaProjects/gatk/fred. Error was: Cannot read non-existent file: /Users/akiezun/IdeaProjects/gatk/fred. ***********************************************************************; Exception in thread ""main"" java.lang.NullPointerException; at org.broadinstitute.hellbender.Main.main(Main.java:93); ```. this is bogus - I don't want any NPEs here",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1598
https://github.com/broadinstitute/gatk/pull/1600:223,Security,authenticat,authenticate,223,"his version includes a fix for the issue we were seeing which required multiple pulls; removed the workaround (fixes #1404). The problem was that we were using an ssh git remote. git-lfs fails in this case because it can't authenticate. (unclear to me if this is a bug or not, I don't know if github requires authentication for ssh access to public repos). In versions <= 1.1.0 git-lfs was falling back to trying over an http connection. They removed this fallback mechanism in 1.1.1. See github/git-lfs#1090 for discussion.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1600
https://github.com/broadinstitute/gatk/pull/1600:309,Security,authenticat,authentication,309,"his version includes a fix for the issue we were seeing which required multiple pulls; removed the workaround (fixes #1404). The problem was that we were using an ssh git remote. git-lfs fails in this case because it can't authenticate. (unclear to me if this is a bug or not, I don't know if github requires authentication for ssh access to public repos). In versions <= 1.1.0 git-lfs was falling back to trying over an http connection. They removed this fallback mechanism in 1.1.1. See github/git-lfs#1090 for discussion.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1600
https://github.com/broadinstitute/gatk/pull/1600:332,Security,access,access,332,"his version includes a fix for the issue we were seeing which required multiple pulls; removed the workaround (fixes #1404). The problem was that we were using an ssh git remote. git-lfs fails in this case because it can't authenticate. (unclear to me if this is a bug or not, I don't know if github requires authentication for ssh access to public repos). In versions <= 1.1.0 git-lfs was falling back to trying over an http connection. They removed this fallback mechanism in 1.1.1. See github/git-lfs#1090 for discussion.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1600
https://github.com/broadinstitute/gatk/pull/1602:141,Energy Efficiency,power,powerful,141,travis support recommended that we move to travis with sudo since the capacity problems have been fixed; the travis nodes with sudo are more powerful now and there shouldn't be long startup delays like there used to be; also moving to the newer ubuntu image,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1602
https://github.com/broadinstitute/gatk/pull/1604:152,Availability,failure,failures,152,gatk-launch wasn't exiting with the same exit code that GATK did; this causes problems for testing infrastructure that expects non-zero exit codes from failures. fixes #1599,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1604
https://github.com/broadinstitute/gatk/pull/1604:91,Testability,test,testing,91,gatk-launch wasn't exiting with the same exit code that GATK did; this causes problems for testing infrastructure that expects non-zero exit codes from failures. fixes #1599,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1604
https://github.com/broadinstitute/gatk/issues/1606:284,Performance,perform,performance,284,I want to establish how well the current async reading and writing of reads and variants works and in what circumstances it should be used. Analysis here is a matrix:; - reads vs variants; - read vs write; - for variants: wide vs narrow; - for variants: compressed vs uncompressed; - performance on multiple cores vs overhead when pinned to 1 core,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1606
https://github.com/broadinstitute/gatk/issues/1607:55,Performance,perform,performance,55,"On a whole genome and exome, the goal is to beat GATK3 performance and memory usage",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1607
https://github.com/broadinstitute/gatk/issues/1608:26,Performance,perform,performance,26,the goal is to beat GATK3 performance and memory usage of GenotypeGVCFs on realistic datasizes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1608
https://github.com/broadinstitute/gatk/issues/1609:30,Performance,perform,performance,30,the goal is to have automated performance tests for representative commandlines and datasizes - details of when to run them etc remain to be decided,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1609
https://github.com/broadinstitute/gatk/issues/1609:42,Testability,test,tests,42,the goal is to have automated performance tests for representative commandlines and datasizes - details of when to run them etc remain to be decided,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1609
https://github.com/broadinstitute/gatk/issues/1610:115,Performance,optimiz,optimize,115,"The goal is to create a way to run, test and profile GATK tools in the cloud, the way that production runs them to optimize for production use",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1610
https://github.com/broadinstitute/gatk/issues/1610:36,Testability,test,test,36,"The goal is to create a way to run, test and profile GATK tools in the cloud, the way that production runs them to optimize for production use",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1610
https://github.com/broadinstitute/gatk/issues/1611:81,Testability,test,test,81,Currently I'm unable to spin up a dataproc cluster succesfully in the broad-gatk-test project. Talking with google about it here: https://groups.google.com/forum/#!topic/cloud-dataproc-discuss/Oi4VKt9RwdU,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1611
https://github.com/broadinstitute/gatk/pull/1616:96,Testability,log,logic,96,This pull request is focused on resolving occurrences of Sonar rule squid:S2178 - Short-circuit logic should be used in boolean contexts. You can find more information about the issue here: https://dev.eclipse.org/sonar/coding_rules#q=squid:S2178. Please let me know if you have any questions. M-Ezzat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1616
https://github.com/broadinstitute/gatk/pull/1618:40,Performance,perform,performance,40,Minor changes to improve Smith-Waterman performance by ~1%.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1618
https://github.com/broadinstitute/gatk/pull/1620:30,Security,hash,hashCode,30,came up in recent profiling - hashCode computation on KMer was very inefficient and wasteful in String creation . @lbergelson can you review?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1620
https://github.com/broadinstitute/gatk/pull/1621:194,Testability,Test,Tests,194,This came up during profiling - containsVertex was creating full lists for vertices all the time only to check membership. Using a scan now (no caching but that's possible too if needed later). Tests for Path at 100% now. @lbergelson can you review?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1621
https://github.com/broadinstitute/gatk/pull/1625:37,Security,hash,hashcode,37,came up on profiling - computing the hashcode over and over is expensive to we precompute it. @lbergelson please review,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1625
https://github.com/broadinstitute/gatk/issues/1626:84,Availability,error,error,84,"I was running BaseRecalibrator with an --interval of GL00207.1:1+ and received this error:. A USER ERROR has occurred: The file GL000207.1:1+ does not exist. It seems like if the contig name has a '.' in it, it thinks it should be looking for a file. This worked for all other contigs that did not have a '.' in the name and it didn't complain about the Y or MT contigs that come before GL00207.1 in the full command:. org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator --useOriginalQualities true --knownSites /cromwell_root/broad-gitc-reference/hg19/Homo_sapiens_assembly19.dbsnp138.vcf --knownSites /cromwell_root/broad-gitc-reference/hg19/Mills_and_1000G_gold_standard.indels.b37.sites.vcf --knownSites /cromwell_root/broad-gitc-reference/hg19/Homo_sapiens_assembly19.known_indels_20120518.vcf --output NA12878.recal_data.csv --intervals Y:1+ --intervals MT:1+ --intervals GL000207.1:1+ --intervals GL000226.1:1+ --intervals GL000229.1:1+ --intervals GL000231.1:1+ --intervals GL000210.1:1+ --intervals GL000239.1:1+ --intervals GL000235.1:1+ --intervals GL000201.1:1+ --intervals GL000247.1:1+ --intervals GL000245.1:1+ --intervals GL000197.1:1+ --intervals GL000203.1:1+ --intervals GL000246.1:1+ --intervals GL000249.1:1+ --intervals GL000196.1:1+ --intervals GL000248.1:1+ --intervals GL000244.1:1+ --intervals GL000238.1:1+ --intervals GL000202.1:1+ --intervals GL000234.1:1+ --intervals GL000232.1:1+ --intervals GL000206.1:1+ --intervals GL000240.1:1+ --intervals GL000236.1:1+ --intervals GL000241.1:1+ --intervals GL000243.1:1+ --intervals GL000242.1:1+ --intervals GL000230.1:1+ --intervals GL000237.1:1+ --intervals GL000233.1:1+ --intervals GL000204.1:1+ --intervals GL000198.1:1+ --intervals GL000208.1:1+ --intervals GL000191.1:1+ --intervals GL000227.1:1+ --intervals GL000228.1:1+ --intervals GL000214.1:1+ --intervals GL000221.1:1+ --intervals GL000209.1:1+ --intervals GL000218.1:1+ --intervals GL000220.1:1+ --intervals GL000213.1:1+ --intervals GL000211.1:1+ --i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1626
https://github.com/broadinstitute/gatk/issues/1626:99,Availability,ERROR,ERROR,99,"I was running BaseRecalibrator with an --interval of GL00207.1:1+ and received this error:. A USER ERROR has occurred: The file GL000207.1:1+ does not exist. It seems like if the contig name has a '.' in it, it thinks it should be looking for a file. This worked for all other contigs that did not have a '.' in the name and it didn't complain about the Y or MT contigs that come before GL00207.1 in the full command:. org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator --useOriginalQualities true --knownSites /cromwell_root/broad-gitc-reference/hg19/Homo_sapiens_assembly19.dbsnp138.vcf --knownSites /cromwell_root/broad-gitc-reference/hg19/Mills_and_1000G_gold_standard.indels.b37.sites.vcf --knownSites /cromwell_root/broad-gitc-reference/hg19/Homo_sapiens_assembly19.known_indels_20120518.vcf --output NA12878.recal_data.csv --intervals Y:1+ --intervals MT:1+ --intervals GL000207.1:1+ --intervals GL000226.1:1+ --intervals GL000229.1:1+ --intervals GL000231.1:1+ --intervals GL000210.1:1+ --intervals GL000239.1:1+ --intervals GL000235.1:1+ --intervals GL000201.1:1+ --intervals GL000247.1:1+ --intervals GL000245.1:1+ --intervals GL000197.1:1+ --intervals GL000203.1:1+ --intervals GL000246.1:1+ --intervals GL000249.1:1+ --intervals GL000196.1:1+ --intervals GL000248.1:1+ --intervals GL000244.1:1+ --intervals GL000238.1:1+ --intervals GL000202.1:1+ --intervals GL000234.1:1+ --intervals GL000232.1:1+ --intervals GL000206.1:1+ --intervals GL000240.1:1+ --intervals GL000236.1:1+ --intervals GL000241.1:1+ --intervals GL000243.1:1+ --intervals GL000242.1:1+ --intervals GL000230.1:1+ --intervals GL000237.1:1+ --intervals GL000233.1:1+ --intervals GL000204.1:1+ --intervals GL000198.1:1+ --intervals GL000208.1:1+ --intervals GL000191.1:1+ --intervals GL000227.1:1+ --intervals GL000228.1:1+ --intervals GL000214.1:1+ --intervals GL000221.1:1+ --intervals GL000209.1:1+ --intervals GL000218.1:1+ --intervals GL000220.1:1+ --intervals GL000213.1:1+ --intervals GL000211.1:1+ --i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1626
https://github.com/broadinstitute/gatk/issues/1629:83,Performance,perform,performance,83,`SWPairwiseAlignment.calculateMatrix` comes up on profiles. We need to investigate performance improvements to it,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1629
https://github.com/broadinstitute/gatk/pull/1630:126,Testability,test,testing,126,"Our launch script gatk-launch now uses a bunch of system properties for things like compression, async reading, deflater. Our testing infrastructure must use the same settings. for @lbergelson",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1630
https://github.com/broadinstitute/gatk/issues/1632:122,Availability,error,errors,122,"Some tools have had usage examples ported from GATK3 that don't work in GATK4. We should fix ; these. . As well as fixing errors, it would be good to change the javadoc so it references parameters by the constant values instead of hardcoding them. (use `{@value StandardArgumentDefinitions#SOME_NAME}` ). These occur in at least the following tools, (found by `find in path -T`):; - [ ] ValidateVariants; - [ ] VariantFiltration; - [ ] AnalyzeCovariates; - [ ] BaseRecalibrator; - [ ] LeftAlignIndels",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1632
https://github.com/broadinstitute/gatk/issues/1632:387,Security,Validat,ValidateVariants,387,"Some tools have had usage examples ported from GATK3 that don't work in GATK4. We should fix ; these. . As well as fixing errors, it would be good to change the javadoc so it references parameters by the constant values instead of hardcoding them. (use `{@value StandardArgumentDefinitions#SOME_NAME}` ). These occur in at least the following tools, (found by `find in path -T`):; - [ ] ValidateVariants; - [ ] VariantFiltration; - [ ] AnalyzeCovariates; - [ ] BaseRecalibrator; - [ ] LeftAlignIndels",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1632
https://github.com/broadinstitute/gatk/pull/1633:162,Availability,error,error,162,making constants BQSR_TABLE_LONG_NAME and BQSR_TABLE_SHORT_NAME in StandardArgumentDefinitions; fixing outdated references to -BQSR -> -bqsr in documentation and error messages; fixes #1631,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1633
https://github.com/broadinstitute/gatk/pull/1633:168,Integrability,message,messages,168,making constants BQSR_TABLE_LONG_NAME and BQSR_TABLE_SHORT_NAME in StandardArgumentDefinitions; fixing outdated references to -BQSR -> -bqsr in documentation and error messages; fixes #1631,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1633
https://github.com/broadinstitute/gatk/pull/1637:16,Testability,test,test,16,document better test selection by gradle test filtering. @lbergelson please review,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1637
https://github.com/broadinstitute/gatk/pull/1637:41,Testability,test,test,41,document better test selection by gradle test filtering. @lbergelson please review,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1637
https://github.com/broadinstitute/gatk/issues/1638:631,Deployability,Integrat,IntegrationTestSpec,631,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:663,Deployability,Integrat,IntegrationTestSpec,663,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:738,Deployability,Integrat,IntegrationTestSpec,738,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:770,Deployability,Integrat,IntegrationTestSpec,770,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:845,Deployability,Integrat,IntegrationTestSpec,845,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:877,Deployability,Integrat,IntegrationTestSpec,877,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:8196,Deployability,Integrat,IntegrationTestSpec,8196,tractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:94); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:517); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:151); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:170); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:69); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:148); ... 49 more; Caused by: java.nio.channels.ClosedChannelException; at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:109); at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:252); at htsjdk.samtools.seekablestream.SeekableFileStream.position(SeekableFileStream.java:64); at htsjdk.tribble.TribbleIndexedFeatureReader$BlockStreamWrapper.read(TribbleIndexedFeatureReader.java:534); at java.io.InputStream.read(InputStream.java:101); at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:127); at htsjdk.tribble.readers.PositionalBufferedStream.read(PositionalBufferedStream.java:79); at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284); at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326); at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178); at java.io.InputStreamReader.read(InputStreamReader.java:184); at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBuffe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:8228,Deployability,Integrat,IntegrationTestSpec,8228,tractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:94); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:517); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:151); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:170); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:69); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:148); ... 49 more; Caused by: java.nio.channels.ClosedChannelException; at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:109); at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:252); at htsjdk.samtools.seekablestream.SeekableFileStream.position(SeekableFileStream.java:64); at htsjdk.tribble.TribbleIndexedFeatureReader$BlockStreamWrapper.read(TribbleIndexedFeatureReader.java:534); at java.io.InputStream.read(InputStream.java:101); at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:127); at htsjdk.tribble.readers.PositionalBufferedStream.read(PositionalBufferedStream.java:79); at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284); at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326); at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178); at java.io.InputStreamReader.read(InputStreamReader.java:184); at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBufferedReader.java:140); at ht,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:631,Integrability,Integrat,IntegrationTestSpec,631,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:663,Integrability,Integrat,IntegrationTestSpec,663,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:738,Integrability,Integrat,IntegrationTestSpec,738,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:770,Integrability,Integrat,IntegrationTestSpec,770,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:845,Integrability,Integrat,IntegrationTestSpec,845,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:877,Integrability,Integrat,IntegrationTestSpec,877,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:4112,Integrability,Message,MessageHub,4112,eflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at htsjdk.tribble.readers.AsynchronousLineReader.checkAndThrowIfWorkerException(AsynchronousLineReader.java:61); at htsjdk.tribble.readers.AsynchronousLineReader.readLine(AsynchronousLineReader.java:43); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); at htsjdk.samtools.util.AbstractIterator.hasNext(AbstractIterator.java:44); at htsjdk.tribble.AsciiFeatureCod,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:4135,Integrability,Message,MessageHub,4135,.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at htsjdk.tribble.readers.AsynchronousLineReader.checkAndThrowIfWorkerException(AsynchronousLineReader.java:61); at htsjdk.tribble.readers.AsynchronousLineReader.readLine(AsynchronousLineReader.java:43); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); at htsjdk.samtools.util.AbstractIterator.hasNext(AbstractIterator.java:44); at htsjdk.tribble.AsciiFeatureCodec.isDone(AsciiFe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:6969,Integrability,wrap,wrapAndCopyInto,6969,ext.getValues(FeatureContext.java:138); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration.areClusteredSNPs(VariantFiltration.java:369); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration.filter(VariantFiltration.java:300); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration.apply(VariantFiltration.java:265); at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$30(VariantWalker.java:96); at org.broadinstitute.hellbender.engine.VariantWalker$$Lambda$415/83249460.accept(Unknown Source); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:94); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:517); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:151); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:170); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:8196,Integrability,Integrat,IntegrationTestSpec,8196,tractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:94); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:517); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:151); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:170); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:69); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:148); ... 49 more; Caused by: java.nio.channels.ClosedChannelException; at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:109); at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:252); at htsjdk.samtools.seekablestream.SeekableFileStream.position(SeekableFileStream.java:64); at htsjdk.tribble.TribbleIndexedFeatureReader$BlockStreamWrapper.read(TribbleIndexedFeatureReader.java:534); at java.io.InputStream.read(InputStream.java:101); at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:127); at htsjdk.tribble.readers.PositionalBufferedStream.read(PositionalBufferedStream.java:79); at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284); at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326); at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178); at java.io.InputStreamReader.read(InputStreamReader.java:184); at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBuffe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:8228,Integrability,Integrat,IntegrationTestSpec,8228,tractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:94); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:517); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:151); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:170); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:69); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:148); ... 49 more; Caused by: java.nio.channels.ClosedChannelException; at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:109); at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:252); at htsjdk.samtools.seekablestream.SeekableFileStream.position(SeekableFileStream.java:64); at htsjdk.tribble.TribbleIndexedFeatureReader$BlockStreamWrapper.read(TribbleIndexedFeatureReader.java:534); at java.io.InputStream.read(InputStream.java:101); at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:127); at htsjdk.tribble.readers.PositionalBufferedStream.read(PositionalBufferedStream.java:79); at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284); at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326); at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178); at java.io.InputStreamReader.read(InputStreamReader.java:184); at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBufferedReader.java:140); at ht,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:4180,Performance,concurren,concurrent,4180,lectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at htsjdk.tribble.readers.AsynchronousLineReader.checkAndThrowIfWorkerException(AsynchronousLineReader.java:61); at htsjdk.tribble.readers.AsynchronousLineReader.readLine(AsynchronousLineReader.java:43); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); at htsjdk.samtools.util.AbstractIterator.hasNext(AbstractIterator.java:44); at htsjdk.tribble.AsciiFeatureCodec.isDone(AsciiFeatureCodec.java:53); at htsjdk.tribble.AsciiFea,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:4287,Performance,concurren,concurrent,4287,spatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at htsjdk.tribble.readers.AsynchronousLineReader.checkAndThrowIfWorkerException(AsynchronousLineReader.java:61); at htsjdk.tribble.readers.AsynchronousLineReader.readLine(AsynchronousLineReader.java:43); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); at htsjdk.samtools.util.AbstractIterator.hasNext(AbstractIterator.java:44); at htsjdk.tribble.AsciiFeatureCodec.isDone(AsciiFeatureCodec.java:53); at htsjdk.tribble.AsciiFeatureCodec.isDone(AsciiFeatureCodec.java:41); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.re,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:4371,Performance,concurren,concurrent,4371,h.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at htsjdk.tribble.readers.AsynchronousLineReader.checkAndThrowIfWorkerException(AsynchronousLineReader.java:61); at htsjdk.tribble.readers.AsynchronousLineReader.readLine(AsynchronousLineReader.java:43); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); at htsjdk.samtools.util.AbstractIterator.hasNext(AbstractIterator.java:44); at htsjdk.tribble.AsciiFeatureCodec.isDone(AsciiFeatureCodec.java:53); at htsjdk.tribble.AsciiFeatureCodec.isDone(AsciiFeatureCodec.java:41); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.readNextRecord(TribbleIndexedFeatureReader.java:447); at htsjdk.tribble.TribbleIndexed,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:4455,Performance,concurren,concurrent,4455,:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at htsjdk.tribble.readers.AsynchronousLineReader.checkAndThrowIfWorkerException(AsynchronousLineReader.java:61); at htsjdk.tribble.readers.AsynchronousLineReader.readLine(AsynchronousLineReader.java:43); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:24); at htsjdk.tribble.readers.LineIteratorImpl.advance(LineIteratorImpl.java:11); at htsjdk.samtools.util.AbstractIterator.hasNext(AbstractIterator.java:44); at htsjdk.tribble.AsciiFeatureCodec.isDone(AsciiFeatureCodec.java:53); at htsjdk.tribble.AsciiFeatureCodec.isDone(AsciiFeatureCodec.java:41); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.readNextRecord(TribbleIndexedFeatureReader.java:447); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.<init>(TribbleIndexedFeatureReader.java:390); at htsjdk.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:166,Security,expose,exposed,166,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:105,Testability,test,tests,105,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:124,Testability,test,test,124,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:212,Testability,log,logs,212,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:275,Testability,test,test,275,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:331,Testability,test,test,331,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:357,Testability,test,tests,357,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:461,Testability,test,testClusteredSnps,461,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:626,Testability,test,test,626,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:733,Testability,test,test,733,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:840,Testability,test,test,840,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:996,Testability,test,testClusteredSnps,996,ranch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:35,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1353,Testability,test,testng,1353,/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.ru,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1445,Testability,test,testng,1445,ionTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1508,Testability,test,testng,1508,: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runT,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1575,Testability,test,testng,1575,thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1644,Testability,test,testng,1644,pec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassPr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1660,Testability,Test,TestMethodWorker,1660,rationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1695,Testability,Test,TestMethodWorker,1695,at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.ta,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1730,Testability,test,testng,1730,test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.sto,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1746,Testability,Test,TestMethodWorker,1746,pec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProce,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1767,Testability,Test,TestMethodWorker,1767,egrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1802,Testability,test,testng,1802,rg.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorI,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1809,Testability,Test,TestRunner,1809,nstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invok,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1831,Testability,Test,TestRunner,1831,er.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1860,Testability,test,testng,1860,c.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAcc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1867,Testability,Test,TestRunner,1867,Test(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1882,Testability,Test,TestRunner,1882,ionTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(Nativ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1911,Testability,test,testng,1911,.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:1968,Testability,test,testng,1968,iltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(Deleg,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2033,Testability,test,testng,2033,rationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.inv,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2093,Testability,test,testng,2093,.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.Refle,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2146,Testability,test,testng,2146,ccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2215,Testability,test,testng,2215,DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(Reflectio,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2279,Testability,test,testng,2279,.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClas,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2286,Testability,Test,TestNG,2286,3); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoader,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2315,Testability,Test,TestNG,2315,eflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2341,Testability,test,testng,2341,; at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2348,Testability,Test,TestNG,2348,g.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2372,Testability,Test,TestNG,2372,l.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messagin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2398,Testability,test,testng,2398,hod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$Dis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2405,Testability,Test,TestNG,2405,hodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$Dispatchin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2416,Testability,Test,TestNG,2416,tionHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocati,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2468,Testability,test,testing,2468,okeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2476,Testability,test,testng,2476,od(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.pro,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2483,Testability,Test,TestNGTestClassProcessor,2483,639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2517,Testability,Test,TestNGTestClassProcessor,2517,nvoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2586,Testability,test,testing,2586,.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2594,Testability,test,testng,2594,r.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2601,Testability,Test,TestNGTestClassProcessor,2601,ods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2631,Testability,Test,TestNGTestClassProcessor,2631,rg.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.in,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:2699,Testability,test,testing,2699,odWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:3546,Testability,test,testing,3546,); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:3561,Testability,Test,TestWorker,3561,.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:3577,Testability,Test,TestWorker,3577,tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1638:8191,Testability,test,test,8191,il.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:94); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:517); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:151); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:170); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:69); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:148); ... 49 more; Caused by: java.nio.channels.ClosedChannelException; at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:109); at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:252); at htsjdk.samtools.seekablestream.SeekableFileStream.position(SeekableFileStream.java:64); at htsjdk.tribble.TribbleIndexedFeatureReader$BlockStreamWrapper.read(TribbleIndexedFeatureReader.java:534); at java.io.InputStream.read(InputStream.java:101); at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:127); at htsjdk.tribble.readers.PositionalBufferedStream.read(PositionalBufferedStream.java:79); at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284); at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326); at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178); at java.io.InputStreamReader.read(InputStreamReader.java:184); at htsjdk.tribble.readers.LongLineBufferedReader.fill(L,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638
https://github.com/broadinstitute/gatk/issues/1640:10,Security,validat,validated,10,Should be validated to the point where production would be willing to use it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1640
https://github.com/broadinstitute/gatk/issues/1642:22,Availability,down,downsampling,22,Let's use a different downsampling strategy than GATK3 for the `HaplotypeCaller` -- perhaps a `ReservoirDownsampler` whose size is proportional to the size of each region. Need to make sure that whatever strategy we use allows us to deal with high-coverage regions without introducing calling artifacts / missed calls.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1642
https://github.com/broadinstitute/gatk/issues/1644:11,Availability,down,downsample,11,"We want to downsample pathological regions at the Hadoop-BAM level, before paying the cost of loading all the reads into an RDD. Probably we want something like a `ReservoirDownsampler` that downsamples on a per-alignment-start basis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1644
https://github.com/broadinstitute/gatk/issues/1644:191,Availability,down,downsamples,191,"We want to downsample pathological regions at the Hadoop-BAM level, before paying the cost of loading all the reads into an RDD. Probably we want something like a `ReservoirDownsampler` that downsamples on a per-alignment-start basis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1644
https://github.com/broadinstitute/gatk/issues/1644:94,Performance,load,loading,94,"We want to downsample pathological regions at the Hadoop-BAM level, before paying the cost of loading all the reads into an RDD. Probably we want something like a `ReservoirDownsampler` that downsamples on a per-alignment-start basis.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1644
https://github.com/broadinstitute/gatk/issues/1652:128,Integrability,interface,interfaces,128,"Currently we have `GATKSparkTool` as a catch-all for all Spark tools. We should have `ReadSparkTool`, `VariantSparkTool`, etc., interfaces with well-defined APIs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1652
https://github.com/broadinstitute/gatk/issues/1653:197,Deployability,update,update,197,now that we're on htsjdk 2.2.1 we should switch to asyncIO for bams (not tribble); - [x] switch to asyncIO for bams (build.gradle); - [x] switch tests to use asyncIO for bams (build.gradle); - [x] update readme to say that we're using async IO; - [x] update startup message to clarify which IO is sync/async. measure and report performance impact on (using JdkDeflater and IntelDeflater); - [x] PrintReads ; - [x] BaseRecalibrator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1653
https://github.com/broadinstitute/gatk/issues/1653:251,Deployability,update,update,251,now that we're on htsjdk 2.2.1 we should switch to asyncIO for bams (not tribble); - [x] switch to asyncIO for bams (build.gradle); - [x] switch tests to use asyncIO for bams (build.gradle); - [x] update readme to say that we're using async IO; - [x] update startup message to clarify which IO is sync/async. measure and report performance impact on (using JdkDeflater and IntelDeflater); - [x] PrintReads ; - [x] BaseRecalibrator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1653
https://github.com/broadinstitute/gatk/issues/1653:266,Integrability,message,message,266,now that we're on htsjdk 2.2.1 we should switch to asyncIO for bams (not tribble); - [x] switch to asyncIO for bams (build.gradle); - [x] switch tests to use asyncIO for bams (build.gradle); - [x] update readme to say that we're using async IO; - [x] update startup message to clarify which IO is sync/async. measure and report performance impact on (using JdkDeflater and IntelDeflater); - [x] PrintReads ; - [x] BaseRecalibrator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1653
https://github.com/broadinstitute/gatk/issues/1653:328,Performance,perform,performance,328,now that we're on htsjdk 2.2.1 we should switch to asyncIO for bams (not tribble); - [x] switch to asyncIO for bams (build.gradle); - [x] switch tests to use asyncIO for bams (build.gradle); - [x] update readme to say that we're using async IO; - [x] update startup message to clarify which IO is sync/async. measure and report performance impact on (using JdkDeflater and IntelDeflater); - [x] PrintReads ; - [x] BaseRecalibrator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1653
https://github.com/broadinstitute/gatk/issues/1653:145,Testability,test,tests,145,now that we're on htsjdk 2.2.1 we should switch to asyncIO for bams (not tribble); - [x] switch to asyncIO for bams (build.gradle); - [x] switch tests to use asyncIO for bams (build.gradle); - [x] update readme to say that we're using async IO; - [x] update startup message to clarify which IO is sync/async. measure and report performance impact on (using JdkDeflater and IntelDeflater); - [x] PrintReads ; - [x] BaseRecalibrator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1653
https://github.com/broadinstitute/gatk/issues/1655:182,Safety,avoid,avoid,182,"In the HaplotypeCaller test, about 80% of the Smith-Waterman calls result in a substring match to the reference without any indels. Add a substring search to SWPairwiseAlignment and avoid running the full Smith-Waterman for these cases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1655
https://github.com/broadinstitute/gatk/issues/1655:23,Testability,test,test,23,"In the HaplotypeCaller test, about 80% of the Smith-Waterman calls result in a substring match to the reference without any indels. Add a substring search to SWPairwiseAlignment and avoid running the full Smith-Waterman for these cases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1655
https://github.com/broadinstitute/gatk/issues/1657:85,Energy Efficiency,reduce,reducers,85,"I'm running with a 35GB file (namesorted), 60 executors, 1 core each, 7 GB each, 277 reducers, and see unexpected issues; - [ ] GC times can go as 20% of execution time in stage2 (treeAggregate); - [ ] task duration goes up to 6.7min (median 1.5 min) - this seems too long; - [ ] peak execution memory can go up to 263.9 GB (!) (median 988.8 MB). this ticket is to track the analysis of those issues. my original run http://dataflow01.broadinstitute.org:18088/history/application_1458881872901_0078/stages/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1657
https://github.com/broadinstitute/gatk/issues/1660:239,Deployability,pipeline,pipeline,239,"`ReadsPipelineSpark` should override `GATKSparkTool.getRecommendedNumReducers()` to provide an appropriate default value for the `numReducers` argument. The default gives us one partition per 10MB of input, which may be too small for this pipeline (it was kept small on purpose due to memory issues, but perhaps these have been resolved?)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1660
https://github.com/broadinstitute/gatk/issues/1661:145,Availability,robust,robust,145,"Despite much development work in the past year to iron the bugs out of htsjdk's cram implementation, there is still a need for much better, more robust tests, as we keep running into new issues. In particular, we need roundtrip bam -> cram -> bam tests on non-trivially-sized inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1661
https://github.com/broadinstitute/gatk/issues/1661:152,Testability,test,tests,152,"Despite much development work in the past year to iron the bugs out of htsjdk's cram implementation, there is still a need for much better, more robust tests, as we keep running into new issues. In particular, we need roundtrip bam -> cram -> bam tests on non-trivially-sized inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1661
https://github.com/broadinstitute/gatk/issues/1661:247,Testability,test,tests,247,"Despite much development work in the past year to iron the bugs out of htsjdk's cram implementation, there is still a need for much better, more robust tests, as we keep running into new issues. In particular, we need roundtrip bam -> cram -> bam tests on non-trivially-sized inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1661
https://github.com/broadinstitute/gatk/pull/1664:190,Testability,test,test,190,this PR adds a hidden option to skip BAQ. This is used in the ongoing investigation of BAQ removal in https://github.com/broadinstitute/gatk/issues/1557. @droazen have a look. I could add a test but the recal table would need to be taken on faith.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1664
https://github.com/broadinstitute/gatk/issues/1666:70,Modifiability,variab,variables,70,We currently use a confusing mix of system properties and environment variables for settings not controlled by command-line arguments. We should choose one or the other.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1666
https://github.com/broadinstitute/gatk/pull/1667:140,Deployability,pipeline,pipelines,140,"Most of the CRAM tests were added before we could write CRAM on Spark, so this fills out the CRAM write tests for the remaining Spark tools/pipelines (ApplyBQSRSpark, BQSRPipelineSpark and ReadsPipelineSpark). Also includes one totally opportunistic deletion of an unused zero-length fasta file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1667
https://github.com/broadinstitute/gatk/pull/1667:17,Testability,test,tests,17,"Most of the CRAM tests were added before we could write CRAM on Spark, so this fills out the CRAM write tests for the remaining Spark tools/pipelines (ApplyBQSRSpark, BQSRPipelineSpark and ReadsPipelineSpark). Also includes one totally opportunistic deletion of an unused zero-length fasta file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1667
https://github.com/broadinstitute/gatk/pull/1667:104,Testability,test,tests,104,"Most of the CRAM tests were added before we could write CRAM on Spark, so this fills out the CRAM write tests for the remaining Spark tools/pipelines (ApplyBQSRSpark, BQSRPipelineSpark and ReadsPipelineSpark). Also includes one totally opportunistic deletion of an unused zero-length fasta file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1667
https://github.com/broadinstitute/gatk/issues/1669:491,Availability,Avail,Available,491,"right now you get this which is bogus on many levels (duplicated and confusing categories, confusing tool names etc). We need to put more order into this. @vdauwera can you help come up with a better scheme of how to organize tools?; Compare to the ADAM project (much much smaller scope of course but very clean UI: https://github.com/bigdatagenomics/adam). ```; /gatk-launch --list; Running:; /Users/akiezun/IdeaProjects/gatk/build/install/gatk/bin/gatk --help; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Copy Number Analysis: Tools to analyze copy number data.; CalculateTargetCoverage Count overlapping reads target by target. --------------------------------------------------------------------------------------; Fasta: Tools for analysis and manipulation of files in fasta format; CreateSequenceDictionary Creates a dict file from reference sequence in fasta format; NormalizeFasta Normalizes lines of sequence in a fasta file to be of the same length. --------------------------------------------------------------------------------------; Intervals: Tools for processing intervals and associated overlapping records; BedToIntervalList Converts a BED file to an Picard Interval List; ExampleIntervalWalker Print intervals with optional contextual data; IntervalListTools General tool for manipulating interval lists; LiftOverIntervalList Lifts over an interval list between genome builds. --------------------------------------------------------------------------------------; QC: Tools for Diagnostics and Quality Control; AnalyzeCovariates Tool to analyze and evaluate base recalibration tables for BQSR; CalculateHsMetrics Produces Hybrid Selection-specific metrics for a SAM/BAM file; CollectAlignmentSummaryMetrics Produces from a SAM/BAM/CRAM file containing summary alignment metrics; CollectBaseDistributionByCycle Produces metrics about nucleotide distribution per cycle in a SAM/BAM/CRAM fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669
https://github.com/broadinstitute/gatk/issues/1669:2472,Availability,error,error,2472,"for manipulating interval lists; LiftOverIntervalList Lifts over an interval list between genome builds. --------------------------------------------------------------------------------------; QC: Tools for Diagnostics and Quality Control; AnalyzeCovariates Tool to analyze and evaluate base recalibration tables for BQSR; CalculateHsMetrics Produces Hybrid Selection-specific metrics for a SAM/BAM file; CollectAlignmentSummaryMetrics Produces from a SAM/BAM/CRAM file containing summary alignment metrics; CollectBaseDistributionByCycle Produces metrics about nucleotide distribution per cycle in a SAM/BAM/CRAM file; CollectGcBiasMetrics Produces metrics about GC bias in the reads in the provided SAM/BAM file; CollectInsertSizeMetrics Produces metrics for insert size distribution for a SAM/BAM/CRAM file; CollectJumpingLibraryMetrics Produces jumping library metrics for the provided SAM/BAMs; CollectMultipleMetrics A ""meta-metrics"" calculating program that produces multiple metrics for the provided SAM/BAM file; CollectOxoGMetrics Produces metrics quantifying the CpCG -> CpCA error rate from the provided SAM/BAM file; CollectQualityYieldMetrics Produces metrics that quantify the quality and yield of sequence data from the provided SAM/BAM/CRAM file; CollectRnaSeqMetrics Produces RNA alignment metrics for a SAM/BAM file; CollectRrbsMetrics Produces metrics about bisulfite conversion for RRBS data; CollectSequencingArtifactMetrics Produces metrics to quantify single-base sequencing artifacts from a SAM/BAM file; CollectTargetedPcrMetrics Produces Targeted PCR-related metrics given the provided SAM/BAM; CollectWgsMetrics Produces metrics related to whole genome sequencing for a SAM/BAM file; MeanQualityByCycle Produces metrics for mean quality by cycle for a SAM/BAM/CRAM file; QualityScoreDistribution Produces metrics for quality score distributions for a SAM/BAM/CRAM file. --------------------------------------------------------------------------------------; SAM/BAM/CRAM: T",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669
https://github.com/broadinstitute/gatk/issues/1669:4270,Availability,Down,DownsampleSam,4270,"oups in a SAM/BAM/CRAM file with a single new read group; ApplyBQSR Applies the BQSR table to the input SAM/BAM/CRAM; BaseRecalibrator Generates recalibration table for BQSR; BuildBamIndex Generates a BAM index (.bai) file; CalculateReadGroupChecksum Creates a hash code based on the read groups (RG) in the SAM/BAM/CRAM header; CleanSam Cleans the provided SAM/BAM/CRAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads; ClipReads Clip reads in a SAM/BAM/CRAM file; CompareBaseQualities Compares base qualities of two input SAM/BAM/CRAM files; CompareSAMs Compares two input SAM/BAM/CRAM files; CountBases Count bases in a SAM/BAM/CRAM file; CountReads Count reads in a SAM/BAM/CRAM file; DownsampleSam Down-sample a SAM/BAM file to retain a random subset of the reads; EstimateLibraryComplexity Estimates library complexity from the sequence of read pairs; ExampleReadWalkerWithReference Print reads with reference context; ExampleReadWalkerWithVariants Print reads with overlapping variants; FastqToSam Converts a fastq file to an unaligned SAM/BAM file; FilterReads Creates a new SAM/BAM/CRAM file by including or excluding aligned reads; FixMateInformation Ensure that all mate-pair information is in sync between each read and its mate pair; FixMisencodedBaseQualityReads Fix Illumina base quality scores in a SAM/BAM/CRAM file; FlagStat A reimplementation of the 'samtools flagstat' subcommand; GatherBQSRReports Gathers scattered BQSR recalibration reports into a single file; GatherBamFiles Concatenates one or more BAM files together as efficiently as possible; LeftAlignIndels Left-aligns indels from reads in a SAM/BAM/CRAM file; MarkDuplicates Examines aligned records in the supplied SAM/BAM/CRAM file to locate duplicate molecules.; MergeBamAlignment Merges alignment data from a SAM/BAM with data in an unmapped SAM/BAM/CRAM file; MergeSamFiles Merges multiple SAM/BAM files into one file; PrintReads Print reads in the SAM/BAM/CRAM file; Reor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669
https://github.com/broadinstitute/gatk/issues/1669:4284,Availability,Down,Down-sample,4284,"oups in a SAM/BAM/CRAM file with a single new read group; ApplyBQSR Applies the BQSR table to the input SAM/BAM/CRAM; BaseRecalibrator Generates recalibration table for BQSR; BuildBamIndex Generates a BAM index (.bai) file; CalculateReadGroupChecksum Creates a hash code based on the read groups (RG) in the SAM/BAM/CRAM header; CleanSam Cleans the provided SAM/BAM/CRAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads; ClipReads Clip reads in a SAM/BAM/CRAM file; CompareBaseQualities Compares base qualities of two input SAM/BAM/CRAM files; CompareSAMs Compares two input SAM/BAM/CRAM files; CountBases Count bases in a SAM/BAM/CRAM file; CountReads Count reads in a SAM/BAM/CRAM file; DownsampleSam Down-sample a SAM/BAM file to retain a random subset of the reads; EstimateLibraryComplexity Estimates library complexity from the sequence of read pairs; ExampleReadWalkerWithReference Print reads with reference context; ExampleReadWalkerWithVariants Print reads with overlapping variants; FastqToSam Converts a fastq file to an unaligned SAM/BAM file; FilterReads Creates a new SAM/BAM/CRAM file by including or excluding aligned reads; FixMateInformation Ensure that all mate-pair information is in sync between each read and its mate pair; FixMisencodedBaseQualityReads Fix Illumina base quality scores in a SAM/BAM/CRAM file; FlagStat A reimplementation of the 'samtools flagstat' subcommand; GatherBQSRReports Gathers scattered BQSR recalibration reports into a single file; GatherBamFiles Concatenates one or more BAM files together as efficiently as possible; LeftAlignIndels Left-aligns indels from reads in a SAM/BAM/CRAM file; MarkDuplicates Examines aligned records in the supplied SAM/BAM/CRAM file to locate duplicate molecules.; MergeBamAlignment Merges alignment data from a SAM/BAM with data in an unmapped SAM/BAM/CRAM file; MergeSamFiles Merges multiple SAM/BAM files into one file; PrintReads Print reads in the SAM/BAM/CRAM file; Reor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669
https://github.com/broadinstitute/gatk/issues/1669:433,Deployability,install,install,433,"right now you get this which is bogus on many levels (duplicated and confusing categories, confusing tool names etc). We need to put more order into this. @vdauwera can you help come up with a better scheme of how to organize tools?; Compare to the ADAM project (much much smaller scope of course but very clean UI: https://github.com/bigdatagenomics/adam). ```; /gatk-launch --list; Running:; /Users/akiezun/IdeaProjects/gatk/build/install/gatk/bin/gatk --help; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Copy Number Analysis: Tools to analyze copy number data.; CalculateTargetCoverage Count overlapping reads target by target. --------------------------------------------------------------------------------------; Fasta: Tools for analysis and manipulation of files in fasta format; CreateSequenceDictionary Creates a dict file from reference sequence in fasta format; NormalizeFasta Normalizes lines of sequence in a fasta file to be of the same length. --------------------------------------------------------------------------------------; Intervals: Tools for processing intervals and associated overlapping records; BedToIntervalList Converts a BED file to an Picard Interval List; ExampleIntervalWalker Print intervals with optional contextual data; IntervalListTools General tool for manipulating interval lists; LiftOverIntervalList Lifts over an interval list between genome builds. --------------------------------------------------------------------------------------; QC: Tools for Diagnostics and Quality Control; AnalyzeCovariates Tool to analyze and evaluate base recalibration tables for BQSR; CalculateHsMetrics Produces Hybrid Selection-specific metrics for a SAM/BAM file; CollectAlignmentSummaryMetrics Produces from a SAM/BAM/CRAM file containing summary alignment metrics; CollectBaseDistributionByCycle Produces metrics about nucleotide distribution per cycle in a SAM/BAM/CRAM fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669
https://github.com/broadinstitute/gatk/issues/1669:6713,Deployability,pipeline,pipelines,6713,"dMateCigar Reverts the original base qualities and adds the mate cigar tag to read-group BAMs; RevertSam Reverts SAM/BAM files to a previous state; SamFormatConverter Convert a SAM/BAM/CRAM file to a SAM/BAM/CRAM file; SamToFastq Converts a SAM/BAM file into a FASTQ; SortSam Sorts a SAM/BAM/CRAM file; SplitNCigarReads Split Reads with N in Cigar; SplitReads Outputs reads from a SAM/BAM/CRAM by read group, sample and library name; UnmarkDuplicates Unmark duplicates in a SAM/BAM/CRAM file; ValidateSamFile Validates a SAM/BAM/CRAM file. --------------------------------------------------------------------------------------; Spark Validation tools: Tools written in Spark to compare aspects of two different files; CompareBaseQualitiesSpark Diff qs of the BAMs; CompareDuplicatesSpark Compares two BAMs for duplicates. --------------------------------------------------------------------------------------; Spark pipelines: Pipelines that combine tools and use Apache Spark for scaling out (experimental); BQSRPipelineSpark Both steps of BQSR (BaseRecalibrator and ApplyBQSR) on Spark; ReadsPipelineSpark Takes aligned reads (likely from BWA) and runs MarkDuplicates and BQSR. The final result is analysis-ready reads. --------------------------------------------------------------------------------------; Spark tools: Tools that use Apache Spark for scaling out (experimental); ApplyBQSRSpark ApplyBQSR on Spark; BaseRecalibratorSpark BaseRecalibrator on Spark; BaseRecalibratorSparkSharded BaseRecalibrator on Spark (experimental sharded implementation); CollectBaseDistributionByCycleSpark CollectBaseDistributionByCycle on Spark; CollectQualityYieldMetricsSpark CollectQualityYieldMetrics on Spark; CountBasesSpark CountBases on Spark; CountReadsSpark CountReads on Spark; CountVariantsSpark CountVariants on Spark; CreateHadoopBamSplittingIndex create a hadoop-bam splitting index; FindBadGenomicKmersSpark find ref kmers with high copy number; FindSVBreakpointsSpark Produce small FASTQs of",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669
https://github.com/broadinstitute/gatk/issues/1669:6724,Deployability,Pipeline,Pipelines,6724,"dMateCigar Reverts the original base qualities and adds the mate cigar tag to read-group BAMs; RevertSam Reverts SAM/BAM files to a previous state; SamFormatConverter Convert a SAM/BAM/CRAM file to a SAM/BAM/CRAM file; SamToFastq Converts a SAM/BAM file into a FASTQ; SortSam Sorts a SAM/BAM/CRAM file; SplitNCigarReads Split Reads with N in Cigar; SplitReads Outputs reads from a SAM/BAM/CRAM by read group, sample and library name; UnmarkDuplicates Unmark duplicates in a SAM/BAM/CRAM file; ValidateSamFile Validates a SAM/BAM/CRAM file. --------------------------------------------------------------------------------------; Spark Validation tools: Tools written in Spark to compare aspects of two different files; CompareBaseQualitiesSpark Diff qs of the BAMs; CompareDuplicatesSpark Compares two BAMs for duplicates. --------------------------------------------------------------------------------------; Spark pipelines: Pipelines that combine tools and use Apache Spark for scaling out (experimental); BQSRPipelineSpark Both steps of BQSR (BaseRecalibrator and ApplyBQSR) on Spark; ReadsPipelineSpark Takes aligned reads (likely from BWA) and runs MarkDuplicates and BQSR. The final result is analysis-ready reads. --------------------------------------------------------------------------------------; Spark tools: Tools that use Apache Spark for scaling out (experimental); ApplyBQSRSpark ApplyBQSR on Spark; BaseRecalibratorSpark BaseRecalibrator on Spark; BaseRecalibratorSparkSharded BaseRecalibrator on Spark (experimental sharded implementation); CollectBaseDistributionByCycleSpark CollectBaseDistributionByCycle on Spark; CollectQualityYieldMetricsSpark CollectQualityYieldMetrics on Spark; CountBasesSpark CountBases on Spark; CountReadsSpark CountReads on Spark; CountVariantsSpark CountVariants on Spark; CreateHadoopBamSplittingIndex create a hadoop-bam splitting index; FindBadGenomicKmersSpark find ref kmers with high copy number; FindSVBreakpointsSpark Produce small FASTQs of",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669
https://github.com/broadinstitute/gatk/issues/1669:5127,Energy Efficiency,efficient,efficiently,5127,"oups in a SAM/BAM/CRAM file with a single new read group; ApplyBQSR Applies the BQSR table to the input SAM/BAM/CRAM; BaseRecalibrator Generates recalibration table for BQSR; BuildBamIndex Generates a BAM index (.bai) file; CalculateReadGroupChecksum Creates a hash code based on the read groups (RG) in the SAM/BAM/CRAM header; CleanSam Cleans the provided SAM/BAM/CRAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads; ClipReads Clip reads in a SAM/BAM/CRAM file; CompareBaseQualities Compares base qualities of two input SAM/BAM/CRAM files; CompareSAMs Compares two input SAM/BAM/CRAM files; CountBases Count bases in a SAM/BAM/CRAM file; CountReads Count reads in a SAM/BAM/CRAM file; DownsampleSam Down-sample a SAM/BAM file to retain a random subset of the reads; EstimateLibraryComplexity Estimates library complexity from the sequence of read pairs; ExampleReadWalkerWithReference Print reads with reference context; ExampleReadWalkerWithVariants Print reads with overlapping variants; FastqToSam Converts a fastq file to an unaligned SAM/BAM file; FilterReads Creates a new SAM/BAM/CRAM file by including or excluding aligned reads; FixMateInformation Ensure that all mate-pair information is in sync between each read and its mate pair; FixMisencodedBaseQualityReads Fix Illumina base quality scores in a SAM/BAM/CRAM file; FlagStat A reimplementation of the 'samtools flagstat' subcommand; GatherBQSRReports Gathers scattered BQSR recalibration reports into a single file; GatherBamFiles Concatenates one or more BAM files together as efficiently as possible; LeftAlignIndels Left-aligns indels from reads in a SAM/BAM/CRAM file; MarkDuplicates Examines aligned records in the supplied SAM/BAM/CRAM file to locate duplicate molecules.; MergeBamAlignment Merges alignment data from a SAM/BAM with data in an unmapped SAM/BAM/CRAM file; MergeSamFiles Merges multiple SAM/BAM files into one file; PrintReads Print reads in the SAM/BAM/CRAM file; Reor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669
https://github.com/broadinstitute/gatk/issues/1669:3800,Security,hash,hash,3800,"oups in a SAM/BAM/CRAM file with a single new read group; ApplyBQSR Applies the BQSR table to the input SAM/BAM/CRAM; BaseRecalibrator Generates recalibration table for BQSR; BuildBamIndex Generates a BAM index (.bai) file; CalculateReadGroupChecksum Creates a hash code based on the read groups (RG) in the SAM/BAM/CRAM header; CleanSam Cleans the provided SAM/BAM/CRAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads; ClipReads Clip reads in a SAM/BAM/CRAM file; CompareBaseQualities Compares base qualities of two input SAM/BAM/CRAM files; CompareSAMs Compares two input SAM/BAM/CRAM files; CountBases Count bases in a SAM/BAM/CRAM file; CountReads Count reads in a SAM/BAM/CRAM file; DownsampleSam Down-sample a SAM/BAM file to retain a random subset of the reads; EstimateLibraryComplexity Estimates library complexity from the sequence of read pairs; ExampleReadWalkerWithReference Print reads with reference context; ExampleReadWalkerWithVariants Print reads with overlapping variants; FastqToSam Converts a fastq file to an unaligned SAM/BAM file; FilterReads Creates a new SAM/BAM/CRAM file by including or excluding aligned reads; FixMateInformation Ensure that all mate-pair information is in sync between each read and its mate pair; FixMisencodedBaseQualityReads Fix Illumina base quality scores in a SAM/BAM/CRAM file; FlagStat A reimplementation of the 'samtools flagstat' subcommand; GatherBQSRReports Gathers scattered BQSR recalibration reports into a single file; GatherBamFiles Concatenates one or more BAM files together as efficiently as possible; LeftAlignIndels Left-aligns indels from reads in a SAM/BAM/CRAM file; MarkDuplicates Examines aligned records in the supplied SAM/BAM/CRAM file to locate duplicate molecules.; MergeBamAlignment Merges alignment data from a SAM/BAM with data in an unmapped SAM/BAM/CRAM file; MergeSamFiles Merges multiple SAM/BAM files into one file; PrintReads Print reads in the SAM/BAM/CRAM file; Reor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669
https://github.com/broadinstitute/gatk/issues/1669:6290,Security,Validat,ValidateSamFile,6290,"MisencodedBaseQualityReads Fix Illumina base quality scores in a SAM/BAM/CRAM file; FlagStat A reimplementation of the 'samtools flagstat' subcommand; GatherBQSRReports Gathers scattered BQSR recalibration reports into a single file; GatherBamFiles Concatenates one or more BAM files together as efficiently as possible; LeftAlignIndels Left-aligns indels from reads in a SAM/BAM/CRAM file; MarkDuplicates Examines aligned records in the supplied SAM/BAM/CRAM file to locate duplicate molecules.; MergeBamAlignment Merges alignment data from a SAM/BAM with data in an unmapped SAM/BAM/CRAM file; MergeSamFiles Merges multiple SAM/BAM files into one file; PrintReads Print reads in the SAM/BAM/CRAM file; ReorderSam Reorders reads in a SAM/BAM file to match ordering in reference; ReplaceSamHeader Replace the SAMFileHeader in a SAM/BAM file with the given header; RevertBaseQualityScores Revert Quality Scores in a SAM/BAM/CRAM file; RevertOriginalBaseQualitiesAndAddMateCigar Reverts the original base qualities and adds the mate cigar tag to read-group BAMs; RevertSam Reverts SAM/BAM files to a previous state; SamFormatConverter Convert a SAM/BAM/CRAM file to a SAM/BAM/CRAM file; SamToFastq Converts a SAM/BAM file into a FASTQ; SortSam Sorts a SAM/BAM/CRAM file; SplitNCigarReads Split Reads with N in Cigar; SplitReads Outputs reads from a SAM/BAM/CRAM by read group, sample and library name; UnmarkDuplicates Unmark duplicates in a SAM/BAM/CRAM file; ValidateSamFile Validates a SAM/BAM/CRAM file. --------------------------------------------------------------------------------------; Spark Validation tools: Tools written in Spark to compare aspects of two different files; CompareBaseQualitiesSpark Diff qs of the BAMs; CompareDuplicatesSpark Compares two BAMs for duplicates. --------------------------------------------------------------------------------------; Spark pipelines: Pipelines that combine tools and use Apache Spark for scaling out (experimental); BQSRPipelineSpark Both st",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669
https://github.com/broadinstitute/gatk/issues/1669:6306,Security,Validat,Validates,6306,"MisencodedBaseQualityReads Fix Illumina base quality scores in a SAM/BAM/CRAM file; FlagStat A reimplementation of the 'samtools flagstat' subcommand; GatherBQSRReports Gathers scattered BQSR recalibration reports into a single file; GatherBamFiles Concatenates one or more BAM files together as efficiently as possible; LeftAlignIndels Left-aligns indels from reads in a SAM/BAM/CRAM file; MarkDuplicates Examines aligned records in the supplied SAM/BAM/CRAM file to locate duplicate molecules.; MergeBamAlignment Merges alignment data from a SAM/BAM with data in an unmapped SAM/BAM/CRAM file; MergeSamFiles Merges multiple SAM/BAM files into one file; PrintReads Print reads in the SAM/BAM/CRAM file; ReorderSam Reorders reads in a SAM/BAM file to match ordering in reference; ReplaceSamHeader Replace the SAMFileHeader in a SAM/BAM file with the given header; RevertBaseQualityScores Revert Quality Scores in a SAM/BAM/CRAM file; RevertOriginalBaseQualitiesAndAddMateCigar Reverts the original base qualities and adds the mate cigar tag to read-group BAMs; RevertSam Reverts SAM/BAM files to a previous state; SamFormatConverter Convert a SAM/BAM/CRAM file to a SAM/BAM/CRAM file; SamToFastq Converts a SAM/BAM file into a FASTQ; SortSam Sorts a SAM/BAM/CRAM file; SplitNCigarReads Split Reads with N in Cigar; SplitReads Outputs reads from a SAM/BAM/CRAM by read group, sample and library name; UnmarkDuplicates Unmark duplicates in a SAM/BAM/CRAM file; ValidateSamFile Validates a SAM/BAM/CRAM file. --------------------------------------------------------------------------------------; Spark Validation tools: Tools written in Spark to compare aspects of two different files; CompareBaseQualitiesSpark Diff qs of the BAMs; CompareDuplicatesSpark Compares two BAMs for duplicates. --------------------------------------------------------------------------------------; Spark pipelines: Pipelines that combine tools and use Apache Spark for scaling out (experimental); BQSRPipelineSpark Both st",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669
https://github.com/broadinstitute/gatk/issues/1669:6431,Security,Validat,Validation,6431,"ne file; PrintReads Print reads in the SAM/BAM/CRAM file; ReorderSam Reorders reads in a SAM/BAM file to match ordering in reference; ReplaceSamHeader Replace the SAMFileHeader in a SAM/BAM file with the given header; RevertBaseQualityScores Revert Quality Scores in a SAM/BAM/CRAM file; RevertOriginalBaseQualitiesAndAddMateCigar Reverts the original base qualities and adds the mate cigar tag to read-group BAMs; RevertSam Reverts SAM/BAM files to a previous state; SamFormatConverter Convert a SAM/BAM/CRAM file to a SAM/BAM/CRAM file; SamToFastq Converts a SAM/BAM file into a FASTQ; SortSam Sorts a SAM/BAM/CRAM file; SplitNCigarReads Split Reads with N in Cigar; SplitReads Outputs reads from a SAM/BAM/CRAM by read group, sample and library name; UnmarkDuplicates Unmark duplicates in a SAM/BAM/CRAM file; ValidateSamFile Validates a SAM/BAM/CRAM file. --------------------------------------------------------------------------------------; Spark Validation tools: Tools written in Spark to compare aspects of two different files; CompareBaseQualitiesSpark Diff qs of the BAMs; CompareDuplicatesSpark Compares two BAMs for duplicates. --------------------------------------------------------------------------------------; Spark pipelines: Pipelines that combine tools and use Apache Spark for scaling out (experimental); BQSRPipelineSpark Both steps of BQSR (BaseRecalibrator and ApplyBQSR) on Spark; ReadsPipelineSpark Takes aligned reads (likely from BWA) and runs MarkDuplicates and BQSR. The final result is analysis-ready reads. --------------------------------------------------------------------------------------; Spark tools: Tools that use Apache Spark for scaling out (experimental); ApplyBQSRSpark ApplyBQSR on Spark; BaseRecalibratorSpark BaseRecalibrator on Spark; BaseRecalibratorSparkSharded BaseRecalibrator on Spark (experimental sharded implementation); CollectBaseDistributionByCycleSpark CollectBaseDistributionByCycle on Spark; CollectQualityYieldMetricsSpark CollectQua",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669
https://github.com/broadinstitute/gatk/issues/1669:9457,Security,Validat,ValidateVariants,9457,Qs of reads sharing kmers with putative SV breakpoints for local assembly; FlagStatSpark FlagStat on Spark; MarkDuplicatesSpark MarkDuplicates on Spark; MeanQualityByCycleSpark MeanQualityByCycle on Spark; PrintReadsSpark PrintReads on Spark; QualityScoreDistributionSpark QualityScoreDistribution on Spark; SortReadFileSpark SortSam on Spark (works on SAM/BAM/CRAM). --------------------------------------------------------------------------------------; Spark tools for structural variation analysis: Structural variation analysis tools that use Apache Spark for scaling out (experimental); CollectInsertSizeMetricsSpark Collect Insert Size Distribution on Spark. --------------------------------------------------------------------------------------; VCF: Tools for manipulating variants and associated metadata; CountVariants Count variants in a VCF file; ExampleVariantWalker Example tool that prints variants with optional contextual data; FilterVcf Hard-filters a VCF file; GatherVcfs Gathers multiple VCF files from a scatter operation into a single VCF file; GenotypeConcordance Calculates the concordance between genotype data for two samples in two different VCFs; IndexFeatureFile Creates indices for Feature-containing files (eg VCF and BED files); LiftOverVcf Lifts a VCF between genome builds; MakeSitesOnlyVcf Creates a VCF bereft of genotype information from an input VCF; MergeVcfs Merges multiple VCF files into one VCF file; RenameSampleInVcf Rename a sample within a VCF; SelectVariants Select a subset of variants from a larger callset in a VCF file; SortVcf Sorts one or more VCF files; SplitVcfs Splits an input VCF file into two VCF files; ValidateVariants Validate VCF; VariantFiltration Hard-filter variants VCF (mark them as FILTER); VariantsToTable Extract specific fields from a VCF file to a tab-delimited table; VcfToIntervalList Converts a VCF file to a Picard Interval List. --------------------------------------------------------------------------------------; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669
https://github.com/broadinstitute/gatk/issues/1669:9474,Security,Validat,Validate,9474,Qs of reads sharing kmers with putative SV breakpoints for local assembly; FlagStatSpark FlagStat on Spark; MarkDuplicatesSpark MarkDuplicates on Spark; MeanQualityByCycleSpark MeanQualityByCycle on Spark; PrintReadsSpark PrintReads on Spark; QualityScoreDistributionSpark QualityScoreDistribution on Spark; SortReadFileSpark SortSam on Spark (works on SAM/BAM/CRAM). --------------------------------------------------------------------------------------; Spark tools for structural variation analysis: Structural variation analysis tools that use Apache Spark for scaling out (experimental); CollectInsertSizeMetricsSpark Collect Insert Size Distribution on Spark. --------------------------------------------------------------------------------------; VCF: Tools for manipulating variants and associated metadata; CountVariants Count variants in a VCF file; ExampleVariantWalker Example tool that prints variants with optional contextual data; FilterVcf Hard-filters a VCF file; GatherVcfs Gathers multiple VCF files from a scatter operation into a single VCF file; GenotypeConcordance Calculates the concordance between genotype data for two samples in two different VCFs; IndexFeatureFile Creates indices for Feature-containing files (eg VCF and BED files); LiftOverVcf Lifts a VCF between genome builds; MakeSitesOnlyVcf Creates a VCF bereft of genotype information from an input VCF; MergeVcfs Merges multiple VCF files into one VCF file; RenameSampleInVcf Rename a sample within a VCF; SelectVariants Select a subset of variants from a larger callset in a VCF file; SortVcf Sorts one or more VCF files; SplitVcfs Splits an input VCF file into two VCF files; ValidateVariants Validate VCF; VariantFiltration Hard-filter variants VCF (mark them as FILTER); VariantsToTable Extract specific fields from a VCF file to a tab-delimited table; VcfToIntervalList Converts a VCF file to a Picard Interval List. --------------------------------------------------------------------------------------; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669
https://github.com/broadinstitute/gatk/issues/1671:257,Testability,test,tests,257,"`-L unmapped` support was enabled in https://github.com/broadinstitute/gatk/pull/1670, but `-XL unmapped` was left disabled. Enabling it should require only minor code changes to `IntervalUtils` and `IntervalArgumentCollection`, but will require heavy-duty tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1671
https://github.com/broadinstitute/gatk/issues/1674:565,Testability,test,testing,565,"ReadsDataSource uses the headers for individual bams when instantiating the individual iterators for each bam, but uses the merged header everywhere else when multiple bams are present. We need to verify that there are no issues/bugs related to contig indices changing in the merged header. There shouldn't be, since we use contig names everywhere except `SamReaderQueryingIterator` (where you definitely want the contig index to match the index from the bam file in question), but it doesn't hurt to spend a little time (no more than 1-2 days) double-checking and testing the case of reading multiple bams where the contig indices in the final merged header differ from the indices in the original files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1674
https://github.com/broadinstitute/gatk/issues/1675:166,Performance,optimiz,optimizations,166,"We need to prove that the broadcast of the reference and variants takes up a non-trivial fraction of the runtime of the Spark BQSR before it's worthwhile to consider optimizations/extra complexity like https://github.com/broadinstitute/gatk/issues/1407. We also need to understand:. -how the cost of the broadcasts scale as you add more nodes; -how the cost of the broadcasts scale as vcf file size increases. Should not be too difficult to determine this, and it seems like a necessary prerequisite to things like https://github.com/broadinstitute/gatk/issues/1407",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1675
https://github.com/broadinstitute/gatk/pull/1677:244,Integrability,depend,dependent,244,"#1629 @akiezun @droazen @lbergelson. Added a substring search to `SWPairwiseAlignment.align`to avoid running the full Smith-Waterman when the query is found in the reference without any indels. The performance benefit of this code will be data dependent. In the current HaplotypeCaller test, >80% of the Smith-Waterman calls are filtered by the substring search. Added tests to cover all of the overhang strategies. **Note:** The substring search only works for the `SOFTCLIP`and `IGNORE`overhang strategies. The `INDEL`and `LEADING_INDEL`can result in more complicated CIGAR strings. See the `SWPairwiseAlignmentUnitTest.testSubstringMatchIndelLong` and `SWPairwiseAlignmentUnitTest.testSubstringMatchLeadingIndelLong` tests for examples.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1677
https://github.com/broadinstitute/gatk/pull/1677:198,Performance,perform,performance,198,"#1629 @akiezun @droazen @lbergelson. Added a substring search to `SWPairwiseAlignment.align`to avoid running the full Smith-Waterman when the query is found in the reference without any indels. The performance benefit of this code will be data dependent. In the current HaplotypeCaller test, >80% of the Smith-Waterman calls are filtered by the substring search. Added tests to cover all of the overhang strategies. **Note:** The substring search only works for the `SOFTCLIP`and `IGNORE`overhang strategies. The `INDEL`and `LEADING_INDEL`can result in more complicated CIGAR strings. See the `SWPairwiseAlignmentUnitTest.testSubstringMatchIndelLong` and `SWPairwiseAlignmentUnitTest.testSubstringMatchLeadingIndelLong` tests for examples.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1677
https://github.com/broadinstitute/gatk/pull/1677:95,Safety,avoid,avoid,95,"#1629 @akiezun @droazen @lbergelson. Added a substring search to `SWPairwiseAlignment.align`to avoid running the full Smith-Waterman when the query is found in the reference without any indels. The performance benefit of this code will be data dependent. In the current HaplotypeCaller test, >80% of the Smith-Waterman calls are filtered by the substring search. Added tests to cover all of the overhang strategies. **Note:** The substring search only works for the `SOFTCLIP`and `IGNORE`overhang strategies. The `INDEL`and `LEADING_INDEL`can result in more complicated CIGAR strings. See the `SWPairwiseAlignmentUnitTest.testSubstringMatchIndelLong` and `SWPairwiseAlignmentUnitTest.testSubstringMatchLeadingIndelLong` tests for examples.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1677
https://github.com/broadinstitute/gatk/pull/1677:286,Testability,test,test,286,"#1629 @akiezun @droazen @lbergelson. Added a substring search to `SWPairwiseAlignment.align`to avoid running the full Smith-Waterman when the query is found in the reference without any indels. The performance benefit of this code will be data dependent. In the current HaplotypeCaller test, >80% of the Smith-Waterman calls are filtered by the substring search. Added tests to cover all of the overhang strategies. **Note:** The substring search only works for the `SOFTCLIP`and `IGNORE`overhang strategies. The `INDEL`and `LEADING_INDEL`can result in more complicated CIGAR strings. See the `SWPairwiseAlignmentUnitTest.testSubstringMatchIndelLong` and `SWPairwiseAlignmentUnitTest.testSubstringMatchLeadingIndelLong` tests for examples.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1677
https://github.com/broadinstitute/gatk/pull/1677:369,Testability,test,tests,369,"#1629 @akiezun @droazen @lbergelson. Added a substring search to `SWPairwiseAlignment.align`to avoid running the full Smith-Waterman when the query is found in the reference without any indels. The performance benefit of this code will be data dependent. In the current HaplotypeCaller test, >80% of the Smith-Waterman calls are filtered by the substring search. Added tests to cover all of the overhang strategies. **Note:** The substring search only works for the `SOFTCLIP`and `IGNORE`overhang strategies. The `INDEL`and `LEADING_INDEL`can result in more complicated CIGAR strings. See the `SWPairwiseAlignmentUnitTest.testSubstringMatchIndelLong` and `SWPairwiseAlignmentUnitTest.testSubstringMatchLeadingIndelLong` tests for examples.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1677
https://github.com/broadinstitute/gatk/pull/1677:622,Testability,test,testSubstringMatchIndelLong,622,"#1629 @akiezun @droazen @lbergelson. Added a substring search to `SWPairwiseAlignment.align`to avoid running the full Smith-Waterman when the query is found in the reference without any indels. The performance benefit of this code will be data dependent. In the current HaplotypeCaller test, >80% of the Smith-Waterman calls are filtered by the substring search. Added tests to cover all of the overhang strategies. **Note:** The substring search only works for the `SOFTCLIP`and `IGNORE`overhang strategies. The `INDEL`and `LEADING_INDEL`can result in more complicated CIGAR strings. See the `SWPairwiseAlignmentUnitTest.testSubstringMatchIndelLong` and `SWPairwiseAlignmentUnitTest.testSubstringMatchLeadingIndelLong` tests for examples.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1677
https://github.com/broadinstitute/gatk/pull/1677:684,Testability,test,testSubstringMatchLeadingIndelLong,684,"#1629 @akiezun @droazen @lbergelson. Added a substring search to `SWPairwiseAlignment.align`to avoid running the full Smith-Waterman when the query is found in the reference without any indels. The performance benefit of this code will be data dependent. In the current HaplotypeCaller test, >80% of the Smith-Waterman calls are filtered by the substring search. Added tests to cover all of the overhang strategies. **Note:** The substring search only works for the `SOFTCLIP`and `IGNORE`overhang strategies. The `INDEL`and `LEADING_INDEL`can result in more complicated CIGAR strings. See the `SWPairwiseAlignmentUnitTest.testSubstringMatchIndelLong` and `SWPairwiseAlignmentUnitTest.testSubstringMatchLeadingIndelLong` tests for examples.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1677
https://github.com/broadinstitute/gatk/pull/1677:720,Testability,test,tests,720,"#1629 @akiezun @droazen @lbergelson. Added a substring search to `SWPairwiseAlignment.align`to avoid running the full Smith-Waterman when the query is found in the reference without any indels. The performance benefit of this code will be data dependent. In the current HaplotypeCaller test, >80% of the Smith-Waterman calls are filtered by the substring search. Added tests to cover all of the overhang strategies. **Note:** The substring search only works for the `SOFTCLIP`and `IGNORE`overhang strategies. The `INDEL`and `LEADING_INDEL`can result in more complicated CIGAR strings. See the `SWPairwiseAlignmentUnitTest.testSubstringMatchIndelLong` and `SWPairwiseAlignmentUnitTest.testSubstringMatchLeadingIndelLong` tests for examples.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1677
https://github.com/broadinstitute/gatk/pull/1681:175,Availability,down,download,175,"previously avx code was sometimes included if installDist had been run prior to running sparkJar, now sparkJar will always contain the native code; fixes #1576. also changing download of inteldeflator.so to only happen if it doesn't exist already",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1681
https://github.com/broadinstitute/gatk/pull/1681:46,Deployability,install,installDist,46,"previously avx code was sometimes included if installDist had been run prior to running sparkJar, now sparkJar will always contain the native code; fixes #1576. also changing download of inteldeflator.so to only happen if it doesn't exist already",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1681
https://github.com/broadinstitute/gatk/issues/1683:31,Safety,detect,detection,31,"Can port the index-out-of-date detection code from GATK3. Unlike GATK3, however, we should not attempt to lock and/or re-generate out-of-date or missing indices on input files. Requested by @eitanbanks",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1683
https://github.com/broadinstitute/gatk/issues/1684:340,Performance,optimiz,optimizations,340,"In `InfiniteRandomMatingPopulationModel` the methods `singleSampleLikelihoods`, `multiSampleHeterogeneousPloidyModelLikelihoods`, and `multiSampleHomogeneousPloidyModelLikelihoods` do almost exactly the same thing with almost identical code. It does not appear that the assumptions of a single sample or homogeneous ploidy are used for any optimizations, unless you count not repeating the ploidy getter. @lbergelson am I missing something?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1684
https://github.com/broadinstitute/gatk/issues/1686:41,Usability,simpl,simpler,41,"`InfiniteRandomMatingPopulationModel` is simpler than its name suggests. The _only_ assumption it makes is that samples have independent genotypes -- after all, the only thing it really does is delegate the calculation, independently for each sample, to `GenotypeLikelihoodCalculator`. Any further assumptions are built into `GenotypeLikelihoodCalculator`. In particular, it is the latter class that assumes Hardy-Weinberg equilibrium (independent alleles). Does anyone else like @vruano or @ldgauthier think that `InfiniteRandomMatingPopulationModel` should be renamed something like `IndependentSamplesModel` and the comment: `And genotypes should exhibit the ratios expected under HWE` moved to `GenotypeLikelihoodCalculator`?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1686
https://github.com/broadinstitute/gatk/issues/1688:310,Energy Efficiency,efficient,efficient,310,"GATK3 is very slow when processing references with large numbers of contigs, such as draft genomes. In the past this mostly affected microbial genomes so we didn't do anything about it, but now the Hg38 has a lot more contigs so we have to make sure that's not going to be a problem with GATK4. . To be clear, efficient processing of reference genomes with thousands of contigs is a must-have. . Efficient processing of e.g. microbial draft genomes with tens of thousands of contigs is a nice-to-have. More than that is just crazy talk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1688
https://github.com/broadinstitute/gatk/issues/1688:396,Energy Efficiency,Efficient,Efficient,396,"GATK3 is very slow when processing references with large numbers of contigs, such as draft genomes. In the past this mostly affected microbial genomes so we didn't do anything about it, but now the Hg38 has a lot more contigs so we have to make sure that's not going to be a problem with GATK4. . To be clear, efficient processing of reference genomes with thousands of contigs is a must-have. . Efficient processing of e.g. microbial draft genomes with tens of thousands of contigs is a nice-to-have. More than that is just crazy talk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1688
https://github.com/broadinstitute/gatk/issues/1688:303,Usability,clear,clear,303,"GATK3 is very slow when processing references with large numbers of contigs, such as draft genomes. In the past this mostly affected microbial genomes so we didn't do anything about it, but now the Hg38 has a lot more contigs so we have to make sure that's not going to be a problem with GATK4. . To be clear, efficient processing of reference genomes with thousands of contigs is a must-have. . Efficient processing of e.g. microbial draft genomes with tens of thousands of contigs is a nice-to-have. More than that is just crazy talk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1688
https://github.com/broadinstitute/gatk/issues/1689:246,Deployability,update,updates,246,figure out how spark-submit --files works:; - [x] does it send files like it promises **yes**; - [x] where does it put them?; **I believe it puts them in the working directory of the executor.**; - [ ] does cache them between runs?; - [ ] how do updates to the files work if it caches them?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1689
https://github.com/broadinstitute/gatk/issues/1689:207,Performance,cache,cache,207,figure out how spark-submit --files works:; - [x] does it send files like it promises **yes**; - [x] where does it put them?; **I believe it puts them in the working directory of the executor.**; - [ ] does cache them between runs?; - [ ] how do updates to the files work if it caches them?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1689
https://github.com/broadinstitute/gatk/issues/1689:278,Performance,cache,caches,278,figure out how spark-submit --files works:; - [x] does it send files like it promises **yes**; - [x] where does it put them?; **I believe it puts them in the working directory of the executor.**; - [ ] does cache them between runs?; - [ ] how do updates to the files work if it caches them?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1689
https://github.com/broadinstitute/gatk/issues/1692:58,Testability,test,test,58,"run from gatk directory; `./gatk-launch PrintReads -I src/test/resources/large/NA12878.RNAseq.bam -O tmp.bam`. you see `IntelDeflater`. now, `cd ..`; `./gatk/gatk-launch PrintReads -I gatk/src/test/resources/large/NA12878.RNAseq.bam -O gatk/tmp.bam`. and you see `JdkDeflater`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1692
https://github.com/broadinstitute/gatk/issues/1692:193,Testability,test,test,193,"run from gatk directory; `./gatk-launch PrintReads -I src/test/resources/large/NA12878.RNAseq.bam -O tmp.bam`. you see `IntelDeflater`. now, `cd ..`; `./gatk/gatk-launch PrintReads -I gatk/src/test/resources/large/NA12878.RNAseq.bam -O gatk/tmp.bam`. and you see `JdkDeflater`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1692
https://github.com/broadinstitute/gatk/issues/1694:40,Availability,error,error,40,GATK launch has a fairly complex set of error conditions it can handle. Since it's in python it's very easy to break it without realizing. We need tests for it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1694
https://github.com/broadinstitute/gatk/issues/1694:147,Testability,test,tests,147,GATK launch has a fairly complex set of error conditions it can handle. Since it's in python it's very easy to break it without realizing. We need tests for it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1694
https://github.com/broadinstitute/gatk/pull/1695:232,Modifiability,config,config,232,"fixes #1653. ok, results are in, with asyncIO GATK4 out of the box beats GATK3 4.53x on PrintReads and 1.48x on BaseRecalibrator!. PrintReads; syncTribble, asyncSamtools, IntelDeflater: 1x baseline (this is the out-of-the box GATK4 config); syncTribble, syncSamtools, IntelDeflater: 1.32x baseline; syncTribble, asyncSamtools, JDKDeflater: 1.68x baseline ; syncTribble, syncSamtools, JDKDeflater: 2.32x baseline. For comparison, GATK3 out of the box is 4.53x baseline on PrintReads. BaseRecalibrator; syncTribble, asyncSamtools, IntelDeflater: 1x baseline (this is the out-of-the box GATK4 config); syncTribble, syncSamtools, IntelDeflater: 1x baseline; asyncTribble, asyncSamtools, IntelDeflater: 2.14x baseline; syncTribble, asyncSamtools, JDKDeflater: 1.04x baseline; syncTribble, syncSamtools, JDKDeflater: 1.03x baseline; asyncTribble, asyncSamtools, JDKDeflater: 2.12x baseline. For comparison, GATK3 out of the box is 1.48x baseline on BaseRecalibrator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1695
https://github.com/broadinstitute/gatk/pull/1695:590,Modifiability,config,config,590,"fixes #1653. ok, results are in, with asyncIO GATK4 out of the box beats GATK3 4.53x on PrintReads and 1.48x on BaseRecalibrator!. PrintReads; syncTribble, asyncSamtools, IntelDeflater: 1x baseline (this is the out-of-the box GATK4 config); syncTribble, syncSamtools, IntelDeflater: 1.32x baseline; syncTribble, asyncSamtools, JDKDeflater: 1.68x baseline ; syncTribble, syncSamtools, JDKDeflater: 2.32x baseline. For comparison, GATK3 out of the box is 4.53x baseline on PrintReads. BaseRecalibrator; syncTribble, asyncSamtools, IntelDeflater: 1x baseline (this is the out-of-the box GATK4 config); syncTribble, syncSamtools, IntelDeflater: 1x baseline; asyncTribble, asyncSamtools, IntelDeflater: 2.14x baseline; syncTribble, asyncSamtools, JDKDeflater: 1.04x baseline; syncTribble, syncSamtools, JDKDeflater: 1.03x baseline; asyncTribble, asyncSamtools, JDKDeflater: 2.12x baseline. For comparison, GATK3 out of the box is 1.48x baseline on BaseRecalibrator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1695
https://github.com/broadinstitute/gatk/issues/1697:612,Availability,repair,repairing,612,"Heng's paper _""A statistical framework for SNP calling. . .""_ presents two methods for calculating a variant QUAL score. The first and simpler approach finds a maximum likelihood allele fraction. The second, which is implemented in the GATK, enumerates the likelihoods of all possible partitions of allele counts among the total ploidy of all samples. The latter approach becomes nightmarish (in terms of speed and code complexity) for non-diploid organisms _or_ multiple alt alleles and scales poorly (quadratically, I believe) with the number of samples. Known bugs in the calculation -- and the difficulty of repairing them -- can be attributed to its complexity. And its treatment of multiple alt alleles involves a major hack. The task is to generalize the allele fraction (as opposed to allele count) equations to non-diploid samples with multiple alt alleles, figure out appropriate priors, and implement it as a new QUAL score calculator.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1697
https://github.com/broadinstitute/gatk/issues/1697:135,Usability,simpl,simpler,135,"Heng's paper _""A statistical framework for SNP calling. . .""_ presents two methods for calculating a variant QUAL score. The first and simpler approach finds a maximum likelihood allele fraction. The second, which is implemented in the GATK, enumerates the likelihoods of all possible partitions of allele counts among the total ploidy of all samples. The latter approach becomes nightmarish (in terms of speed and code complexity) for non-diploid organisms _or_ multiple alt alleles and scales poorly (quadratically, I believe) with the number of samples. Known bugs in the calculation -- and the difficulty of repairing them -- can be attributed to its complexity. And its treatment of multiple alt alleles involves a major hack. The task is to generalize the allele fraction (as opposed to allele count) equations to non-diploid samples with multiple alt alleles, figure out appropriate priors, and implement it as a new QUAL score calculator.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1697
https://github.com/broadinstitute/gatk/pull/1698:57,Security,Hash,Hashtables,57,"there was some not nice code in IntervalsSkipList (using Hashtables (sic! we had Hashtables in 2016), declaring things as ArrayList, verbose comparators etc). This is a small cleanup",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1698
https://github.com/broadinstitute/gatk/pull/1698:81,Security,Hash,Hashtables,81,"there was some not nice code in IntervalsSkipList (using Hashtables (sic! we had Hashtables in 2016), declaring things as ArrayList, verbose comparators etc). This is a small cleanup",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1698
https://github.com/broadinstitute/gatk/issues/1699:335,Performance,Queue,Queue,335,"hmm, async bam writing causes this:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.read.ArtificialBAMBuilderUnitTest.testBamProvider[4](ArtificialBAMBuilder{samples=[sample0], readLength=10, alignmentStart=1, skipNLoci=0, nLoci=100, nReadsPerLocus=10}, 10, 0, 1, 1, 10, 100) FAILED; java.lang.RuntimeException: Queue should be empty but is size: 276; at htsjdk.samtools.util.AbstractAsyncWriter.close(AbstractAsyncWriter.java:75); at org.broadinstitute.hellbender.utils.read.ArtificialBAMBuilder.makeBAMFile(ArtificialBAMBuilder.java:143); at org.broadinstitute.hellbender.utils.read.ArtificialBAMBuilder.makeTemporaryBAMFile(ArtificialBAMBuilder.java:130); at org.broadinstitute.hellbender.utils.read.ArtificialBAMBuilderUnitTest.testBamProvider(ArtificialBAMBuilderUnitTest.java:69); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1699
https://github.com/broadinstitute/gatk/issues/1699:64,Testability,test,test,64,"hmm, async bam writing causes this:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.read.ArtificialBAMBuilderUnitTest.testBamProvider[4](ArtificialBAMBuilder{samples=[sample0], readLength=10, alignmentStart=1, skipNLoci=0, nLoci=100, nReadsPerLocus=10}, 10, 0, 1, 1, 10, 100) FAILED; java.lang.RuntimeException: Queue should be empty but is size: 276; at htsjdk.samtools.util.AbstractAsyncWriter.close(AbstractAsyncWriter.java:75); at org.broadinstitute.hellbender.utils.read.ArtificialBAMBuilder.makeBAMFile(ArtificialBAMBuilder.java:143); at org.broadinstitute.hellbender.utils.read.ArtificialBAMBuilder.makeTemporaryBAMFile(ArtificialBAMBuilder.java:130); at org.broadinstitute.hellbender.utils.read.ArtificialBAMBuilderUnitTest.testBamProvider(ArtificialBAMBuilderUnitTest.java:69); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1699
https://github.com/broadinstitute/gatk/issues/1699:141,Testability,test,testBamProvider,141,"hmm, async bam writing causes this:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.read.ArtificialBAMBuilderUnitTest.testBamProvider[4](ArtificialBAMBuilder{samples=[sample0], readLength=10, alignmentStart=1, skipNLoci=0, nLoci=100, nReadsPerLocus=10}, 10, 0, 1, 1, 10, 100) FAILED; java.lang.RuntimeException: Queue should be empty but is size: 276; at htsjdk.samtools.util.AbstractAsyncWriter.close(AbstractAsyncWriter.java:75); at org.broadinstitute.hellbender.utils.read.ArtificialBAMBuilder.makeBAMFile(ArtificialBAMBuilder.java:143); at org.broadinstitute.hellbender.utils.read.ArtificialBAMBuilder.makeTemporaryBAMFile(ArtificialBAMBuilder.java:130); at org.broadinstitute.hellbender.utils.read.ArtificialBAMBuilderUnitTest.testBamProvider(ArtificialBAMBuilderUnitTest.java:69); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1699
https://github.com/broadinstitute/gatk/issues/1699:755,Testability,test,testBamProvider,755,"hmm, async bam writing causes this:. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.utils.read.ArtificialBAMBuilderUnitTest.testBamProvider[4](ArtificialBAMBuilder{samples=[sample0], readLength=10, alignmentStart=1, skipNLoci=0, nLoci=100, nReadsPerLocus=10}, 10, 0, 1, 1, 10, 100) FAILED; java.lang.RuntimeException: Queue should be empty but is size: 276; at htsjdk.samtools.util.AbstractAsyncWriter.close(AbstractAsyncWriter.java:75); at org.broadinstitute.hellbender.utils.read.ArtificialBAMBuilder.makeBAMFile(ArtificialBAMBuilder.java:143); at org.broadinstitute.hellbender.utils.read.ArtificialBAMBuilder.makeTemporaryBAMFile(ArtificialBAMBuilder.java:130); at org.broadinstitute.hellbender.utils.read.ArtificialBAMBuilderUnitTest.testBamProvider(ArtificialBAMBuilderUnitTest.java:69); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1699
https://github.com/broadinstitute/gatk/issues/1700:825,Usability,simpl,simple,825,"HaplotypeCaller expends massive effort to generate a set of local candidate haplotypes along with their read likelihoods. After that, however, it throws away the haplotypes, using them _only_ for their CIGAR strings from which individual and independent variants are extracted. While this approach probably has some merits, it would be nice if HaplotypeCaller had a ""haplotype"" mode in which it genotypes and calls whole local haplotypes, basically by _not_ splitting up the haplotypes and defining each haplotype as a `VariantContext` (although perhaps splitting these by CIGAR string for the final vcf). In addition to giving some amount of phasing (about as much as `ReadBackedPhasing` currently does) for free, this would let nearby variants share statistical strength. All the calls that we reject upon manual review by simple phasing considerations would be correct.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1700
https://github.com/broadinstitute/gatk/pull/1704:41,Testability,test,test,41,"""SNP"" issue and not-exactly-reproducible test results (see #1805) still under investigation. Changed API to input. Thanks for taking a look, @cwhelan!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1704
https://github.com/broadinstitute/gatk/pull/1706:73,Availability,down,downloading,73,also updating build to extract the intelDeflater from the jar instead of downloading it; removed the htsjdkVersion property I had recently added since it was no longer necessary with the update to include the .so in the jar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1706
https://github.com/broadinstitute/gatk/pull/1706:187,Deployability,update,update,187,also updating build to extract the intelDeflater from the jar instead of downloading it; removed the htsjdkVersion property I had recently added since it was no longer necessary with the update to include the .so in the jar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1706
https://github.com/broadinstitute/gatk/pull/1708:441,Availability,avail,available,441,"New implementation of `SlidingWindowWalker` with some ideas from the discussion in #1528. The thinks that are requested in #1198 still holds, but now it is more general: padding option is added and construction of windows are done by interval. The code contain a lot of TODO because it relies on changes implemented in #1567, and because it is suppose to be a walker over `ReadWindow` instead of `SimpleInterval`+`ReadsContext` if reads are available. I think that with these changes it could be general to be extended by `ReadWindowWalker` and by users that needs a different way of ""slide"" over intervals.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708
https://github.com/broadinstitute/gatk/pull/1708:510,Modifiability,extend,extended,510,"New implementation of `SlidingWindowWalker` with some ideas from the discussion in #1528. The thinks that are requested in #1198 still holds, but now it is more general: padding option is added and construction of windows are done by interval. The code contain a lot of TODO because it relies on changes implemented in #1567, and because it is suppose to be a walker over `ReadWindow` instead of `SimpleInterval`+`ReadsContext` if reads are available. I think that with these changes it could be general to be extended by `ReadWindowWalker` and by users that needs a different way of ""slide"" over intervals.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708
https://github.com/broadinstitute/gatk/pull/1708:397,Usability,Simpl,SimpleInterval,397,"New implementation of `SlidingWindowWalker` with some ideas from the discussion in #1528. The thinks that are requested in #1198 still holds, but now it is more general: padding option is added and construction of windows are done by interval. The code contain a lot of TODO because it relies on changes implemented in #1567, and because it is suppose to be a walker over `ReadWindow` instead of `SimpleInterval`+`ReadsContext` if reads are available. I think that with these changes it could be general to be extended by `ReadWindowWalker` and by users that needs a different way of ""slide"" over intervals.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1708
https://github.com/broadinstitute/gatk/issues/1710:779,Modifiability,extend,extended,779,"In my case, as an API user, my main usage of GATK is for traverse `GATKRead` and `VariantContext`, so I would like to have in `GATKTool` a simpler way of access to the `FeatureInput<VariantContext>` instead of getting them from `FeatureManager features`. It will be useful in the `VariantWalker` as a step to issue #692, to get all the variants provided by the user in the same walker. My idea is modify the `GATKTool` to include:; - A `public abstract boolean requiresVariant()`, which will be used to determine if we should detach or not all the variants inputs from the `FeatureManager features`.; - A `private void initializeVariants()`, which will implement a way to extract the `FeatureInput<VariantContext>` from `features` and initialize a `FeatureManager variants` or a extended class which includes only `VariantContext` inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1710
https://github.com/broadinstitute/gatk/issues/1710:154,Security,access,access,154,"In my case, as an API user, my main usage of GATK is for traverse `GATKRead` and `VariantContext`, so I would like to have in `GATKTool` a simpler way of access to the `FeatureInput<VariantContext>` instead of getting them from `FeatureManager features`. It will be useful in the `VariantWalker` as a step to issue #692, to get all the variants provided by the user in the same walker. My idea is modify the `GATKTool` to include:; - A `public abstract boolean requiresVariant()`, which will be used to determine if we should detach or not all the variants inputs from the `FeatureManager features`.; - A `private void initializeVariants()`, which will implement a way to extract the `FeatureInput<VariantContext>` from `features` and initialize a `FeatureManager variants` or a extended class which includes only `VariantContext` inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1710
https://github.com/broadinstitute/gatk/issues/1710:139,Usability,simpl,simpler,139,"In my case, as an API user, my main usage of GATK is for traverse `GATKRead` and `VariantContext`, so I would like to have in `GATKTool` a simpler way of access to the `FeatureInput<VariantContext>` instead of getting them from `FeatureManager features`. It will be useful in the `VariantWalker` as a step to issue #692, to get all the variants provided by the user in the same walker. My idea is modify the `GATKTool` to include:; - A `public abstract boolean requiresVariant()`, which will be used to determine if we should detach or not all the variants inputs from the `FeatureManager features`.; - A `private void initializeVariants()`, which will implement a way to extract the `FeatureInput<VariantContext>` from `features` and initialize a `FeatureManager variants` or a extended class which includes only `VariantContext` inputs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1710
https://github.com/broadinstitute/gatk/issues/1711:134,Availability,FAILURE,FAILURE,134,"A user building GATK on a POWER system tried to set `GATK_SKIP_NATIVE_BUILD=true`, but it didn't prevent the build from failing. ```; FAILURE: Build failed with an exception. * What went wrong:; A problem occurred configuring root project 'gatk'.; > Exception thrown while executing model rule: NativeComponentModelPlugin.Rules#createBinaries; > Invalid NativePlatform: linux_ppc64. BUILD FAILED; ```. the temporary workaround was the following change:. ```; $ diff build.gradle.org build.gradle; 406c406,408; < VectorLoglessPairHMM(NativeLibrarySpec) {. ---; > if(System.properties[""os.arch""] != ""ppc64""); > {; > VectorLoglessPairHMM(NativeLibrarySpec) {; 458a461; > }; ```. This is a bug and is likely to cause failures on other systems as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1711
https://github.com/broadinstitute/gatk/issues/1711:713,Availability,failure,failures,713,"A user building GATK on a POWER system tried to set `GATK_SKIP_NATIVE_BUILD=true`, but it didn't prevent the build from failing. ```; FAILURE: Build failed with an exception. * What went wrong:; A problem occurred configuring root project 'gatk'.; > Exception thrown while executing model rule: NativeComponentModelPlugin.Rules#createBinaries; > Invalid NativePlatform: linux_ppc64. BUILD FAILED; ```. the temporary workaround was the following change:. ```; $ diff build.gradle.org build.gradle; 406c406,408; < VectorLoglessPairHMM(NativeLibrarySpec) {. ---; > if(System.properties[""os.arch""] != ""ppc64""); > {; > VectorLoglessPairHMM(NativeLibrarySpec) {; 458a461; > }; ```. This is a bug and is likely to cause failures on other systems as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1711
https://github.com/broadinstitute/gatk/issues/1711:26,Energy Efficiency,POWER,POWER,26,"A user building GATK on a POWER system tried to set `GATK_SKIP_NATIVE_BUILD=true`, but it didn't prevent the build from failing. ```; FAILURE: Build failed with an exception. * What went wrong:; A problem occurred configuring root project 'gatk'.; > Exception thrown while executing model rule: NativeComponentModelPlugin.Rules#createBinaries; > Invalid NativePlatform: linux_ppc64. BUILD FAILED; ```. the temporary workaround was the following change:. ```; $ diff build.gradle.org build.gradle; 406c406,408; < VectorLoglessPairHMM(NativeLibrarySpec) {. ---; > if(System.properties[""os.arch""] != ""ppc64""); > {; > VectorLoglessPairHMM(NativeLibrarySpec) {; 458a461; > }; ```. This is a bug and is likely to cause failures on other systems as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1711
https://github.com/broadinstitute/gatk/issues/1711:214,Modifiability,config,configuring,214,"A user building GATK on a POWER system tried to set `GATK_SKIP_NATIVE_BUILD=true`, but it didn't prevent the build from failing. ```; FAILURE: Build failed with an exception. * What went wrong:; A problem occurred configuring root project 'gatk'.; > Exception thrown while executing model rule: NativeComponentModelPlugin.Rules#createBinaries; > Invalid NativePlatform: linux_ppc64. BUILD FAILED; ```. the temporary workaround was the following change:. ```; $ diff build.gradle.org build.gradle; 406c406,408; < VectorLoglessPairHMM(NativeLibrarySpec) {. ---; > if(System.properties[""os.arch""] != ""ppc64""); > {; > VectorLoglessPairHMM(NativeLibrarySpec) {; 458a461; > }; ```. This is a bug and is likely to cause failures on other systems as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1711
https://github.com/broadinstitute/gatk/pull/1714:41,Integrability,depend,dependencies,41,"Included in this PR are the engine-level dependencies of the HaplotypeCaller; from the dr_runnable_haplotypecaller branch. This includes all parts; of that branch that lie outside of the haplotypecaller package itself, and; mostly consists of a new traversal (AssemblyRegionWalker), additions to; the genotyper package, and various additions/tweaks to common utilities; and engine-level classes. The haplotypecaller package itself has been removed in preparation for its; migration to the gatk-protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1714
https://github.com/broadinstitute/gatk/issues/1718:308,Performance,perform,performance,308,"the 2bit fasta isa bit of a pain to deal with. https://github.com/broadinstitute/gatk/issues/1580 shows that size-wise bgzip fasta is comparable. . This ticket is to:; - [ ] establish whether we can use a bgzipped+index fasta as a reference (htjdk support and speed); - [ ] if it works for walkers, evaluate performance of distributing the reference+index to spark executors and using it as a replacement for 2 bit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1718
https://github.com/broadinstitute/gatk/pull/1719:31,Testability,test,tests,31,"Before this change you can run tests only once before gradle decides that the task is ""UP-TO-DATE"". I want to be able to rerun tests multiple times (bonus: guess what I'm testing) and so this caching got in the way. With this change, tasks are always rerunnable. Test it by running this twice:. ```; ./gradlew test --tests *VariantFiltrationIntegrationTest*; ```. @lbergelson can you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1719
https://github.com/broadinstitute/gatk/pull/1719:127,Testability,test,tests,127,"Before this change you can run tests only once before gradle decides that the task is ""UP-TO-DATE"". I want to be able to rerun tests multiple times (bonus: guess what I'm testing) and so this caching got in the way. With this change, tasks are always rerunnable. Test it by running this twice:. ```; ./gradlew test --tests *VariantFiltrationIntegrationTest*; ```. @lbergelson can you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1719
https://github.com/broadinstitute/gatk/pull/1719:171,Testability,test,testing,171,"Before this change you can run tests only once before gradle decides that the task is ""UP-TO-DATE"". I want to be able to rerun tests multiple times (bonus: guess what I'm testing) and so this caching got in the way. With this change, tasks are always rerunnable. Test it by running this twice:. ```; ./gradlew test --tests *VariantFiltrationIntegrationTest*; ```. @lbergelson can you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1719
https://github.com/broadinstitute/gatk/pull/1719:263,Testability,Test,Test,263,"Before this change you can run tests only once before gradle decides that the task is ""UP-TO-DATE"". I want to be able to rerun tests multiple times (bonus: guess what I'm testing) and so this caching got in the way. With this change, tasks are always rerunnable. Test it by running this twice:. ```; ./gradlew test --tests *VariantFiltrationIntegrationTest*; ```. @lbergelson can you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1719
https://github.com/broadinstitute/gatk/pull/1719:310,Testability,test,test,310,"Before this change you can run tests only once before gradle decides that the task is ""UP-TO-DATE"". I want to be able to rerun tests multiple times (bonus: guess what I'm testing) and so this caching got in the way. With this change, tasks are always rerunnable. Test it by running this twice:. ```; ./gradlew test --tests *VariantFiltrationIntegrationTest*; ```. @lbergelson can you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1719
https://github.com/broadinstitute/gatk/pull/1719:317,Testability,test,tests,317,"Before this change you can run tests only once before gradle decides that the task is ""UP-TO-DATE"". I want to be able to rerun tests multiple times (bonus: guess what I'm testing) and so this caching got in the way. With this change, tasks are always rerunnable. Test it by running this twice:. ```; ./gradlew test --tests *VariantFiltrationIntegrationTest*; ```. @lbergelson can you review?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1719
https://github.com/broadinstitute/gatk/pull/1720:62,Testability,test,tests,62,fixes https://github.com/broadinstitute/gatk/issues/1713. the tests fail unless the 1 line in `GATKVCFHeaderLines` is added. @droazen please review,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1720
https://github.com/broadinstitute/gatk/issues/1723:650,Deployability,pipeline,pipelines,650,"annoyingly, spark UI and Hadoop UI claim the job succeeds. see output here: https://console.cloud.google.com/dataproc/jobs/813fba95-8b19-4b4d-9cd1-b5bb94e4e52c?project=broad-dsde-dev&authuser=1&graph=GCE_CPU&duration=PT1H&tab=output. ```; java.lang.ArrayIndexOutOfBoundsException: 18; at org.broadinstitute.hellbender.utils.recalibration.EventType.eventFrom(EventType.java:35); at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generateReportTables(RecalUtils.java:255); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:50); at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:109); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1723
https://github.com/broadinstitute/gatk/issues/1723:1675,Deployability,deploy,deploy,1675,b succeeds. see output here: https://console.cloud.google.com/dataproc/jobs/813fba95-8b19-4b4d-9cd1-b5bb94e4e52c?project=broad-dsde-dev&authuser=1&graph=GCE_CPU&duration=PT1H&tab=output. ```; java.lang.ArrayIndexOutOfBoundsException: 18; at org.broadinstitute.hellbender.utils.recalibration.EventType.eventFrom(EventType.java:35); at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generateReportTables(RecalUtils.java:255); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:50); at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:109); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1723
https://github.com/broadinstitute/gatk/issues/1723:1712,Deployability,deploy,deploy,1712,b succeeds. see output here: https://console.cloud.google.com/dataproc/jobs/813fba95-8b19-4b4d-9cd1-b5bb94e4e52c?project=broad-dsde-dev&authuser=1&graph=GCE_CPU&duration=PT1H&tab=output. ```; java.lang.ArrayIndexOutOfBoundsException: 18; at org.broadinstitute.hellbender.utils.recalibration.EventType.eventFrom(EventType.java:35); at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generateReportTables(RecalUtils.java:255); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:50); at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:109); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1723
https://github.com/broadinstitute/gatk/issues/1723:1784,Deployability,deploy,deploy,1784,b succeeds. see output here: https://console.cloud.google.com/dataproc/jobs/813fba95-8b19-4b4d-9cd1-b5bb94e4e52c?project=broad-dsde-dev&authuser=1&graph=GCE_CPU&duration=PT1H&tab=output. ```; java.lang.ArrayIndexOutOfBoundsException: 18; at org.broadinstitute.hellbender.utils.recalibration.EventType.eventFrom(EventType.java:35); at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generateReportTables(RecalUtils.java:255); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:50); at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:109); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1723
https://github.com/broadinstitute/gatk/issues/1723:1860,Deployability,deploy,deploy,1860,b succeeds. see output here: https://console.cloud.google.com/dataproc/jobs/813fba95-8b19-4b4d-9cd1-b5bb94e4e52c?project=broad-dsde-dev&authuser=1&graph=GCE_CPU&duration=PT1H&tab=output. ```; java.lang.ArrayIndexOutOfBoundsException: 18; at org.broadinstitute.hellbender.utils.recalibration.EventType.eventFrom(EventType.java:35); at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generateReportTables(RecalUtils.java:255); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:50); at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:109); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1723
https://github.com/broadinstitute/gatk/issues/1723:1931,Deployability,deploy,deploy,1931,b succeeds. see output here: https://console.cloud.google.com/dataproc/jobs/813fba95-8b19-4b4d-9cd1-b5bb94e4e52c?project=broad-dsde-dev&authuser=1&graph=GCE_CPU&duration=PT1H&tab=output. ```; java.lang.ArrayIndexOutOfBoundsException: 18; at org.broadinstitute.hellbender.utils.recalibration.EventType.eventFrom(EventType.java:35); at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generateReportTables(RecalUtils.java:255); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:50); at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:109); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1723
https://github.com/broadinstitute/gatk/issues/1723:2000,Deployability,deploy,deploy,2000,b succeeds. see output here: https://console.cloud.google.com/dataproc/jobs/813fba95-8b19-4b4d-9cd1-b5bb94e4e52c?project=broad-dsde-dev&authuser=1&graph=GCE_CPU&duration=PT1H&tab=output. ```; java.lang.ArrayIndexOutOfBoundsException: 18; at org.broadinstitute.hellbender.utils.recalibration.EventType.eventFrom(EventType.java:35); at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generateReportTables(RecalUtils.java:255); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:50); at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:109); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1723
https://github.com/broadinstitute/gatk/issues/1727:36,Testability,test,test,36,```; ./gatk-launch FilterVcf -I src/test/resources/org/broadinstitute/hellbender/tools/walkers/filters/VariantFiltration/expected/testVariantFiltration_testGenotypeFilters1.vcf -O a.vcf; ```. ```; java.lang.NullPointerException; at org.broadinstitute.hellbender.tools.picard.vcf.filter.AlleleBalanceFilter.filter(AlleleBalanceFilter.java:50); at org.broadinstitute.hellbender.tools.picard.vcf.filter.FilterApplyingVariantIterator.next(FilterApplyingVariantIterator.java:52); at org.broadinstitute.hellbender.tools.picard.vcf.filter.FilterVcf.doWork(FilterVcf.java:85); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:61); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); ```. the bug is in `AlleleBalanceFilter` - it assumes that all genotypes have 2 AD fields and crashes and burns if they don't.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1727
https://github.com/broadinstitute/gatk/issues/1728:36,Testability,test,test,36,"```; ./gatk-launch FilterVcf -I src/test/resources/large/dbsnp_138.b37.20.21.vcf -O a.vcf; ```. ```; java.lang.IllegalArgumentException: A reference dictionary is required for creating Tribble indices on the fly; at htsjdk.variant.variantcontext.writer.VariantContextWriterBuilder.build(VariantContextWriterBuilder.java:411); at org.broadinstitute.hellbender.tools.picard.vcf.filter.FilterVcf.doWork(FilterVcf.java:71); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:61); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); ```. This file does have a sequence dictionary and the Picard original FilterVcf does not blows up on this file but does blow up if file has no sequence dictionary, eg; `java -jar ~/bin/picard-tools-2.0.1/picard.jar FilterVcf I=src/test/resources/empty.vcf O=a.vcf`. ```; Exception in thread ""main"" picard.PicardException: The input vcf must have a sequence dictionary in order to create indexed vcf or bcfs.; at picard.vcf.filter.FilterVcf.doWork(FilterVcf.java:106); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:209); at picard.cmdline.PicardCommandLine.instanceMain(PicardCommandLine.java:95); at picard.cmdline.PicardCommandLine.main(PicardCommandLine.java:105); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1728
https://github.com/broadinstitute/gatk/issues/1728:1104,Testability,test,test,1104,"```; ./gatk-launch FilterVcf -I src/test/resources/large/dbsnp_138.b37.20.21.vcf -O a.vcf; ```. ```; java.lang.IllegalArgumentException: A reference dictionary is required for creating Tribble indices on the fly; at htsjdk.variant.variantcontext.writer.VariantContextWriterBuilder.build(VariantContextWriterBuilder.java:411); at org.broadinstitute.hellbender.tools.picard.vcf.filter.FilterVcf.doWork(FilterVcf.java:71); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:61); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); ```. This file does have a sequence dictionary and the Picard original FilterVcf does not blows up on this file but does blow up if file has no sequence dictionary, eg; `java -jar ~/bin/picard-tools-2.0.1/picard.jar FilterVcf I=src/test/resources/empty.vcf O=a.vcf`. ```; Exception in thread ""main"" picard.PicardException: The input vcf must have a sequence dictionary in order to create indexed vcf or bcfs.; at picard.vcf.filter.FilterVcf.doWork(FilterVcf.java:106); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:209); at picard.cmdline.PicardCommandLine.instanceMain(PicardCommandLine.java:95); at picard.cmdline.PicardCommandLine.main(PicardCommandLine.java:105); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1728
https://github.com/broadinstitute/gatk/pull/1729:85,Energy Efficiency,allocate,allocated,85,This was debugged while testing https://github.com/samtools/htsjdk/pull/576. Readers allocated in `AbstractMarkDuplicatesCommandLineProgram.openInputs` were never closed. In the asyncIO realm (if we switch to async reading) this is a big problem because one worker thread is then created and abandoned (it keeps living after its master is long gone - unless the master is closed which will inform the worker to finish). The diffs are much more trivial than it looks in github gui - essentially 2 things were put in try-with-resources (two lines like this) and lot of white-space shifting followed.; `final SamHeaderAndIterator headerAndIterator = openInputs();`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1729
https://github.com/broadinstitute/gatk/pull/1729:24,Testability,test,testing,24,This was debugged while testing https://github.com/samtools/htsjdk/pull/576. Readers allocated in `AbstractMarkDuplicatesCommandLineProgram.openInputs` were never closed. In the asyncIO realm (if we switch to async reading) this is a big problem because one worker thread is then created and abandoned (it keeps living after its master is long gone - unless the master is closed which will inform the worker to finish). The diffs are much more trivial than it looks in github gui - essentially 2 things were put in try-with-resources (two lines like this) and lot of white-space shifting followed.; `final SamHeaderAndIterator headerAndIterator = openInputs();`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1729
https://github.com/broadinstitute/gatk/issues/1732:95,Deployability,integrat,integration,95,"The current port of the `HaplotypeCaller` in `dr_runnable_haplotypecaller` has several ""fuzzy"" integration tests, in addition to traditional ""exact match"" integration tests, that test that we're above a certain % of concordance with a known good set (currently, GATK 3 output) using selected parts of the records (eg., alleles, genotypes, start/end positions). We should try to expand this strategy to include fuzzy testing for other parts of the output as well, such as annotations, with the ultimate goal of moving away from exact-match testing for the `HaplotypeCaller`. Will need to be done in consultation with methods people, particularly @ldgauthier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1732
https://github.com/broadinstitute/gatk/issues/1732:155,Deployability,integrat,integration,155,"The current port of the `HaplotypeCaller` in `dr_runnable_haplotypecaller` has several ""fuzzy"" integration tests, in addition to traditional ""exact match"" integration tests, that test that we're above a certain % of concordance with a known good set (currently, GATK 3 output) using selected parts of the records (eg., alleles, genotypes, start/end positions). We should try to expand this strategy to include fuzzy testing for other parts of the output as well, such as annotations, with the ultimate goal of moving away from exact-match testing for the `HaplotypeCaller`. Will need to be done in consultation with methods people, particularly @ldgauthier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1732
https://github.com/broadinstitute/gatk/issues/1732:95,Integrability,integrat,integration,95,"The current port of the `HaplotypeCaller` in `dr_runnable_haplotypecaller` has several ""fuzzy"" integration tests, in addition to traditional ""exact match"" integration tests, that test that we're above a certain % of concordance with a known good set (currently, GATK 3 output) using selected parts of the records (eg., alleles, genotypes, start/end positions). We should try to expand this strategy to include fuzzy testing for other parts of the output as well, such as annotations, with the ultimate goal of moving away from exact-match testing for the `HaplotypeCaller`. Will need to be done in consultation with methods people, particularly @ldgauthier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1732
https://github.com/broadinstitute/gatk/issues/1732:155,Integrability,integrat,integration,155,"The current port of the `HaplotypeCaller` in `dr_runnable_haplotypecaller` has several ""fuzzy"" integration tests, in addition to traditional ""exact match"" integration tests, that test that we're above a certain % of concordance with a known good set (currently, GATK 3 output) using selected parts of the records (eg., alleles, genotypes, start/end positions). We should try to expand this strategy to include fuzzy testing for other parts of the output as well, such as annotations, with the ultimate goal of moving away from exact-match testing for the `HaplotypeCaller`. Will need to be done in consultation with methods people, particularly @ldgauthier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1732
https://github.com/broadinstitute/gatk/issues/1732:107,Testability,test,tests,107,"The current port of the `HaplotypeCaller` in `dr_runnable_haplotypecaller` has several ""fuzzy"" integration tests, in addition to traditional ""exact match"" integration tests, that test that we're above a certain % of concordance with a known good set (currently, GATK 3 output) using selected parts of the records (eg., alleles, genotypes, start/end positions). We should try to expand this strategy to include fuzzy testing for other parts of the output as well, such as annotations, with the ultimate goal of moving away from exact-match testing for the `HaplotypeCaller`. Will need to be done in consultation with methods people, particularly @ldgauthier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1732
https://github.com/broadinstitute/gatk/issues/1732:167,Testability,test,tests,167,"The current port of the `HaplotypeCaller` in `dr_runnable_haplotypecaller` has several ""fuzzy"" integration tests, in addition to traditional ""exact match"" integration tests, that test that we're above a certain % of concordance with a known good set (currently, GATK 3 output) using selected parts of the records (eg., alleles, genotypes, start/end positions). We should try to expand this strategy to include fuzzy testing for other parts of the output as well, such as annotations, with the ultimate goal of moving away from exact-match testing for the `HaplotypeCaller`. Will need to be done in consultation with methods people, particularly @ldgauthier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1732
https://github.com/broadinstitute/gatk/issues/1732:179,Testability,test,test,179,"The current port of the `HaplotypeCaller` in `dr_runnable_haplotypecaller` has several ""fuzzy"" integration tests, in addition to traditional ""exact match"" integration tests, that test that we're above a certain % of concordance with a known good set (currently, GATK 3 output) using selected parts of the records (eg., alleles, genotypes, start/end positions). We should try to expand this strategy to include fuzzy testing for other parts of the output as well, such as annotations, with the ultimate goal of moving away from exact-match testing for the `HaplotypeCaller`. Will need to be done in consultation with methods people, particularly @ldgauthier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1732
https://github.com/broadinstitute/gatk/issues/1732:416,Testability,test,testing,416,"The current port of the `HaplotypeCaller` in `dr_runnable_haplotypecaller` has several ""fuzzy"" integration tests, in addition to traditional ""exact match"" integration tests, that test that we're above a certain % of concordance with a known good set (currently, GATK 3 output) using selected parts of the records (eg., alleles, genotypes, start/end positions). We should try to expand this strategy to include fuzzy testing for other parts of the output as well, such as annotations, with the ultimate goal of moving away from exact-match testing for the `HaplotypeCaller`. Will need to be done in consultation with methods people, particularly @ldgauthier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1732
https://github.com/broadinstitute/gatk/issues/1732:539,Testability,test,testing,539,"The current port of the `HaplotypeCaller` in `dr_runnable_haplotypecaller` has several ""fuzzy"" integration tests, in addition to traditional ""exact match"" integration tests, that test that we're above a certain % of concordance with a known good set (currently, GATK 3 output) using selected parts of the records (eg., alleles, genotypes, start/end positions). We should try to expand this strategy to include fuzzy testing for other parts of the output as well, such as annotations, with the ultimate goal of moving away from exact-match testing for the `HaplotypeCaller`. Will need to be done in consultation with methods people, particularly @ldgauthier",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1732
https://github.com/broadinstitute/gatk/pull/1734:285,Safety,risk,risky,285,hashtable lookups are expensive in Kryo and they add up to 15% or more of runtime (top hotspot on Xprof). https://twitter.com/aphyr/status/478638361150636032. This PR turns off reference tracking in Kryo which speeds things up ~7.2mins vs 7.4mins on MarkDuplicatesSpark but it's a bit risky because I think it may result in an infinite loop for cyclic object graphs. We do not have any cyclic object graphs now and so it's fine. The PR is to open a convo about this.; @tomwhite @droazen @laserson wdyt?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1734
https://github.com/broadinstitute/gatk/pull/1734:0,Security,hash,hashtable,0,hashtable lookups are expensive in Kryo and they add up to 15% or more of runtime (top hotspot on Xprof). https://twitter.com/aphyr/status/478638361150636032. This PR turns off reference tracking in Kryo which speeds things up ~7.2mins vs 7.4mins on MarkDuplicatesSpark but it's a bit risky because I think it may result in an infinite loop for cyclic object graphs. We do not have any cyclic object graphs now and so it's fine. The PR is to open a convo about this.; @tomwhite @droazen @laserson wdyt?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1734
https://github.com/broadinstitute/gatk/pull/1736:97,Availability,down,down,97,fixes https://github.com/broadinstitute/gatk/issues/1702. Speed up in writing - from 3.1 minutes down to 2.4 minutes in MarkDuplicatesSpark; @lbergelson can you review?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1736
https://github.com/broadinstitute/gatk/pull/1737:47,Testability,test,tested,47,added info about dynamic allocation on YARN (I tested it on Google and it works nicely). @lbergelson please review,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1737
https://github.com/broadinstitute/gatk/issues/1738:46,Performance,load,loadable,46,We need a test in travis that the deflator is loadable. I recently broke it's extraction and nothing failed. We need to prevent this from happening in the future.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1738
https://github.com/broadinstitute/gatk/issues/1738:10,Testability,test,test,10,We need a test in travis that the deflator is loadable. I recently broke it's extraction and nothing failed. We need to prevent this from happening in the future.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1738
https://github.com/broadinstitute/gatk/issues/1739:156,Integrability,message,message,156,"after a clean build, build/libIntelDeflater.so is a directory (!) and not a file and running gatk does not use the IntelDeflater (plus prints out a (bogus) message) - `Java HotSpot(TM) 64-Bit Server VM warning: You have loaded library /local/akiezun/gatk4_bqsr_deleteIndels_v2/gatk/build/libIntelDeflater.so which might have disabled stack guard. The VM will try to fix the stack guard now.; It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'`. @lbergelson please fix (ideally prevent from happening in the future or at least enter a ticket to future proof it)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1739
https://github.com/broadinstitute/gatk/issues/1739:220,Performance,load,loaded,220,"after a clean build, build/libIntelDeflater.so is a directory (!) and not a file and running gatk does not use the IntelDeflater (plus prints out a (bogus) message) - `Java HotSpot(TM) 64-Bit Server VM warning: You have loaded library /local/akiezun/gatk4_bqsr_deleteIndels_v2/gatk/build/libIntelDeflater.so which might have disabled stack guard. The VM will try to fix the stack guard now.; It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'`. @lbergelson please fix (ideally prevent from happening in the future or at least enter a ticket to future proof it)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1739
https://github.com/broadinstitute/gatk/issues/1740:329,Energy Efficiency,monitor,monitor,329,"I find hundreds of those in jstack dump created towards the end of our test run. The wait has no timeout and so these threads never die. ```; ""OutputCapture-3-stdout-Test worker-12"" #1118 daemon prio=5 os_prio=31 tid=0x00007f818d05c000 nid=0xde23 in Object.wait() [0x0000000127aae000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at org.broadinstitute.hellbender.utils.runtime.ProcessController$OutputCapture.run(ProcessController.java:315); - locked <0x000000077cf28d98> (a java.util.EnumMap); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1740
https://github.com/broadinstitute/gatk/issues/1740:97,Safety,timeout,timeout,97,"I find hundreds of those in jstack dump created towards the end of our test run. The wait has no timeout and so these threads never die. ```; ""OutputCapture-3-stdout-Test worker-12"" #1118 daemon prio=5 os_prio=31 tid=0x00007f818d05c000 nid=0xde23 in Object.wait() [0x0000000127aae000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at org.broadinstitute.hellbender.utils.runtime.ProcessController$OutputCapture.run(ProcessController.java:315); - locked <0x000000077cf28d98> (a java.util.EnumMap); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1740
https://github.com/broadinstitute/gatk/issues/1740:71,Testability,test,test,71,"I find hundreds of those in jstack dump created towards the end of our test run. The wait has no timeout and so these threads never die. ```; ""OutputCapture-3-stdout-Test worker-12"" #1118 daemon prio=5 os_prio=31 tid=0x00007f818d05c000 nid=0xde23 in Object.wait() [0x0000000127aae000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at org.broadinstitute.hellbender.utils.runtime.ProcessController$OutputCapture.run(ProcessController.java:315); - locked <0x000000077cf28d98> (a java.util.EnumMap); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1740
https://github.com/broadinstitute/gatk/issues/1740:166,Testability,Test,Test,166,"I find hundreds of those in jstack dump created towards the end of our test run. The wait has no timeout and so these threads never die. ```; ""OutputCapture-3-stdout-Test worker-12"" #1118 daemon prio=5 os_prio=31 tid=0x00007f818d05c000 nid=0xde23 in Object.wait() [0x0000000127aae000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at org.broadinstitute.hellbender.utils.runtime.ProcessController$OutputCapture.run(ProcessController.java:315); - locked <0x000000077cf28d98> (a java.util.EnumMap); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1740
https://github.com/broadinstitute/gatk/issues/1741:304,Performance,concurren,concurrent,304,"this guy never dies. ```; ""SAMFileWriterThread-187"" #471 daemon prio=5 os_prio=31 tid=0x00007f818d4b3000 nid=0xd98b waiting on condition [0x0000000126530000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000007507b5178> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:116); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1741
https://github.com/broadinstitute/gatk/issues/1741:379,Performance,concurren,concurrent,379,"this guy never dies. ```; ""SAMFileWriterThread-187"" #471 daemon prio=5 os_prio=31 tid=0x00007f818d4b3000 nid=0xd98b waiting on condition [0x0000000126530000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000007507b5178> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:116); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1741
https://github.com/broadinstitute/gatk/issues/1741:454,Performance,concurren,concurrent,454,"this guy never dies. ```; ""SAMFileWriterThread-187"" #471 daemon prio=5 os_prio=31 tid=0x00007f818d4b3000 nid=0xd98b waiting on condition [0x0000000126530000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000007507b5178> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:116); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1741
https://github.com/broadinstitute/gatk/issues/1741:577,Performance,concurren,concurrent,577,"this guy never dies. ```; ""SAMFileWriterThread-187"" #471 daemon prio=5 os_prio=31 tid=0x00007f818d4b3000 nid=0xd98b waiting on condition [0x0000000126530000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000007507b5178> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:116); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1741
https://github.com/broadinstitute/gatk/issues/1741:220,Safety,Unsafe,Unsafe,220,"this guy never dies. ```; ""SAMFileWriterThread-187"" #471 daemon prio=5 os_prio=31 tid=0x00007f818d4b3000 nid=0xd98b waiting on condition [0x0000000126530000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000007507b5178> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:116); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1741
https://github.com/broadinstitute/gatk/pull/1742:24,Testability,test,test,24,"the first commit adds a test, which should fail; the second should fix the test. It's currently failing though, so I'm not sure what's happening yet. It works on gsa5.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1742
https://github.com/broadinstitute/gatk/pull/1742:75,Testability,test,test,75,"the first commit adds a test, which should fail; the second should fix the test. It's currently failing though, so I'm not sure what's happening yet. It works on gsa5.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1742
https://github.com/broadinstitute/gatk/pull/1745:247,Testability,test,tested,247,"saves 10%-20% of runtime of BaseRecalibator. Intentionally done as a conservative and optional change so that we can evaluate it properly. Analyses done in https://github.com/broadinstitute/gatk/issues/1056 show that resulting bams are identical (tested on bams up to 30GB of size so far). Added a test for it - note that it uses a 27MB file that is not in LFS, which may be a problem. (we could switch to using the standard BAM and using -L to limit territory. Using full file takes 3-4 minutes which is too slow for testing). Also added some utility methods to a few testing classes so that we can test this properly. Added full tests for added functionality. . @lbergelson please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1745
https://github.com/broadinstitute/gatk/pull/1745:298,Testability,test,test,298,"saves 10%-20% of runtime of BaseRecalibator. Intentionally done as a conservative and optional change so that we can evaluate it properly. Analyses done in https://github.com/broadinstitute/gatk/issues/1056 show that resulting bams are identical (tested on bams up to 30GB of size so far). Added a test for it - note that it uses a 27MB file that is not in LFS, which may be a problem. (we could switch to using the standard BAM and using -L to limit territory. Using full file takes 3-4 minutes which is too slow for testing). Also added some utility methods to a few testing classes so that we can test this properly. Added full tests for added functionality. . @lbergelson please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1745
https://github.com/broadinstitute/gatk/pull/1745:518,Testability,test,testing,518,"saves 10%-20% of runtime of BaseRecalibator. Intentionally done as a conservative and optional change so that we can evaluate it properly. Analyses done in https://github.com/broadinstitute/gatk/issues/1056 show that resulting bams are identical (tested on bams up to 30GB of size so far). Added a test for it - note that it uses a 27MB file that is not in LFS, which may be a problem. (we could switch to using the standard BAM and using -L to limit territory. Using full file takes 3-4 minutes which is too slow for testing). Also added some utility methods to a few testing classes so that we can test this properly. Added full tests for added functionality. . @lbergelson please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1745
https://github.com/broadinstitute/gatk/pull/1745:569,Testability,test,testing,569,"saves 10%-20% of runtime of BaseRecalibator. Intentionally done as a conservative and optional change so that we can evaluate it properly. Analyses done in https://github.com/broadinstitute/gatk/issues/1056 show that resulting bams are identical (tested on bams up to 30GB of size so far). Added a test for it - note that it uses a 27MB file that is not in LFS, which may be a problem. (we could switch to using the standard BAM and using -L to limit territory. Using full file takes 3-4 minutes which is too slow for testing). Also added some utility methods to a few testing classes so that we can test this properly. Added full tests for added functionality. . @lbergelson please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1745
https://github.com/broadinstitute/gatk/pull/1745:600,Testability,test,test,600,"saves 10%-20% of runtime of BaseRecalibator. Intentionally done as a conservative and optional change so that we can evaluate it properly. Analyses done in https://github.com/broadinstitute/gatk/issues/1056 show that resulting bams are identical (tested on bams up to 30GB of size so far). Added a test for it - note that it uses a 27MB file that is not in LFS, which may be a problem. (we could switch to using the standard BAM and using -L to limit territory. Using full file takes 3-4 minutes which is too slow for testing). Also added some utility methods to a few testing classes so that we can test this properly. Added full tests for added functionality. . @lbergelson please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1745
https://github.com/broadinstitute/gatk/pull/1745:631,Testability,test,tests,631,"saves 10%-20% of runtime of BaseRecalibator. Intentionally done as a conservative and optional change so that we can evaluate it properly. Analyses done in https://github.com/broadinstitute/gatk/issues/1056 show that resulting bams are identical (tested on bams up to 30GB of size so far). Added a test for it - note that it uses a 27MB file that is not in LFS, which may be a problem. (we could switch to using the standard BAM and using -L to limit territory. Using full file takes 3-4 minutes which is too slow for testing). Also added some utility methods to a few testing classes so that we can test this properly. Added full tests for added functionality. . @lbergelson please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1745
https://github.com/broadinstitute/gatk/pull/1747:127,Integrability,message,messages,127,This PR adds sputnik CI https://sputnik.ci as a code reviewer on pull Reqs. I have configured it to use only FindBugs to limit messages to potentially useful ones. @droazen @lbergelson wdyt? we could give it a try and see if it helps us or annoys us.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1747
https://github.com/broadinstitute/gatk/pull/1747:83,Modifiability,config,configured,83,This PR adds sputnik CI https://sputnik.ci as a code reviewer on pull Reqs. I have configured it to use only FindBugs to limit messages to potentially useful ones. @droazen @lbergelson wdyt? we could give it a try and see if it helps us or annoys us.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1747
https://github.com/broadinstitute/gatk/pull/1748:407,Availability,down,downloadGsaLibFile,407,- Modified the files so that we can build libVectorLoglessPairHMM.so on Ubuntu/ppc64le platform; - Restored and modified the files for 128-bit vector that are on GATK3; - Added a new file to replace AVX with POWER8 vector instructions; - [Question] Is any unit test included in the repository to test the library?; - Confirmed that the library was built on Ubuntu 15.10/ppc64le. ```; ./gradlew installAll; :downloadGsaLibFile UP-TO-DATE; :extractIntelDeflater; :compileJava; :processResources; :classes; :compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp; :linkVectorLoglessPairHMMSharedLibrary; :copySharedLib; :jar; :startScripts; :installDist; :sparkJar; :installSpark; :installAll. BUILD SUCCESSFUL; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748
https://github.com/broadinstitute/gatk/pull/1748:394,Deployability,install,installAll,394,- Modified the files so that we can build libVectorLoglessPairHMM.so on Ubuntu/ppc64le platform; - Restored and modified the files for 128-bit vector that are on GATK3; - Added a new file to replace AVX with POWER8 vector instructions; - [Question] Is any unit test included in the repository to test the library?; - Confirmed that the library was built on Ubuntu 15.10/ppc64le. ```; ./gradlew installAll; :downloadGsaLibFile UP-TO-DATE; :extractIntelDeflater; :compileJava; :processResources; :classes; :compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp; :linkVectorLoglessPairHMMSharedLibrary; :copySharedLib; :jar; :startScripts; :installDist; :sparkJar; :installSpark; :installAll. BUILD SUCCESSFUL; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748
https://github.com/broadinstitute/gatk/pull/1748:648,Deployability,install,installDist,648,- Modified the files so that we can build libVectorLoglessPairHMM.so on Ubuntu/ppc64le platform; - Restored and modified the files for 128-bit vector that are on GATK3; - Added a new file to replace AVX with POWER8 vector instructions; - [Question] Is any unit test included in the repository to test the library?; - Confirmed that the library was built on Ubuntu 15.10/ppc64le. ```; ./gradlew installAll; :downloadGsaLibFile UP-TO-DATE; :extractIntelDeflater; :compileJava; :processResources; :classes; :compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp; :linkVectorLoglessPairHMMSharedLibrary; :copySharedLib; :jar; :startScripts; :installDist; :sparkJar; :installSpark; :installAll. BUILD SUCCESSFUL; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748
https://github.com/broadinstitute/gatk/pull/1748:673,Deployability,install,installSpark,673,- Modified the files so that we can build libVectorLoglessPairHMM.so on Ubuntu/ppc64le platform; - Restored and modified the files for 128-bit vector that are on GATK3; - Added a new file to replace AVX with POWER8 vector instructions; - [Question] Is any unit test included in the repository to test the library?; - Confirmed that the library was built on Ubuntu 15.10/ppc64le. ```; ./gradlew installAll; :downloadGsaLibFile UP-TO-DATE; :extractIntelDeflater; :compileJava; :processResources; :classes; :compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp; :linkVectorLoglessPairHMMSharedLibrary; :copySharedLib; :jar; :startScripts; :installDist; :sparkJar; :installSpark; :installAll. BUILD SUCCESSFUL; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748
https://github.com/broadinstitute/gatk/pull/1748:688,Deployability,install,installAll,688,- Modified the files so that we can build libVectorLoglessPairHMM.so on Ubuntu/ppc64le platform; - Restored and modified the files for 128-bit vector that are on GATK3; - Added a new file to replace AVX with POWER8 vector instructions; - [Question] Is any unit test included in the repository to test the library?; - Confirmed that the library was built on Ubuntu 15.10/ppc64le. ```; ./gradlew installAll; :downloadGsaLibFile UP-TO-DATE; :extractIntelDeflater; :compileJava; :processResources; :classes; :compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp; :linkVectorLoglessPairHMMSharedLibrary; :copySharedLib; :jar; :startScripts; :installDist; :sparkJar; :installSpark; :installAll. BUILD SUCCESSFUL; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748
https://github.com/broadinstitute/gatk/pull/1748:261,Testability,test,test,261,- Modified the files so that we can build libVectorLoglessPairHMM.so on Ubuntu/ppc64le platform; - Restored and modified the files for 128-bit vector that are on GATK3; - Added a new file to replace AVX with POWER8 vector instructions; - [Question] Is any unit test included in the repository to test the library?; - Confirmed that the library was built on Ubuntu 15.10/ppc64le. ```; ./gradlew installAll; :downloadGsaLibFile UP-TO-DATE; :extractIntelDeflater; :compileJava; :processResources; :classes; :compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp; :linkVectorLoglessPairHMMSharedLibrary; :copySharedLib; :jar; :startScripts; :installDist; :sparkJar; :installSpark; :installAll. BUILD SUCCESSFUL; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748
https://github.com/broadinstitute/gatk/pull/1748:296,Testability,test,test,296,- Modified the files so that we can build libVectorLoglessPairHMM.so on Ubuntu/ppc64le platform; - Restored and modified the files for 128-bit vector that are on GATK3; - Added a new file to replace AVX with POWER8 vector instructions; - [Question] Is any unit test included in the repository to test the library?; - Confirmed that the library was built on Ubuntu 15.10/ppc64le. ```; ./gradlew installAll; :downloadGsaLibFile UP-TO-DATE; :extractIntelDeflater; :compileJava; :processResources; :classes; :compileVectorLoglessPairHMMSharedLibraryVectorLoglessPairHMMCpp; :linkVectorLoglessPairHMMSharedLibrary; :copySharedLib; :jar; :startScripts; :installDist; :sparkJar; :installSpark; :installAll. BUILD SUCCESSFUL; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1748
https://github.com/broadinstitute/gatk/pull/1750:419,Deployability,integrat,integrated,419,"This is an experiment to see if it's possible to run BWA-MEM on Spark. (Please don't merge.) The basic idea is that it uses JNI to call BWA-MEM's align function to align a batch of read pairs in one go. I think it should complement the work that @SHuang-Broad has been doing in #1701. It would be great to get your (and @akiezun's) feedback on the direction here. A few comments; - Building the native libraries is not integrated, and it's not using the Apache 2 licensed code. I think this could use some of the changes in #1701.; - The ref is assumed to be on the local FS for the moment - it should really be loaded from HDFS. Also, the output is a single SAM file on the local FS, not a sharded BAM as for the rest of the GATK Spark tools.; - It is assumed that read pairs are interleaved and reads in a pair are placed in the same split (by setting `hadoopbam.bam.keep-paired-reads-together`). However, that property only works for queryname sorted BAMs, which isn't the case here, so we need to relax that requirement in Hadoop-BAM.; - I haven't tried this on large inputs, so I don't know how well it performs. To run, I used the following on a cluster. ```; ./gatk-launch BwaSpark \; --ref /home/tom/workspace/jbwa/test/ref.fa \; --input hdfs:///user/$USER/bwa/R.bam \; --output /tmp/bwa.sam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 1 \; --executor-cores 1 \; --executor-memory 3G \; --archives jbwa-native.tar#jbwa-native \; --conf 'spark.executor.extraLibraryPath=jbwa-native'; ```. The interesting bit is the use of Spark's `--archives` flag to copy a tarball of native libraries (which I built manually) to every executor, and unpacks it in the working directory. Then `spark.executor.extraLibraryPath` is set to add that path to the library path of the executor. This means that you don't have to rely on the native libraries being installed on every node in the cluster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750
https://github.com/broadinstitute/gatk/pull/1750:1900,Deployability,install,installed,1900,"This is an experiment to see if it's possible to run BWA-MEM on Spark. (Please don't merge.) The basic idea is that it uses JNI to call BWA-MEM's align function to align a batch of read pairs in one go. I think it should complement the work that @SHuang-Broad has been doing in #1701. It would be great to get your (and @akiezun's) feedback on the direction here. A few comments; - Building the native libraries is not integrated, and it's not using the Apache 2 licensed code. I think this could use some of the changes in #1701.; - The ref is assumed to be on the local FS for the moment - it should really be loaded from HDFS. Also, the output is a single SAM file on the local FS, not a sharded BAM as for the rest of the GATK Spark tools.; - It is assumed that read pairs are interleaved and reads in a pair are placed in the same split (by setting `hadoopbam.bam.keep-paired-reads-together`). However, that property only works for queryname sorted BAMs, which isn't the case here, so we need to relax that requirement in Hadoop-BAM.; - I haven't tried this on large inputs, so I don't know how well it performs. To run, I used the following on a cluster. ```; ./gatk-launch BwaSpark \; --ref /home/tom/workspace/jbwa/test/ref.fa \; --input hdfs:///user/$USER/bwa/R.bam \; --output /tmp/bwa.sam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 1 \; --executor-cores 1 \; --executor-memory 3G \; --archives jbwa-native.tar#jbwa-native \; --conf 'spark.executor.extraLibraryPath=jbwa-native'; ```. The interesting bit is the use of Spark's `--archives` flag to copy a tarball of native libraries (which I built manually) to every executor, and unpacks it in the working directory. Then `spark.executor.extraLibraryPath` is set to add that path to the library path of the executor. This means that you don't have to rely on the native libraries being installed on every node in the cluster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750
https://github.com/broadinstitute/gatk/pull/1750:419,Integrability,integrat,integrated,419,"This is an experiment to see if it's possible to run BWA-MEM on Spark. (Please don't merge.) The basic idea is that it uses JNI to call BWA-MEM's align function to align a batch of read pairs in one go. I think it should complement the work that @SHuang-Broad has been doing in #1701. It would be great to get your (and @akiezun's) feedback on the direction here. A few comments; - Building the native libraries is not integrated, and it's not using the Apache 2 licensed code. I think this could use some of the changes in #1701.; - The ref is assumed to be on the local FS for the moment - it should really be loaded from HDFS. Also, the output is a single SAM file on the local FS, not a sharded BAM as for the rest of the GATK Spark tools.; - It is assumed that read pairs are interleaved and reads in a pair are placed in the same split (by setting `hadoopbam.bam.keep-paired-reads-together`). However, that property only works for queryname sorted BAMs, which isn't the case here, so we need to relax that requirement in Hadoop-BAM.; - I haven't tried this on large inputs, so I don't know how well it performs. To run, I used the following on a cluster. ```; ./gatk-launch BwaSpark \; --ref /home/tom/workspace/jbwa/test/ref.fa \; --input hdfs:///user/$USER/bwa/R.bam \; --output /tmp/bwa.sam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 1 \; --executor-cores 1 \; --executor-memory 3G \; --archives jbwa-native.tar#jbwa-native \; --conf 'spark.executor.extraLibraryPath=jbwa-native'; ```. The interesting bit is the use of Spark's `--archives` flag to copy a tarball of native libraries (which I built manually) to every executor, and unpacks it in the working directory. Then `spark.executor.extraLibraryPath` is set to add that path to the library path of the executor. This means that you don't have to rely on the native libraries being installed on every node in the cluster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750
https://github.com/broadinstitute/gatk/pull/1750:612,Performance,load,loaded,612,"This is an experiment to see if it's possible to run BWA-MEM on Spark. (Please don't merge.) The basic idea is that it uses JNI to call BWA-MEM's align function to align a batch of read pairs in one go. I think it should complement the work that @SHuang-Broad has been doing in #1701. It would be great to get your (and @akiezun's) feedback on the direction here. A few comments; - Building the native libraries is not integrated, and it's not using the Apache 2 licensed code. I think this could use some of the changes in #1701.; - The ref is assumed to be on the local FS for the moment - it should really be loaded from HDFS. Also, the output is a single SAM file on the local FS, not a sharded BAM as for the rest of the GATK Spark tools.; - It is assumed that read pairs are interleaved and reads in a pair are placed in the same split (by setting `hadoopbam.bam.keep-paired-reads-together`). However, that property only works for queryname sorted BAMs, which isn't the case here, so we need to relax that requirement in Hadoop-BAM.; - I haven't tried this on large inputs, so I don't know how well it performs. To run, I used the following on a cluster. ```; ./gatk-launch BwaSpark \; --ref /home/tom/workspace/jbwa/test/ref.fa \; --input hdfs:///user/$USER/bwa/R.bam \; --output /tmp/bwa.sam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 1 \; --executor-cores 1 \; --executor-memory 3G \; --archives jbwa-native.tar#jbwa-native \; --conf 'spark.executor.extraLibraryPath=jbwa-native'; ```. The interesting bit is the use of Spark's `--archives` flag to copy a tarball of native libraries (which I built manually) to every executor, and unpacks it in the working directory. Then `spark.executor.extraLibraryPath` is set to add that path to the library path of the executor. This means that you don't have to rely on the native libraries being installed on every node in the cluster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750
https://github.com/broadinstitute/gatk/pull/1750:1108,Performance,perform,performs,1108,"This is an experiment to see if it's possible to run BWA-MEM on Spark. (Please don't merge.) The basic idea is that it uses JNI to call BWA-MEM's align function to align a batch of read pairs in one go. I think it should complement the work that @SHuang-Broad has been doing in #1701. It would be great to get your (and @akiezun's) feedback on the direction here. A few comments; - Building the native libraries is not integrated, and it's not using the Apache 2 licensed code. I think this could use some of the changes in #1701.; - The ref is assumed to be on the local FS for the moment - it should really be loaded from HDFS. Also, the output is a single SAM file on the local FS, not a sharded BAM as for the rest of the GATK Spark tools.; - It is assumed that read pairs are interleaved and reads in a pair are placed in the same split (by setting `hadoopbam.bam.keep-paired-reads-together`). However, that property only works for queryname sorted BAMs, which isn't the case here, so we need to relax that requirement in Hadoop-BAM.; - I haven't tried this on large inputs, so I don't know how well it performs. To run, I used the following on a cluster. ```; ./gatk-launch BwaSpark \; --ref /home/tom/workspace/jbwa/test/ref.fa \; --input hdfs:///user/$USER/bwa/R.bam \; --output /tmp/bwa.sam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 1 \; --executor-cores 1 \; --executor-memory 3G \; --archives jbwa-native.tar#jbwa-native \; --conf 'spark.executor.extraLibraryPath=jbwa-native'; ```. The interesting bit is the use of Spark's `--archives` flag to copy a tarball of native libraries (which I built manually) to every executor, and unpacks it in the working directory. Then `spark.executor.extraLibraryPath` is set to add that path to the library path of the executor. This means that you don't have to rely on the native libraries being installed on every node in the cluster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750
https://github.com/broadinstitute/gatk/pull/1750:1223,Testability,test,test,1223,"This is an experiment to see if it's possible to run BWA-MEM on Spark. (Please don't merge.) The basic idea is that it uses JNI to call BWA-MEM's align function to align a batch of read pairs in one go. I think it should complement the work that @SHuang-Broad has been doing in #1701. It would be great to get your (and @akiezun's) feedback on the direction here. A few comments; - Building the native libraries is not integrated, and it's not using the Apache 2 licensed code. I think this could use some of the changes in #1701.; - The ref is assumed to be on the local FS for the moment - it should really be loaded from HDFS. Also, the output is a single SAM file on the local FS, not a sharded BAM as for the rest of the GATK Spark tools.; - It is assumed that read pairs are interleaved and reads in a pair are placed in the same split (by setting `hadoopbam.bam.keep-paired-reads-together`). However, that property only works for queryname sorted BAMs, which isn't the case here, so we need to relax that requirement in Hadoop-BAM.; - I haven't tried this on large inputs, so I don't know how well it performs. To run, I used the following on a cluster. ```; ./gatk-launch BwaSpark \; --ref /home/tom/workspace/jbwa/test/ref.fa \; --input hdfs:///user/$USER/bwa/R.bam \; --output /tmp/bwa.sam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 1 \; --executor-cores 1 \; --executor-memory 3G \; --archives jbwa-native.tar#jbwa-native \; --conf 'spark.executor.extraLibraryPath=jbwa-native'; ```. The interesting bit is the use of Spark's `--archives` flag to copy a tarball of native libraries (which I built manually) to every executor, and unpacks it in the working directory. Then `spark.executor.extraLibraryPath` is set to add that path to the library path of the executor. This means that you don't have to rely on the native libraries being installed on every node in the cluster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750
https://github.com/broadinstitute/gatk/pull/1750:332,Usability,feedback,feedback,332,"This is an experiment to see if it's possible to run BWA-MEM on Spark. (Please don't merge.) The basic idea is that it uses JNI to call BWA-MEM's align function to align a batch of read pairs in one go. I think it should complement the work that @SHuang-Broad has been doing in #1701. It would be great to get your (and @akiezun's) feedback on the direction here. A few comments; - Building the native libraries is not integrated, and it's not using the Apache 2 licensed code. I think this could use some of the changes in #1701.; - The ref is assumed to be on the local FS for the moment - it should really be loaded from HDFS. Also, the output is a single SAM file on the local FS, not a sharded BAM as for the rest of the GATK Spark tools.; - It is assumed that read pairs are interleaved and reads in a pair are placed in the same split (by setting `hadoopbam.bam.keep-paired-reads-together`). However, that property only works for queryname sorted BAMs, which isn't the case here, so we need to relax that requirement in Hadoop-BAM.; - I haven't tried this on large inputs, so I don't know how well it performs. To run, I used the following on a cluster. ```; ./gatk-launch BwaSpark \; --ref /home/tom/workspace/jbwa/test/ref.fa \; --input hdfs:///user/$USER/bwa/R.bam \; --output /tmp/bwa.sam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --driver-memory 3G \; --num-executors 1 \; --executor-cores 1 \; --executor-memory 3G \; --archives jbwa-native.tar#jbwa-native \; --conf 'spark.executor.extraLibraryPath=jbwa-native'; ```. The interesting bit is the use of Spark's `--archives` flag to copy a tarball of native libraries (which I built manually) to every executor, and unpacks it in the working directory. Then `spark.executor.extraLibraryPath` is set to add that path to the library path of the executor. This means that you don't have to rely on the native libraries being installed on every node in the cluster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1750
https://github.com/broadinstitute/gatk/issues/1751:70,Availability,error,error,70,"When the user includes an unknown argument in a tool command line the error message only indicates the first letter of the unknown argument as supposed to the full name. . At the very least the message should read ""unknown argument STARTING with 'x'"". However I would say that is far better if the whole argument name is output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1751
https://github.com/broadinstitute/gatk/issues/1751:76,Integrability,message,message,76,"When the user includes an unknown argument in a tool command line the error message only indicates the first letter of the unknown argument as supposed to the full name. . At the very least the message should read ""unknown argument STARTING with 'x'"". However I would say that is far better if the whole argument name is output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1751
https://github.com/broadinstitute/gatk/issues/1751:194,Integrability,message,message,194,"When the user includes an unknown argument in a tool command line the error message only indicates the first letter of the unknown argument as supposed to the full name. . At the very least the message should read ""unknown argument STARTING with 'x'"". However I would say that is far better if the whole argument name is output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1751
https://github.com/broadinstitute/gatk/issues/1752:536,Availability,error,error,536,"I was developing a `LocusWalker` (#1707) when I found that if several BAM files are provided, the `LocusIteratorByState` (LIBS) returns only a `AlignmentContext` with associated `ReadPileup` with only one sample. I realized that in the LIBS there is a commented exception thrown about that multi-sample is not supported. Because it is commented, the LIBS is providing an `AlignmentContext` for the next sample if the first of them does not have coverage. This is misleading for an API user (it took me some time to understand where the error comes from). I was thinking to do a pull request (or include this in #1707) to solve the issue. There are two ways of doing this:; - As in GATK3, implement an internal `PerSampleReadPileup` that extends the `ReadPileup` and provides an efficient way of separate sample-specific pileups.; - If there is no plan to support multi-sample pileups (I'm worried about this, because I will need it), construct the `AlignmentContext` in the LIBS from all samples. Then, the method `makeFilteredPileup` could be used to extract (in a complicated way) a per-sample pileup by the user side. Because the current implementation was done by @akiezun, could you please give me some feedback? I will need it for my stuff, and I will be very grateful if I can solve this as soon as possible...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1752
https://github.com/broadinstitute/gatk/issues/1752:778,Energy Efficiency,efficient,efficient,778,"I was developing a `LocusWalker` (#1707) when I found that if several BAM files are provided, the `LocusIteratorByState` (LIBS) returns only a `AlignmentContext` with associated `ReadPileup` with only one sample. I realized that in the LIBS there is a commented exception thrown about that multi-sample is not supported. Because it is commented, the LIBS is providing an `AlignmentContext` for the next sample if the first of them does not have coverage. This is misleading for an API user (it took me some time to understand where the error comes from). I was thinking to do a pull request (or include this in #1707) to solve the issue. There are two ways of doing this:; - As in GATK3, implement an internal `PerSampleReadPileup` that extends the `ReadPileup` and provides an efficient way of separate sample-specific pileups.; - If there is no plan to support multi-sample pileups (I'm worried about this, because I will need it), construct the `AlignmentContext` in the LIBS from all samples. Then, the method `makeFilteredPileup` could be used to extract (in a complicated way) a per-sample pileup by the user side. Because the current implementation was done by @akiezun, could you please give me some feedback? I will need it for my stuff, and I will be very grateful if I can solve this as soon as possible...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1752
https://github.com/broadinstitute/gatk/issues/1752:737,Modifiability,extend,extends,737,"I was developing a `LocusWalker` (#1707) when I found that if several BAM files are provided, the `LocusIteratorByState` (LIBS) returns only a `AlignmentContext` with associated `ReadPileup` with only one sample. I realized that in the LIBS there is a commented exception thrown about that multi-sample is not supported. Because it is commented, the LIBS is providing an `AlignmentContext` for the next sample if the first of them does not have coverage. This is misleading for an API user (it took me some time to understand where the error comes from). I was thinking to do a pull request (or include this in #1707) to solve the issue. There are two ways of doing this:; - As in GATK3, implement an internal `PerSampleReadPileup` that extends the `ReadPileup` and provides an efficient way of separate sample-specific pileups.; - If there is no plan to support multi-sample pileups (I'm worried about this, because I will need it), construct the `AlignmentContext` in the LIBS from all samples. Then, the method `makeFilteredPileup` could be used to extract (in a complicated way) a per-sample pileup by the user side. Because the current implementation was done by @akiezun, could you please give me some feedback? I will need it for my stuff, and I will be very grateful if I can solve this as soon as possible...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1752
https://github.com/broadinstitute/gatk/issues/1752:1208,Usability,feedback,feedback,1208,"I was developing a `LocusWalker` (#1707) when I found that if several BAM files are provided, the `LocusIteratorByState` (LIBS) returns only a `AlignmentContext` with associated `ReadPileup` with only one sample. I realized that in the LIBS there is a commented exception thrown about that multi-sample is not supported. Because it is commented, the LIBS is providing an `AlignmentContext` for the next sample if the first of them does not have coverage. This is misleading for an API user (it took me some time to understand where the error comes from). I was thinking to do a pull request (or include this in #1707) to solve the issue. There are two ways of doing this:; - As in GATK3, implement an internal `PerSampleReadPileup` that extends the `ReadPileup` and provides an efficient way of separate sample-specific pileups.; - If there is no plan to support multi-sample pileups (I'm worried about this, because I will need it), construct the `AlignmentContext` in the LIBS from all samples. Then, the method `makeFilteredPileup` could be used to extract (in a complicated way) a per-sample pileup by the user side. Because the current implementation was done by @akiezun, could you please give me some feedback? I will need it for my stuff, and I will be very grateful if I can solve this as soon as possible...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1752
https://github.com/broadinstitute/gatk/issues/1754:1,Security,Hash,HashedListTargetCollection,1,"`HashedListTargetCollection` sorts targets by `IntervalUtils.LEXICOGRAPHICAL_ORDER_COMPARATOR` i.e. ASCII order. Any tool that uses this class to store its targets outputs chromosomes in the order 1, 10, 11 . . . 19, 2, 20, 21, 22, 3 . . .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1754
https://github.com/broadinstitute/gatk/issues/1755:472,Testability,test,test,472,"Running on GCS (cluster created by dataproc), the GATK spark tools run much faster on HDFS than on files stored on GS. HDFS 1.15 minutes. ```; /gatk-launch CountReadsSpark -I /user/akiezun/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam --apiKey <myAPIKEY> -- --sparkRunner GCS --cluster dataproc-cluster-3 --executor-cores 3 --executor-memory 25G --conf spark.yarn.executor.memoryOverhead=2500; ```. GCS 7.50 minutes. ```; ./gatk-launch CountReadsSpark -I gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam --apiKey <myAPIKEY> -- --sparkRunner GCS --cluster dataproc-cluster-3 --executor-cores 3 --executor-memory 25G --conf spark.yarn.executor.memoryOverhead=2500; ```. @lbergelson @jean-philippe-martin is this a known thing? If this expected",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755
https://github.com/broadinstitute/gatk/issues/1755:487,Testability,benchmark,benchmark,487,"Running on GCS (cluster created by dataproc), the GATK spark tools run much faster on HDFS than on files stored on GS. HDFS 1.15 minutes. ```; /gatk-launch CountReadsSpark -I /user/akiezun/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam --apiKey <myAPIKEY> -- --sparkRunner GCS --cluster dataproc-cluster-3 --executor-cores 3 --executor-memory 25G --conf spark.yarn.executor.memoryOverhead=2500; ```. GCS 7.50 minutes. ```; ./gatk-launch CountReadsSpark -I gs://hellbender/test/resources/benchmark/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam --apiKey <myAPIKEY> -- --sparkRunner GCS --cluster dataproc-cluster-3 --executor-cores 3 --executor-memory 25G --conf spark.yarn.executor.memoryOverhead=2500; ```. @lbergelson @jean-philippe-martin is this a known thing? If this expected",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1755
https://github.com/broadinstitute/gatk/pull/1757:111,Deployability,patch,patch,111,"As noted in #1752, only the first sample with coverage is returned in the `AlignmentContext`. This is a simple patch to make the `LoscusIteratorByState` returns an `AlignmentContext` with all the information in the provided iterator.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1757
https://github.com/broadinstitute/gatk/pull/1757:104,Usability,simpl,simple,104,"As noted in #1752, only the first sample with coverage is returned in the `AlignmentContext`. This is a simple patch to make the `LoscusIteratorByState` returns an `AlignmentContext` with all the information in the provided iterator.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1757
https://github.com/broadinstitute/gatk/pull/1760:4,Testability,test,tests,4,our tests used to fail when run a root (because root can create files that others can't etc). This is a fix for it. Tested on a docker vm running gatk as root. @lbergelson can you review?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1760
https://github.com/broadinstitute/gatk/pull/1760:116,Testability,Test,Tested,116,our tests used to fail when run a root (because root can create files that others can't etc). This is a fix for it. Tested on a docker vm running gatk as root. @lbergelson can you review?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1760
https://github.com/broadinstitute/gatk/pull/1761:135,Availability,toler,tolerance,135,"The fix will enable run gradle build and test on PowerPC while the pairHMM native binary build is being sorted out. It also introduced tolerance when compare two floating point numbers, which caused test failure on PowerPC.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1761
https://github.com/broadinstitute/gatk/pull/1761:204,Availability,failure,failure,204,"The fix will enable run gradle build and test on PowerPC while the pairHMM native binary build is being sorted out. It also introduced tolerance when compare two floating point numbers, which caused test failure on PowerPC.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1761
https://github.com/broadinstitute/gatk/pull/1761:49,Energy Efficiency,Power,PowerPC,49,"The fix will enable run gradle build and test on PowerPC while the pairHMM native binary build is being sorted out. It also introduced tolerance when compare two floating point numbers, which caused test failure on PowerPC.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1761
https://github.com/broadinstitute/gatk/pull/1761:215,Energy Efficiency,Power,PowerPC,215,"The fix will enable run gradle build and test on PowerPC while the pairHMM native binary build is being sorted out. It also introduced tolerance when compare two floating point numbers, which caused test failure on PowerPC.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1761
https://github.com/broadinstitute/gatk/pull/1761:41,Testability,test,test,41,"The fix will enable run gradle build and test on PowerPC while the pairHMM native binary build is being sorted out. It also introduced tolerance when compare two floating point numbers, which caused test failure on PowerPC.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1761
https://github.com/broadinstitute/gatk/pull/1761:199,Testability,test,test,199,"The fix will enable run gradle build and test on PowerPC while the pairHMM native binary build is being sorted out. It also introduced tolerance when compare two floating point numbers, which caused test failure on PowerPC.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1761
https://github.com/broadinstitute/gatk/pull/1764:59,Deployability,integrat,integration,59,this solves a nasty precision issue in the HaplotypeCaller integration tests where tests would pass or fail depending on the order in which they ran!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1764
https://github.com/broadinstitute/gatk/pull/1764:59,Integrability,integrat,integration,59,this solves a nasty precision issue in the HaplotypeCaller integration tests where tests would pass or fail depending on the order in which they ran!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1764
https://github.com/broadinstitute/gatk/pull/1764:108,Integrability,depend,depending,108,this solves a nasty precision issue in the HaplotypeCaller integration tests where tests would pass or fail depending on the order in which they ran!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1764
https://github.com/broadinstitute/gatk/pull/1764:71,Testability,test,tests,71,this solves a nasty precision issue in the HaplotypeCaller integration tests where tests would pass or fail depending on the order in which they ran!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1764
https://github.com/broadinstitute/gatk/pull/1764:83,Testability,test,tests,83,this solves a nasty precision issue in the HaplotypeCaller integration tests where tests would pass or fail depending on the order in which they ran!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1764
https://github.com/broadinstitute/gatk/pull/1765:18,Energy Efficiency,reduce,reduce,18,ongoing effort to reduce dependency on dataflow packages. @lbergelson please have a look,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1765
https://github.com/broadinstitute/gatk/pull/1765:25,Integrability,depend,dependency,25,ongoing effort to reduce dependency on dataflow packages. @lbergelson please have a look,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1765
https://github.com/broadinstitute/gatk/pull/1766:82,Security,access,access,82,add IBM. @frank-y-liu please review (I can't assign to you because you need write access for that),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1766
https://github.com/broadinstitute/gatk/issues/1768:927,Availability,avail,available,927,"@akiezun @lbergelson Changed the Spark context configuration from ""local[*]"" to ""local[N]"", where N is specified by a environmental variable. Ran gradle test with ""--tests _SparkIntegration_"". Out of 203 tests, one failed: "" testBulkFragmentsNoDuplicates"", the rest passed. Here is the snippet of code change. Any suggestions?. ```; private static JavaSparkContext createTestSparkContext(Map<String, String> overridingProperties) {; determineSparkMaster();; final SparkConf sparkConf = setupSparkConf(""TestContext"", DEFAULT_SPARK_MASTER, DEFAULT_TEST_PROPERTIES, overridingProperties);; return new JavaSparkContext(sparkConf);; }. /**; * Determine the number of cores Spark master should use. Only used in Spark Test; * Read the specification from the environmental variable GATK_TEST_SPARK_CORES; * If the value is a valid positive integer, use it; * If the value is bogus (strings, etc), or the env. var. is not set, use all available cores, as in ""local[*]""; */. private static void determineSparkMaster() {; int foo = 0;; try {; foo = Integer.parseInt( System.getenv(""GATK_TEST_SPARK_CORES"") );; } catch ( NumberFormatException e ) {}; String numSparkCores;; if ( foo > 0 ) {; numSparkCores = String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:1320,Availability,Error,Error,1320," ```; private static JavaSparkContext createTestSparkContext(Map<String, String> overridingProperties) {; determineSparkMaster();; final SparkConf sparkConf = setupSparkConf(""TestContext"", DEFAULT_SPARK_MASTER, DEFAULT_TEST_PROPERTIES, overridingProperties);; return new JavaSparkContext(sparkConf);; }. /**; * Determine the number of cores Spark master should use. Only used in Spark Test; * Read the specification from the environmental variable GATK_TEST_SPARK_CORES; * If the value is a valid positive integer, use it; * If the value is bogus (strings, etc), or the env. var. is not set, use all available cores, as in ""local[*]""; */. private static void determineSparkMaster() {; int foo = 0;; try {; foo = Integer.parseInt( System.getenv(""GATK_TEST_SPARK_CORES"") );; } catch ( NumberFormatException e ) {}; String numSparkCores;; if ( foo > 0 ) {; numSparkCores = String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesComman",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:47,Deployability,configurat,configuration,47,"@akiezun @lbergelson Changed the Spark context configuration from ""local[*]"" to ""local[N]"", where N is specified by a environmental variable. Ran gradle test with ""--tests _SparkIntegration_"". Out of 203 tests, one failed: "" testBulkFragmentsNoDuplicates"", the rest passed. Here is the snippet of code change. Any suggestions?. ```; private static JavaSparkContext createTestSparkContext(Map<String, String> overridingProperties) {; determineSparkMaster();; final SparkConf sparkConf = setupSparkConf(""TestContext"", DEFAULT_SPARK_MASTER, DEFAULT_TEST_PROPERTIES, overridingProperties);; return new JavaSparkContext(sparkConf);; }. /**; * Determine the number of cores Spark master should use. Only used in Spark Test; * Read the specification from the environmental variable GATK_TEST_SPARK_CORES; * If the value is a valid positive integer, use it; * If the value is bogus (strings, etc), or the env. var. is not set, use all available cores, as in ""local[*]""; */. private static void determineSparkMaster() {; int foo = 0;; try {; foo = Integer.parseInt( System.getenv(""GATK_TEST_SPARK_CORES"") );; } catch ( NumberFormatException e ) {}; String numSparkCores;; if ( foo > 0 ) {; numSparkCores = String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:1326,Integrability,message,messages,1326," ```; private static JavaSparkContext createTestSparkContext(Map<String, String> overridingProperties) {; determineSparkMaster();; final SparkConf sparkConf = setupSparkConf(""TestContext"", DEFAULT_SPARK_MASTER, DEFAULT_TEST_PROPERTIES, overridingProperties);; return new JavaSparkContext(sparkConf);; }. /**; * Determine the number of cores Spark master should use. Only used in Spark Test; * Read the specification from the environmental variable GATK_TEST_SPARK_CORES; * If the value is a valid positive integer, use it; * If the value is bogus (strings, etc), or the env. var. is not set, use all available cores, as in ""local[*]""; */. private static void determineSparkMaster() {; int foo = 0;; try {; foo = Integer.parseInt( System.getenv(""GATK_TEST_SPARK_CORES"") );; } catch ( NumberFormatException e ) {}; String numSparkCores;; if ( foo > 0 ) {; numSparkCores = String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesComman",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:5486,Integrability,Message,MessageHub,5486,internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:5509,Integrability,Message,MessageHub,5509,internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:47,Modifiability,config,configuration,47,"@akiezun @lbergelson Changed the Spark context configuration from ""local[*]"" to ""local[N]"", where N is specified by a environmental variable. Ran gradle test with ""--tests _SparkIntegration_"". Out of 203 tests, one failed: "" testBulkFragmentsNoDuplicates"", the rest passed. Here is the snippet of code change. Any suggestions?. ```; private static JavaSparkContext createTestSparkContext(Map<String, String> overridingProperties) {; determineSparkMaster();; final SparkConf sparkConf = setupSparkConf(""TestContext"", DEFAULT_SPARK_MASTER, DEFAULT_TEST_PROPERTIES, overridingProperties);; return new JavaSparkContext(sparkConf);; }. /**; * Determine the number of cores Spark master should use. Only used in Spark Test; * Read the specification from the environmental variable GATK_TEST_SPARK_CORES; * If the value is a valid positive integer, use it; * If the value is bogus (strings, etc), or the env. var. is not set, use all available cores, as in ""local[*]""; */. private static void determineSparkMaster() {; int foo = 0;; try {; foo = Integer.parseInt( System.getenv(""GATK_TEST_SPARK_CORES"") );; } catch ( NumberFormatException e ) {}; String numSparkCores;; if ( foo > 0 ) {; numSparkCores = String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:132,Modifiability,variab,variable,132,"@akiezun @lbergelson Changed the Spark context configuration from ""local[*]"" to ""local[N]"", where N is specified by a environmental variable. Ran gradle test with ""--tests _SparkIntegration_"". Out of 203 tests, one failed: "" testBulkFragmentsNoDuplicates"", the rest passed. Here is the snippet of code change. Any suggestions?. ```; private static JavaSparkContext createTestSparkContext(Map<String, String> overridingProperties) {; determineSparkMaster();; final SparkConf sparkConf = setupSparkConf(""TestContext"", DEFAULT_SPARK_MASTER, DEFAULT_TEST_PROPERTIES, overridingProperties);; return new JavaSparkContext(sparkConf);; }. /**; * Determine the number of cores Spark master should use. Only used in Spark Test; * Read the specification from the environmental variable GATK_TEST_SPARK_CORES; * If the value is a valid positive integer, use it; * If the value is bogus (strings, etc), or the env. var. is not set, use all available cores, as in ""local[*]""; */. private static void determineSparkMaster() {; int foo = 0;; try {; foo = Integer.parseInt( System.getenv(""GATK_TEST_SPARK_CORES"") );; } catch ( NumberFormatException e ) {}; String numSparkCores;; if ( foo > 0 ) {; numSparkCores = String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:766,Modifiability,variab,variable,766,"@akiezun @lbergelson Changed the Spark context configuration from ""local[*]"" to ""local[N]"", where N is specified by a environmental variable. Ran gradle test with ""--tests _SparkIntegration_"". Out of 203 tests, one failed: "" testBulkFragmentsNoDuplicates"", the rest passed. Here is the snippet of code change. Any suggestions?. ```; private static JavaSparkContext createTestSparkContext(Map<String, String> overridingProperties) {; determineSparkMaster();; final SparkConf sparkConf = setupSparkConf(""TestContext"", DEFAULT_SPARK_MASTER, DEFAULT_TEST_PROPERTIES, overridingProperties);; return new JavaSparkContext(sparkConf);; }. /**; * Determine the number of cores Spark master should use. Only used in Spark Test; * Read the specification from the environmental variable GATK_TEST_SPARK_CORES; * If the value is a valid positive integer, use it; * If the value is bogus (strings, etc), or the env. var. is not set, use all available cores, as in ""local[*]""; */. private static void determineSparkMaster() {; int foo = 0;; try {; foo = Integer.parseInt( System.getenv(""GATK_TEST_SPARK_CORES"") );; } catch ( NumberFormatException e ) {}; String numSparkCores;; if ( foo > 0 ) {; numSparkCores = String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:5554,Performance,concurren,concurrent,5554,internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:5661,Performance,concurren,concurrent,5661,internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:5745,Performance,concurren,concurrent,5745,internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:5829,Performance,concurren,concurrent,5829,internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:153,Testability,test,test,153,"@akiezun @lbergelson Changed the Spark context configuration from ""local[*]"" to ""local[N]"", where N is specified by a environmental variable. Ran gradle test with ""--tests _SparkIntegration_"". Out of 203 tests, one failed: "" testBulkFragmentsNoDuplicates"", the rest passed. Here is the snippet of code change. Any suggestions?. ```; private static JavaSparkContext createTestSparkContext(Map<String, String> overridingProperties) {; determineSparkMaster();; final SparkConf sparkConf = setupSparkConf(""TestContext"", DEFAULT_SPARK_MASTER, DEFAULT_TEST_PROPERTIES, overridingProperties);; return new JavaSparkContext(sparkConf);; }. /**; * Determine the number of cores Spark master should use. Only used in Spark Test; * Read the specification from the environmental variable GATK_TEST_SPARK_CORES; * If the value is a valid positive integer, use it; * If the value is bogus (strings, etc), or the env. var. is not set, use all available cores, as in ""local[*]""; */. private static void determineSparkMaster() {; int foo = 0;; try {; foo = Integer.parseInt( System.getenv(""GATK_TEST_SPARK_CORES"") );; } catch ( NumberFormatException e ) {}; String numSparkCores;; if ( foo > 0 ) {; numSparkCores = String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:166,Testability,test,tests,166,"@akiezun @lbergelson Changed the Spark context configuration from ""local[*]"" to ""local[N]"", where N is specified by a environmental variable. Ran gradle test with ""--tests _SparkIntegration_"". Out of 203 tests, one failed: "" testBulkFragmentsNoDuplicates"", the rest passed. Here is the snippet of code change. Any suggestions?. ```; private static JavaSparkContext createTestSparkContext(Map<String, String> overridingProperties) {; determineSparkMaster();; final SparkConf sparkConf = setupSparkConf(""TestContext"", DEFAULT_SPARK_MASTER, DEFAULT_TEST_PROPERTIES, overridingProperties);; return new JavaSparkContext(sparkConf);; }. /**; * Determine the number of cores Spark master should use. Only used in Spark Test; * Read the specification from the environmental variable GATK_TEST_SPARK_CORES; * If the value is a valid positive integer, use it; * If the value is bogus (strings, etc), or the env. var. is not set, use all available cores, as in ""local[*]""; */. private static void determineSparkMaster() {; int foo = 0;; try {; foo = Integer.parseInt( System.getenv(""GATK_TEST_SPARK_CORES"") );; } catch ( NumberFormatException e ) {}; String numSparkCores;; if ( foo > 0 ) {; numSparkCores = String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:204,Testability,test,tests,204,"@akiezun @lbergelson Changed the Spark context configuration from ""local[*]"" to ""local[N]"", where N is specified by a environmental variable. Ran gradle test with ""--tests _SparkIntegration_"". Out of 203 tests, one failed: "" testBulkFragmentsNoDuplicates"", the rest passed. Here is the snippet of code change. Any suggestions?. ```; private static JavaSparkContext createTestSparkContext(Map<String, String> overridingProperties) {; determineSparkMaster();; final SparkConf sparkConf = setupSparkConf(""TestContext"", DEFAULT_SPARK_MASTER, DEFAULT_TEST_PROPERTIES, overridingProperties);; return new JavaSparkContext(sparkConf);; }. /**; * Determine the number of cores Spark master should use. Only used in Spark Test; * Read the specification from the environmental variable GATK_TEST_SPARK_CORES; * If the value is a valid positive integer, use it; * If the value is bogus (strings, etc), or the env. var. is not set, use all available cores, as in ""local[*]""; */. private static void determineSparkMaster() {; int foo = 0;; try {; foo = Integer.parseInt( System.getenv(""GATK_TEST_SPARK_CORES"") );; } catch ( NumberFormatException e ) {}; String numSparkCores;; if ( foo > 0 ) {; numSparkCores = String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:225,Testability,test,testBulkFragmentsNoDuplicates,225,"@akiezun @lbergelson Changed the Spark context configuration from ""local[*]"" to ""local[N]"", where N is specified by a environmental variable. Ran gradle test with ""--tests _SparkIntegration_"". Out of 203 tests, one failed: "" testBulkFragmentsNoDuplicates"", the rest passed. Here is the snippet of code change. Any suggestions?. ```; private static JavaSparkContext createTestSparkContext(Map<String, String> overridingProperties) {; determineSparkMaster();; final SparkConf sparkConf = setupSparkConf(""TestContext"", DEFAULT_SPARK_MASTER, DEFAULT_TEST_PROPERTIES, overridingProperties);; return new JavaSparkContext(sparkConf);; }. /**; * Determine the number of cores Spark master should use. Only used in Spark Test; * Read the specification from the environmental variable GATK_TEST_SPARK_CORES; * If the value is a valid positive integer, use it; * If the value is bogus (strings, etc), or the env. var. is not set, use all available cores, as in ""local[*]""; */. private static void determineSparkMaster() {; int foo = 0;; try {; foo = Integer.parseInt( System.getenv(""GATK_TEST_SPARK_CORES"") );; } catch ( NumberFormatException e ) {}; String numSparkCores;; if ( foo > 0 ) {; numSparkCores = String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:502,Testability,Test,TestContext,502,"@akiezun @lbergelson Changed the Spark context configuration from ""local[*]"" to ""local[N]"", where N is specified by a environmental variable. Ran gradle test with ""--tests _SparkIntegration_"". Out of 203 tests, one failed: "" testBulkFragmentsNoDuplicates"", the rest passed. Here is the snippet of code change. Any suggestions?. ```; private static JavaSparkContext createTestSparkContext(Map<String, String> overridingProperties) {; determineSparkMaster();; final SparkConf sparkConf = setupSparkConf(""TestContext"", DEFAULT_SPARK_MASTER, DEFAULT_TEST_PROPERTIES, overridingProperties);; return new JavaSparkContext(sparkConf);; }. /**; * Determine the number of cores Spark master should use. Only used in Spark Test; * Read the specification from the environmental variable GATK_TEST_SPARK_CORES; * If the value is a valid positive integer, use it; * If the value is bogus (strings, etc), or the env. var. is not set, use all available cores, as in ""local[*]""; */. private static void determineSparkMaster() {; int foo = 0;; try {; foo = Integer.parseInt( System.getenv(""GATK_TEST_SPARK_CORES"") );; } catch ( NumberFormatException e ) {}; String numSparkCores;; if ( foo > 0 ) {; numSparkCores = String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:712,Testability,Test,Test,712,"@akiezun @lbergelson Changed the Spark context configuration from ""local[*]"" to ""local[N]"", where N is specified by a environmental variable. Ran gradle test with ""--tests _SparkIntegration_"". Out of 203 tests, one failed: "" testBulkFragmentsNoDuplicates"", the rest passed. Here is the snippet of code change. Any suggestions?. ```; private static JavaSparkContext createTestSparkContext(Map<String, String> overridingProperties) {; determineSparkMaster();; final SparkConf sparkConf = setupSparkConf(""TestContext"", DEFAULT_SPARK_MASTER, DEFAULT_TEST_PROPERTIES, overridingProperties);; return new JavaSparkContext(sparkConf);; }. /**; * Determine the number of cores Spark master should use. Only used in Spark Test; * Read the specification from the environmental variable GATK_TEST_SPARK_CORES; * If the value is a valid positive integer, use it; * If the value is bogus (strings, etc), or the env. var. is not set, use all available cores, as in ""local[*]""; */. private static void determineSparkMaster() {; int foo = 0;; try {; foo = Integer.parseInt( System.getenv(""GATK_TEST_SPARK_CORES"") );; } catch ( NumberFormatException e ) {}; String numSparkCores;; if ( foo > 0 ) {; numSparkCores = String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:2188,Testability,test,test,2188,"ores = String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:2193,Testability,test,testers,2193,"String.format(""[%d]"", foo);; } else {; numSparkCores = ""[*]"";; }; DEFAULT_SPARK_MASTER = ""local"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.pri",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:2287,Testability,test,test,2287,"cal"" + numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:2292,Testability,test,testers,2292,numSparkCores;; }. ```. Error messages:. ```; java.lang.NullPointerException at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.Suit,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:2345,Testability,test,testBulkFragmentsNoDuplicates,2345,oadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:77); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:2727,Testability,test,testng,2727,line.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.ru,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:2819,Testability,test,testng,2819,institute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:2882,Testability,test,testng,2882,mmandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runT,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:2949,Testability,test,testng,2949,nstanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3018,Testability,test,testng,3018,anceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassPr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3034,Testability,Test,TestMethodWorker,3034,); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3069,Testability,Test,TestMethodWorker,3069,ellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.ta,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3104,Testability,test,testng,3104,mandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.sto,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3120,Testability,Test,TestMethodWorker,3120,rogramTest.java:65); at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProce,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3141,Testability,Test,TestMethodWorker,3141,; at org.broadinstitute.hellbender.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3176,Testability,test,testng,3176,er.utils.test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorI,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3183,Testability,Test,TestRunner,3183,test.testers.SamFileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invok,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3205,Testability,Test,TestRunner,3205,ileTester.runTest(SamFileTester.java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3234,Testability,test,testng,3234,java:263); at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAcc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3241,Testability,Test,TestRunner,3241,; at org.broadinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3256,Testability,Test,TestRunner,3256,dinstitute.hellbender.utils.test.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(Nativ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3285,Testability,test,testng,3285,est.testers.AbstractMarkDuplicatesCommandLineProgramTest.testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3342,Testability,test,testng,3342,testBulkFragmentsNoDuplicates(AbstractMarkDuplicatesCommandLineProgramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(Deleg,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3407,Testability,test,testng,3407,ogramTest.java:460); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.inv,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3467,Testability,test,testng,3467,.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.Refle,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3520,Testability,test,testng,3520,ccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3589,Testability,test,testng,3589,DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(Reflectio,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3653,Testability,test,testng,3653,.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClas,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3660,Testability,Test,TestNG,3660,3); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoader,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3689,Testability,Test,TestNG,3689,eflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3715,Testability,test,testng,3715,; at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3722,Testability,Test,TestNG,3722,g.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3746,Testability,Test,TestNG,3746,l.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messagin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3772,Testability,test,testng,3772,hod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$Dis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3779,Testability,Test,TestNG,3779,hodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$Dispatchin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3790,Testability,Test,TestNG,3790,tionHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocati,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3842,Testability,test,testing,3842,okeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3850,Testability,test,testng,3850,od(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.pro,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3857,Testability,Test,TestNGTestClassProcessor,3857,639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3891,Testability,Test,TestNGTestClassProcessor,3891,nvoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3960,Testability,test,testing,3960,.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3968,Testability,test,testng,3968,r.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:3975,Testability,Test,TestNGTestClassProcessor,3975,ods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:4005,Testability,Test,TestNGTestClassProcessor,4005,rg.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.in,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:4073,Testability,test,testing,4073,odWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:352); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:310); at org.testng.SuiteRunner.run(SuiteRunner.java:259); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:133); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:4920,Testability,test,testing,4920,); at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:4935,Testability,Test,TestWorker,4935,.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1768:4951,Testability,Test,TestWorker,4951,internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:83); at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); at com.sun.proxy.$Proxy2.stop(Unknown Source); at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360); at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1768
https://github.com/broadinstitute/gatk/issues/1769:163,Safety,safe,safe,163,"Travis test suites runtimes have been creeping upwards in both gatk and gatk-protected, and have now reached painful levels. Let's parallelize the test suite in a safe way that won't require us to make all test classes thread-safe. In the past, we successfully parallelized by test class, with one process per test class, and aggregated the results at the end.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1769
https://github.com/broadinstitute/gatk/issues/1769:226,Safety,safe,safe,226,"Travis test suites runtimes have been creeping upwards in both gatk and gatk-protected, and have now reached painful levels. Let's parallelize the test suite in a safe way that won't require us to make all test classes thread-safe. In the past, we successfully parallelized by test class, with one process per test class, and aggregated the results at the end.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1769
https://github.com/broadinstitute/gatk/issues/1769:7,Testability,test,test,7,"Travis test suites runtimes have been creeping upwards in both gatk and gatk-protected, and have now reached painful levels. Let's parallelize the test suite in a safe way that won't require us to make all test classes thread-safe. In the past, we successfully parallelized by test class, with one process per test class, and aggregated the results at the end.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1769
https://github.com/broadinstitute/gatk/issues/1769:147,Testability,test,test,147,"Travis test suites runtimes have been creeping upwards in both gatk and gatk-protected, and have now reached painful levels. Let's parallelize the test suite in a safe way that won't require us to make all test classes thread-safe. In the past, we successfully parallelized by test class, with one process per test class, and aggregated the results at the end.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1769
https://github.com/broadinstitute/gatk/issues/1769:206,Testability,test,test,206,"Travis test suites runtimes have been creeping upwards in both gatk and gatk-protected, and have now reached painful levels. Let's parallelize the test suite in a safe way that won't require us to make all test classes thread-safe. In the past, we successfully parallelized by test class, with one process per test class, and aggregated the results at the end.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1769
https://github.com/broadinstitute/gatk/issues/1769:277,Testability,test,test,277,"Travis test suites runtimes have been creeping upwards in both gatk and gatk-protected, and have now reached painful levels. Let's parallelize the test suite in a safe way that won't require us to make all test classes thread-safe. In the past, we successfully parallelized by test class, with one process per test class, and aggregated the results at the end.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1769
https://github.com/broadinstitute/gatk/issues/1769:310,Testability,test,test,310,"Travis test suites runtimes have been creeping upwards in both gatk and gatk-protected, and have now reached painful levels. Let's parallelize the test suite in a safe way that won't require us to make all test classes thread-safe. In the past, we successfully parallelized by test class, with one process per test class, and aggregated the results at the end.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1769
https://github.com/broadinstitute/gatk/issues/1771:588,Availability,failure,failure,588,"We recently determined that the FTZ setting gets cleared during integration tests for unknown reasons. We temporarily fixed this by explicitly turning FTZ on in every call to `jniComputeLikelihoods()` (https://github.com/broadinstitute/gatk/pull/1764), but this might be inefficient, and even if it isn't it would be good to understand what's going on. Without the fix in https://github.com/broadinstitute/gatk/pull/1764, if you run `HaplotypeCallerIntegrationTest`, the ""consistent with past results"" tests will either succeed or fail depending on whether they run first or not, and the failure is definitely due to FTZ somehow getting unset between tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771
https://github.com/broadinstitute/gatk/issues/1771:64,Deployability,integrat,integration,64,"We recently determined that the FTZ setting gets cleared during integration tests for unknown reasons. We temporarily fixed this by explicitly turning FTZ on in every call to `jniComputeLikelihoods()` (https://github.com/broadinstitute/gatk/pull/1764), but this might be inefficient, and even if it isn't it would be good to understand what's going on. Without the fix in https://github.com/broadinstitute/gatk/pull/1764, if you run `HaplotypeCallerIntegrationTest`, the ""consistent with past results"" tests will either succeed or fail depending on whether they run first or not, and the failure is definitely due to FTZ somehow getting unset between tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771
https://github.com/broadinstitute/gatk/issues/1771:64,Integrability,integrat,integration,64,"We recently determined that the FTZ setting gets cleared during integration tests for unknown reasons. We temporarily fixed this by explicitly turning FTZ on in every call to `jniComputeLikelihoods()` (https://github.com/broadinstitute/gatk/pull/1764), but this might be inefficient, and even if it isn't it would be good to understand what's going on. Without the fix in https://github.com/broadinstitute/gatk/pull/1764, if you run `HaplotypeCallerIntegrationTest`, the ""consistent with past results"" tests will either succeed or fail depending on whether they run first or not, and the failure is definitely due to FTZ somehow getting unset between tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771
https://github.com/broadinstitute/gatk/issues/1771:536,Integrability,depend,depending,536,"We recently determined that the FTZ setting gets cleared during integration tests for unknown reasons. We temporarily fixed this by explicitly turning FTZ on in every call to `jniComputeLikelihoods()` (https://github.com/broadinstitute/gatk/pull/1764), but this might be inefficient, and even if it isn't it would be good to understand what's going on. Without the fix in https://github.com/broadinstitute/gatk/pull/1764, if you run `HaplotypeCallerIntegrationTest`, the ""consistent with past results"" tests will either succeed or fail depending on whether they run first or not, and the failure is definitely due to FTZ somehow getting unset between tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771
https://github.com/broadinstitute/gatk/issues/1771:76,Testability,test,tests,76,"We recently determined that the FTZ setting gets cleared during integration tests for unknown reasons. We temporarily fixed this by explicitly turning FTZ on in every call to `jniComputeLikelihoods()` (https://github.com/broadinstitute/gatk/pull/1764), but this might be inefficient, and even if it isn't it would be good to understand what's going on. Without the fix in https://github.com/broadinstitute/gatk/pull/1764, if you run `HaplotypeCallerIntegrationTest`, the ""consistent with past results"" tests will either succeed or fail depending on whether they run first or not, and the failure is definitely due to FTZ somehow getting unset between tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771
https://github.com/broadinstitute/gatk/issues/1771:502,Testability,test,tests,502,"We recently determined that the FTZ setting gets cleared during integration tests for unknown reasons. We temporarily fixed this by explicitly turning FTZ on in every call to `jniComputeLikelihoods()` (https://github.com/broadinstitute/gatk/pull/1764), but this might be inefficient, and even if it isn't it would be good to understand what's going on. Without the fix in https://github.com/broadinstitute/gatk/pull/1764, if you run `HaplotypeCallerIntegrationTest`, the ""consistent with past results"" tests will either succeed or fail depending on whether they run first or not, and the failure is definitely due to FTZ somehow getting unset between tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771
https://github.com/broadinstitute/gatk/issues/1771:651,Testability,test,tests,651,"We recently determined that the FTZ setting gets cleared during integration tests for unknown reasons. We temporarily fixed this by explicitly turning FTZ on in every call to `jniComputeLikelihoods()` (https://github.com/broadinstitute/gatk/pull/1764), but this might be inefficient, and even if it isn't it would be good to understand what's going on. Without the fix in https://github.com/broadinstitute/gatk/pull/1764, if you run `HaplotypeCallerIntegrationTest`, the ""consistent with past results"" tests will either succeed or fail depending on whether they run first or not, and the failure is definitely due to FTZ somehow getting unset between tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771
https://github.com/broadinstitute/gatk/issues/1771:49,Usability,clear,cleared,49,"We recently determined that the FTZ setting gets cleared during integration tests for unknown reasons. We temporarily fixed this by explicitly turning FTZ on in every call to `jniComputeLikelihoods()` (https://github.com/broadinstitute/gatk/pull/1764), but this might be inefficient, and even if it isn't it would be good to understand what's going on. Without the fix in https://github.com/broadinstitute/gatk/pull/1764, if you run `HaplotypeCallerIntegrationTest`, the ""consistent with past results"" tests will either succeed or fail depending on whether they run first or not, and the failure is definitely due to FTZ somehow getting unset between tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771
https://github.com/broadinstitute/gatk/pull/1772:46,Availability,avail,available,46,packaging gatk-launch in our jar so that it's available to downstream projects; some build.gradle refactoring,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1772
https://github.com/broadinstitute/gatk/pull/1772:59,Availability,down,downstream,59,packaging gatk-launch in our jar so that it's available to downstream projects; some build.gradle refactoring,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1772
https://github.com/broadinstitute/gatk/pull/1772:98,Modifiability,refactor,refactoring,98,packaging gatk-launch in our jar so that it's available to downstream projects; some build.gradle refactoring,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1772
https://github.com/broadinstitute/gatk/pull/1776:77,Modifiability,variab,variable,77,"1. User can define the number of Spark cores in gradle test by environmental variable GATK_TEST_SPARK_CORES. If the variable is not defined, or the value is bogus, will fall back to default of ""local[*]""; 2. Skip intelDeflator test on PPC platforms.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1776
https://github.com/broadinstitute/gatk/pull/1776:116,Modifiability,variab,variable,116,"1. User can define the number of Spark cores in gradle test by environmental variable GATK_TEST_SPARK_CORES. If the variable is not defined, or the value is bogus, will fall back to default of ""local[*]""; 2. Skip intelDeflator test on PPC platforms.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1776
https://github.com/broadinstitute/gatk/pull/1776:55,Testability,test,test,55,"1. User can define the number of Spark cores in gradle test by environmental variable GATK_TEST_SPARK_CORES. If the variable is not defined, or the value is bogus, will fall back to default of ""local[*]""; 2. Skip intelDeflator test on PPC platforms.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1776
https://github.com/broadinstitute/gatk/pull/1776:227,Testability,test,test,227,"1. User can define the number of Spark cores in gradle test by environmental variable GATK_TEST_SPARK_CORES. If the variable is not defined, or the value is bogus, will fall back to default of ""local[*]""; 2. Skip intelDeflator test on PPC platforms.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1776
https://github.com/broadinstitute/gatk/issues/1777:204,Testability,test,test,204,"CRAMInputFormat doesn't support several of the new features in BAMInputFormat, specifically:; - interval filtering https://github.com/HadoopGenomics/Hadoop-BAM/pull/59; - keep read pairs together (should test queryname sorted CRAM in Hadoop-BAM) https://github.com/HadoopGenomics/Hadoop-BAM/pull/49; - we should also test this issue on CRAM https://github.com/HadoopGenomics/Hadoop-BAM/pull/80",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1777
https://github.com/broadinstitute/gatk/issues/1777:317,Testability,test,test,317,"CRAMInputFormat doesn't support several of the new features in BAMInputFormat, specifically:; - interval filtering https://github.com/HadoopGenomics/Hadoop-BAM/pull/59; - keep read pairs together (should test queryname sorted CRAM in Hadoop-BAM) https://github.com/HadoopGenomics/Hadoop-BAM/pull/49; - we should also test this issue on CRAM https://github.com/HadoopGenomics/Hadoop-BAM/pull/80",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1777
https://github.com/broadinstitute/gatk/issues/1779:0,Deployability,Update,Update,0,"Update the gradle distribution commands to include the sparkJar, gatk-launch, and settings.gradle in the zipped archive so we can have a single command to build a distributable zip.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1779
https://github.com/broadinstitute/gatk/issues/1780:242,Availability,ERROR,ERROR,242,```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```. fails with . ```; 16/04/27 18:49:12 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:279,Availability,Error,Error,279,```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```. fails with . ```; 16/04/27 18:49:12 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:3797,Availability,ERROR,ERROR,3797,r.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/04/27 18:49:12 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1231); at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229); at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:5995,Availability,down,down,5995,"PostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:9643,Availability,ERROR,ERROR,9643," at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [828b3d22-2109-4128-b4af-427a9d410db0] entered state [ERROR] while waiting for [DONE].; ```. Likely because of . ```; ""--conf"", ""spark.yarn.dist.files="" + script + ""/build/libIntelDeflater.so"",; ```. in gatk-launch. This likely needs special handling for dataproc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:9744,Availability,ERROR,ERROR,9744," at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [828b3d22-2109-4128-b4af-427a9d410db0] entered state [ERROR] while waiting for [DONE].; ```. Likely because of . ```; ""--conf"", ""spark.yarn.dist.files="" + script + ""/build/libIntelDeflater.so"",; ```. in gatk-launch. This likely needs special handling for dataproc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:937,Deployability,deploy,deploy,937,```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```. fails with . ```; 16/04/27 18:49:12 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:1012,Deployability,deploy,deploy,1012,nch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```. fails with . ```; 16/04/27 18:49:12 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.clus,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:1048,Deployability,deploy,deploy,1048,resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```. fails with . ```; 16/04/27 18:49:12 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnCli,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:1120,Deployability,deploy,deploy,1120,t -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```. fails with . ```; 16/04/27 18:49:12 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.Tas,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:1234,Deployability,deploy,deploy,1234,9:12 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:1505,Deployability,deploy,deploy,1505,alFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSpa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:1602,Deployability,deploy,deploy,1602,ocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkComman,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:1759,Deployability,deploy,deploy,1759,FileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:1839,Deployability,deploy,deploy,1839,eUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:1926,Deployability,deploy,deploy,1926,e.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProg,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:3410,Deployability,deploy,deploy,3410,ntextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/04/27 18:49:12 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1231); at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229); at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); at ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:3447,Deployability,deploy,deploy,3447,.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/04/27 18:49:12 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1231); at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229); at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); at org.apache.spark.api.java.JavaSparkContext.<i,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:3519,Deployability,deploy,deploy,3519,rkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/04/27 18:49:12 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1231); at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229); at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbende,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:3595,Deployability,deploy,deploy,3595,rkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/04/27 18:49:12 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1231); at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229); at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:3666,Deployability,deploy,deploy,3666,oadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/04/27 18:49:12 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1231); at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229); at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFact,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:3735,Deployability,deploy,deploy,3735,eProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/04/27 18:49:12 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1231); at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229); at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:5581,Deployability,deploy,deploy,5581,"ntextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:5618,Deployability,deploy,deploy,5618,".hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFile",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:5690,Deployability,deploy,deploy,5690,"rkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileU",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:5766,Deployability,deploy,deploy,5766,"rkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:5837,Deployability,deploy,deploy,5837,"oadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Clie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:5906,Deployability,deploy,deploy,5906,"eProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:6082,Deployability,pipeline,pipelines,6082,"ommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:6801,Deployability,deploy,deploy,6801,".scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Cli",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:6876,Deployability,deploy,deploy,6876,":121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.clus",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:6912,Deployability,deploy,deploy,6912,"t.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnCli",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:6984,Deployability,deploy,deploy,6984,"hutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.Tas",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:7098,Deployability,deploy,deploy,7098,sSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:7369,Deployability,deploy,deploy,7369,alFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSpa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:7466,Deployability,deploy,deploy,7466,ocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkComman,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:7623,Deployability,deploy,deploy,7623,FileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:7703,Deployability,deploy,deploy,7703,eUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:7790,Deployability,deploy,deploy,7790,e.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProg,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:9274,Deployability,deploy,deploy,9274," at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [828b3d22-2109-4128-b4af-427a9d410db0] entered state [ERROR] while waiting for [DONE].; ```. Likely because of . ```; ""--conf"", ""spark.yarn.dist.files="" + script + ""/build/libIntelDeflater.so"",; ```. in gatk-launch. This likely needs special handling for dataproc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:9311,Deployability,deploy,deploy,9311," at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [828b3d22-2109-4128-b4af-427a9d410db0] entered state [ERROR] while waiting for [DONE].; ```. Likely because of . ```; ""--conf"", ""spark.yarn.dist.files="" + script + ""/build/libIntelDeflater.so"",; ```. in gatk-launch. This likely needs special handling for dataproc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:9383,Deployability,deploy,deploy,9383," at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [828b3d22-2109-4128-b4af-427a9d410db0] entered state [ERROR] while waiting for [DONE].; ```. Likely because of . ```; ""--conf"", ""spark.yarn.dist.files="" + script + ""/build/libIntelDeflater.so"",; ```. in gatk-launch. This likely needs special handling for dataproc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:9459,Deployability,deploy,deploy,9459," at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [828b3d22-2109-4128-b4af-427a9d410db0] entered state [ERROR] while waiting for [DONE].; ```. Likely because of . ```; ""--conf"", ""spark.yarn.dist.files="" + script + ""/build/libIntelDeflater.so"",; ```. in gatk-launch. This likely needs special handling for dataproc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:9530,Deployability,deploy,deploy,9530," at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [828b3d22-2109-4128-b4af-427a9d410db0] entered state [ERROR] while waiting for [DONE].; ```. Likely because of . ```; ""--conf"", ""spark.yarn.dist.files="" + script + ""/build/libIntelDeflater.so"",; ```. in gatk-launch. This likely needs special handling for dataproc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:9599,Deployability,deploy,deploy,9599," at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [828b3d22-2109-4128-b4af-427a9d410db0] entered state [ERROR] while waiting for [DONE].; ```. Likely because of . ```; ""--conf"", ""spark.yarn.dist.files="" + script + ""/build/libIntelDeflater.so"",; ```. in gatk-launch. This likely needs special handling for dataproc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:2002,Energy Efficiency,schedul,scheduler,2002,park.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbend,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:2111,Energy Efficiency,schedul,scheduler,2111,ark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:7866,Energy Efficiency,schedul,scheduler,7866,park.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbend,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:7975,Energy Efficiency,schedul,scheduler,7975,ark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/issues/1780:54,Testability,test,test,54,```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```. fails with . ```; 16/04/27 18:49:12 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780
https://github.com/broadinstitute/gatk/pull/1781:32,Testability,test,test,32,"adding cram version of our main test bam. @lbergelson, ok?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1781
https://github.com/broadinstitute/gatk/issues/1783:782,Integrability,message,message,782,"`./gatk-launch BuildBamIndex -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram`. blows up with a bogus exception. ```; htsjdk.samtools.SAMException: Input file must be bam file, not sam file.; at org.broadinstitute.hellbender.tools.picard.sam.BuildBamIndex.doWork(BuildBamIndex.java:101); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:61); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); ```. desired outcome: message that using files that are not in BAM format is not supported. No exception!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1783
https://github.com/broadinstitute/gatk/issues/1783:36,Testability,test,test,36,"`./gatk-launch BuildBamIndex -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram`. blows up with a bogus exception. ```; htsjdk.samtools.SAMException: Input file must be bam file, not sam file.; at org.broadinstitute.hellbender.tools.picard.sam.BuildBamIndex.doWork(BuildBamIndex.java:101); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:61); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); ```. desired outcome: message that using files that are not in BAM format is not supported. No exception!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1783
https://github.com/broadinstitute/gatk/pull/1785:628,Deployability,pipeline,pipelines,628,fixes #1784 and #1780 which both caused problems with the intel deflater. note: this incorporates the changes in https://github.com/broadinstitute/gatk/pull/1717 because without them it fails with. ```; java.lang.UnsupportedOperationException: Directory size listing not supported on GCS.; at org.broadinstitute.hellbender.utils.gcs.BucketUtils.dirSize(BucketUtils.java:301); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getRecommendedNumReducers(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:237); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1785
https://github.com/broadinstitute/gatk/pull/1785:1646,Deployability,deploy,deploy,1646,d on GCS.; at org.broadinstitute.hellbender.utils.gcs.BucketUtils.dirSize(BucketUtils.java:301); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getRecommendedNumReducers(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:237); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. example commandline:. ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output --apiKey $HELLBENDER_TEST_APIKEY -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1785
https://github.com/broadinstitute/gatk/pull/1785:1683,Deployability,deploy,deploy,1683,d on GCS.; at org.broadinstitute.hellbender.utils.gcs.BucketUtils.dirSize(BucketUtils.java:301); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getRecommendedNumReducers(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:237); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. example commandline:. ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output --apiKey $HELLBENDER_TEST_APIKEY -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1785
https://github.com/broadinstitute/gatk/pull/1785:1755,Deployability,deploy,deploy,1755,d on GCS.; at org.broadinstitute.hellbender.utils.gcs.BucketUtils.dirSize(BucketUtils.java:301); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getRecommendedNumReducers(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:237); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. example commandline:. ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output --apiKey $HELLBENDER_TEST_APIKEY -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1785
https://github.com/broadinstitute/gatk/pull/1785:1831,Deployability,deploy,deploy,1831,d on GCS.; at org.broadinstitute.hellbender.utils.gcs.BucketUtils.dirSize(BucketUtils.java:301); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getRecommendedNumReducers(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:237); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. example commandline:. ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output --apiKey $HELLBENDER_TEST_APIKEY -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1785
https://github.com/broadinstitute/gatk/pull/1785:1902,Deployability,deploy,deploy,1902,d on GCS.; at org.broadinstitute.hellbender.utils.gcs.BucketUtils.dirSize(BucketUtils.java:301); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getRecommendedNumReducers(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:237); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. example commandline:. ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output --apiKey $HELLBENDER_TEST_APIKEY -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1785
https://github.com/broadinstitute/gatk/pull/1785:1971,Deployability,deploy,deploy,1971,d on GCS.; at org.broadinstitute.hellbender.utils.gcs.BucketUtils.dirSize(BucketUtils.java:301); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getRecommendedNumReducers(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:237); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. example commandline:. ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output --apiKey $HELLBENDER_TEST_APIKEY -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1785
https://github.com/broadinstitute/gatk/pull/1785:2096,Testability,test,test,2096,d on GCS.; at org.broadinstitute.hellbender.utils.gcs.BucketUtils.dirSize(BucketUtils.java:301); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getRecommendedNumReducers(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:237); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. example commandline:. ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output --apiKey $HELLBENDER_TEST_APIKEY -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1785
https://github.com/broadinstitute/gatk/issues/1787:228,Testability,test,test,228,CountReads . CRAM is 17x slower here! (in user time) . using interval list from `ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/2.8/b37/Broad.human.exome.b37.interval_list.gz`. ```; time ./gatk-launch CountReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram -L Broad.human.exome.b37.interval_list -R src/test/resources/large/human_g1k_v37.20.21.fasta; real 1m12.498s; user 1m23.140s; ```. ```; time ./gatk-launch CountReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L Broad.human.exome.b37.interval_list; real 0m2.178s; user 0m4.853s; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1787
https://github.com/broadinstitute/gatk/issues/1787:336,Testability,test,test,336,CountReads . CRAM is 17x slower here! (in user time) . using interval list from `ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/2.8/b37/Broad.human.exome.b37.interval_list.gz`. ```; time ./gatk-launch CountReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram -L Broad.human.exome.b37.interval_list -R src/test/resources/large/human_g1k_v37.20.21.fasta; real 1m12.498s; user 1m23.140s; ```. ```; time ./gatk-launch CountReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L Broad.human.exome.b37.interval_list; real 0m2.178s; user 0m4.853s; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1787
https://github.com/broadinstitute/gatk/issues/1787:463,Testability,test,test,463,CountReads . CRAM is 17x slower here! (in user time) . using interval list from `ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/2.8/b37/Broad.human.exome.b37.interval_list.gz`. ```; time ./gatk-launch CountReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram -L Broad.human.exome.b37.interval_list -R src/test/resources/large/human_g1k_v37.20.21.fasta; real 1m12.498s; user 1m23.140s; ```. ```; time ./gatk-launch CountReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -L Broad.human.exome.b37.interval_list; real 0m2.178s; user 0m4.853s; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1787
https://github.com/broadinstitute/gatk/pull/1790:45,Deployability,release,releases,45,"this is necessary for sonatype to accept our releases, I forgot to commit it last time i did a release",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1790
https://github.com/broadinstitute/gatk/pull/1790:95,Deployability,release,release,95,"this is necessary for sonatype to accept our releases, I forgot to commit it last time i did a release",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1790
https://github.com/broadinstitute/gatk/issues/1791:16,Deployability,release,release,16,"Currently every release is a ""snapshot"" unless you manually change a flag in the build script. It should automatically detect ""snapshot status"" from the version.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1791
https://github.com/broadinstitute/gatk/issues/1791:119,Safety,detect,detect,119,"Currently every release is a ""snapshot"" unless you manually change a flag in the build script. It should automatically detect ""snapshot status"" from the version.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1791
https://github.com/broadinstitute/gatk/pull/1794:39,Security,access,accessing,39,PileupElement: save memory and time by accessing bases and quals directly without copying. Showed up on profile in HC,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1794
https://github.com/broadinstitute/gatk/pull/1795:54,Energy Efficiency,allocate,allocated,54,those came up clearly on HaplotypeCaller profiles and allocated many megabytes of objects for now reason,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1795
https://github.com/broadinstitute/gatk/pull/1795:14,Usability,clear,clearly,14,those came up clearly on HaplotypeCaller profiles and allocated many megabytes of objects for now reason,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1795
https://github.com/broadinstitute/gatk/issues/1799:212,Testability,test,test,212,The GATK3 v3.5-0-g36282e4 commandlines and numbers:. running on gsa5 (has AVX). VCF mode on 32GB of ram. ```; time java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta --out a.gatk3.vcf; ...; real 1m52.247s; user 2m32.789s; ```. Note:The VCF mode is not what production is running so it's lower priority than GVCF,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1799
https://github.com/broadinstitute/gatk/issues/1799:280,Testability,test,test,280,The GATK3 v3.5-0-g36282e4 commandlines and numbers:. running on gsa5 (has AVX). VCF mode on 32GB of ram. ```; time java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta --out a.gatk3.vcf; ...; real 1m52.247s; user 2m32.789s; ```. Note:The VCF mode is not what production is running so it's lower priority than GVCF,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1799
https://github.com/broadinstitute/gatk/issues/1800:213,Testability,test,test,213,The GATK3 v3.5-0-g36282e4 commandlines and numbers:. running on gsa5 (has AVX). GVCF mode on 32GB of ram. ```; time java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --out a.gatk3.g.vcf; ...; real 3m48.076s; user 8m45.049s; ```. GVCF mode on 10GB of ram. ```; time java -Xmx10g -Xms10g -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --out a.gatk3.g.vcf; ...; real 3m39.496s; user 10m16.387s; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1800
https://github.com/broadinstitute/gatk/issues/1800:281,Testability,test,test,281,The GATK3 v3.5-0-g36282e4 commandlines and numbers:. running on gsa5 (has AVX). GVCF mode on 32GB of ram. ```; time java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --out a.gatk3.g.vcf; ...; real 3m48.076s; user 8m45.049s; ```. GVCF mode on 10GB of ram. ```; time java -Xmx10g -Xms10g -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --out a.gatk3.g.vcf; ...; real 3m39.496s; user 10m16.387s; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1800
https://github.com/broadinstitute/gatk/issues/1800:550,Testability,test,test,550,The GATK3 v3.5-0-g36282e4 commandlines and numbers:. running on gsa5 (has AVX). GVCF mode on 32GB of ram. ```; time java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --out a.gatk3.g.vcf; ...; real 3m48.076s; user 8m45.049s; ```. GVCF mode on 10GB of ram. ```; time java -Xmx10g -Xms10g -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --out a.gatk3.g.vcf; ...; real 3m39.496s; user 10m16.387s; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1800
https://github.com/broadinstitute/gatk/issues/1800:618,Testability,test,test,618,The GATK3 v3.5-0-g36282e4 commandlines and numbers:. running on gsa5 (has AVX). GVCF mode on 32GB of ram. ```; time java -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --out a.gatk3.g.vcf; ...; real 3m48.076s; user 8m45.049s; ```. GVCF mode on 10GB of ram. ```; time java -Xmx10g -Xms10g -jar /humgen/gsa-hpprojects/GATK/bin/current/GenomeAnalysisTK.jar -T HaplotypeCaller -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta -ERC GVCF --out a.gatk3.g.vcf; ...; real 3m39.496s; user 10m16.387s; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1800
https://github.com/broadinstitute/gatk/issues/1801:157,Integrability,interface,interface,157,"Initial version should consist of:; - A superinterface called `NativeLibrary` that has `getLibraryPath()` and `isSupported()` methods.; - A `PairHmmBinding` interface (name open to negotiation!) that extends `NativeLibrary` and has signatures for `jniComputeLikelihoods()` and other PairHmm JNI methods. Once created, we need to publish a jar on maven for this repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1801
https://github.com/broadinstitute/gatk/issues/1801:200,Modifiability,extend,extends,200,"Initial version should consist of:; - A superinterface called `NativeLibrary` that has `getLibraryPath()` and `isSupported()` methods.; - A `PairHmmBinding` interface (name open to negotiation!) that extends `NativeLibrary` and has signatures for `jniComputeLikelihoods()` and other PairHmm JNI methods. Once created, we need to publish a jar on maven for this repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1801
https://github.com/broadinstitute/gatk/issues/1802:118,Testability,test,test,118,this used to work (just 2 weeks ago when https://github.com/broadinstitute/gatk/pull/1719 was merged). ```; ./gradlew test --tests *VariantFiltrationIntegrationTest*; ```. now i get:. ```; > No tests found for given includes: [*VariantFiltrationIntegrationTest*]; ```. very annoying,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1802
https://github.com/broadinstitute/gatk/issues/1802:125,Testability,test,tests,125,this used to work (just 2 weeks ago when https://github.com/broadinstitute/gatk/pull/1719 was merged). ```; ./gradlew test --tests *VariantFiltrationIntegrationTest*; ```. now i get:. ```; > No tests found for given includes: [*VariantFiltrationIntegrationTest*]; ```. very annoying,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1802
https://github.com/broadinstitute/gatk/issues/1802:194,Testability,test,tests,194,this used to work (just 2 weeks ago when https://github.com/broadinstitute/gatk/pull/1719 was merged). ```; ./gradlew test --tests *VariantFiltrationIntegrationTest*; ```. now i get:. ```; > No tests found for given includes: [*VariantFiltrationIntegrationTest*]; ```. very annoying,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1802
https://github.com/broadinstitute/gatk/pull/1803:218,Integrability,depend,dependency,218,"The CollectInsertSizeMetrics test cram file used to be zero-length, and was recently regenerated (most likely with Picard since it has BAM contents) but either way its STILL not a CRAM file. Also removed a stray junit dependency that crept in to CompareBaseQualities.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1803
https://github.com/broadinstitute/gatk/pull/1803:29,Testability,test,test,29,"The CollectInsertSizeMetrics test cram file used to be zero-length, and was recently regenerated (most likely with Picard since it has BAM contents) but either way its STILL not a CRAM file. Also removed a stray junit dependency that crept in to CompareBaseQualities.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1803
https://github.com/broadinstitute/gatk/issues/1808:22,Energy Efficiency,power,power,22,"To test GATK on IBM's power systems we need to run an automated test suite off-travis (they do not have power support). We have an account at http://osuosl.org/ - @droazen has an account. Ideally, this would run on all pushes to all branches (+ add a badge to our repo) but we can start with a nightly build of master",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1808
https://github.com/broadinstitute/gatk/issues/1808:104,Energy Efficiency,power,power,104,"To test GATK on IBM's power systems we need to run an automated test suite off-travis (they do not have power support). We have an account at http://osuosl.org/ - @droazen has an account. Ideally, this would run on all pushes to all branches (+ add a badge to our repo) but we can start with a nightly build of master",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1808
https://github.com/broadinstitute/gatk/issues/1808:3,Testability,test,test,3,"To test GATK on IBM's power systems we need to run an automated test suite off-travis (they do not have power support). We have an account at http://osuosl.org/ - @droazen has an account. Ideally, this would run on all pushes to all branches (+ add a badge to our repo) but we can start with a nightly build of master",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1808
https://github.com/broadinstitute/gatk/issues/1808:64,Testability,test,test,64,"To test GATK on IBM's power systems we need to run an automated test suite off-travis (they do not have power support). We have an account at http://osuosl.org/ - @droazen has an account. Ideally, this would run on all pushes to all branches (+ add a badge to our repo) but we can start with a nightly build of master",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1808
https://github.com/broadinstitute/gatk/issues/1809:172,Deployability,pipeline,pipeline,172,"Experiment with different algorithms and settings for compression of shuffle files (ie not only algorithm but buffer sizes, file sizes etc). Use MarkDuplicatesSpark as the pipeline and run on broad cluster and cloud cluster. Remember to try ""no compression"" too.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1809
https://github.com/broadinstitute/gatk/issues/1811:120,Performance,cache,cache,120,"as it is now, we read the data twice in MarkDuplicatesSpark because we filter it using predicate `p` and `!p`. We could cache the RDD but it's not clear if it's faster than just re-reading.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1811
https://github.com/broadinstitute/gatk/issues/1811:147,Usability,clear,clear,147,"as it is now, we read the data twice in MarkDuplicatesSpark because we filter it using predicate `p` and `!p`. We could cache the RDD but it's not clear if it's faster than just re-reading.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1811
https://github.com/broadinstitute/gatk/issues/1812:141,Performance,perform,performance,141,https://github.com/mengyao/complete-striped-smith-waterman-library; I've successfully built and run it on linux and mac. Need to investigate performance and usability for us. see also #1629,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1812
https://github.com/broadinstitute/gatk/issues/1812:157,Usability,usab,usability,157,https://github.com/mengyao/complete-striped-smith-waterman-library; I've successfully built and run it on linux and mac. Need to investigate performance and usability for us. see also #1629,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1812
https://github.com/broadinstitute/gatk/pull/1813:431,Availability,avail,available,431,"Changes to enable multi-threaded native AVX PairHMM using OpenMP. Also includes a performance improvement in the native C++ `Context` class. `VectorLoglessPairHMM.java` is hardcoded to set the maximum number of PairHMM threads (`maxNumberOfThreads`) to 100. This is the maximum number of threads **allowed** by GATK, not the number of threads **requested**. C code in the native library will query OpenMP for the number of threads available on the platform, and use min(OpenMP threads available, `maxNumberOfThreads`) threads. **Measured Speedup**; Command. ```; ./gatk-launch HaplotypeCaller -R src/test/resources/large/human_g1k_v37.20.21.fasta -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.g.vcf -ERC GVCF; ```. 1 thread; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 36.882098080000006; 2 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 18.160468659000003; 3 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 12.541517043; 4 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 9.727374342000001. **Potential issues**; - The target platform running GATK must have OpenMP installed; - The code has not been tested on Mac. **Todo**; - New Java code to allow the user to specify `maxNumberOfThreads` variable in `VectorLoglessPairHMM.java`.; - Move `maxNumberOfThreads` to the native `initialize` function, once we migrate to the new native library.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813
https://github.com/broadinstitute/gatk/pull/1813:485,Availability,avail,available,485,"Changes to enable multi-threaded native AVX PairHMM using OpenMP. Also includes a performance improvement in the native C++ `Context` class. `VectorLoglessPairHMM.java` is hardcoded to set the maximum number of PairHMM threads (`maxNumberOfThreads`) to 100. This is the maximum number of threads **allowed** by GATK, not the number of threads **requested**. C code in the native library will query OpenMP for the number of threads available on the platform, and use min(OpenMP threads available, `maxNumberOfThreads`) threads. **Measured Speedup**; Command. ```; ./gatk-launch HaplotypeCaller -R src/test/resources/large/human_g1k_v37.20.21.fasta -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.g.vcf -ERC GVCF; ```. 1 thread; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 36.882098080000006; 2 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 18.160468659000003; 3 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 12.541517043; 4 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 9.727374342000001. **Potential issues**; - The target platform running GATK must have OpenMP installed; - The code has not been tested on Mac. **Todo**; - New Java code to allow the user to specify `maxNumberOfThreads` variable in `VectorLoglessPairHMM.java`.; - Move `maxNumberOfThreads` to the native `initialize` function, once we migrate to the new native library.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813
https://github.com/broadinstitute/gatk/pull/1813:1218,Deployability,install,installed,1218,"Changes to enable multi-threaded native AVX PairHMM using OpenMP. Also includes a performance improvement in the native C++ `Context` class. `VectorLoglessPairHMM.java` is hardcoded to set the maximum number of PairHMM threads (`maxNumberOfThreads`) to 100. This is the maximum number of threads **allowed** by GATK, not the number of threads **requested**. C code in the native library will query OpenMP for the number of threads available on the platform, and use min(OpenMP threads available, `maxNumberOfThreads`) threads. **Measured Speedup**; Command. ```; ./gatk-launch HaplotypeCaller -R src/test/resources/large/human_g1k_v37.20.21.fasta -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.g.vcf -ERC GVCF; ```. 1 thread; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 36.882098080000006; 2 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 18.160468659000003; 3 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 12.541517043; 4 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 9.727374342000001. **Potential issues**; - The target platform running GATK must have OpenMP installed; - The code has not been tested on Mac. **Todo**; - New Java code to allow the user to specify `maxNumberOfThreads` variable in `VectorLoglessPairHMM.java`.; - Move `maxNumberOfThreads` to the native `initialize` function, once we migrate to the new native library.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813
https://github.com/broadinstitute/gatk/pull/1813:1344,Modifiability,variab,variable,1344,"Changes to enable multi-threaded native AVX PairHMM using OpenMP. Also includes a performance improvement in the native C++ `Context` class. `VectorLoglessPairHMM.java` is hardcoded to set the maximum number of PairHMM threads (`maxNumberOfThreads`) to 100. This is the maximum number of threads **allowed** by GATK, not the number of threads **requested**. C code in the native library will query OpenMP for the number of threads available on the platform, and use min(OpenMP threads available, `maxNumberOfThreads`) threads. **Measured Speedup**; Command. ```; ./gatk-launch HaplotypeCaller -R src/test/resources/large/human_g1k_v37.20.21.fasta -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.g.vcf -ERC GVCF; ```. 1 thread; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 36.882098080000006; 2 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 18.160468659000003; 3 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 12.541517043; 4 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 9.727374342000001. **Potential issues**; - The target platform running GATK must have OpenMP installed; - The code has not been tested on Mac. **Todo**; - New Java code to allow the user to specify `maxNumberOfThreads` variable in `VectorLoglessPairHMM.java`.; - Move `maxNumberOfThreads` to the native `initialize` function, once we migrate to the new native library.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813
https://github.com/broadinstitute/gatk/pull/1813:18,Performance,multi-thread,multi-threaded,18,"Changes to enable multi-threaded native AVX PairHMM using OpenMP. Also includes a performance improvement in the native C++ `Context` class. `VectorLoglessPairHMM.java` is hardcoded to set the maximum number of PairHMM threads (`maxNumberOfThreads`) to 100. This is the maximum number of threads **allowed** by GATK, not the number of threads **requested**. C code in the native library will query OpenMP for the number of threads available on the platform, and use min(OpenMP threads available, `maxNumberOfThreads`) threads. **Measured Speedup**; Command. ```; ./gatk-launch HaplotypeCaller -R src/test/resources/large/human_g1k_v37.20.21.fasta -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.g.vcf -ERC GVCF; ```. 1 thread; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 36.882098080000006; 2 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 18.160468659000003; 3 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 12.541517043; 4 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 9.727374342000001. **Potential issues**; - The target platform running GATK must have OpenMP installed; - The code has not been tested on Mac. **Todo**; - New Java code to allow the user to specify `maxNumberOfThreads` variable in `VectorLoglessPairHMM.java`.; - Move `maxNumberOfThreads` to the native `initialize` function, once we migrate to the new native library.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813
https://github.com/broadinstitute/gatk/pull/1813:82,Performance,perform,performance,82,"Changes to enable multi-threaded native AVX PairHMM using OpenMP. Also includes a performance improvement in the native C++ `Context` class. `VectorLoglessPairHMM.java` is hardcoded to set the maximum number of PairHMM threads (`maxNumberOfThreads`) to 100. This is the maximum number of threads **allowed** by GATK, not the number of threads **requested**. C code in the native library will query OpenMP for the number of threads available on the platform, and use min(OpenMP threads available, `maxNumberOfThreads`) threads. **Measured Speedup**; Command. ```; ./gatk-launch HaplotypeCaller -R src/test/resources/large/human_g1k_v37.20.21.fasta -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.g.vcf -ERC GVCF; ```. 1 thread; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 36.882098080000006; 2 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 18.160468659000003; 3 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 12.541517043; 4 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 9.727374342000001. **Potential issues**; - The target platform running GATK must have OpenMP installed; - The code has not been tested on Mac. **Todo**; - New Java code to allow the user to specify `maxNumberOfThreads` variable in `VectorLoglessPairHMM.java`.; - Move `maxNumberOfThreads` to the native `initialize` function, once we migrate to the new native library.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813
https://github.com/broadinstitute/gatk/pull/1813:600,Testability,test,test,600,"Changes to enable multi-threaded native AVX PairHMM using OpenMP. Also includes a performance improvement in the native C++ `Context` class. `VectorLoglessPairHMM.java` is hardcoded to set the maximum number of PairHMM threads (`maxNumberOfThreads`) to 100. This is the maximum number of threads **allowed** by GATK, not the number of threads **requested**. C code in the native library will query OpenMP for the number of threads available on the platform, and use min(OpenMP threads available, `maxNumberOfThreads`) threads. **Measured Speedup**; Command. ```; ./gatk-launch HaplotypeCaller -R src/test/resources/large/human_g1k_v37.20.21.fasta -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.g.vcf -ERC GVCF; ```. 1 thread; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 36.882098080000006; 2 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 18.160468659000003; 3 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 12.541517043; 4 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 9.727374342000001. **Potential issues**; - The target platform running GATK must have OpenMP installed; - The code has not been tested on Mac. **Todo**; - New Java code to allow the user to specify `maxNumberOfThreads` variable in `VectorLoglessPairHMM.java`.; - Move `maxNumberOfThreads` to the native `initialize` function, once we migrate to the new native library.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813
https://github.com/broadinstitute/gatk/pull/1813:654,Testability,test,test,654,"Changes to enable multi-threaded native AVX PairHMM using OpenMP. Also includes a performance improvement in the native C++ `Context` class. `VectorLoglessPairHMM.java` is hardcoded to set the maximum number of PairHMM threads (`maxNumberOfThreads`) to 100. This is the maximum number of threads **allowed** by GATK, not the number of threads **requested**. C code in the native library will query OpenMP for the number of threads available on the platform, and use min(OpenMP threads available, `maxNumberOfThreads`) threads. **Measured Speedup**; Command. ```; ./gatk-launch HaplotypeCaller -R src/test/resources/large/human_g1k_v37.20.21.fasta -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.g.vcf -ERC GVCF; ```. 1 thread; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 36.882098080000006; 2 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 18.160468659000003; 3 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 12.541517043; 4 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 9.727374342000001. **Potential issues**; - The target platform running GATK must have OpenMP installed; - The code has not been tested on Mac. **Todo**; - New Java code to allow the user to specify `maxNumberOfThreads` variable in `VectorLoglessPairHMM.java`.; - Move `maxNumberOfThreads` to the native `initialize` function, once we migrate to the new native library.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813
https://github.com/broadinstitute/gatk/pull/1813:1253,Testability,test,tested,1253,"Changes to enable multi-threaded native AVX PairHMM using OpenMP. Also includes a performance improvement in the native C++ `Context` class. `VectorLoglessPairHMM.java` is hardcoded to set the maximum number of PairHMM threads (`maxNumberOfThreads`) to 100. This is the maximum number of threads **allowed** by GATK, not the number of threads **requested**. C code in the native library will query OpenMP for the number of threads available on the platform, and use min(OpenMP threads available, `maxNumberOfThreads`) threads. **Measured Speedup**; Command. ```; ./gatk-launch HaplotypeCaller -R src/test/resources/large/human_g1k_v37.20.21.fasta -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.g.vcf -ERC GVCF; ```. 1 thread; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 36.882098080000006; 2 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 18.160468659000003; 3 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 12.541517043; 4 threads; INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 9.727374342000001. **Potential issues**; - The target platform running GATK must have OpenMP installed; - The code has not been tested on Mac. **Todo**; - New Java code to allow the user to specify `maxNumberOfThreads` variable in `VectorLoglessPairHMM.java`.; - Move `maxNumberOfThreads` to the native `initialize` function, once we migrate to the new native library.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1813
https://github.com/broadinstitute/gatk/pull/1814:39,Testability,test,tests,39,"adding ""spark"" tag to additional spark tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1814
https://github.com/broadinstitute/gatk/issues/1815:48,Availability,down,down,48,"11:05:38.056 INFO CountVariantsSpark - Shutting down engine; [May 12, 2016 11:05:38 AM AST] org.broadinstitute.hellbender.tools.spark.pipelines.CountVariantsSpark done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=3114270720; htsjdk.tribble.TribbleException: Input stream does not contain a BCF encoded file; BCF magic header info not found, at record 0 with position 0:; at htsjdk.variant.bcf2.BCF2Codec.error(BCF2Codec.java:492); at htsjdk.variant.bcf2.BCF2Codec.readHeader(BCF2Codec.java:153); at org.seqdoop.hadoop_bam.BCFSplitGuesser.<init>(BCFSplitGuesser.java:109); at org.seqdoop.hadoop_bam.BCFSplitGuesser.<init>(BCFSplitGuesser.java:89); at org.seqdoop.hadoop_bam.VCFInputFormat.addGuessedSplits(VCFInputFormat.java:254); at org.seqdoop.hadoop_bam.VCFInputFormat.fixBCFSplits(VCFInputFormat.java:242); at org.seqdoop.hadoop_bam.VCFInputFormat.getSplits(VCFInputFormat.java:221); at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:95); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1910); at org.apache.spark.rdd.RDD.count(RDD.scala:1121); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:445); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:47); at org.broadinstitute.hellbender.tools.spark.pipelines.CountVariantsSpark.runTool(CountVariantsSpark.java:39); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1815
https://github.com/broadinstitute/gatk/issues/1815:411,Availability,error,error,411,"11:05:38.056 INFO CountVariantsSpark - Shutting down engine; [May 12, 2016 11:05:38 AM AST] org.broadinstitute.hellbender.tools.spark.pipelines.CountVariantsSpark done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=3114270720; htsjdk.tribble.TribbleException: Input stream does not contain a BCF encoded file; BCF magic header info not found, at record 0 with position 0:; at htsjdk.variant.bcf2.BCF2Codec.error(BCF2Codec.java:492); at htsjdk.variant.bcf2.BCF2Codec.readHeader(BCF2Codec.java:153); at org.seqdoop.hadoop_bam.BCFSplitGuesser.<init>(BCFSplitGuesser.java:109); at org.seqdoop.hadoop_bam.BCFSplitGuesser.<init>(BCFSplitGuesser.java:89); at org.seqdoop.hadoop_bam.VCFInputFormat.addGuessedSplits(VCFInputFormat.java:254); at org.seqdoop.hadoop_bam.VCFInputFormat.fixBCFSplits(VCFInputFormat.java:242); at org.seqdoop.hadoop_bam.VCFInputFormat.getSplits(VCFInputFormat.java:221); at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:95); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1910); at org.apache.spark.rdd.RDD.count(RDD.scala:1121); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:445); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:47); at org.broadinstitute.hellbender.tools.spark.pipelines.CountVariantsSpark.runTool(CountVariantsSpark.java:39); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1815
https://github.com/broadinstitute/gatk/issues/1815:134,Deployability,pipeline,pipelines,134,"11:05:38.056 INFO CountVariantsSpark - Shutting down engine; [May 12, 2016 11:05:38 AM AST] org.broadinstitute.hellbender.tools.spark.pipelines.CountVariantsSpark done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=3114270720; htsjdk.tribble.TribbleException: Input stream does not contain a BCF encoded file; BCF magic header info not found, at record 0 with position 0:; at htsjdk.variant.bcf2.BCF2Codec.error(BCF2Codec.java:492); at htsjdk.variant.bcf2.BCF2Codec.readHeader(BCF2Codec.java:153); at org.seqdoop.hadoop_bam.BCFSplitGuesser.<init>(BCFSplitGuesser.java:109); at org.seqdoop.hadoop_bam.BCFSplitGuesser.<init>(BCFSplitGuesser.java:89); at org.seqdoop.hadoop_bam.VCFInputFormat.addGuessedSplits(VCFInputFormat.java:254); at org.seqdoop.hadoop_bam.VCFInputFormat.fixBCFSplits(VCFInputFormat.java:242); at org.seqdoop.hadoop_bam.VCFInputFormat.getSplits(VCFInputFormat.java:221); at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:95); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1910); at org.apache.spark.rdd.RDD.count(RDD.scala:1121); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:445); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:47); at org.broadinstitute.hellbender.tools.spark.pipelines.CountVariantsSpark.runTool(CountVariantsSpark.java:39); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1815
https://github.com/broadinstitute/gatk/issues/1815:1858,Deployability,pipeline,pipelines,1858,a:109); at org.seqdoop.hadoop_bam.BCFSplitGuesser.<init>(BCFSplitGuesser.java:89); at org.seqdoop.hadoop_bam.VCFInputFormat.addGuessedSplits(VCFInputFormat.java:254); at org.seqdoop.hadoop_bam.VCFInputFormat.fixBCFSplits(VCFInputFormat.java:242); at org.seqdoop.hadoop_bam.VCFInputFormat.getSplits(VCFInputFormat.java:221); at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:95); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1910); at org.apache.spark.rdd.RDD.count(RDD.scala:1121); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:445); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:47); at org.broadinstitute.hellbender.tools.spark.pipelines.CountVariantsSpark.runTool(CountVariantsSpark.java:39); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:69); at org.broadinstitute.hellbender.Main.main(Main.java:84),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1815
https://github.com/broadinstitute/gatk/issues/1816:65,Testability,test,test,65,can we set up travis on the Mac? If we support the Mac we should test it automatically. @lbergelson is it doable/hard?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1816
https://github.com/broadinstitute/gatk/pull/1817:30,Deployability,release,released,30,"Hadoop-BAM 7.5.0 has not been released yet, so this will fail.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1817
https://github.com/broadinstitute/gatk/issues/1819:48,Availability,avail,available,48,"What is the current behavior when OpenMP is not available or is the wrong version on a Linux system? If the answer is ""it blows up with a gross error"", then let's patch the code so that it gracefully falls back to the single-threaded version, as on a Mac.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1819
https://github.com/broadinstitute/gatk/issues/1819:144,Availability,error,error,144,"What is the current behavior when OpenMP is not available or is the wrong version on a Linux system? If the answer is ""it blows up with a gross error"", then let's patch the code so that it gracefully falls back to the single-threaded version, as on a Mac.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1819
https://github.com/broadinstitute/gatk/issues/1819:163,Deployability,patch,patch,163,"What is the current behavior when OpenMP is not available or is the wrong version on a Linux system? If the answer is ""it blows up with a gross error"", then let's patch the code so that it gracefully falls back to the single-threaded version, as on a Mac.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1819
https://github.com/broadinstitute/gatk/issues/1820:13,Testability,test,tests,13,see disabled tests like `testBQSRBucket(BQSRTest params)` in `BaseRecalibratorSparkIntegrationTest.java` - they fail to read data from buckets,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1820
https://github.com/broadinstitute/gatk/issues/1820:25,Testability,test,testBQSRBucket,25,see disabled tests like `testBQSRBucket(BQSRTest params)` in `BaseRecalibratorSparkIntegrationTest.java` - they fail to read data from buckets,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1820
https://github.com/broadinstitute/gatk/pull/1825:823,Deployability,integrat,integration,823,"this is the initial port of the Allele Specific annotation for HaplotypeCaller. It mostly focuses on the GVCF mode (ie outputs the 'raw' data). I have a branch in protected https://github.com/broadinstitute/gatk-protected/tree/ak_haplotypecaller_allele_specific_annotations that uses those and I verified that the annotations are correctly output and their values are much closer that before to those from GATK3.5. I did not port any code related to combining the annotations in GenotypeGVCFs or CombinedGVCFs etc. Also, no code for VariantAnnotator or UnifiedGenotyper was ported - gatk4 does not have those tools right now. @droazen can you review? Sorry, this is a whole bunch of code and it's not the final version yet (in particular, little effort was put into redesigning the framework - that will wait until we have integration tests so we can keep the results stable while improving design and code). We also need to add tickets to:; - turn dithering off/on in RankSum tests (it's always off for now to simplify testing); - use AlleleSpecific annotations in the VCF mode; - (later) port code for combining annotations",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1825
https://github.com/broadinstitute/gatk/pull/1825:823,Integrability,integrat,integration,823,"this is the initial port of the Allele Specific annotation for HaplotypeCaller. It mostly focuses on the GVCF mode (ie outputs the 'raw' data). I have a branch in protected https://github.com/broadinstitute/gatk-protected/tree/ak_haplotypecaller_allele_specific_annotations that uses those and I verified that the annotations are correctly output and their values are much closer that before to those from GATK3.5. I did not port any code related to combining the annotations in GenotypeGVCFs or CombinedGVCFs etc. Also, no code for VariantAnnotator or UnifiedGenotyper was ported - gatk4 does not have those tools right now. @droazen can you review? Sorry, this is a whole bunch of code and it's not the final version yet (in particular, little effort was put into redesigning the framework - that will wait until we have integration tests so we can keep the results stable while improving design and code). We also need to add tickets to:; - turn dithering off/on in RankSum tests (it's always off for now to simplify testing); - use AlleleSpecific annotations in the VCF mode; - (later) port code for combining annotations",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1825
https://github.com/broadinstitute/gatk/pull/1825:835,Testability,test,tests,835,"this is the initial port of the Allele Specific annotation for HaplotypeCaller. It mostly focuses on the GVCF mode (ie outputs the 'raw' data). I have a branch in protected https://github.com/broadinstitute/gatk-protected/tree/ak_haplotypecaller_allele_specific_annotations that uses those and I verified that the annotations are correctly output and their values are much closer that before to those from GATK3.5. I did not port any code related to combining the annotations in GenotypeGVCFs or CombinedGVCFs etc. Also, no code for VariantAnnotator or UnifiedGenotyper was ported - gatk4 does not have those tools right now. @droazen can you review? Sorry, this is a whole bunch of code and it's not the final version yet (in particular, little effort was put into redesigning the framework - that will wait until we have integration tests so we can keep the results stable while improving design and code). We also need to add tickets to:; - turn dithering off/on in RankSum tests (it's always off for now to simplify testing); - use AlleleSpecific annotations in the VCF mode; - (later) port code for combining annotations",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1825
https://github.com/broadinstitute/gatk/pull/1825:977,Testability,test,tests,977,"this is the initial port of the Allele Specific annotation for HaplotypeCaller. It mostly focuses on the GVCF mode (ie outputs the 'raw' data). I have a branch in protected https://github.com/broadinstitute/gatk-protected/tree/ak_haplotypecaller_allele_specific_annotations that uses those and I verified that the annotations are correctly output and their values are much closer that before to those from GATK3.5. I did not port any code related to combining the annotations in GenotypeGVCFs or CombinedGVCFs etc. Also, no code for VariantAnnotator or UnifiedGenotyper was ported - gatk4 does not have those tools right now. @droazen can you review? Sorry, this is a whole bunch of code and it's not the final version yet (in particular, little effort was put into redesigning the framework - that will wait until we have integration tests so we can keep the results stable while improving design and code). We also need to add tickets to:; - turn dithering off/on in RankSum tests (it's always off for now to simplify testing); - use AlleleSpecific annotations in the VCF mode; - (later) port code for combining annotations",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1825
https://github.com/broadinstitute/gatk/pull/1825:1020,Testability,test,testing,1020,"this is the initial port of the Allele Specific annotation for HaplotypeCaller. It mostly focuses on the GVCF mode (ie outputs the 'raw' data). I have a branch in protected https://github.com/broadinstitute/gatk-protected/tree/ak_haplotypecaller_allele_specific_annotations that uses those and I verified that the annotations are correctly output and their values are much closer that before to those from GATK3.5. I did not port any code related to combining the annotations in GenotypeGVCFs or CombinedGVCFs etc. Also, no code for VariantAnnotator or UnifiedGenotyper was ported - gatk4 does not have those tools right now. @droazen can you review? Sorry, this is a whole bunch of code and it's not the final version yet (in particular, little effort was put into redesigning the framework - that will wait until we have integration tests so we can keep the results stable while improving design and code). We also need to add tickets to:; - turn dithering off/on in RankSum tests (it's always off for now to simplify testing); - use AlleleSpecific annotations in the VCF mode; - (later) port code for combining annotations",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1825
https://github.com/broadinstitute/gatk/pull/1825:1011,Usability,simpl,simplify,1011,"this is the initial port of the Allele Specific annotation for HaplotypeCaller. It mostly focuses on the GVCF mode (ie outputs the 'raw' data). I have a branch in protected https://github.com/broadinstitute/gatk-protected/tree/ak_haplotypecaller_allele_specific_annotations that uses those and I verified that the annotations are correctly output and their values are much closer that before to those from GATK3.5. I did not port any code related to combining the annotations in GenotypeGVCFs or CombinedGVCFs etc. Also, no code for VariantAnnotator or UnifiedGenotyper was ported - gatk4 does not have those tools right now. @droazen can you review? Sorry, this is a whole bunch of code and it's not the final version yet (in particular, little effort was put into redesigning the framework - that will wait until we have integration tests so we can keep the results stable while improving design and code). We also need to add tickets to:; - turn dithering off/on in RankSum tests (it's always off for now to simplify testing); - use AlleleSpecific annotations in the VCF mode; - (later) port code for combining annotations",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1825
https://github.com/broadinstitute/gatk/pull/1827:10,Availability,checkpoint,checkpoint,10,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesn’t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827
https://github.com/broadinstitute/gatk/pull/1827:207,Deployability,pipeline,pipelined,207,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesn’t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827
https://github.com/broadinstitute/gatk/pull/1827:428,Energy Efficiency,efficient,efficient,428,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesn’t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827
https://github.com/broadinstitute/gatk/pull/1827:446,Energy Efficiency,reduce,reduce,446,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesn’t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827
https://github.com/broadinstitute/gatk/pull/1827:537,Integrability,interface,interface,537,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesn’t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827
https://github.com/broadinstitute/gatk/pull/1827:807,Integrability,interface,interfaces,807,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesn’t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827
https://github.com/broadinstitute/gatk/pull/1827:922,Integrability,interface,interfaces,922,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesn’t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827
https://github.com/broadinstitute/gatk/pull/1827:1185,Integrability,interface,interfaces,1185,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesn’t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827
https://github.com/broadinstitute/gatk/pull/1827:1504,Integrability,interface,interfaces,1504,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesn’t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827
https://github.com/broadinstitute/gatk/pull/1827:161,Modifiability,refactor,refactoring,161,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesn’t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827
https://github.com/broadinstitute/gatk/pull/1827:343,Modifiability,extend,extend,343,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesn’t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827
https://github.com/broadinstitute/gatk/pull/1827:1095,Modifiability,refactor,refactoring,1095,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesn’t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827
https://github.com/broadinstitute/gatk/pull/1827:1515,Modifiability,evolve,evolve,1515,"This is a checkpoint PR for https://github.com/broadinstitute/gatk/issues/1237 and https://github.com/broadinstitute/gatk/issues/1643. This is the first step in refactoring metrics collectors so they can be pipelined in Spark and reuse RDDs, but still share metrics computation code between walker and Spark versions. The next step will be to extend MultilevelCollector to be able to merge its own instances in order to support efficient map and reduce phases for multi level collectors. Suggested review order:. -MetricsCollectorSpark: interface to be implemented by all Spark collectors; -MetricsArgs:base class for all collector argument sets; -MetricsCollectorToolSpark: base class for all Spark metrics collector tools; -CollectQualityYieldMetrics: Spark version of QualityYieldMetrics using these new interfaces; -CollectInsertSizeMetricsSpark: existing Spark version of InsertSizeMetrics collector ported; to these interfaces; -CollectMultipleMetricsSpark: Spark version of CollectMultipleMetrics; currently only works; on QualityYieldMetrics and InsertSizeMetrics. The rest of the PR is refactoring existing to get QualityYieldMetrics and InsertSizeMetrics to conform to these interfaces (moving CollectInsertSizeMetrics out of the sv package and Program Groups, etc.). Note that the existing InsertSizeMetrics Spark collector doesn’t really share code with the walker; version (and their command line param sets are way out of sync) but this should be fixed separately from these changes as the interfaces evolve.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1827
https://github.com/broadinstitute/gatk/pull/1828:396,Deployability,upgrade,upgrades,396,"by a quirk of travis this includes pull request builds, so if you need a snapshot you can open a pull request and the head of it will be build into a snapshot on artifactory. the version identifier should will appear in the output of the ""after_sucess"" block on travis; this will only package the linux version of the pairHmm library, to package both versions you must do a manual snapshot. also upgrades gradle to 2.13 to use the new findProperty function. DON'T MERGE UNTIL THE NEW ARTIFACT IS CONFIRMED TO BE IN ARTIFACTORY FOR THIS PR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1828
https://github.com/broadinstitute/gatk/pull/1833:74,Usability,simpl,simple,74,In #1774 I added to LIBS the possibility to keep reads with Ns. This is a simple commit to allow `LocusWalker` implementations to use this behaviour (switch off by default).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1833
https://github.com/broadinstitute/gatk/pull/1834:65,Availability,avail,available,65,"@tomwhite can you review? We have CHD 5.7 running now and 1.6 is available on the cloud so no reason to not upgrade AFAIK. For some reason, the lists returned from `.collect` are no longer mutable so i have to make copies in 1 test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1834
https://github.com/broadinstitute/gatk/pull/1834:108,Deployability,upgrade,upgrade,108,"@tomwhite can you review? We have CHD 5.7 running now and 1.6 is available on the cloud so no reason to not upgrade AFAIK. For some reason, the lists returned from `.collect` are no longer mutable so i have to make copies in 1 test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1834
https://github.com/broadinstitute/gatk/pull/1834:227,Testability,test,test,227,"@tomwhite can you review? We have CHD 5.7 running now and 1.6 is available on the cloud so no reason to not upgrade AFAIK. For some reason, the lists returned from `.collect` are no longer mutable so i have to make copies in 1 test",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1834
https://github.com/broadinstitute/gatk/pull/1835:24,Testability,test,test,24,fixes #1506 : re-enable test for blocked vcfs on spark. @tomwhite can you have a look?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1835
https://github.com/broadinstitute/gatk/pull/1836:72,Testability,test,tests,72,Porting Pileup (https://github.com/broadinstitute/gatk/issues/1581) and tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1836
https://github.com/broadinstitute/gatk/issues/1839:233,Deployability,pipeline,pipelines,233,"latest master 4353d54fd2d64ea1b4c8429986f83eb873a4d687. `/gatk-launch CountReadsSpark -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam`. ```; [May 18, 2016 5:10:55 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=318242816; java.net.BindException: Failed to bind to: /10.1.5.39:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1839
https://github.com/broadinstitute/gatk/issues/1839:854,Performance,concurren,concurrent,854,"latest master 4353d54fd2d64ea1b4c8429986f83eb873a4d687. `/gatk-launch CountReadsSpark -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam`. ```; [May 18, 2016 5:10:55 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=318242816; java.net.BindException: Failed to bind to: /10.1.5.39:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1839
https://github.com/broadinstitute/gatk/issues/1839:921,Performance,concurren,concurrent,921,"latest master 4353d54fd2d64ea1b4c8429986f83eb873a4d687. `/gatk-launch CountReadsSpark -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam`. ```; [May 18, 2016 5:10:55 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=318242816; java.net.BindException: Failed to bind to: /10.1.5.39:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1839
https://github.com/broadinstitute/gatk/issues/1839:988,Performance,concurren,concurrent,988,"latest master 4353d54fd2d64ea1b4c8429986f83eb873a4d687. `/gatk-launch CountReadsSpark -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam`. ```; [May 18, 2016 5:10:55 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=318242816; java.net.BindException: Failed to bind to: /10.1.5.39:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1839
https://github.com/broadinstitute/gatk/issues/1839:1443,Performance,concurren,concurrent,1443,".b37.NA12878.20.21.bam`. ```; [May 18, 2016 5:10:55 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=318242816; java.net.BindException: Failed to bind to: /10.1.5.39:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. we should make it not fail or fail more gracefully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1839
https://github.com/broadinstitute/gatk/issues/1839:1764,Performance,concurren,concurrent,1764,".b37.NA12878.20.21.bam`. ```; [May 18, 2016 5:10:55 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=318242816; java.net.BindException: Failed to bind to: /10.1.5.39:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. we should make it not fail or fail more gracefully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1839
https://github.com/broadinstitute/gatk/issues/1839:1837,Performance,concurren,concurrent,1837,".b37.NA12878.20.21.bam`. ```; [May 18, 2016 5:10:55 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=318242816; java.net.BindException: Failed to bind to: /10.1.5.39:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. we should make it not fail or fail more gracefully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1839
https://github.com/broadinstitute/gatk/issues/1839:1922,Performance,concurren,concurrent,1922,".b37.NA12878.20.21.bam`. ```; [May 18, 2016 5:10:55 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=318242816; java.net.BindException: Failed to bind to: /10.1.5.39:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. we should make it not fail or fail more gracefully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1839
https://github.com/broadinstitute/gatk/issues/1839:1999,Performance,concurren,concurrent,1999,".b37.NA12878.20.21.bam`. ```; [May 18, 2016 5:10:55 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=318242816; java.net.BindException: Failed to bind to: /10.1.5.39:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. we should make it not fail or fail more gracefully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1839
https://github.com/broadinstitute/gatk/issues/1839:93,Testability,test,test,93,"latest master 4353d54fd2d64ea1b4c8429986f83eb873a4d687. `/gatk-launch CountReadsSpark -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam`. ```; [May 18, 2016 5:10:55 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=318242816; java.net.BindException: Failed to bind to: /10.1.5.39:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1839
https://github.com/broadinstitute/gatk/issues/1841:5,Availability,error,error,5,"```; error: unmappable character for encoding ASCII; * @author Daniel G??mez-S??nchez (magicDGS); ```. affected files: IntervalOverlappingIterator.java:17, LocusWalker.java:27, ExampleLocusWalker.java:22. @magicDGS can you fix this?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1841
https://github.com/broadinstitute/gatk/issues/1843:267,Testability,test,test,267,"BaseRecalibratorIntegrationTest ; `{new BQSRTest(GRCh37Ref_chr2021, trickyBam_chr20, dbSNPb37_chr20, ""-indelBQSR "", getResourceDir() + ""expected.CEUTrio.HiSeq.WGS.b37.ch20.4379150-4379157.recal.txt"")},`. the bam file and the vcf file have no overlap which makes this test less useful than it should be",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1843
https://github.com/broadinstitute/gatk/pull/1844:42,Energy Efficiency,reduce,reduce,42,using linked or sorted data structures to reduce our dependency on the arbitrary changes in iterator orders (happens everytime you move from a jvm to another jvm),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1844
https://github.com/broadinstitute/gatk/pull/1844:53,Integrability,depend,dependency,53,using linked or sorted data structures to reduce our dependency on the arbitrary changes in iterator orders (happens everytime you move from a jvm to another jvm),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1844
https://github.com/broadinstitute/gatk/pull/1845:357,Deployability,integrat,integration,357,"Implements https://github.com/broadinstitute/gatk/issues/1382.; Remove diploid assumptions during allele subsetting. **Summary**; - Changed `GenotypeLikelihoods.GenotypeLikelihoodsAllelePair GenotypeLikelihoods.getAllelePair(int PLindex)` to `ArrayList<Integer> GenotypeLikelihoods.getAlleles(int PLindex, int ploidy)` in GATKVariantContextUtils. ; - Added integration and unit tests.tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1845
https://github.com/broadinstitute/gatk/pull/1845:357,Integrability,integrat,integration,357,"Implements https://github.com/broadinstitute/gatk/issues/1382.; Remove diploid assumptions during allele subsetting. **Summary**; - Changed `GenotypeLikelihoods.GenotypeLikelihoodsAllelePair GenotypeLikelihoods.getAllelePair(int PLindex)` to `ArrayList<Integer> GenotypeLikelihoods.getAlleles(int PLindex, int ploidy)` in GATKVariantContextUtils. ; - Added integration and unit tests.tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1845
https://github.com/broadinstitute/gatk/pull/1845:378,Testability,test,tests,378,"Implements https://github.com/broadinstitute/gatk/issues/1382.; Remove diploid assumptions during allele subsetting. **Summary**; - Changed `GenotypeLikelihoods.GenotypeLikelihoodsAllelePair GenotypeLikelihoods.getAllelePair(int PLindex)` to `ArrayList<Integer> GenotypeLikelihoods.getAlleles(int PLindex, int ploidy)` in GATKVariantContextUtils. ; - Added integration and unit tests.tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1845
https://github.com/broadinstitute/gatk/pull/1845:384,Testability,test,tests,384,"Implements https://github.com/broadinstitute/gatk/issues/1382.; Remove diploid assumptions during allele subsetting. **Summary**; - Changed `GenotypeLikelihoods.GenotypeLikelihoodsAllelePair GenotypeLikelihoods.getAllelePair(int PLindex)` to `ArrayList<Integer> GenotypeLikelihoods.getAlleles(int PLindex, int ploidy)` in GATKVariantContextUtils. ; - Added integration and unit tests.tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1845
https://github.com/broadinstitute/gatk/pull/1846:14,Availability,down,down,14,Pushing MLLib down to public to make it easier for people to use with GATK4. Took SVD along for the ride (to test that it works).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1846
https://github.com/broadinstitute/gatk/pull/1846:109,Testability,test,test,109,Pushing MLLib down to public to make it easier for people to use with GATK4. Took SVD along for the ride (to test that it works).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1846
https://github.com/broadinstitute/gatk/pull/1847:66,Integrability,depend,depend,66,"-Published a jbwa snapshot to the Broad artifactory, which we now depend on; via gradle. This snapshot contains builds of the native jbwa code for both; Mac and Linux. -Added utility methods to NativeUtils to load this library at runtime,; and a test proving that it can be loaded successfully. Also switched; to the new NativeUtils methods for loading the PairHMM, and confirmed; that it loads successfully with the HaplotypeCaller in protected. -Included a gradle script to publish a new jbwa snapshot, should it; become necessary, along with instructions on how to do so. Resolves #1838",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847
https://github.com/broadinstitute/gatk/pull/1847:209,Performance,load,load,209,"-Published a jbwa snapshot to the Broad artifactory, which we now depend on; via gradle. This snapshot contains builds of the native jbwa code for both; Mac and Linux. -Added utility methods to NativeUtils to load this library at runtime,; and a test proving that it can be loaded successfully. Also switched; to the new NativeUtils methods for loading the PairHMM, and confirmed; that it loads successfully with the HaplotypeCaller in protected. -Included a gradle script to publish a new jbwa snapshot, should it; become necessary, along with instructions on how to do so. Resolves #1838",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847
https://github.com/broadinstitute/gatk/pull/1847:274,Performance,load,loaded,274,"-Published a jbwa snapshot to the Broad artifactory, which we now depend on; via gradle. This snapshot contains builds of the native jbwa code for both; Mac and Linux. -Added utility methods to NativeUtils to load this library at runtime,; and a test proving that it can be loaded successfully. Also switched; to the new NativeUtils methods for loading the PairHMM, and confirmed; that it loads successfully with the HaplotypeCaller in protected. -Included a gradle script to publish a new jbwa snapshot, should it; become necessary, along with instructions on how to do so. Resolves #1838",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847
https://github.com/broadinstitute/gatk/pull/1847:345,Performance,load,loading,345,"-Published a jbwa snapshot to the Broad artifactory, which we now depend on; via gradle. This snapshot contains builds of the native jbwa code for both; Mac and Linux. -Added utility methods to NativeUtils to load this library at runtime,; and a test proving that it can be loaded successfully. Also switched; to the new NativeUtils methods for loading the PairHMM, and confirmed; that it loads successfully with the HaplotypeCaller in protected. -Included a gradle script to publish a new jbwa snapshot, should it; become necessary, along with instructions on how to do so. Resolves #1838",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847
https://github.com/broadinstitute/gatk/pull/1847:389,Performance,load,loads,389,"-Published a jbwa snapshot to the Broad artifactory, which we now depend on; via gradle. This snapshot contains builds of the native jbwa code for both; Mac and Linux. -Added utility methods to NativeUtils to load this library at runtime,; and a test proving that it can be loaded successfully. Also switched; to the new NativeUtils methods for loading the PairHMM, and confirmed; that it loads successfully with the HaplotypeCaller in protected. -Included a gradle script to publish a new jbwa snapshot, should it; become necessary, along with instructions on how to do so. Resolves #1838",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847
https://github.com/broadinstitute/gatk/pull/1847:246,Testability,test,test,246,"-Published a jbwa snapshot to the Broad artifactory, which we now depend on; via gradle. This snapshot contains builds of the native jbwa code for both; Mac and Linux. -Added utility methods to NativeUtils to load this library at runtime,; and a test proving that it can be loaded successfully. Also switched; to the new NativeUtils methods for loading the PairHMM, and confirmed; that it loads successfully with the HaplotypeCaller in protected. -Included a gradle script to publish a new jbwa snapshot, should it; become necessary, along with instructions on how to do so. Resolves #1838",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1847
https://github.com/broadinstitute/gatk/issues/1848:85,Integrability,depend,dependency,85,"Once https://github.com/broadinstitute/gatk-native-bindings is on maven and a formal dependency of the GATK, move the `NativeUtils` class there. This class (as of https://github.com/broadinstitute/gatk/pull/1847) contains a method `loadLibraryFromClasspath()` to extract and load native libraries from the classpath that will be needed by implementors of our `NativeLibrary` interface.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1848
https://github.com/broadinstitute/gatk/issues/1848:375,Integrability,interface,interface,375,"Once https://github.com/broadinstitute/gatk-native-bindings is on maven and a formal dependency of the GATK, move the `NativeUtils` class there. This class (as of https://github.com/broadinstitute/gatk/pull/1847) contains a method `loadLibraryFromClasspath()` to extract and load native libraries from the classpath that will be needed by implementors of our `NativeLibrary` interface.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1848
https://github.com/broadinstitute/gatk/issues/1848:232,Performance,load,loadLibraryFromClasspath,232,"Once https://github.com/broadinstitute/gatk-native-bindings is on maven and a formal dependency of the GATK, move the `NativeUtils` class there. This class (as of https://github.com/broadinstitute/gatk/pull/1847) contains a method `loadLibraryFromClasspath()` to extract and load native libraries from the classpath that will be needed by implementors of our `NativeLibrary` interface.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1848
https://github.com/broadinstitute/gatk/issues/1848:275,Performance,load,load,275,"Once https://github.com/broadinstitute/gatk-native-bindings is on maven and a formal dependency of the GATK, move the `NativeUtils` class there. This class (as of https://github.com/broadinstitute/gatk/pull/1847) contains a method `loadLibraryFromClasspath()` to extract and load native libraries from the classpath that will be needed by implementors of our `NativeLibrary` interface.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1848
https://github.com/broadinstitute/gatk/pull/1849:45,Testability,test,tests,45,"SparkTestUtils contains a useful method that tests whether a class is; serializable/deserializable in Kryo. This method is needed by the; gatk-protected test suite, but is not currently packaged in the gatk-public; jar. Moving to utils/test so that it will be packaged.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1849
https://github.com/broadinstitute/gatk/pull/1849:153,Testability,test,test,153,"SparkTestUtils contains a useful method that tests whether a class is; serializable/deserializable in Kryo. This method is needed by the; gatk-protected test suite, but is not currently packaged in the gatk-public; jar. Moving to utils/test so that it will be packaged.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1849
https://github.com/broadinstitute/gatk/pull/1849:236,Testability,test,test,236,"SparkTestUtils contains a useful method that tests whether a class is; serializable/deserializable in Kryo. This method is needed by the; gatk-protected test suite, but is not currently packaged in the gatk-public; jar. Moving to utils/test so that it will be packaged.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1849
https://github.com/broadinstitute/gatk/pull/1852:137,Deployability,update,update,137,"Implements #1552 . When the StrandAlleleCountsBySample (SAC) annotation is present in VCFs, allele subsetting by SelectVariants will now update this field in the final VCF. **Summary of changes**; - `SelectVariants.subsetRecord()` uses the updated `GATKVariantContextUtils.updatePLsSACsAD()`, which contains the machinery to subset SAC by the used alleles.; - Added unit and integration tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852
https://github.com/broadinstitute/gatk/pull/1852:240,Deployability,update,updated,240,"Implements #1552 . When the StrandAlleleCountsBySample (SAC) annotation is present in VCFs, allele subsetting by SelectVariants will now update this field in the final VCF. **Summary of changes**; - `SelectVariants.subsetRecord()` uses the updated `GATKVariantContextUtils.updatePLsSACsAD()`, which contains the machinery to subset SAC by the used alleles.; - Added unit and integration tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852
https://github.com/broadinstitute/gatk/pull/1852:273,Deployability,update,updatePLsSACsAD,273,"Implements #1552 . When the StrandAlleleCountsBySample (SAC) annotation is present in VCFs, allele subsetting by SelectVariants will now update this field in the final VCF. **Summary of changes**; - `SelectVariants.subsetRecord()` uses the updated `GATKVariantContextUtils.updatePLsSACsAD()`, which contains the machinery to subset SAC by the used alleles.; - Added unit and integration tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852
https://github.com/broadinstitute/gatk/pull/1852:375,Deployability,integrat,integration,375,"Implements #1552 . When the StrandAlleleCountsBySample (SAC) annotation is present in VCFs, allele subsetting by SelectVariants will now update this field in the final VCF. **Summary of changes**; - `SelectVariants.subsetRecord()` uses the updated `GATKVariantContextUtils.updatePLsSACsAD()`, which contains the machinery to subset SAC by the used alleles.; - Added unit and integration tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852
https://github.com/broadinstitute/gatk/pull/1852:375,Integrability,integrat,integration,375,"Implements #1552 . When the StrandAlleleCountsBySample (SAC) annotation is present in VCFs, allele subsetting by SelectVariants will now update this field in the final VCF. **Summary of changes**; - `SelectVariants.subsetRecord()` uses the updated `GATKVariantContextUtils.updatePLsSACsAD()`, which contains the machinery to subset SAC by the used alleles.; - Added unit and integration tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852
https://github.com/broadinstitute/gatk/pull/1852:387,Testability,test,tests,387,"Implements #1552 . When the StrandAlleleCountsBySample (SAC) annotation is present in VCFs, allele subsetting by SelectVariants will now update this field in the final VCF. **Summary of changes**; - `SelectVariants.subsetRecord()` uses the updated `GATKVariantContextUtils.updatePLsSACsAD()`, which contains the machinery to subset SAC by the used alleles.; - Added unit and integration tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1852
https://github.com/broadinstitute/gatk/issues/1856:194,Availability,error,error,194,"The problem was ported from GATK 3 ([here](https://github.com/broadinstitute/gsa-unstable/pull/1369) and [here](https://github.com/broadinstitute/gsa-unstable/issues/1368)), and seems like more error(s) are introduced. Essentially, the fields`cachePloidyCapacity` and `cacheAlleleCountCapacity` are mixed, and fixing should be easy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1856
https://github.com/broadinstitute/gatk/issues/1856:243,Performance,cache,cachePloidyCapacity,243,"The problem was ported from GATK 3 ([here](https://github.com/broadinstitute/gsa-unstable/pull/1369) and [here](https://github.com/broadinstitute/gsa-unstable/issues/1368)), and seems like more error(s) are introduced. Essentially, the fields`cachePloidyCapacity` and `cacheAlleleCountCapacity` are mixed, and fixing should be easy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1856
https://github.com/broadinstitute/gatk/issues/1856:269,Performance,cache,cacheAlleleCountCapacity,269,"The problem was ported from GATK 3 ([here](https://github.com/broadinstitute/gsa-unstable/pull/1369) and [here](https://github.com/broadinstitute/gsa-unstable/issues/1368)), and seems like more error(s) are introduced. Essentially, the fields`cachePloidyCapacity` and `cacheAlleleCountCapacity` are mixed, and fixing should be easy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1856
https://github.com/broadinstitute/gatk/issues/1857:418,Energy Efficiency,allocate,allocate,418,"(added here for easier tracking). In bwajni.c; - [ ] lift `(*env)->GetArrayLength(env,baseArray)` out of loops; - [ ] pass pointers directly not wrapped in classes, eg. BwaIndex could be passed as a pointer; - [ ] cache all fieldIDs, methodIDs and classes; - [ ] pass in data directly without the ShortRead; - [ ] batch multiple calls to align (1 read) ; - [ ] pass in a struct for the native code to fill rather then allocate a new AlnRgn everytime",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1857
https://github.com/broadinstitute/gatk/issues/1857:145,Integrability,wrap,wrapped,145,"(added here for easier tracking). In bwajni.c; - [ ] lift `(*env)->GetArrayLength(env,baseArray)` out of loops; - [ ] pass pointers directly not wrapped in classes, eg. BwaIndex could be passed as a pointer; - [ ] cache all fieldIDs, methodIDs and classes; - [ ] pass in data directly without the ShortRead; - [ ] batch multiple calls to align (1 read) ; - [ ] pass in a struct for the native code to fill rather then allocate a new AlnRgn everytime",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1857
https://github.com/broadinstitute/gatk/issues/1857:214,Performance,cache,cache,214,"(added here for easier tracking). In bwajni.c; - [ ] lift `(*env)->GetArrayLength(env,baseArray)` out of loops; - [ ] pass pointers directly not wrapped in classes, eg. BwaIndex could be passed as a pointer; - [ ] cache all fieldIDs, methodIDs and classes; - [ ] pass in data directly without the ShortRead; - [ ] batch multiple calls to align (1 read) ; - [ ] pass in a struct for the native code to fill rather then allocate a new AlnRgn everytime",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1857
https://github.com/broadinstitute/gatk/issues/1858:249,Modifiability,refactor,refactoring,249,Issue spotted in GATK3 and fixed by pull-request https://github.com/broadinstitute/gsa-unstable/pull/1377. Original issue: https://github.com/broadinstitute/gsa-unstable/issues/1340. Needs to be ported to GATK4 as part of a larger fix involving the refactoring of AFCalculators (Issue TBA).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1858
https://github.com/broadinstitute/gatk/pull/1861:65,Performance,perform,performance,65,improvement proposed by folks at Intel - in my tests it improves performance of MarkDuplicatesSpark by >10% . @gspowley can you review and/or ask Eric+Lucy ?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1861
https://github.com/broadinstitute/gatk/pull/1861:47,Testability,test,tests,47,improvement proposed by folks at Intel - in my tests it improves performance of MarkDuplicatesSpark by >10% . @gspowley can you review and/or ask Eric+Lucy ?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1861
https://github.com/broadinstitute/gatk/pull/1862:60,Deployability,pipeline,pipeline,60,"Because I'm implementing a program in GATK that comes for a pipeline using `samtools mpileup` and post-processing the file, I need a way to check if the internal `ReadPileup` generated by the tool is indeed the same as the one obtained for samtools. Thus, I ported the `CheckPileup` tool from GATK3 in this PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1862
https://github.com/broadinstitute/gatk/issues/1868:565,Availability,down,down,565,"While working on a GATK-3 bug, broadinstitute/gsa-unstable#855, @SHuang-Broad noticed that the natural place to avoid an explosion of genotypes (i.e. the size of the GLs vector) for a large number of alleles is right after computing haplotype likelihoods. In GATK-4 this is in `HaplotypeCallerEngine::callRegion` after the line `final ReadLikelihoods<Haplotype> readLikelihoods =; likelihoodCalculationEngine.computeReadLikelihoods(assemblyResult, samplesList, reads)`. . However, at this point there is the following comment:. ``` java; // Note: we used to subset down at this point to only the ""best"" haplotypes in all samples for genotyping, but there; // was a bad interaction between that selection and the marginalization that happens over each event when computing; // GLs. In particular, for samples that are heterozygous non-reference (B/C) the marginalization for B treats the; // haplotype containing C as reference (and vice versa). Now this is fine if all possible haplotypes are included; // in the genotyping, but we lose information if we select down to a few haplotypes. [EB]; ```. If I understand EB's (@eitanbanks?) point right, subsetting here is dangerous only because of our downstream hack to turn multiallelic genotyping into a biallelic problem. The new AF/QUAL model will perform honest multiallelic genotyping, hence it makes sense to revisit whether to subset haplotypes at this point.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1868
https://github.com/broadinstitute/gatk/issues/1868:1062,Availability,down,down,1062,"While working on a GATK-3 bug, broadinstitute/gsa-unstable#855, @SHuang-Broad noticed that the natural place to avoid an explosion of genotypes (i.e. the size of the GLs vector) for a large number of alleles is right after computing haplotype likelihoods. In GATK-4 this is in `HaplotypeCallerEngine::callRegion` after the line `final ReadLikelihoods<Haplotype> readLikelihoods =; likelihoodCalculationEngine.computeReadLikelihoods(assemblyResult, samplesList, reads)`. . However, at this point there is the following comment:. ``` java; // Note: we used to subset down at this point to only the ""best"" haplotypes in all samples for genotyping, but there; // was a bad interaction between that selection and the marginalization that happens over each event when computing; // GLs. In particular, for samples that are heterozygous non-reference (B/C) the marginalization for B treats the; // haplotype containing C as reference (and vice versa). Now this is fine if all possible haplotypes are included; // in the genotyping, but we lose information if we select down to a few haplotypes. [EB]; ```. If I understand EB's (@eitanbanks?) point right, subsetting here is dangerous only because of our downstream hack to turn multiallelic genotyping into a biallelic problem. The new AF/QUAL model will perform honest multiallelic genotyping, hence it makes sense to revisit whether to subset haplotypes at this point.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1868
https://github.com/broadinstitute/gatk/issues/1868:1197,Availability,down,downstream,1197,"While working on a GATK-3 bug, broadinstitute/gsa-unstable#855, @SHuang-Broad noticed that the natural place to avoid an explosion of genotypes (i.e. the size of the GLs vector) for a large number of alleles is right after computing haplotype likelihoods. In GATK-4 this is in `HaplotypeCallerEngine::callRegion` after the line `final ReadLikelihoods<Haplotype> readLikelihoods =; likelihoodCalculationEngine.computeReadLikelihoods(assemblyResult, samplesList, reads)`. . However, at this point there is the following comment:. ``` java; // Note: we used to subset down at this point to only the ""best"" haplotypes in all samples for genotyping, but there; // was a bad interaction between that selection and the marginalization that happens over each event when computing; // GLs. In particular, for samples that are heterozygous non-reference (B/C) the marginalization for B treats the; // haplotype containing C as reference (and vice versa). Now this is fine if all possible haplotypes are included; // in the genotyping, but we lose information if we select down to a few haplotypes. [EB]; ```. If I understand EB's (@eitanbanks?) point right, subsetting here is dangerous only because of our downstream hack to turn multiallelic genotyping into a biallelic problem. The new AF/QUAL model will perform honest multiallelic genotyping, hence it makes sense to revisit whether to subset haplotypes at this point.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1868
https://github.com/broadinstitute/gatk/issues/1868:1298,Performance,perform,perform,1298,"While working on a GATK-3 bug, broadinstitute/gsa-unstable#855, @SHuang-Broad noticed that the natural place to avoid an explosion of genotypes (i.e. the size of the GLs vector) for a large number of alleles is right after computing haplotype likelihoods. In GATK-4 this is in `HaplotypeCallerEngine::callRegion` after the line `final ReadLikelihoods<Haplotype> readLikelihoods =; likelihoodCalculationEngine.computeReadLikelihoods(assemblyResult, samplesList, reads)`. . However, at this point there is the following comment:. ``` java; // Note: we used to subset down at this point to only the ""best"" haplotypes in all samples for genotyping, but there; // was a bad interaction between that selection and the marginalization that happens over each event when computing; // GLs. In particular, for samples that are heterozygous non-reference (B/C) the marginalization for B treats the; // haplotype containing C as reference (and vice versa). Now this is fine if all possible haplotypes are included; // in the genotyping, but we lose information if we select down to a few haplotypes. [EB]; ```. If I understand EB's (@eitanbanks?) point right, subsetting here is dangerous only because of our downstream hack to turn multiallelic genotyping into a biallelic problem. The new AF/QUAL model will perform honest multiallelic genotyping, hence it makes sense to revisit whether to subset haplotypes at this point.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1868
https://github.com/broadinstitute/gatk/issues/1868:112,Safety,avoid,avoid,112,"While working on a GATK-3 bug, broadinstitute/gsa-unstable#855, @SHuang-Broad noticed that the natural place to avoid an explosion of genotypes (i.e. the size of the GLs vector) for a large number of alleles is right after computing haplotype likelihoods. In GATK-4 this is in `HaplotypeCallerEngine::callRegion` after the line `final ReadLikelihoods<Haplotype> readLikelihoods =; likelihoodCalculationEngine.computeReadLikelihoods(assemblyResult, samplesList, reads)`. . However, at this point there is the following comment:. ``` java; // Note: we used to subset down at this point to only the ""best"" haplotypes in all samples for genotyping, but there; // was a bad interaction between that selection and the marginalization that happens over each event when computing; // GLs. In particular, for samples that are heterozygous non-reference (B/C) the marginalization for B treats the; // haplotype containing C as reference (and vice versa). Now this is fine if all possible haplotypes are included; // in the genotyping, but we lose information if we select down to a few haplotypes. [EB]; ```. If I understand EB's (@eitanbanks?) point right, subsetting here is dangerous only because of our downstream hack to turn multiallelic genotyping into a biallelic problem. The new AF/QUAL model will perform honest multiallelic genotyping, hence it makes sense to revisit whether to subset haplotypes at this point.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1868
https://github.com/broadinstitute/gatk/issues/1869:241,Availability,error,errors,241,"While @ldgauthier was presenting LobSTR, an STR indel caller, @fleharty and I were discussing how their probabilistic model was in some sense trying to convey a gap opening penalty and a gap continuation penalty for PCR slippage and related errors. This sort of suggests that one could go whole hog and build this into an enhanced pair-HMM that is aware of multiple error modes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1869
https://github.com/broadinstitute/gatk/issues/1869:366,Availability,error,error,366,"While @ldgauthier was presenting LobSTR, an STR indel caller, @fleharty and I were discussing how their probabilistic model was in some sense trying to convey a gap opening penalty and a gap continuation penalty for PCR slippage and related errors. This sort of suggests that one could go whole hog and build this into an enhanced pair-HMM that is aware of multiple error modes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1869
https://github.com/broadinstitute/gatk/issues/1869:322,Modifiability,enhance,enhanced,322,"While @ldgauthier was presenting LobSTR, an STR indel caller, @fleharty and I were discussing how their probabilistic model was in some sense trying to convey a gap opening penalty and a gap continuation penalty for PCR slippage and related errors. This sort of suggests that one could go whole hog and build this into an enhanced pair-HMM that is aware of multiple error modes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1869
https://github.com/broadinstitute/gatk/issues/1870:53,Availability,error,error,53,"git clone https://github.com/lindenb/jbwa. and make, error message is:. ksw.c:29:23: fatal error: emmintrin.h: No such file or directory; # include <emmintrin.h>. ^; compilation terminated.; make[1]: **\* [ksw.o] Error 1; make[1]: Leaving directory `/home/dlspark/gatk/lib/jbwa/bwa-8e2da1e407972170d1a660286f07a3a3a71ee6fb'; make: **\* [bwa-8e2da1e407972170d1a660286f07a3a3a71ee6fb/libbwa.a] Error 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1870
https://github.com/broadinstitute/gatk/issues/1870:91,Availability,error,error,91,"git clone https://github.com/lindenb/jbwa. and make, error message is:. ksw.c:29:23: fatal error: emmintrin.h: No such file or directory; # include <emmintrin.h>. ^; compilation terminated.; make[1]: **\* [ksw.o] Error 1; make[1]: Leaving directory `/home/dlspark/gatk/lib/jbwa/bwa-8e2da1e407972170d1a660286f07a3a3a71ee6fb'; make: **\* [bwa-8e2da1e407972170d1a660286f07a3a3a71ee6fb/libbwa.a] Error 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1870
https://github.com/broadinstitute/gatk/issues/1870:213,Availability,Error,Error,213,"git clone https://github.com/lindenb/jbwa. and make, error message is:. ksw.c:29:23: fatal error: emmintrin.h: No such file or directory; # include <emmintrin.h>. ^; compilation terminated.; make[1]: **\* [ksw.o] Error 1; make[1]: Leaving directory `/home/dlspark/gatk/lib/jbwa/bwa-8e2da1e407972170d1a660286f07a3a3a71ee6fb'; make: **\* [bwa-8e2da1e407972170d1a660286f07a3a3a71ee6fb/libbwa.a] Error 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1870
https://github.com/broadinstitute/gatk/issues/1870:392,Availability,Error,Error,392,"git clone https://github.com/lindenb/jbwa. and make, error message is:. ksw.c:29:23: fatal error: emmintrin.h: No such file or directory; # include <emmintrin.h>. ^; compilation terminated.; make[1]: **\* [ksw.o] Error 1; make[1]: Leaving directory `/home/dlspark/gatk/lib/jbwa/bwa-8e2da1e407972170d1a660286f07a3a3a71ee6fb'; make: **\* [bwa-8e2da1e407972170d1a660286f07a3a3a71ee6fb/libbwa.a] Error 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1870
https://github.com/broadinstitute/gatk/issues/1870:59,Integrability,message,message,59,"git clone https://github.com/lindenb/jbwa. and make, error message is:. ksw.c:29:23: fatal error: emmintrin.h: No such file or directory; # include <emmintrin.h>. ^; compilation terminated.; make[1]: **\* [ksw.o] Error 1; make[1]: Leaving directory `/home/dlspark/gatk/lib/jbwa/bwa-8e2da1e407972170d1a660286f07a3a3a71ee6fb'; make: **\* [bwa-8e2da1e407972170d1a660286f07a3a3a71ee6fb/libbwa.a] Error 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1870
https://github.com/broadinstitute/gatk/pull/1871:739,Deployability,update,updateChromosomeCountsInfo,739,"Fixes #1731 . **Summary of changes**; - SelectVariants.java; - createVCFHeaderLineList() - ensure allele count INFO annotations (AC, AN, AF) are in the output VCF header if --setFilteredGtToNocall ; - setFilteredGenotypeToNocall() - recompute AC, AN and AF if filtered genotypes are set to no-call, add them to the genotype builder.; - VariantFiltration.java; - initializeVcfWriter() - ensure allele count INFO annotations (AC, AN, AF) are in the output VCF header if --setFilteredGtToNocall ; - makeGenotypes() - recompute AC, AN and AF if filtered genotypes are set to no-call, add them to the genotype builder.; - GATKVariantContextUtils.java; - incrementChromosomeCountsInfo() - count the total and alternate alleles for a genotype; - updateChromosomeCountsInfo() - update AC, AN and AF with the computed count of total and alternate alleles; - selectVariantsInfoField.vcf\* and variantFiltrationInfoField.vcf*; - User data from https://github.com/broadinstitute/gsa-unstable/issues/1325; - VariantFiltrationIntegrationTest.java; - Tests that show the fix for https://github.com/broadinstitute/gsa-unstable/issues/1325; - SelectVariantsIntegrationTest.java; - MD5 changed in testSetFilteredGtoNocall() because AC, AN and AF are always in the VCF if --setFilteredGtToNocall; - GATKVariantContextUtilsUnitTest.java; - Test new GATKVariantContextUtils methods",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1871
https://github.com/broadinstitute/gatk/pull/1871:770,Deployability,update,update,770,"Fixes #1731 . **Summary of changes**; - SelectVariants.java; - createVCFHeaderLineList() - ensure allele count INFO annotations (AC, AN, AF) are in the output VCF header if --setFilteredGtToNocall ; - setFilteredGenotypeToNocall() - recompute AC, AN and AF if filtered genotypes are set to no-call, add them to the genotype builder.; - VariantFiltration.java; - initializeVcfWriter() - ensure allele count INFO annotations (AC, AN, AF) are in the output VCF header if --setFilteredGtToNocall ; - makeGenotypes() - recompute AC, AN and AF if filtered genotypes are set to no-call, add them to the genotype builder.; - GATKVariantContextUtils.java; - incrementChromosomeCountsInfo() - count the total and alternate alleles for a genotype; - updateChromosomeCountsInfo() - update AC, AN and AF with the computed count of total and alternate alleles; - selectVariantsInfoField.vcf\* and variantFiltrationInfoField.vcf*; - User data from https://github.com/broadinstitute/gsa-unstable/issues/1325; - VariantFiltrationIntegrationTest.java; - Tests that show the fix for https://github.com/broadinstitute/gsa-unstable/issues/1325; - SelectVariantsIntegrationTest.java; - MD5 changed in testSetFilteredGtoNocall() because AC, AN and AF are always in the VCF if --setFilteredGtToNocall; - GATKVariantContextUtilsUnitTest.java; - Test new GATKVariantContextUtils methods",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1871
https://github.com/broadinstitute/gatk/pull/1871:1036,Testability,Test,Tests,1036,"Fixes #1731 . **Summary of changes**; - SelectVariants.java; - createVCFHeaderLineList() - ensure allele count INFO annotations (AC, AN, AF) are in the output VCF header if --setFilteredGtToNocall ; - setFilteredGenotypeToNocall() - recompute AC, AN and AF if filtered genotypes are set to no-call, add them to the genotype builder.; - VariantFiltration.java; - initializeVcfWriter() - ensure allele count INFO annotations (AC, AN, AF) are in the output VCF header if --setFilteredGtToNocall ; - makeGenotypes() - recompute AC, AN and AF if filtered genotypes are set to no-call, add them to the genotype builder.; - GATKVariantContextUtils.java; - incrementChromosomeCountsInfo() - count the total and alternate alleles for a genotype; - updateChromosomeCountsInfo() - update AC, AN and AF with the computed count of total and alternate alleles; - selectVariantsInfoField.vcf\* and variantFiltrationInfoField.vcf*; - User data from https://github.com/broadinstitute/gsa-unstable/issues/1325; - VariantFiltrationIntegrationTest.java; - Tests that show the fix for https://github.com/broadinstitute/gsa-unstable/issues/1325; - SelectVariantsIntegrationTest.java; - MD5 changed in testSetFilteredGtoNocall() because AC, AN and AF are always in the VCF if --setFilteredGtToNocall; - GATKVariantContextUtilsUnitTest.java; - Test new GATKVariantContextUtils methods",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1871
https://github.com/broadinstitute/gatk/pull/1871:1179,Testability,test,testSetFilteredGtoNocall,1179,"Fixes #1731 . **Summary of changes**; - SelectVariants.java; - createVCFHeaderLineList() - ensure allele count INFO annotations (AC, AN, AF) are in the output VCF header if --setFilteredGtToNocall ; - setFilteredGenotypeToNocall() - recompute AC, AN and AF if filtered genotypes are set to no-call, add them to the genotype builder.; - VariantFiltration.java; - initializeVcfWriter() - ensure allele count INFO annotations (AC, AN, AF) are in the output VCF header if --setFilteredGtToNocall ; - makeGenotypes() - recompute AC, AN and AF if filtered genotypes are set to no-call, add them to the genotype builder.; - GATKVariantContextUtils.java; - incrementChromosomeCountsInfo() - count the total and alternate alleles for a genotype; - updateChromosomeCountsInfo() - update AC, AN and AF with the computed count of total and alternate alleles; - selectVariantsInfoField.vcf\* and variantFiltrationInfoField.vcf*; - User data from https://github.com/broadinstitute/gsa-unstable/issues/1325; - VariantFiltrationIntegrationTest.java; - Tests that show the fix for https://github.com/broadinstitute/gsa-unstable/issues/1325; - SelectVariantsIntegrationTest.java; - MD5 changed in testSetFilteredGtoNocall() because AC, AN and AF are always in the VCF if --setFilteredGtToNocall; - GATKVariantContextUtilsUnitTest.java; - Test new GATKVariantContextUtils methods",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1871
https://github.com/broadinstitute/gatk/pull/1871:1320,Testability,Test,Test,1320,"Fixes #1731 . **Summary of changes**; - SelectVariants.java; - createVCFHeaderLineList() - ensure allele count INFO annotations (AC, AN, AF) are in the output VCF header if --setFilteredGtToNocall ; - setFilteredGenotypeToNocall() - recompute AC, AN and AF if filtered genotypes are set to no-call, add them to the genotype builder.; - VariantFiltration.java; - initializeVcfWriter() - ensure allele count INFO annotations (AC, AN, AF) are in the output VCF header if --setFilteredGtToNocall ; - makeGenotypes() - recompute AC, AN and AF if filtered genotypes are set to no-call, add them to the genotype builder.; - GATKVariantContextUtils.java; - incrementChromosomeCountsInfo() - count the total and alternate alleles for a genotype; - updateChromosomeCountsInfo() - update AC, AN and AF with the computed count of total and alternate alleles; - selectVariantsInfoField.vcf\* and variantFiltrationInfoField.vcf*; - User data from https://github.com/broadinstitute/gsa-unstable/issues/1325; - VariantFiltrationIntegrationTest.java; - Tests that show the fix for https://github.com/broadinstitute/gsa-unstable/issues/1325; - SelectVariantsIntegrationTest.java; - MD5 changed in testSetFilteredGtoNocall() because AC, AN and AF are always in the VCF if --setFilteredGtToNocall; - GATKVariantContextUtilsUnitTest.java; - Test new GATKVariantContextUtils methods",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1871
https://github.com/broadinstitute/gatk/issues/1873:38,Performance,perform,performance,38,using snappy and 512k blocks improves performance of MarkDuplicatesSpark by >10% but the problem is that it fails to work on our in-house cluster (works fine on dataproc on google cloud). The PRs to look at are https://github.com/broadinstitute/gatk/pull/1861 and https://github.com/broadinstitute/gatk/issues/1872,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1873
https://github.com/broadinstitute/gatk/pull/1874:119,Availability,down,down,119,@gspowley can you review and/or delegate to Eric and Lucy?. I confirmed the speedup on MarkDuplicatesSpark - from 9:23 down to 8:54 seconds,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1874
https://github.com/broadinstitute/gatk/issues/1876:708,Availability,error,error,708,"I have tested several program functions by GATK4, some of them work pretty well. GATK4 does a great job to organize all steps by different tools like BWA, samtools, picard. Besides, it seems that there is also some optimization inside. Like ""cleanSam"" step, GATK4 cuts the time half compared to the one using Picard. (From 8 mins to 4 mins on same data) . However, the problem about GATK4 is that some programs fail due to java related problem (Maybe some reasons else). So far, the functions I failed are ""FastqToSam"" and ""ReadsPipelineSpark "". . Note that, I am using spark locally not scale-out cluster. So the command I am running ReadsPipelineSpark is as below(with oracle java8). Then it gives me java error. . ./bin/gatk/gatk­launch \; ReadsPipelineSpark \; -O hdfs:<PATH>CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr \; -I hdfs:<PATH>/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam \; -R hdfs:<PATH>human_g1k_v37.2bit \; --knownSites hdfs:<PATH>dbsnp_138.b37.excluding_sites_after_129.vcf \; --shardedOutput true \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES. All the failure seems to do with the java.lang.IllegalArgumentException with different error causes:; 1. readpipeline: java.lang.IllegalArgumentException: Null object is not allowed here; 2. fastqtosam: java.lang.IllegalArgumentException: Self-suppression not permitted",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1876
https://github.com/broadinstitute/gatk/issues/1876:1119,Availability,failure,failure,1119,"I have tested several program functions by GATK4, some of them work pretty well. GATK4 does a great job to organize all steps by different tools like BWA, samtools, picard. Besides, it seems that there is also some optimization inside. Like ""cleanSam"" step, GATK4 cuts the time half compared to the one using Picard. (From 8 mins to 4 mins on same data) . However, the problem about GATK4 is that some programs fail due to java related problem (Maybe some reasons else). So far, the functions I failed are ""FastqToSam"" and ""ReadsPipelineSpark "". . Note that, I am using spark locally not scale-out cluster. So the command I am running ReadsPipelineSpark is as below(with oracle java8). Then it gives me java error. . ./bin/gatk/gatk­launch \; ReadsPipelineSpark \; -O hdfs:<PATH>CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr \; -I hdfs:<PATH>/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam \; -R hdfs:<PATH>human_g1k_v37.2bit \; --knownSites hdfs:<PATH>dbsnp_138.b37.excluding_sites_after_129.vcf \; --shardedOutput true \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES. All the failure seems to do with the java.lang.IllegalArgumentException with different error causes:; 1. readpipeline: java.lang.IllegalArgumentException: Null object is not allowed here; 2. fastqtosam: java.lang.IllegalArgumentException: Self-suppression not permitted",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1876
https://github.com/broadinstitute/gatk/issues/1876:1198,Availability,error,error,1198,"I have tested several program functions by GATK4, some of them work pretty well. GATK4 does a great job to organize all steps by different tools like BWA, samtools, picard. Besides, it seems that there is also some optimization inside. Like ""cleanSam"" step, GATK4 cuts the time half compared to the one using Picard. (From 8 mins to 4 mins on same data) . However, the problem about GATK4 is that some programs fail due to java related problem (Maybe some reasons else). So far, the functions I failed are ""FastqToSam"" and ""ReadsPipelineSpark "". . Note that, I am using spark locally not scale-out cluster. So the command I am running ReadsPipelineSpark is as below(with oracle java8). Then it gives me java error. . ./bin/gatk/gatk­launch \; ReadsPipelineSpark \; -O hdfs:<PATH>CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr \; -I hdfs:<PATH>/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam \; -R hdfs:<PATH>human_g1k_v37.2bit \; --knownSites hdfs:<PATH>dbsnp_138.b37.excluding_sites_after_129.vcf \; --shardedOutput true \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES. All the failure seems to do with the java.lang.IllegalArgumentException with different error causes:; 1. readpipeline: java.lang.IllegalArgumentException: Null object is not allowed here; 2. fastqtosam: java.lang.IllegalArgumentException: Self-suppression not permitted",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1876
https://github.com/broadinstitute/gatk/issues/1876:215,Performance,optimiz,optimization,215,"I have tested several program functions by GATK4, some of them work pretty well. GATK4 does a great job to organize all steps by different tools like BWA, samtools, picard. Besides, it seems that there is also some optimization inside. Like ""cleanSam"" step, GATK4 cuts the time half compared to the one using Picard. (From 8 mins to 4 mins on same data) . However, the problem about GATK4 is that some programs fail due to java related problem (Maybe some reasons else). So far, the functions I failed are ""FastqToSam"" and ""ReadsPipelineSpark "". . Note that, I am using spark locally not scale-out cluster. So the command I am running ReadsPipelineSpark is as below(with oracle java8). Then it gives me java error. . ./bin/gatk/gatk­launch \; ReadsPipelineSpark \; -O hdfs:<PATH>CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr \; -I hdfs:<PATH>/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam \; -R hdfs:<PATH>human_g1k_v37.2bit \; --knownSites hdfs:<PATH>dbsnp_138.b37.excluding_sites_after_129.vcf \; --shardedOutput true \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES. All the failure seems to do with the java.lang.IllegalArgumentException with different error causes:; 1. readpipeline: java.lang.IllegalArgumentException: Null object is not allowed here; 2. fastqtosam: java.lang.IllegalArgumentException: Self-suppression not permitted",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1876
https://github.com/broadinstitute/gatk/issues/1876:7,Testability,test,tested,7,"I have tested several program functions by GATK4, some of them work pretty well. GATK4 does a great job to organize all steps by different tools like BWA, samtools, picard. Besides, it seems that there is also some optimization inside. Like ""cleanSam"" step, GATK4 cuts the time half compared to the one using Picard. (From 8 mins to 4 mins on same data) . However, the problem about GATK4 is that some programs fail due to java related problem (Maybe some reasons else). So far, the functions I failed are ""FastqToSam"" and ""ReadsPipelineSpark "". . Note that, I am using spark locally not scale-out cluster. So the command I am running ReadsPipelineSpark is as below(with oracle java8). Then it gives me java error. . ./bin/gatk/gatk­launch \; ReadsPipelineSpark \; -O hdfs:<PATH>CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr \; -I hdfs:<PATH>/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam \; -R hdfs:<PATH>human_g1k_v37.2bit \; --knownSites hdfs:<PATH>dbsnp_138.b37.excluding_sites_after_129.vcf \; --shardedOutput true \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES. All the failure seems to do with the java.lang.IllegalArgumentException with different error causes:; 1. readpipeline: java.lang.IllegalArgumentException: Null object is not allowed here; 2. fastqtosam: java.lang.IllegalArgumentException: Self-suppression not permitted",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1876
https://github.com/broadinstitute/gatk/issues/1877:388,Performance,perform,performance,388,"Reading the same number of files by `JavaSparkContext.wholeTextFiles` and counting the number of lines in each file takes approximately 20 min. But running locally assembly on these files takes more than 24 hours, which based on experience is way too much longer than expected. In addition, running the same program on ~10 files takes ~5 min. . Need to see if there's a way to debug this performance issue (get information from executors).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1877
https://github.com/broadinstitute/gatk/pull/1878:249,Availability,down,down,249,"This affects `RunBWAMEMViaCommandLine.java` and `RunSGAViaProcessBuilderOnSpark.java`, where stdio are always captured.; Capturing BWA MEM's stdout is a must because the result is piped to stdout.; For SGA, this helps debugging. But this also slows down the performance. Hence an option would be useful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1878
https://github.com/broadinstitute/gatk/pull/1878:258,Performance,perform,performance,258,"This affects `RunBWAMEMViaCommandLine.java` and `RunSGAViaProcessBuilderOnSpark.java`, where stdio are always captured.; Capturing BWA MEM's stdout is a must because the result is piped to stdout.; For SGA, this helps debugging. But this also slows down the performance. Hence an option would be useful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1878
https://github.com/broadinstitute/gatk/issues/1882:20,Availability,redundant,redundant,20,there's some highly redundant code in those classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1882
https://github.com/broadinstitute/gatk/issues/1882:20,Safety,redund,redundant,20,there's some highly redundant code in those classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1882
https://github.com/broadinstitute/gatk/issues/1887:46,Integrability,depend,depend,46,"Seems a shame to have every Spark application depend on GCS code, just to have access to HDFS. Maybe we could bust this into two pieces: separate out a spark.utils.HDFSUtils that knows nothing about GCS but can handle ""file:"" and ""hdfs:"" URLs, leaving the original gcs.BucketUtils that handles only ""gcs:"" URLs, and delegates non-gcs URLs to HDFSUtils.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1887
https://github.com/broadinstitute/gatk/issues/1887:79,Security,access,access,79,"Seems a shame to have every Spark application depend on GCS code, just to have access to HDFS. Maybe we could bust this into two pieces: separate out a spark.utils.HDFSUtils that knows nothing about GCS but can handle ""file:"" and ""hdfs:"" URLs, leaving the original gcs.BucketUtils that handles only ""gcs:"" URLs, and delegates non-gcs URLs to HDFSUtils.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1887
https://github.com/broadinstitute/gatk/pull/1888:113,Safety,risk,risk,113,partially addressed https://github.com/broadinstitute/gatk/issues/1427 (let's do this in a few steps to minimize risk). @droazen have a look; (will remove them from protected when this is merged and we're reved up),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1888
https://github.com/broadinstitute/gatk/issues/1889:248,Energy Efficiency,efficient,efficiently,248,"We're trying to chop out huge pile-ups, and genomically ubiquitous kmers, and high frequency kmers in the read set, and then we yet again eliminate kmers that appear in numerous intervals. Can't we do something simpler that cleans up the drek more efficiently?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1889
https://github.com/broadinstitute/gatk/issues/1889:211,Usability,simpl,simpler,211,"We're trying to chop out huge pile-ups, and genomically ubiquitous kmers, and high frequency kmers in the read set, and then we yet again eliminate kmers that appear in numerous intervals. Can't we do something simpler that cleans up the drek more efficiently?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1889
https://github.com/broadinstitute/gatk/issues/1890:66,Performance,optimiz,optimized,66,They appear to have been pulled out of a hat. And they need to be optimized.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1890
https://github.com/broadinstitute/gatk/issues/1891:59,Energy Efficiency,reduce,reduceScope,59,"`AFCalculator` has a couple of methods that don't belong: `reduceScope()`, which is called _before_ the AF calculation to reduce its computational burden, and `subsetAlleles()`, which is called _after_ the calculation to eliminate alleles that don't exist in called genotypes (in conformance with the dubious VCF spec). These methods are chronologically distinct from the rest of `AFCalculator` and do not appear to use any private variables. Thus they could easily be turned into static methods and removed from `AFCalculator`. Furthermore, `reduceScope()` and `subsetAlleles()` each have two implementations, in `DiploidExactAFCalculator` and `GeneralPloidyExactAFCalculator`. Since these two cases are complementary, they could easily be merged in a single method with an `if (ploidy == 2) . . .`. Finally, the general ploidy code is more complicated than it needs to be and needs editing. Beyond general housekeeping, the main motivation here is to untangle the AF/qual code as much as possible _without_ changing behavior before introducing the new model into the mix (issue #1697).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1891
https://github.com/broadinstitute/gatk/issues/1891:122,Energy Efficiency,reduce,reduce,122,"`AFCalculator` has a couple of methods that don't belong: `reduceScope()`, which is called _before_ the AF calculation to reduce its computational burden, and `subsetAlleles()`, which is called _after_ the calculation to eliminate alleles that don't exist in called genotypes (in conformance with the dubious VCF spec). These methods are chronologically distinct from the rest of `AFCalculator` and do not appear to use any private variables. Thus they could easily be turned into static methods and removed from `AFCalculator`. Furthermore, `reduceScope()` and `subsetAlleles()` each have two implementations, in `DiploidExactAFCalculator` and `GeneralPloidyExactAFCalculator`. Since these two cases are complementary, they could easily be merged in a single method with an `if (ploidy == 2) . . .`. Finally, the general ploidy code is more complicated than it needs to be and needs editing. Beyond general housekeeping, the main motivation here is to untangle the AF/qual code as much as possible _without_ changing behavior before introducing the new model into the mix (issue #1697).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1891
https://github.com/broadinstitute/gatk/issues/1891:543,Energy Efficiency,reduce,reduceScope,543,"`AFCalculator` has a couple of methods that don't belong: `reduceScope()`, which is called _before_ the AF calculation to reduce its computational burden, and `subsetAlleles()`, which is called _after_ the calculation to eliminate alleles that don't exist in called genotypes (in conformance with the dubious VCF spec). These methods are chronologically distinct from the rest of `AFCalculator` and do not appear to use any private variables. Thus they could easily be turned into static methods and removed from `AFCalculator`. Furthermore, `reduceScope()` and `subsetAlleles()` each have two implementations, in `DiploidExactAFCalculator` and `GeneralPloidyExactAFCalculator`. Since these two cases are complementary, they could easily be merged in a single method with an `if (ploidy == 2) . . .`. Finally, the general ploidy code is more complicated than it needs to be and needs editing. Beyond general housekeeping, the main motivation here is to untangle the AF/qual code as much as possible _without_ changing behavior before introducing the new model into the mix (issue #1697).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1891
https://github.com/broadinstitute/gatk/issues/1891:432,Modifiability,variab,variables,432,"`AFCalculator` has a couple of methods that don't belong: `reduceScope()`, which is called _before_ the AF calculation to reduce its computational burden, and `subsetAlleles()`, which is called _after_ the calculation to eliminate alleles that don't exist in called genotypes (in conformance with the dubious VCF spec). These methods are chronologically distinct from the rest of `AFCalculator` and do not appear to use any private variables. Thus they could easily be turned into static methods and removed from `AFCalculator`. Furthermore, `reduceScope()` and `subsetAlleles()` each have two implementations, in `DiploidExactAFCalculator` and `GeneralPloidyExactAFCalculator`. Since these two cases are complementary, they could easily be merged in a single method with an `if (ploidy == 2) . . .`. Finally, the general ploidy code is more complicated than it needs to be and needs editing. Beyond general housekeeping, the main motivation here is to untangle the AF/qual code as much as possible _without_ changing behavior before introducing the new model into the mix (issue #1697).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1891
https://github.com/broadinstitute/gatk/pull/1892:0,Energy Efficiency,reduce,reduced,0,reduced memory consumption: identified by profiling HC. ; - added a bunch of tests while I was there. note: the equals/hashcode changes are just for clean code - no perf improvements. @lbergelson can you have a look?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1892
https://github.com/broadinstitute/gatk/pull/1892:15,Energy Efficiency,consumption,consumption,15,reduced memory consumption: identified by profiling HC. ; - added a bunch of tests while I was there. note: the equals/hashcode changes are just for clean code - no perf improvements. @lbergelson can you have a look?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1892
https://github.com/broadinstitute/gatk/pull/1892:119,Security,hash,hashcode,119,reduced memory consumption: identified by profiling HC. ; - added a bunch of tests while I was there. note: the equals/hashcode changes are just for clean code - no perf improvements. @lbergelson can you have a look?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1892
https://github.com/broadinstitute/gatk/pull/1892:77,Testability,test,tests,77,reduced memory consumption: identified by profiling HC. ; - added a bunch of tests while I was there. note: the equals/hashcode changes are just for clean code - no perf improvements. @lbergelson can you have a look?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1892
https://github.com/broadinstitute/gatk/issues/1893:218,Availability,down,down,218,"Our goal is to have the combination step for allele-specific annotations handled by TileDB, but we should still port this code to GATK4 for the following reasons:. -We can likely simplify the code greatly, reducing it down to the three cases of List concatenation, sum, and contingency table combination, making it easier for Intel to replicate in TileDB. -It will be good to have a non-TileDB way to combine gvcfs as a model implementation and fallback option.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1893
https://github.com/broadinstitute/gatk/issues/1893:179,Usability,simpl,simplify,179,"Our goal is to have the combination step for allele-specific annotations handled by TileDB, but we should still port this code to GATK4 for the following reasons:. -We can likely simplify the code greatly, reducing it down to the three cases of List concatenation, sum, and contingency table combination, making it easier for Intel to replicate in TileDB. -It will be good to have a non-TileDB way to combine gvcfs as a model implementation and fallback option.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1893
https://github.com/broadinstitute/gatk/issues/1896:91,Availability,ERROR,ERROR,91,```; ./gatk-launch CountVariants -V fred -L 20; ```. blows up with a message . ```; A USER ERROR has occurred: We currently require a sequence dictionary (from a reference or source of reads) to process intervals. This restriction may be removed in the future.; ```. which makes no sense because `fred` does not even exist,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1896
https://github.com/broadinstitute/gatk/issues/1896:69,Integrability,message,message,69,```; ./gatk-launch CountVariants -V fred -L 20; ```. blows up with a message . ```; A USER ERROR has occurred: We currently require a sequence dictionary (from a reference or source of reads) to process intervals. This restriction may be removed in the future.; ```. which makes no sense because `fred` does not even exist,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1896
https://github.com/broadinstitute/gatk/pull/1900:91,Deployability,integrat,integrated,91,"Fixes https://github.com/broadinstitute/gatk/issues/589 (Walkers and Spark tools only, not integrated with Picard tools).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1900
https://github.com/broadinstitute/gatk/pull/1900:91,Integrability,integrat,integrated,91,"Fixes https://github.com/broadinstitute/gatk/issues/589 (Walkers and Spark tools only, not integrated with Picard tools).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1900
https://github.com/broadinstitute/gatk/issues/1902:445,Availability,error,error,445,"The gap-opening and gap-continuation parameters of Smith-Waterman realignment depend on PCA slippage and other stuff that depends on the sequencing platform and sample prep. In other words, they are not global parameters (_note: Smith-Waterman is often used to determine sequence similarity between individuals or species in which case its parameters are constants of the population. But a read differs from a candidate haplotype via sequencing error, not mutation_). @ronlevine suggested (and I am reporting because I like the idea) that we probably have sufficient data to learn these parameters for each sample.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1902
https://github.com/broadinstitute/gatk/issues/1902:78,Integrability,depend,depend,78,"The gap-opening and gap-continuation parameters of Smith-Waterman realignment depend on PCA slippage and other stuff that depends on the sequencing platform and sample prep. In other words, they are not global parameters (_note: Smith-Waterman is often used to determine sequence similarity between individuals or species in which case its parameters are constants of the population. But a read differs from a candidate haplotype via sequencing error, not mutation_). @ronlevine suggested (and I am reporting because I like the idea) that we probably have sufficient data to learn these parameters for each sample.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1902
https://github.com/broadinstitute/gatk/issues/1902:122,Integrability,depend,depends,122,"The gap-opening and gap-continuation parameters of Smith-Waterman realignment depend on PCA slippage and other stuff that depends on the sequencing platform and sample prep. In other words, they are not global parameters (_note: Smith-Waterman is often used to determine sequence similarity between individuals or species in which case its parameters are constants of the population. But a read differs from a candidate haplotype via sequencing error, not mutation_). @ronlevine suggested (and I am reporting because I like the idea) that we probably have sufficient data to learn these parameters for each sample.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1902
https://github.com/broadinstitute/gatk/issues/1902:575,Usability,learn,learn,575,"The gap-opening and gap-continuation parameters of Smith-Waterman realignment depend on PCA slippage and other stuff that depends on the sequencing platform and sample prep. In other words, they are not global parameters (_note: Smith-Waterman is often used to determine sequence similarity between individuals or species in which case its parameters are constants of the population. But a read differs from a candidate haplotype via sequencing error, not mutation_). @ronlevine suggested (and I am reporting because I like the idea) that we probably have sufficient data to learn these parameters for each sample.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1902
https://github.com/broadinstitute/gatk/issues/1907:155,Usability,simpl,simply,155,"In the genotyping code, we represent and iterate over all possible genotypes with a given number of alleles and ploidy in two different ways. The first is simply an `alleleCounts` array of type `int[numAlleles]`, where `alleleCounts[n]` is the number of times allele `n` appears in the (unphased) genotype. This is implemented in the nested `SumIterator` class in `GeneralPloidyExactAFCalculator`. The second is as a `GenotypeAlleleCounts` object, which encapsulates a rather different `int[]` called `sortedAlleleCounts`. A value of `{1 2 4 5}` means a ploidy-7 genotype in which allele 1 appears 2 times and allele 4 appears 5 times. It doesn't seem right to have both of these. Does anyone have an opinion which should stay and which should go? I'm especially hoping that @vruano can advise.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1907
https://github.com/broadinstitute/gatk/issues/1908:183,Integrability,wrap,wrapper,183,"Currently the call out to SGA modules are launched step by step via Process Builder. May be too slow. Now that we have an experience of how it may run on Spark, replace with a single wrapper script make improve runtime.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1908
https://github.com/broadinstitute/gatk/issues/1909:102,Availability,ERROR,ERROR,102,"If an invalid path is given to `Pileup`, the exception is not informative for the final user: `A USER ERROR has occurred: Invalid command line: Argument output has a bad value: /some/invalid/path. Problem constructing PrintStream from the string '/some/invalid/path'.`. Although #121 should correct this issue, in the meanwhile it could be better to use `File` in the argument and generate the stream in `onTraversalStart` to throw a more informative `CouldNotCreateOutputFile`. Thanks to @lbergelson for pointing it out when reviewing #1862.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1909
https://github.com/broadinstitute/gatk/issues/1911:376,Integrability,interface,interface,376,"The fact that `ReadCoordinateComparator` does not exactly match the ordering of htsjdk's `SAMRecordCoordinateComparator` has been the cause of a few bugs. It sorts all unmapped reads after mapped reads, whereas `SAMRecordCoordinateComparator` sorts unmapped reads that are assigned the positions of their mapped mates with their mapped mates. The issue is that the `GATKRead` interface does not allow unmapped reads to have a position. Ie., even if an unmapped `SAMRecord` is assigned the position of its mapped mate, calling `getContig()`/`getStart()` on the unmapped read via the `GATKRead` interface will return `null`/`0`. This was done mainly for consistency reasons and to simplify client code. Perhaps we could add `getAssignedContig()`, `getAssignedStart()`, etc. methods to GATKRead to expose the positions that unmapped reads with mapped mates get assigned for sorting purposes, and use these in `ReadCoordinateComparator`. This should allow us to match `SAMRecordCoordinateComparator` exactly, and then `ReadCoordinateComparator` could be used even when sorting for the purpose of writing a bam.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1911
https://github.com/broadinstitute/gatk/issues/1911:593,Integrability,interface,interface,593,"The fact that `ReadCoordinateComparator` does not exactly match the ordering of htsjdk's `SAMRecordCoordinateComparator` has been the cause of a few bugs. It sorts all unmapped reads after mapped reads, whereas `SAMRecordCoordinateComparator` sorts unmapped reads that are assigned the positions of their mapped mates with their mapped mates. The issue is that the `GATKRead` interface does not allow unmapped reads to have a position. Ie., even if an unmapped `SAMRecord` is assigned the position of its mapped mate, calling `getContig()`/`getStart()` on the unmapped read via the `GATKRead` interface will return `null`/`0`. This was done mainly for consistency reasons and to simplify client code. Perhaps we could add `getAssignedContig()`, `getAssignedStart()`, etc. methods to GATKRead to expose the positions that unmapped reads with mapped mates get assigned for sorting purposes, and use these in `ReadCoordinateComparator`. This should allow us to match `SAMRecordCoordinateComparator` exactly, and then `ReadCoordinateComparator` could be used even when sorting for the purpose of writing a bam.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1911
https://github.com/broadinstitute/gatk/issues/1911:795,Security,expose,expose,795,"The fact that `ReadCoordinateComparator` does not exactly match the ordering of htsjdk's `SAMRecordCoordinateComparator` has been the cause of a few bugs. It sorts all unmapped reads after mapped reads, whereas `SAMRecordCoordinateComparator` sorts unmapped reads that are assigned the positions of their mapped mates with their mapped mates. The issue is that the `GATKRead` interface does not allow unmapped reads to have a position. Ie., even if an unmapped `SAMRecord` is assigned the position of its mapped mate, calling `getContig()`/`getStart()` on the unmapped read via the `GATKRead` interface will return `null`/`0`. This was done mainly for consistency reasons and to simplify client code. Perhaps we could add `getAssignedContig()`, `getAssignedStart()`, etc. methods to GATKRead to expose the positions that unmapped reads with mapped mates get assigned for sorting purposes, and use these in `ReadCoordinateComparator`. This should allow us to match `SAMRecordCoordinateComparator` exactly, and then `ReadCoordinateComparator` could be used even when sorting for the purpose of writing a bam.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1911
https://github.com/broadinstitute/gatk/issues/1911:679,Usability,simpl,simplify,679,"The fact that `ReadCoordinateComparator` does not exactly match the ordering of htsjdk's `SAMRecordCoordinateComparator` has been the cause of a few bugs. It sorts all unmapped reads after mapped reads, whereas `SAMRecordCoordinateComparator` sorts unmapped reads that are assigned the positions of their mapped mates with their mapped mates. The issue is that the `GATKRead` interface does not allow unmapped reads to have a position. Ie., even if an unmapped `SAMRecord` is assigned the position of its mapped mate, calling `getContig()`/`getStart()` on the unmapped read via the `GATKRead` interface will return `null`/`0`. This was done mainly for consistency reasons and to simplify client code. Perhaps we could add `getAssignedContig()`, `getAssignedStart()`, etc. methods to GATKRead to expose the positions that unmapped reads with mapped mates get assigned for sorting purposes, and use these in `ReadCoordinateComparator`. This should allow us to match `SAMRecordCoordinateComparator` exactly, and then `ReadCoordinateComparator` could be used even when sorting for the purpose of writing a bam.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1911
https://github.com/broadinstitute/gatk/issues/1913:5,Performance,perform,performance,5,"Gatk performance work, do we have output files included in our data bucket? We need to add those so people can compare.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1913
https://github.com/broadinstitute/gatk/issues/1914:7,Deployability,update,update,7,Please update jbwa with the current github version. A patch for PPC64 is merged with jbwa now. Thanks!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1914
https://github.com/broadinstitute/gatk/issues/1914:54,Deployability,patch,patch,54,Please update jbwa with the current github version. A patch for PPC64 is merged with jbwa now. Thanks!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1914
https://github.com/broadinstitute/gatk/pull/1915:136,Modifiability,variab,variable,136,"This pull request is focused on resolving occurrences of Sonar rule squid:S1197 - Array designators ""[]"" should be on the type, not the variable. You can find more information about the issue here: https://dev.eclipse.org/sonar/coding_rules#q=squid:S1197. Please let me know if you have any questions. M-Ezzat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1915
https://github.com/broadinstitute/gatk/issues/1916:20,Availability,error,error,20,"I got the following error when running on a cluster (Centos 6.6). The file _/lib64/libz.so.1_ is symlinked to version 1.2.3, which is too low (<1.2.3.3). When I rebuilt jbwa locally it worked fine. ```; java.lang.UnsatisfiedLinkError: /data/11/yarn/nm/usercache/tom/appcache/application_1460561740089_0118/container_1460561740089_0118_01_000002/tmp/libbwajni.5713518835392178075.so: /lib64/libz.so.1: version `ZLIB_1.2.3.3' not found (required by /data/11/yarn/nm/usercache/tom/appcache/application_1460561740089_0118/container_1460561740089_0118_01_000002/tmp/libbwajni.5713518835392178075.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1937); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1822); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at org.broadinstitute.hellbender.utils.NativeUtils.loadLibraryFromClasspath(NativeUtils.java:63) ; at org.broadinstitute.hellbender.utils.bwa.BWANativeLibrary.load(BWANativeLibrary.java:14); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1916
https://github.com/broadinstitute/gatk/issues/1916:635,Performance,load,load,635,"I got the following error when running on a cluster (Centos 6.6). The file _/lib64/libz.so.1_ is symlinked to version 1.2.3, which is too low (<1.2.3.3). When I rebuilt jbwa locally it worked fine. ```; java.lang.UnsatisfiedLinkError: /data/11/yarn/nm/usercache/tom/appcache/application_1460561740089_0118/container_1460561740089_0118_01_000002/tmp/libbwajni.5713518835392178075.so: /lib64/libz.so.1: version `ZLIB_1.2.3.3' not found (required by /data/11/yarn/nm/usercache/tom/appcache/application_1460561740089_0118/container_1460561740089_0118_01_000002/tmp/libbwajni.5713518835392178075.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1937); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1822); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at org.broadinstitute.hellbender.utils.NativeUtils.loadLibraryFromClasspath(NativeUtils.java:63) ; at org.broadinstitute.hellbender.utils.bwa.BWANativeLibrary.load(BWANativeLibrary.java:14); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1916
https://github.com/broadinstitute/gatk/issues/1916:743,Performance,load,loadLibrary,743,"I got the following error when running on a cluster (Centos 6.6). The file _/lib64/libz.so.1_ is symlinked to version 1.2.3, which is too low (<1.2.3.3). When I rebuilt jbwa locally it worked fine. ```; java.lang.UnsatisfiedLinkError: /data/11/yarn/nm/usercache/tom/appcache/application_1460561740089_0118/container_1460561740089_0118_01_000002/tmp/libbwajni.5713518835392178075.so: /lib64/libz.so.1: version `ZLIB_1.2.3.3' not found (required by /data/11/yarn/nm/usercache/tom/appcache/application_1460561740089_0118/container_1460561740089_0118_01_000002/tmp/libbwajni.5713518835392178075.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1937); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1822); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at org.broadinstitute.hellbender.utils.NativeUtils.loadLibraryFromClasspath(NativeUtils.java:63) ; at org.broadinstitute.hellbender.utils.bwa.BWANativeLibrary.load(BWANativeLibrary.java:14); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1916
https://github.com/broadinstitute/gatk/issues/1916:845,Performance,load,load,845,"I got the following error when running on a cluster (Centos 6.6). The file _/lib64/libz.so.1_ is symlinked to version 1.2.3, which is too low (<1.2.3.3). When I rebuilt jbwa locally it worked fine. ```; java.lang.UnsatisfiedLinkError: /data/11/yarn/nm/usercache/tom/appcache/application_1460561740089_0118/container_1460561740089_0118_01_000002/tmp/libbwajni.5713518835392178075.so: /lib64/libz.so.1: version `ZLIB_1.2.3.3' not found (required by /data/11/yarn/nm/usercache/tom/appcache/application_1460561740089_0118/container_1460561740089_0118_01_000002/tmp/libbwajni.5713518835392178075.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1937); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1822); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at org.broadinstitute.hellbender.utils.NativeUtils.loadLibraryFromClasspath(NativeUtils.java:63) ; at org.broadinstitute.hellbender.utils.bwa.BWANativeLibrary.load(BWANativeLibrary.java:14); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1916
https://github.com/broadinstitute/gatk/issues/1916:920,Performance,load,loadLibraryFromClasspath,920,"I got the following error when running on a cluster (Centos 6.6). The file _/lib64/libz.so.1_ is symlinked to version 1.2.3, which is too low (<1.2.3.3). When I rebuilt jbwa locally it worked fine. ```; java.lang.UnsatisfiedLinkError: /data/11/yarn/nm/usercache/tom/appcache/application_1460561740089_0118/container_1460561740089_0118_01_000002/tmp/libbwajni.5713518835392178075.so: /lib64/libz.so.1: version `ZLIB_1.2.3.3' not found (required by /data/11/yarn/nm/usercache/tom/appcache/application_1460561740089_0118/container_1460561740089_0118_01_000002/tmp/libbwajni.5713518835392178075.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1937); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1822); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at org.broadinstitute.hellbender.utils.NativeUtils.loadLibraryFromClasspath(NativeUtils.java:63) ; at org.broadinstitute.hellbender.utils.bwa.BWANativeLibrary.load(BWANativeLibrary.java:14); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1916
https://github.com/broadinstitute/gatk/issues/1916:1028,Performance,load,load,1028,"I got the following error when running on a cluster (Centos 6.6). The file _/lib64/libz.so.1_ is symlinked to version 1.2.3, which is too low (<1.2.3.3). When I rebuilt jbwa locally it worked fine. ```; java.lang.UnsatisfiedLinkError: /data/11/yarn/nm/usercache/tom/appcache/application_1460561740089_0118/container_1460561740089_0118_01_000002/tmp/libbwajni.5713518835392178075.so: /lib64/libz.so.1: version `ZLIB_1.2.3.3' not found (required by /data/11/yarn/nm/usercache/tom/appcache/application_1460561740089_0118/container_1460561740089_0118_01_000002/tmp/libbwajni.5713518835392178075.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1937); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1822); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at org.broadinstitute.hellbender.utils.NativeUtils.loadLibraryFromClasspath(NativeUtils.java:63) ; at org.broadinstitute.hellbender.utils.bwa.BWANativeLibrary.load(BWANativeLibrary.java:14); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1916
https://github.com/broadinstitute/gatk/pull/1919:53,Integrability,interface,interface,53,I refactored SplitNCigar reads to use the ReadWalker interface,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1919
https://github.com/broadinstitute/gatk/pull/1919:2,Modifiability,refactor,refactored,2,I refactored SplitNCigar reads to use the ReadWalker interface,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1919
https://github.com/broadinstitute/gatk/issues/1920:399,Testability,test,test,399,Command line from a gatk-protected jar file:. ```; /dsde/working/lichtens/wgs$ gatk-protected/gatk-launch Pileup -R /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta \; -I /seq/picard_aggregation/G23062/TCGA-BA-4077-01B-01D-2268-08/current/TCGA-BA-4077-01B-01D-2268-08.bam \; -L gatk_intervals_allchr.1kg.phase3.v5a.snp.maf10.biallelic.recode.fixed.prune5.interval_list \; -O test.txt; ```. Uses `'org.broadinstitute:gatk:4.alpha.1-104-g213595d-20160606.150121-2'`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1920
https://github.com/broadinstitute/gatk/pull/1921:58,Security,validat,validation,58,I checked that it runs now with picard 2.0.1 and GATK3.5 (validation fails though),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1921
https://github.com/broadinstitute/gatk/issues/1922:63,Deployability,update,updated,63,the tests written by David A a while back have not been run or updated and they fail (we now compare more stringently so maybe that's why). The ticket is to figure out why and fix if possible. Depends on code changes in https://github.com/broadinstitute/gatk/pull/1921,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1922
https://github.com/broadinstitute/gatk/issues/1922:193,Integrability,Depend,Depends,193,the tests written by David A a while back have not been run or updated and they fail (we now compare more stringently so maybe that's why). The ticket is to figure out why and fix if possible. Depends on code changes in https://github.com/broadinstitute/gatk/pull/1921,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1922
https://github.com/broadinstitute/gatk/issues/1922:4,Testability,test,tests,4,the tests written by David A a while back have not been run or updated and they fail (we now compare more stringently so maybe that's why). The ticket is to figure out why and fix if possible. Depends on code changes in https://github.com/broadinstitute/gatk/pull/1921,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1922
https://github.com/broadinstitute/gatk/pull/1924:239,Integrability,depend,depends,239,"Most changes are trivial, except `computeMaxAcceptableAlleleCount`, which is exactly the same code as in the GATK3 fix. The part that's missing is changes made to `HaplotypeCallerGenotyingEngine`, which is in GATK-protected, which in turn depends on these classes in GATK. So these changes have to go in first.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1924
https://github.com/broadinstitute/gatk/issues/1925:104,Availability,down,down,104,We might be able to compute the microhomology/microsinertion length based on the two cigars and nail it down to an exact breakpoint or range specific to that clip.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1925
https://github.com/broadinstitute/gatk/issues/1926:320,Testability,test,tests,320,"It looks like the mapPartitions call in BwaSpark results in a larger serialized object graph than the other Spark tools. I discovered this by accident when I added a non-serializeable read filter object to the ReadInput argument collection in GATK Spark tool, which caused the BwaSparkIntegrationTest to fail. All other tests passed. It can be reproduced it by just adding an instance of anything that's not serializeable to BwaSpark or GATKSparkTool and running the test BwaSparkIntegrationTest. At the moment the following line will do it since ReadGroupBlackListReadFilter is currently not serializeable (though anything containing a MapEntry will do):. ReadGroupBlackListReadFilter prf = new ReadGroupBlackListReadFilter(Collections.singletonList(""RG:test""), getHeaderForReads());",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1926
https://github.com/broadinstitute/gatk/issues/1926:467,Testability,test,test,467,"It looks like the mapPartitions call in BwaSpark results in a larger serialized object graph than the other Spark tools. I discovered this by accident when I added a non-serializeable read filter object to the ReadInput argument collection in GATK Spark tool, which caused the BwaSparkIntegrationTest to fail. All other tests passed. It can be reproduced it by just adding an instance of anything that's not serializeable to BwaSpark or GATKSparkTool and running the test BwaSparkIntegrationTest. At the moment the following line will do it since ReadGroupBlackListReadFilter is currently not serializeable (though anything containing a MapEntry will do):. ReadGroupBlackListReadFilter prf = new ReadGroupBlackListReadFilter(Collections.singletonList(""RG:test""), getHeaderForReads());",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1926
https://github.com/broadinstitute/gatk/issues/1926:755,Testability,test,test,755,"It looks like the mapPartitions call in BwaSpark results in a larger serialized object graph than the other Spark tools. I discovered this by accident when I added a non-serializeable read filter object to the ReadInput argument collection in GATK Spark tool, which caused the BwaSparkIntegrationTest to fail. All other tests passed. It can be reproduced it by just adding an instance of anything that's not serializeable to BwaSpark or GATKSparkTool and running the test BwaSparkIntegrationTest. At the moment the following line will do it since ReadGroupBlackListReadFilter is currently not serializeable (though anything containing a MapEntry will do):. ReadGroupBlackListReadFilter prf = new ReadGroupBlackListReadFilter(Collections.singletonList(""RG:test""), getHeaderForReads());",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1926
https://github.com/broadinstitute/gatk/pull/1927:268,Deployability,pipeline,pipeline,268,this PR puts together BWA and MarkDuplicates. This is a prelude to BWA+MD+BQSR (but simpler because BQSR requires a 2bit reference and bwa wants a fasta reference). It extracts the core BWA/Spark code into a BwaSparkEngine and call that from both BwaSpark and the new pipeline. . It also improves the handling of sorting order for spark writing - adds a way to sort by queryname (relevant for mark duplicates). @tomwhite can you review?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1927
https://github.com/broadinstitute/gatk/pull/1927:84,Usability,simpl,simpler,84,this PR puts together BWA and MarkDuplicates. This is a prelude to BWA+MD+BQSR (but simpler because BQSR requires a 2bit reference and bwa wants a fasta reference). It extracts the core BWA/Spark code into a BwaSparkEngine and call that from both BwaSpark and the new pipeline. . It also improves the handling of sorting order for spark writing - adds a way to sort by queryname (relevant for mark duplicates). @tomwhite can you review?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1927
https://github.com/broadinstitute/gatk/pull/1929:51,Deployability,integrat,integration,51,"Preparation for some refactoring related to TileDB integration. Extracted classes are package-protected for now, since they are not intended for direct use outside of the engine package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1929
https://github.com/broadinstitute/gatk/pull/1929:51,Integrability,integrat,integration,51,"Preparation for some refactoring related to TileDB integration. Extracted classes are package-protected for now, since they are not intended for direct use outside of the engine package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1929
https://github.com/broadinstitute/gatk/pull/1929:21,Modifiability,refactor,refactoring,21,"Preparation for some refactoring related to TileDB integration. Extracted classes are package-protected for now, since they are not intended for direct use outside of the engine package.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1929
https://github.com/broadinstitute/gatk/issues/1930:75,Availability,error,error,75,"When I tried to run the latest build of the spark gatk I got the following error message on both dataproc and the onprem cluster.; `File Not Found: [/Users/emeryj/IdeaProjects/gatk/build/libIntelDeflater.so]`. Making the following changes to gatk-launch seems to fix it. ```; @@ -23,14 +23,13 @@ BUILD_LOCATION = script +""/build/install/"" + projectName + ""/bin/""; GATK_RUN_SCRIPT = BUILD_LOCATION + projectName; BIN_PATH = script + ""/build/libs"". -EXTRA_JAVA_OPTIONS=""-Dsamjdk.intel_deflater_so_path=libIntelDeflater.so -Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true ""; +EXTRA_JAVA_OPTIONS=""-DGATK_STACKTRACE_ON_USER_EXCEPTION=true "". DEFAULT_SPARK_ARGS = [""--conf"", ""spark.kryoserializer.buffer.max=512m"",; ""--conf"", ""spark.driver.maxResultSize=0"",; ""--conf"", ""spark.driver.userClassPathFirst=true"",; ""--conf"", ""spark.io.compression.codec=lzf"",; ""--conf"", ""spark.yarn.executor.memoryOverhead=600"",; -""--conf"", ""spark.yarn.dist.files="" + script + ""/build/libIntelDeflater.so"",; ""--conf"", ""spark.driver.extraJavaOptions="" + EXTRA_JAVA_OPTIONS,; ""--conf"", ""spark.executor.extraJavaOptions="" + EXTRA_JAVA_OPTIONS]; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1930
https://github.com/broadinstitute/gatk/issues/1930:329,Deployability,install,install,329,"When I tried to run the latest build of the spark gatk I got the following error message on both dataproc and the onprem cluster.; `File Not Found: [/Users/emeryj/IdeaProjects/gatk/build/libIntelDeflater.so]`. Making the following changes to gatk-launch seems to fix it. ```; @@ -23,14 +23,13 @@ BUILD_LOCATION = script +""/build/install/"" + projectName + ""/bin/""; GATK_RUN_SCRIPT = BUILD_LOCATION + projectName; BIN_PATH = script + ""/build/libs"". -EXTRA_JAVA_OPTIONS=""-Dsamjdk.intel_deflater_so_path=libIntelDeflater.so -Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true ""; +EXTRA_JAVA_OPTIONS=""-DGATK_STACKTRACE_ON_USER_EXCEPTION=true "". DEFAULT_SPARK_ARGS = [""--conf"", ""spark.kryoserializer.buffer.max=512m"",; ""--conf"", ""spark.driver.maxResultSize=0"",; ""--conf"", ""spark.driver.userClassPathFirst=true"",; ""--conf"", ""spark.io.compression.codec=lzf"",; ""--conf"", ""spark.yarn.executor.memoryOverhead=600"",; -""--conf"", ""spark.yarn.dist.files="" + script + ""/build/libIntelDeflater.so"",; ""--conf"", ""spark.driver.extraJavaOptions="" + EXTRA_JAVA_OPTIONS,; ""--conf"", ""spark.executor.extraJavaOptions="" + EXTRA_JAVA_OPTIONS]; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1930
https://github.com/broadinstitute/gatk/issues/1930:81,Integrability,message,message,81,"When I tried to run the latest build of the spark gatk I got the following error message on both dataproc and the onprem cluster.; `File Not Found: [/Users/emeryj/IdeaProjects/gatk/build/libIntelDeflater.so]`. Making the following changes to gatk-launch seems to fix it. ```; @@ -23,14 +23,13 @@ BUILD_LOCATION = script +""/build/install/"" + projectName + ""/bin/""; GATK_RUN_SCRIPT = BUILD_LOCATION + projectName; BIN_PATH = script + ""/build/libs"". -EXTRA_JAVA_OPTIONS=""-Dsamjdk.intel_deflater_so_path=libIntelDeflater.so -Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true ""; +EXTRA_JAVA_OPTIONS=""-DGATK_STACKTRACE_ON_USER_EXCEPTION=true "". DEFAULT_SPARK_ARGS = [""--conf"", ""spark.kryoserializer.buffer.max=512m"",; ""--conf"", ""spark.driver.maxResultSize=0"",; ""--conf"", ""spark.driver.userClassPathFirst=true"",; ""--conf"", ""spark.io.compression.codec=lzf"",; ""--conf"", ""spark.yarn.executor.memoryOverhead=600"",; -""--conf"", ""spark.yarn.dist.files="" + script + ""/build/libIntelDeflater.so"",; ""--conf"", ""spark.driver.extraJavaOptions="" + EXTRA_JAVA_OPTIONS,; ""--conf"", ""spark.executor.extraJavaOptions="" + EXTRA_JAVA_OPTIONS]; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1930
https://github.com/broadinstitute/gatk/pull/1932:139,Testability,test,tests,139,trying to fix the coveralls insanity about coverage dropping to 12% all the time. I think it may be because we send results from the cloud tests which are tiny,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1932
https://github.com/broadinstitute/gatk/pull/1935:10,Deployability,integrat,integration,10,"**Initial integration of GKL**; - Removed native build related items from `build.gradle`; - Removed native code from src tree; - Refactored `PairHMM.java` and `VectorLoglessPairHMM.java` to use GKL; - Updated `VectorPairHMMUnitTest.java` to use GKL; - Added integration tests to `IntelDeflaterIntegrationTest.java`. **Notes**; - PairHMM has been tested in HaplotypeCaller and GVCF output is md5sum equivalent to the PairHMM currently in GATK; - PairHMM in GKL is still single threaded, but about **_1.4x faster**_ than existing PairHMM, due to fixing a performance issue in the native code; - Next steps are captured in #1903 #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1935
https://github.com/broadinstitute/gatk/pull/1935:201,Deployability,Update,Updated,201,"**Initial integration of GKL**; - Removed native build related items from `build.gradle`; - Removed native code from src tree; - Refactored `PairHMM.java` and `VectorLoglessPairHMM.java` to use GKL; - Updated `VectorPairHMMUnitTest.java` to use GKL; - Added integration tests to `IntelDeflaterIntegrationTest.java`. **Notes**; - PairHMM has been tested in HaplotypeCaller and GVCF output is md5sum equivalent to the PairHMM currently in GATK; - PairHMM in GKL is still single threaded, but about **_1.4x faster**_ than existing PairHMM, due to fixing a performance issue in the native code; - Next steps are captured in #1903 #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1935
https://github.com/broadinstitute/gatk/pull/1935:258,Deployability,integrat,integration,258,"**Initial integration of GKL**; - Removed native build related items from `build.gradle`; - Removed native code from src tree; - Refactored `PairHMM.java` and `VectorLoglessPairHMM.java` to use GKL; - Updated `VectorPairHMMUnitTest.java` to use GKL; - Added integration tests to `IntelDeflaterIntegrationTest.java`. **Notes**; - PairHMM has been tested in HaplotypeCaller and GVCF output is md5sum equivalent to the PairHMM currently in GATK; - PairHMM in GKL is still single threaded, but about **_1.4x faster**_ than existing PairHMM, due to fixing a performance issue in the native code; - Next steps are captured in #1903 #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1935
https://github.com/broadinstitute/gatk/pull/1935:10,Integrability,integrat,integration,10,"**Initial integration of GKL**; - Removed native build related items from `build.gradle`; - Removed native code from src tree; - Refactored `PairHMM.java` and `VectorLoglessPairHMM.java` to use GKL; - Updated `VectorPairHMMUnitTest.java` to use GKL; - Added integration tests to `IntelDeflaterIntegrationTest.java`. **Notes**; - PairHMM has been tested in HaplotypeCaller and GVCF output is md5sum equivalent to the PairHMM currently in GATK; - PairHMM in GKL is still single threaded, but about **_1.4x faster**_ than existing PairHMM, due to fixing a performance issue in the native code; - Next steps are captured in #1903 #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1935
https://github.com/broadinstitute/gatk/pull/1935:258,Integrability,integrat,integration,258,"**Initial integration of GKL**; - Removed native build related items from `build.gradle`; - Removed native code from src tree; - Refactored `PairHMM.java` and `VectorLoglessPairHMM.java` to use GKL; - Updated `VectorPairHMMUnitTest.java` to use GKL; - Added integration tests to `IntelDeflaterIntegrationTest.java`. **Notes**; - PairHMM has been tested in HaplotypeCaller and GVCF output is md5sum equivalent to the PairHMM currently in GATK; - PairHMM in GKL is still single threaded, but about **_1.4x faster**_ than existing PairHMM, due to fixing a performance issue in the native code; - Next steps are captured in #1903 #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1935
https://github.com/broadinstitute/gatk/pull/1935:129,Modifiability,Refactor,Refactored,129,"**Initial integration of GKL**; - Removed native build related items from `build.gradle`; - Removed native code from src tree; - Refactored `PairHMM.java` and `VectorLoglessPairHMM.java` to use GKL; - Updated `VectorPairHMMUnitTest.java` to use GKL; - Added integration tests to `IntelDeflaterIntegrationTest.java`. **Notes**; - PairHMM has been tested in HaplotypeCaller and GVCF output is md5sum equivalent to the PairHMM currently in GATK; - PairHMM in GKL is still single threaded, but about **_1.4x faster**_ than existing PairHMM, due to fixing a performance issue in the native code; - Next steps are captured in #1903 #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1935
https://github.com/broadinstitute/gatk/pull/1935:553,Performance,perform,performance,553,"**Initial integration of GKL**; - Removed native build related items from `build.gradle`; - Removed native code from src tree; - Refactored `PairHMM.java` and `VectorLoglessPairHMM.java` to use GKL; - Updated `VectorPairHMMUnitTest.java` to use GKL; - Added integration tests to `IntelDeflaterIntegrationTest.java`. **Notes**; - PairHMM has been tested in HaplotypeCaller and GVCF output is md5sum equivalent to the PairHMM currently in GATK; - PairHMM in GKL is still single threaded, but about **_1.4x faster**_ than existing PairHMM, due to fixing a performance issue in the native code; - Next steps are captured in #1903 #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1935
https://github.com/broadinstitute/gatk/pull/1935:270,Testability,test,tests,270,"**Initial integration of GKL**; - Removed native build related items from `build.gradle`; - Removed native code from src tree; - Refactored `PairHMM.java` and `VectorLoglessPairHMM.java` to use GKL; - Updated `VectorPairHMMUnitTest.java` to use GKL; - Added integration tests to `IntelDeflaterIntegrationTest.java`. **Notes**; - PairHMM has been tested in HaplotypeCaller and GVCF output is md5sum equivalent to the PairHMM currently in GATK; - PairHMM in GKL is still single threaded, but about **_1.4x faster**_ than existing PairHMM, due to fixing a performance issue in the native code; - Next steps are captured in #1903 #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1935
https://github.com/broadinstitute/gatk/pull/1935:346,Testability,test,tested,346,"**Initial integration of GKL**; - Removed native build related items from `build.gradle`; - Removed native code from src tree; - Refactored `PairHMM.java` and `VectorLoglessPairHMM.java` to use GKL; - Updated `VectorPairHMMUnitTest.java` to use GKL; - Added integration tests to `IntelDeflaterIntegrationTest.java`. **Notes**; - PairHMM has been tested in HaplotypeCaller and GVCF output is md5sum equivalent to the PairHMM currently in GATK; - PairHMM in GKL is still single threaded, but about **_1.4x faster**_ than existing PairHMM, due to fixing a performance issue in the native code; - Next steps are captured in #1903 #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1935
https://github.com/broadinstitute/gatk/pull/1938:36,Performance,tune,tuned,36,"Some hard-coded parameters that are tuned based on eye-balling inversion test cases.; Also removed some duplicated code.; @cwhelan , would you please take a look? Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1938
https://github.com/broadinstitute/gatk/pull/1938:73,Testability,test,test,73,"Some hard-coded parameters that are tuned based on eye-balling inversion test cases.; Also removed some duplicated code.; @cwhelan , would you please take a look? Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1938
https://github.com/broadinstitute/gatk/issues/1942:85,Performance,tune,tune,85,"For the SV project, we would like to be able to change BWA's alignment parameters to tune them for the problem of aligning assembled contigs to the reference. In particular, we'd like to be able to set these parameters:. `-B9 -O16 -E1`. This will require adding some way to pass parameters to BWA instance creation to jBWA.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1942
https://github.com/broadinstitute/gatk/issues/1943:8,Energy Efficiency,monitor,monitor,8,"current monitor always says ""Records Processed Records/Minute""; this is less good than ; ""Reads Processed Reads/Minute""; ""Assembly Regions Processed Assembly Regions/Minute""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1943
https://github.com/broadinstitute/gatk/pull/1944:121,Testability,test,test,121,This pull request is focused on resolving occurrences of Sonar rule squid:S1155 - Collection.isEmpty() should be used to test for emptiness. You can find more information about the issue here: https://dev.eclipse.org/sonar/coding_rules#q=squid:S1155. Please let me know if you have any questions. M-Ezzat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1944
https://github.com/broadinstitute/gatk/issues/1946:285,Performance,load,load,285,"- Create `PairHMMNativeArguments` in HaplotypeCaller and pass to `VectorLoglessPairHMM` in `PairHMM.java`.; - Supply GATK temp directory in `VectorLoglessPairHMM.java`. Currently passing `null`, which uses the system temp directory. ```; final boolean isSupported = new IntelPairHmm().load(null);; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1946
https://github.com/broadinstitute/gatk/pull/1947:258,Availability,avail,available,258,Currently the behavior is not guaranteed in the situation where the user doesn't specify either spark.executor.cores or spark.executor.memory and is running with autoscaling enabled. Additionally the program will return a different value (the maximum memory available for caching) if the user is running in local mode.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1947
https://github.com/broadinstitute/gatk/issues/1949:953,Availability,ERROR,ERROR,953,"Hi,; While I test a new data set from fastq using bwamem, samtools and gatk4, I use the following scripts:. % step1: bwa mem. > bwa mem -t 12 -Ma -R '@RG ID:HCC1954 LB:HCC1954 SM:HCC1954' human_g1k_v37.fasta HCC1954_1.fq HCC1954_2.fq > HCC1954.sam. % step2: sort by queryname. > samtools sort -n HCC1954.sam -@ 24 > HCC1954.readnamesort.bam . % step3: gatk4 ReadsPipelineSpark. > /gatk-launch \; > ReadsPipelineSpark \; > -I HCC1954.readnamesort.bam \; > -R /benchmark/human_g1k_v37.2bit \; > -O HCC1954.bam \; > --knownSites /benchmark/dbsnp_138.b37.excluding_sites_after_129.vcf \; > --shardedOutput false \; > --emit_original_quals \; > --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES \; > --sparkRunner SPARK \; > --driver-memory 8G \; > --executor-memory 60g \; > --num-executors 1 \; > --executor-cores 4 \; > --sparkMaster local[4]. I encounter problem like this:. org.broadinstitute.hellbender.exceptions.UserException$MalformedRead: A USER ERROR has occurred: Read C097FACXX111207:2:2301:17281:179267 1:1139151-1139251 is malformed: The input .bam file contains reads with no platform information. First observed at read with name = C097FACXX111207:2:2301:17281:179267; at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.parsePlatformForRead(RecalUtils.java:510); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:122); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:33); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apach",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1949
https://github.com/broadinstitute/gatk/issues/1949:2526,Energy Efficiency,schedul,scheduler,2526,"; at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:122); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:33); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Then I check HCC1954.sam, and found . @RG ID:HCC1954 LB:HCC1954 SM:HCC1954. Since it complains no Platform information, I manually add PL:illumina to this line. then step3 works. . My question is do you also manually add PL, platform information to the generated sam file from bwa mem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1949
https://github.com/broadinstitute/gatk/issues/1949:2605,Energy Efficiency,schedul,scheduler,2605,"; at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:122); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:33); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Then I check HCC1954.sam, and found . @RG ID:HCC1954 LB:HCC1954 SM:HCC1954. Since it complains no Platform information, I manually add PL:illumina to this line. then step3 works. . My question is do you also manually add PL, platform information to the generated sam file from bwa mem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1949
https://github.com/broadinstitute/gatk/issues/1949:2684,Energy Efficiency,schedul,scheduler,2684,"; at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:122); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:33); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Then I check HCC1954.sam, and found . @RG ID:HCC1954 LB:HCC1954 SM:HCC1954. Since it complains no Platform information, I manually add PL:illumina to this line. then step3 works. . My question is do you also manually add PL, platform information to the generated sam file from bwa mem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1949
https://github.com/broadinstitute/gatk/issues/1949:2806,Performance,concurren,concurrent,2806,"; at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:122); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:33); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Then I check HCC1954.sam, and found . @RG ID:HCC1954 LB:HCC1954 SM:HCC1954. Since it complains no Platform information, I manually add PL:illumina to this line. then step3 works. . My question is do you also manually add PL, platform information to the generated sam file from bwa mem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1949
https://github.com/broadinstitute/gatk/issues/1949:2890,Performance,concurren,concurrent,2890,"; at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:122); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:33); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Then I check HCC1954.sam, and found . @RG ID:HCC1954 LB:HCC1954 SM:HCC1954. Since it complains no Platform information, I manually add PL:illumina to this line. then step3 works. . My question is do you also manually add PL, platform information to the generated sam file from bwa mem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1949
https://github.com/broadinstitute/gatk/issues/1949:13,Testability,test,test,13,"Hi,; While I test a new data set from fastq using bwamem, samtools and gatk4, I use the following scripts:. % step1: bwa mem. > bwa mem -t 12 -Ma -R '@RG ID:HCC1954 LB:HCC1954 SM:HCC1954' human_g1k_v37.fasta HCC1954_1.fq HCC1954_2.fq > HCC1954.sam. % step2: sort by queryname. > samtools sort -n HCC1954.sam -@ 24 > HCC1954.readnamesort.bam . % step3: gatk4 ReadsPipelineSpark. > /gatk-launch \; > ReadsPipelineSpark \; > -I HCC1954.readnamesort.bam \; > -R /benchmark/human_g1k_v37.2bit \; > -O HCC1954.bam \; > --knownSites /benchmark/dbsnp_138.b37.excluding_sites_after_129.vcf \; > --shardedOutput false \; > --emit_original_quals \; > --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES \; > --sparkRunner SPARK \; > --driver-memory 8G \; > --executor-memory 60g \; > --num-executors 1 \; > --executor-cores 4 \; > --sparkMaster local[4]. I encounter problem like this:. org.broadinstitute.hellbender.exceptions.UserException$MalformedRead: A USER ERROR has occurred: Read C097FACXX111207:2:2301:17281:179267 1:1139151-1139251 is malformed: The input .bam file contains reads with no platform information. First observed at read with name = C097FACXX111207:2:2301:17281:179267; at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.parsePlatformForRead(RecalUtils.java:510); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:122); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:33); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apach",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1949
https://github.com/broadinstitute/gatk/issues/1949:459,Testability,benchmark,benchmark,459,"Hi,; While I test a new data set from fastq using bwamem, samtools and gatk4, I use the following scripts:. % step1: bwa mem. > bwa mem -t 12 -Ma -R '@RG ID:HCC1954 LB:HCC1954 SM:HCC1954' human_g1k_v37.fasta HCC1954_1.fq HCC1954_2.fq > HCC1954.sam. % step2: sort by queryname. > samtools sort -n HCC1954.sam -@ 24 > HCC1954.readnamesort.bam . % step3: gatk4 ReadsPipelineSpark. > /gatk-launch \; > ReadsPipelineSpark \; > -I HCC1954.readnamesort.bam \; > -R /benchmark/human_g1k_v37.2bit \; > -O HCC1954.bam \; > --knownSites /benchmark/dbsnp_138.b37.excluding_sites_after_129.vcf \; > --shardedOutput false \; > --emit_original_quals \; > --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES \; > --sparkRunner SPARK \; > --driver-memory 8G \; > --executor-memory 60g \; > --num-executors 1 \; > --executor-cores 4 \; > --sparkMaster local[4]. I encounter problem like this:. org.broadinstitute.hellbender.exceptions.UserException$MalformedRead: A USER ERROR has occurred: Read C097FACXX111207:2:2301:17281:179267 1:1139151-1139251 is malformed: The input .bam file contains reads with no platform information. First observed at read with name = C097FACXX111207:2:2301:17281:179267; at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.parsePlatformForRead(RecalUtils.java:510); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:122); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:33); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apach",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1949
https://github.com/broadinstitute/gatk/issues/1949:527,Testability,benchmark,benchmark,527,"Hi,; While I test a new data set from fastq using bwamem, samtools and gatk4, I use the following scripts:. % step1: bwa mem. > bwa mem -t 12 -Ma -R '@RG ID:HCC1954 LB:HCC1954 SM:HCC1954' human_g1k_v37.fasta HCC1954_1.fq HCC1954_2.fq > HCC1954.sam. % step2: sort by queryname. > samtools sort -n HCC1954.sam -@ 24 > HCC1954.readnamesort.bam . % step3: gatk4 ReadsPipelineSpark. > /gatk-launch \; > ReadsPipelineSpark \; > -I HCC1954.readnamesort.bam \; > -R /benchmark/human_g1k_v37.2bit \; > -O HCC1954.bam \; > --knownSites /benchmark/dbsnp_138.b37.excluding_sites_after_129.vcf \; > --shardedOutput false \; > --emit_original_quals \; > --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES \; > --sparkRunner SPARK \; > --driver-memory 8G \; > --executor-memory 60g \; > --num-executors 1 \; > --executor-cores 4 \; > --sparkMaster local[4]. I encounter problem like this:. org.broadinstitute.hellbender.exceptions.UserException$MalformedRead: A USER ERROR has occurred: Read C097FACXX111207:2:2301:17281:179267 1:1139151-1139251 is malformed: The input .bam file contains reads with no platform information. First observed at read with name = C097FACXX111207:2:2301:17281:179267; at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.parsePlatformForRead(RecalUtils.java:510); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:122); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:33); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apach",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1949
https://github.com/broadinstitute/gatk/issues/1955:29,Deployability,install,install,29,javadoc checks in `./gradlew install` should be turned off for now because it's just cluttering the output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1955
https://github.com/broadinstitute/gatk/pull/1959:546,Testability,test,tests,546,"- The main change is the use of a reducible multi level collector; currently there is only one consumer which is InsertizeMetricsCollector.; - There is a long comment and diagram at the top of MetricsCollectorSpark that describes the roles and relationships of the various component classes that make up a collector. Its a good place to start to understand these changes.; - The Spark and walker versions of CollectInsertSizeMetrics now use the same MultiLevelCollectorReducible-derived implementation (InsertSizeCollector), and exactly the same tests and expected results.; - All of the CollectInsertSizeMetricsSpark code and corresponding unit tests from the previous implementation are removed in this PR.; - Fixes https://github.com/broadinstitute/gatk/issues/1512.; - Both CollectMultipleMetrics (Spark and walker) implementations include support and tests for external/custom collectors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1959
https://github.com/broadinstitute/gatk/pull/1959:646,Testability,test,tests,646,"- The main change is the use of a reducible multi level collector; currently there is only one consumer which is InsertizeMetricsCollector.; - There is a long comment and diagram at the top of MetricsCollectorSpark that describes the roles and relationships of the various component classes that make up a collector. Its a good place to start to understand these changes.; - The Spark and walker versions of CollectInsertSizeMetrics now use the same MultiLevelCollectorReducible-derived implementation (InsertSizeCollector), and exactly the same tests and expected results.; - All of the CollectInsertSizeMetricsSpark code and corresponding unit tests from the previous implementation are removed in this PR.; - Fixes https://github.com/broadinstitute/gatk/issues/1512.; - Both CollectMultipleMetrics (Spark and walker) implementations include support and tests for external/custom collectors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1959
https://github.com/broadinstitute/gatk/pull/1959:856,Testability,test,tests,856,"- The main change is the use of a reducible multi level collector; currently there is only one consumer which is InsertizeMetricsCollector.; - There is a long comment and diagram at the top of MetricsCollectorSpark that describes the roles and relationships of the various component classes that make up a collector. Its a good place to start to understand these changes.; - The Spark and walker versions of CollectInsertSizeMetrics now use the same MultiLevelCollectorReducible-derived implementation (InsertSizeCollector), and exactly the same tests and expected results.; - All of the CollectInsertSizeMetricsSpark code and corresponding unit tests from the previous implementation are removed in this PR.; - Fixes https://github.com/broadinstitute/gatk/issues/1512.; - Both CollectMultipleMetrics (Spark and walker) implementations include support and tests for external/custom collectors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1959
https://github.com/broadinstitute/gatk/issues/1960:896,Testability,test,test-jenkins,896,"```; gcloud dataproc jobs submit spark --cluster markeddupe --properties spark.kryoserializer.buffer.max=512m,spark.driver.maxResultSize=0,spark.driver.userClassPathFirst=true,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-Dsamjdk.intel_deflater_so_path=libIntelDeflater.so -Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true ,spark.executor.extraJavaOptions=-Dsamjdk.intel_deflater_so_path=libIntelDeflater.so -Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true ,spark.driver.memory=8G,spark.executor.cores=3,spark.executor.memory=25G,spark.yarn.executor.memoryOverhead=2500 --files /app/build/libIntelDeflater.so --jar /app/build/libs/gatk-all-4.alpha.1-159-gd0db982-SNAPSHOT-spark.jar MarkDuplicatesSpark --shardedOutput true -O /scratch/tmp.md.bam --numReducers 0 --apiKey <API_KEY> -I gs://broad-gatk-test-jenkins/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam --sparkMaster yarn-client; ```. related to https://github.com/broadinstitute/gatk/issues/1903",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1960
https://github.com/broadinstitute/gatk/pull/1962:77,Deployability,update,update,77,https://github.com/broadinstitute/gatk/issues/1673 was fixed with the htsjdk update; this re-enables the test.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1962
https://github.com/broadinstitute/gatk/pull/1962:105,Testability,test,test,105,https://github.com/broadinstitute/gatk/issues/1673 was fixed with the htsjdk update; this re-enables the test.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1962
https://github.com/broadinstitute/gatk/pull/1964:63,Deployability,update,updated,63,fixes https://github.com/broadinstitute/gatk/issues/1903. also updated the system properties in build.gradle and gatk-launch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1964
https://github.com/broadinstitute/gatk/issues/1967:206,Energy Efficiency,reduce,reduce,206,"We currently repartition the data into a fixed number of assemblies per partition. Once getExecutorCores is implemented in https://github.com/broadinstitute/gatk/pull/1947, we could make sure that we don't reduce the number of partitions to less than the number of cores.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1967
https://github.com/broadinstitute/gatk/issues/1969:89,Deployability,update,updated,89,"in htsjdk 2.5.0, the deflater factory needs to be set up by hand. Hadoop-BAM needs to be updated to allow this and when it is (https://github.com/HadoopGenomics/Hadoop-BAM/issues/109), gatk should use the API to specify the IntelDeflater",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1969
https://github.com/broadinstitute/gatk/pull/1972:228,Integrability,interface,interface,228,"-Hooked up a new PositionalDownsampler to AssemblyRegionWalker controlled; via an argument --maxReadsPerAlignmentStart (tool sets the default value). -As a side effect, exposed the ""assigned"" position of a read in the; GATKRead interface. This lets us query the nominal positions of unmapped; reads that have been assigned a position for sorting purposes. -As a second side effect, modified ReadCoordinateComparator to sort; unmapped reads with positions in the correct bam/sam file order. Resolves #1642; Resolves #1911",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1972
https://github.com/broadinstitute/gatk/pull/1972:169,Security,expose,exposed,169,"-Hooked up a new PositionalDownsampler to AssemblyRegionWalker controlled; via an argument --maxReadsPerAlignmentStart (tool sets the default value). -As a side effect, exposed the ""assigned"" position of a read in the; GATKRead interface. This lets us query the nominal positions of unmapped; reads that have been assigned a position for sorting purposes. -As a second side effect, modified ReadCoordinateComparator to sort; unmapped reads with positions in the correct bam/sam file order. Resolves #1642; Resolves #1911",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1972
https://github.com/broadinstitute/gatk/pull/1975:594,Availability,error,errors,594,this is enabled for the driving variants of VariantWalker as well as any auxiliary FeatureInput. a genomicsdb workspace is referenced by putting the loader.json that was used to create the arrays as well as a query.json into a directory; this is then specified with a url of the form gendb://path/to/directory; i.e; /myfiles/mygendbfiles/loader.json; /myfiles/mygendbfiles/query.json. ```; SomeVariantWalker -V gendb:///myfiles/mygendbfiles; ```. FeatureWalker isn't yet wired to support gendb urls; performance is untested. invalid input files are likely to result in Segfaults or non-helpful errors. resolves #1647,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1975
https://github.com/broadinstitute/gatk/pull/1975:149,Performance,load,loader,149,this is enabled for the driving variants of VariantWalker as well as any auxiliary FeatureInput. a genomicsdb workspace is referenced by putting the loader.json that was used to create the arrays as well as a query.json into a directory; this is then specified with a url of the form gendb://path/to/directory; i.e; /myfiles/mygendbfiles/loader.json; /myfiles/mygendbfiles/query.json. ```; SomeVariantWalker -V gendb:///myfiles/mygendbfiles; ```. FeatureWalker isn't yet wired to support gendb urls; performance is untested. invalid input files are likely to result in Segfaults or non-helpful errors. resolves #1647,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1975
https://github.com/broadinstitute/gatk/pull/1975:338,Performance,load,loader,338,this is enabled for the driving variants of VariantWalker as well as any auxiliary FeatureInput. a genomicsdb workspace is referenced by putting the loader.json that was used to create the arrays as well as a query.json into a directory; this is then specified with a url of the form gendb://path/to/directory; i.e; /myfiles/mygendbfiles/loader.json; /myfiles/mygendbfiles/query.json. ```; SomeVariantWalker -V gendb:///myfiles/mygendbfiles; ```. FeatureWalker isn't yet wired to support gendb urls; performance is untested. invalid input files are likely to result in Segfaults or non-helpful errors. resolves #1647,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1975
https://github.com/broadinstitute/gatk/pull/1975:500,Performance,perform,performance,500,this is enabled for the driving variants of VariantWalker as well as any auxiliary FeatureInput. a genomicsdb workspace is referenced by putting the loader.json that was used to create the arrays as well as a query.json into a directory; this is then specified with a url of the form gendb://path/to/directory; i.e; /myfiles/mygendbfiles/loader.json; /myfiles/mygendbfiles/query.json. ```; SomeVariantWalker -V gendb:///myfiles/mygendbfiles; ```. FeatureWalker isn't yet wired to support gendb urls; performance is untested. invalid input files are likely to result in Segfaults or non-helpful errors. resolves #1647,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1975
https://github.com/broadinstitute/gatk/pull/1977:22,Testability,test,testBadOffsetWithinBounds,22,PileupElementUnitTest.testBadOffsetWithinBounds was being skipped because the data provider name was wrong.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1977
https://github.com/broadinstitute/gatk/pull/1979:66,Security,validat,validateArg,66,"This consists almost entirely of using `Utils.nonNull` and `Utils.validateArg` in code ported from GATK 3. There are less trivial but straightforward simplifications of code in `MathUtils` and `ReadLikelihoods`. @droazen and @lbergelson is one of you willing to review this mind-numbing PR, or suggest a victim? It should be quick.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1979
https://github.com/broadinstitute/gatk/pull/1979:150,Usability,simpl,simplifications,150,"This consists almost entirely of using `Utils.nonNull` and `Utils.validateArg` in code ported from GATK 3. There are less trivial but straightforward simplifications of code in `MathUtils` and `ReadLikelihoods`. @droazen and @lbergelson is one of you willing to review this mind-numbing PR, or suggest a victim? It should be quick.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1979
https://github.com/broadinstitute/gatk/issues/1980:472,Testability,test,test,472,"a call to jbwa `BwaMem.align2` may return a String that corresponds to multiple SAMRecords (as per comment in bwamem.h (https://github.com/lh3/bwa/blob/5961611c358e480110793bbf241523a3cfac049b/bwamem.h). ```; Note that $seqs[i].sam may consist of several SAM lines if the corresponding sequence has multiple primary hits.; ```. currently, the code assumes that the string will result in 1 `SAMRecord` because it calls SAMLineParser.parseLine. The task here is to create a test case that exhibits this problem and then code up a solution to it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1980
https://github.com/broadinstitute/gatk/pull/1981:359,Deployability,pipeline,pipeline,359,"fixes https://github.com/broadinstitute/gatk/issues/1904. 2 hacks needed to be done for now:; 1) bwa loses tags so we keep track of which original read corresponds to which aligned read by creating a subclass of `ShortRead` (the class that jBWA uses for JNI) and putting the original read in a pointer in there.; 2) bwa requires a fasta reference while reads pipeline wants a 2bit reference (in the BROADCAST mode, which is the default). The workaround for now is to pass two reference files one of each kind",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1981
https://github.com/broadinstitute/gatk/issues/1983:2459,Integrability,wrap,wrapAndCopyInto,2459,eryAndPrefetch(FeatureDataSource.java:302); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:162); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:114); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:253); at org.broadinstitute.hellbender.tools.examples.ExampleReadWalkerWithVariants.apply(ExampleReadWalkerWithVariants.java:71); at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$0(ReadWalker.java:80); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:78); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:508); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1983
https://github.com/broadinstitute/gatk/issues/1983:56,Testability,test,test,56,"```; ./gatk-launch ExampleReadWalkerWithVariants -I src/test/resources/org/broadinstitute/hellbender/engine/reads_data_source_test1.bam -V ./src/test/resources/org/broadinstitute/hellbender/engine/example_variants.vcf; ```. blows up . ```; htsjdk.tribble.TribbleException: Line 12: there aren't enough columns for line 4 (we expected 9 tokens, and saw 1 ), for input source: /Users/akiezun/IdeaProjects/gatk/./src/test/resources/org/broadinstitute/hellbender/engine/example_variants.vcf; at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:276); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:60); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:74); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:36); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.readNextRecord(TribbleIndexedFeatureReader.java:472); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.next(TribbleIndexedFeatureReader.java:428); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.next(TribbleIndexedFeatureReader.java:396); at org.broadinstitute.hellbender.engine.FeatureCache.fill(FeatureCache.java:141); at org.broadinstitute.hellbender.engine.FeatureDataSource.refillQueryCache(FeatureDataSource.java:334); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:302); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:162); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:114); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:253); at org.broadinstitute.hellbender.tools.examples.ExampleReadWalkerWithVariants.apply(ExampleReadWalkerWithVariants.java:71); at org.broadinstitute.hellbender.engine.Read",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1983
https://github.com/broadinstitute/gatk/issues/1983:145,Testability,test,test,145,"```; ./gatk-launch ExampleReadWalkerWithVariants -I src/test/resources/org/broadinstitute/hellbender/engine/reads_data_source_test1.bam -V ./src/test/resources/org/broadinstitute/hellbender/engine/example_variants.vcf; ```. blows up . ```; htsjdk.tribble.TribbleException: Line 12: there aren't enough columns for line 4 (we expected 9 tokens, and saw 1 ), for input source: /Users/akiezun/IdeaProjects/gatk/./src/test/resources/org/broadinstitute/hellbender/engine/example_variants.vcf; at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:276); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:60); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:74); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:36); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.readNextRecord(TribbleIndexedFeatureReader.java:472); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.next(TribbleIndexedFeatureReader.java:428); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.next(TribbleIndexedFeatureReader.java:396); at org.broadinstitute.hellbender.engine.FeatureCache.fill(FeatureCache.java:141); at org.broadinstitute.hellbender.engine.FeatureDataSource.refillQueryCache(FeatureDataSource.java:334); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:302); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:162); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:114); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:253); at org.broadinstitute.hellbender.tools.examples.ExampleReadWalkerWithVariants.apply(ExampleReadWalkerWithVariants.java:71); at org.broadinstitute.hellbender.engine.Read",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1983
https://github.com/broadinstitute/gatk/issues/1983:414,Testability,test,test,414,"```; ./gatk-launch ExampleReadWalkerWithVariants -I src/test/resources/org/broadinstitute/hellbender/engine/reads_data_source_test1.bam -V ./src/test/resources/org/broadinstitute/hellbender/engine/example_variants.vcf; ```. blows up . ```; htsjdk.tribble.TribbleException: Line 12: there aren't enough columns for line 4 (we expected 9 tokens, and saw 1 ), for input source: /Users/akiezun/IdeaProjects/gatk/./src/test/resources/org/broadinstitute/hellbender/engine/example_variants.vcf; at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:276); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:60); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:74); at htsjdk.tribble.AsciiFeatureCodec.decode(AsciiFeatureCodec.java:36); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.readNextRecord(TribbleIndexedFeatureReader.java:472); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.next(TribbleIndexedFeatureReader.java:428); at htsjdk.tribble.TribbleIndexedFeatureReader$QueryIterator.next(TribbleIndexedFeatureReader.java:396); at org.broadinstitute.hellbender.engine.FeatureCache.fill(FeatureCache.java:141); at org.broadinstitute.hellbender.engine.FeatureDataSource.refillQueryCache(FeatureDataSource.java:334); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:302); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:264); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:162); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:114); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:253); at org.broadinstitute.hellbender.tools.examples.ExampleReadWalkerWithVariants.apply(ExampleReadWalkerWithVariants.java:71); at org.broadinstitute.hellbender.engine.Read",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1983
https://github.com/broadinstitute/gatk/issues/1985:152,Availability,error,error,152,"Using the latest `master` version of GATK4 as dependency in gradle (using [jitpack](https://jitpack.io/)) to build a shadow jar generetes the following error when a custom walker run using `java -jar shadowJar.jar CustomTool`, but also with public tools. Only if `--use_jdk_deflater true` is provided, it works. . The log is the following (there is no other log):. ```; 14:57:19.102 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985
https://github.com/broadinstitute/gatk/issues/1985:586,Availability,error,error,586,"Using the latest `master` version of GATK4 as dependency in gradle (using [jitpack](https://jitpack.io/)) to build a shadow jar generetes the following error when a custom walker run using `java -jar shadowJar.jar CustomTool`, but also with public tools. Only if `--use_jdk_deflater true` is provided, it works. . The log is the following (there is no other log):. ```; 14:57:19.102 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985
https://github.com/broadinstitute/gatk/issues/1985:1063,Availability,error,error,1063,"build a shadow jar generetes the following error when a custom walker run using `java -jar shadowJar.jar CustomTool`, but also with public tools. Only if `--use_jdk_deflater true` is provided, it works. . The log is the following (there is no other log):. ```; 14:57:19.102 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the library without assuming that the final user will have support for the native code or not. Could this be do",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985
https://github.com/broadinstitute/gatk/issues/1985:1887,Availability,error,error,1887," IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the library without assuming that the final user will have support for the native code or not. Could this be done? I prefer not to remove the faster code by intel because I know that some users will benefit from it. Just in case it is needed, my system is a Mac OS X (10.11.5) with Darwin Kernel Version 15.5.0 (root:xnu-3248.50.21~8/RELEASE_X86_64 x86_64). Thank you very much in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985
https://github.com/broadinstitute/gatk/issues/1985:46,Integrability,depend,dependency,46,"Using the latest `master` version of GATK4 as dependency in gradle (using [jitpack](https://jitpack.io/)) to build a shadow jar generetes the following error when a custom walker run using `java -jar shadowJar.jar CustomTool`, but also with public tools. Only if `--use_jdk_deflater true` is provided, it works. . The log is the following (there is no other log):. ```; 14:57:19.102 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985
https://github.com/broadinstitute/gatk/issues/1985:1522,Integrability,depend,dependency,1522," IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the library without assuming that the final user will have support for the native code or not. Could this be done? I prefer not to remove the faster code by intel because I know that some users will benefit from it. Just in case it is needed, my system is a Mac OS X (10.11.5) with Darwin Kernel Version 15.5.0 (root:xnu-3248.50.21~8/RELEASE_X86_64 x86_64). Thank you very much in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985
https://github.com/broadinstitute/gatk/issues/1985:1762,Integrability,depend,dependency,1762," IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the library without assuming that the final user will have support for the native code or not. Could this be done? I prefer not to remove the faster code by intel because I know that some users will benefit from it. Just in case it is needed, my system is a Mac OS X (10.11.5) with Darwin Kernel Version 15.5.0 (root:xnu-3248.50.21~8/RELEASE_X86_64 x86_64). Thank you very much in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985
https://github.com/broadinstitute/gatk/issues/1985:414,Performance,load,load,414,"Using the latest `master` version of GATK4 as dependency in gradle (using [jitpack](https://jitpack.io/)) to build a shadow jar generetes the following error when a custom walker run using `java -jar shadowJar.jar CustomTool`, but also with public tools. Only if `--use_jdk_deflater true` is provided, it works. . The log is the following (there is no other log):. ```; 14:57:19.102 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985
https://github.com/broadinstitute/gatk/issues/1985:1613,Performance,load,load,1613," IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the library without assuming that the final user will have support for the native code or not. Could this be done? I prefer not to remove the faster code by intel because I know that some users will benefit from it. Just in case it is needed, my system is a Mac OS X (10.11.5) with Darwin Kernel Version 15.5.0 (root:xnu-3248.50.21~8/RELEASE_X86_64 x86_64). Thank you very much in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985
https://github.com/broadinstitute/gatk/issues/1985:601,Safety,detect,detected,601,"Using the latest `master` version of GATK4 as dependency in gradle (using [jitpack](https://jitpack.io/)) to build a shadow jar generetes the following error when a custom walker run using `java -jar shadowJar.jar CustomTool`, but also with public tools. Only if `--use_jdk_deflater true` is provided, it works. . The log is the following (there is no other log):. ```; 14:57:19.102 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985
https://github.com/broadinstitute/gatk/issues/1985:1414,Safety,Abort,Abort,1414," IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the library without assuming that the final user will have support for the native code or not. Could this be done? I prefer not to remove the faster code by intel because I know that some users will benefit from it. Just in case it is needed, my system is a Mac OS X (10.11.5) with Darwin Kernel Version 15.5.0 (root:xnu-3248.50.21~8/RELEASE_X86_64 x86_64). Thank you very much in advance.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985
https://github.com/broadinstitute/gatk/issues/1985:318,Testability,log,log,318,"Using the latest `master` version of GATK4 as dependency in gradle (using [jitpack](https://jitpack.io/)) to build a shadow jar generetes the following error when a custom walker run using `java -jar shadowJar.jar CustomTool`, but also with public tools. Only if `--use_jdk_deflater true` is provided, it works. . The log is the following (there is no other log):. ```; 14:57:19.102 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985
https://github.com/broadinstitute/gatk/issues/1985:358,Testability,log,log,358,"Using the latest `master` version of GATK4 as dependency in gradle (using [jitpack](https://jitpack.io/)) to build a shadow jar generetes the following error when a custom walker run using `java -jar shadowJar.jar CustomTool`, but also with public tools. Only if `--use_jdk_deflater true` is provided, it works. . The log is the following (there is no other log):. ```; 14:57:19.102 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985
https://github.com/broadinstitute/gatk/issues/1985:1170,Testability,log,log,1170," CustomTool`, but also with public tools. Only if `--use_jdk_deflater true` is provided, it works. . The log is the following (there is no other log):. ```; 14:57:19.102 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/daniel/workspaces/gatk4test/build/libs/shadowJar-0.0.1-SNAPSHOT-all.jar!/com/intel/gkl/native/libIntelGKL.dylib; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGILL (0x4) at pc=0x0000000128c014d0, pid=31197, tid=5891; #; # JRE version: Java(TM) SE Runtime Environment (8.0_60-b27) (build 1.8.0_60-b27); # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode bsd-amd64 compressed oops); # Problematic frame:; # C [libIntelGKL8818190486223479934.dylib+0xe4d0] _ZN7ContextIfEC2Ev+0x30; #; # Core dump written. Default location: /cores/core or core.31197; #; # An error report file with more information is saved as:; # /Users/daniel/workspaces/gatk4test/hs_err_pid31197.log; #; # If you would like to submit a bug report, please visit:; # http://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #; Abort trap: 6 (core dumped); ```. To fix it, I tried by excluding `com.intel.gkl` from GATK and add it as a dependency to my program, but it blows up anyway. In addition, I tried a sample program to load the PairHMM fastest implementation by `PairHMM.Implementation.FASTEST_AVAILABLE.makeNewHMM()`, and it also blows up. If I remove completely the dependency in my shadow jar, the command line blows up because the gkl `IntelDeflaterFactory` is not found. I guess that the error in the library is GKL-related, but in the case of the GATK framework I would like to have a way of using the library without assuming that the final user will have support for the native code or not. Could this be done? I prefer not to remove the faster code by intel because I know that some users will benefit from it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1985
https://github.com/broadinstitute/gatk/issues/1988:94,Performance,load,loading,94,"We have a somewhat urgent need for a Spark partitioner (or something similar) that can handle loading of records for regions that overlap. For example, we'd like to be able to have a partition with all reads in `1:900-2100`, and a separate partition with reads in `1:1900-3100`, and some reads in the overlapping regions would appear in both partitions. . Such a partitioner is needed for several current GATK projects, including:. https://github.com/broadinstitute/gatk/issues/1639 Spark version of HaplotypeCaller (the prototype currently uses an inefficient `groupBy` operation to accomplish this). https://github.com/broadinstitute/gatk/issues/1558 Make pileups in parallel on Spark . It also might be needed for avoiding boundary artifacts in the SV caller.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1988
https://github.com/broadinstitute/gatk/issues/1988:717,Safety,avoid,avoiding,717,"We have a somewhat urgent need for a Spark partitioner (or something similar) that can handle loading of records for regions that overlap. For example, we'd like to be able to have a partition with all reads in `1:900-2100`, and a separate partition with reads in `1:1900-3100`, and some reads in the overlapping regions would appear in both partitions. . Such a partitioner is needed for several current GATK projects, including:. https://github.com/broadinstitute/gatk/issues/1639 Spark version of HaplotypeCaller (the prototype currently uses an inefficient `groupBy` operation to accomplish this). https://github.com/broadinstitute/gatk/issues/1558 Make pileups in parallel on Spark . It also might be needed for avoiding boundary artifacts in the SV caller.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1988
https://github.com/broadinstitute/gatk/issues/1990:18,Usability,simpl,simplify,18,This will greatly simplify setup for gatk-protected.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1990
https://github.com/broadinstitute/gatk/issues/1993:220,Testability,log,log,220,"For an unknown reason master snapshots are failing. They're trying to upload with the version ""unspecified-SNAPSHOT"". I don't know why they're not setting the version properly. . The last good build was ; f3cd74 [travis-log](https://travis-ci.org/broadinstitute/gatk/jobs/143069453); the next build has nothing suspicious in it as far as I can tell; 69a0f89 [travis-log](https://travis-ci.org/broadinstitute/gatk/jobs/143174237). I suspect it may have something to do with the shallow clone that travis does. I suspect that with the clone being shallow, the last tag was lost and git describe is unable to describe. I'm not sure why it occurred on 203 though, since the clone is 50 deep. In any case, using the clone command from github results in a clone that reports an undefined version. . `git clone --depth=50 --branch=master https://github.com/broadinstitute/gatk.git broadinstitute/gatk`. Not sure what the best solution is, but the easiest is to just set the version manually.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1993
https://github.com/broadinstitute/gatk/issues/1993:366,Testability,log,log,366,"For an unknown reason master snapshots are failing. They're trying to upload with the version ""unspecified-SNAPSHOT"". I don't know why they're not setting the version properly. . The last good build was ; f3cd74 [travis-log](https://travis-ci.org/broadinstitute/gatk/jobs/143069453); the next build has nothing suspicious in it as far as I can tell; 69a0f89 [travis-log](https://travis-ci.org/broadinstitute/gatk/jobs/143174237). I suspect it may have something to do with the shallow clone that travis does. I suspect that with the clone being shallow, the last tag was lost and git describe is unable to describe. I'm not sure why it occurred on 203 though, since the clone is 50 deep. In any case, using the clone command from github results in a clone that reports an undefined version. . `git clone --depth=50 --branch=master https://github.com/broadinstitute/gatk.git broadinstitute/gatk`. Not sure what the best solution is, but the easiest is to just set the version manually.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1993
https://github.com/broadinstitute/gatk/pull/1994:6,Integrability,depend,dependency,6,Add a dependency to the official NIO jar and include tests for gcloud-java-nio. This is based on the earlier NIO example. Now that NIO support is official we can move this test to our main branch. CC: @lbergelson @droazen,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1994
https://github.com/broadinstitute/gatk/pull/1994:53,Testability,test,tests,53,Add a dependency to the official NIO jar and include tests for gcloud-java-nio. This is based on the earlier NIO example. Now that NIO support is official we can move this test to our main branch. CC: @lbergelson @droazen,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1994
https://github.com/broadinstitute/gatk/pull/1994:172,Testability,test,test,172,Add a dependency to the official NIO jar and include tests for gcloud-java-nio. This is based on the earlier NIO example. Now that NIO support is official we can move this test to our main branch. CC: @lbergelson @droazen,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1994
https://github.com/broadinstitute/gatk/issues/1999:451,Availability,avail,available,451,"As of https://github.com/broadinstitute/gatk/pull/1424, `VariantWalker` prefers the sequence dictionary from the driving variants in `getBestAvailableSequenceDictionary()` over other sequence dictionaries, as it should. However, if the driving variants input does not have a declared sequence dictionary, but does have an index file, we end up creating an incomplete sequence dictionary from the index with no contig lengths and using it as the ""best available"" dictionary, even if better dictionaries from the reference or reads are available. This is discussed in https://github.com/broadinstitute/gatk/pull/1424#discussion_r70674157, and is replicated in the test `ExampleVariantWalkerIntegrationTest.testExampleVariantWalker_UndefinedContigLengthsInDictionary`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1999
https://github.com/broadinstitute/gatk/issues/1999:534,Availability,avail,available,534,"As of https://github.com/broadinstitute/gatk/pull/1424, `VariantWalker` prefers the sequence dictionary from the driving variants in `getBestAvailableSequenceDictionary()` over other sequence dictionaries, as it should. However, if the driving variants input does not have a declared sequence dictionary, but does have an index file, we end up creating an incomplete sequence dictionary from the index with no contig lengths and using it as the ""best available"" dictionary, even if better dictionaries from the reference or reads are available. This is discussed in https://github.com/broadinstitute/gatk/pull/1424#discussion_r70674157, and is replicated in the test `ExampleVariantWalkerIntegrationTest.testExampleVariantWalker_UndefinedContigLengthsInDictionary`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1999
https://github.com/broadinstitute/gatk/issues/1999:662,Testability,test,test,662,"As of https://github.com/broadinstitute/gatk/pull/1424, `VariantWalker` prefers the sequence dictionary from the driving variants in `getBestAvailableSequenceDictionary()` over other sequence dictionaries, as it should. However, if the driving variants input does not have a declared sequence dictionary, but does have an index file, we end up creating an incomplete sequence dictionary from the index with no contig lengths and using it as the ""best available"" dictionary, even if better dictionaries from the reference or reads are available. This is discussed in https://github.com/broadinstitute/gatk/pull/1424#discussion_r70674157, and is replicated in the test `ExampleVariantWalkerIntegrationTest.testExampleVariantWalker_UndefinedContigLengthsInDictionary`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1999
https://github.com/broadinstitute/gatk/issues/2000:177,Testability,log,logical,177,Trying to use an url of the form hdfs://server:port/path/to/file as an input to a spark program. This used to work. Now we're confused by the possibility that we're declaring a logical name.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2000
https://github.com/broadinstitute/gatk/issues/2001:4,Deployability,update,updated,4,"The updated jBWA code allows changing scoring parameters for `bwa mem`. ; Specifically, the `BwaMem` class now has a method `updateScoringParameters()`. This ticket is to remind the SV group to update, accordingly, the class `AlignContigsAndCallBreakpointsSpark` and `ContigAligner.java`. Closes #1942 .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2001
https://github.com/broadinstitute/gatk/issues/2001:125,Deployability,update,updateScoringParameters,125,"The updated jBWA code allows changing scoring parameters for `bwa mem`. ; Specifically, the `BwaMem` class now has a method `updateScoringParameters()`. This ticket is to remind the SV group to update, accordingly, the class `AlignContigsAndCallBreakpointsSpark` and `ContigAligner.java`. Closes #1942 .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2001
https://github.com/broadinstitute/gatk/issues/2001:194,Deployability,update,update,194,"The updated jBWA code allows changing scoring parameters for `bwa mem`. ; Specifically, the `BwaMem` class now has a method `updateScoringParameters()`. This ticket is to remind the SV group to update, accordingly, the class `AlignContigsAndCallBreakpointsSpark` and `ContigAligner.java`. Closes #1942 .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2001
https://github.com/broadinstitute/gatk/issues/2003:3133,Integrability,interface,interface,3133,"eProgramTest.java:76); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:80); at org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCountsIntegrationTest.testSparkGenomeReadCounts(SparkGenomeReadCountsIntegrationTest.java:28). Caused by:; java.io.NotSerializableException: org.broadinstitute.hellbender.engine.TaggedInputFileArgument; Serialization stack:; - object not serializable (class: org.broadinstitute.hellbender.engine.TaggedInputFileArgument, value: /home/travis/build/broadinstitute/gatk-protected/src/test/resources/org/broadinstitute/hellbender/tools/genome/HCC1143_chr3_1K_11K.tiny.bam); - writeObject data (class: java.util.ArrayList); - object (class java.util.ArrayList, [/home/travis/build/broadinstitute/gatk-protected/src/test/resources/org/broadinstitute/hellbender/tools/genome/HCC1143_chr3_1K_11K.tiny.bam]); - field (class: org.broadinstitute.hellbender.cmdline.argumentcollections.OptionalReadInputArgumentCollection, name: readInputs, type: interface java.util.List); - object (class org.broadinstitute.hellbender.cmdline.argumentcollections.OptionalReadInputArgumentCollection, org.broadinstitute.hellbender.cmdline.argumentcollections.OptionalReadInputArgumentCollection@21d212c8); - field (class: org.broadinstitute.hellbender.engine.spark.GATKSparkTool, name: readArguments, type: class org.broadinstitute.hellbender.cmdline.argumentcollections.ReadInputArgumentCollection); - object (class org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts, org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts@5aef1838); - element of array (index: 0); - array (class [Ljava.lang.Object;, size 2); - field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;); - object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts, functionalInterfaceMethod=org/apache/spark/ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2003
https://github.com/broadinstitute/gatk/issues/2003:4967,Integrability,interface,interface,4967,"er.cmdline.argumentcollections.ReadInputArgumentCollection); - object (class org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts, org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts@5aef1838); - element of array (index: 0); - array (class [Ljava.lang.Object;, size 2); - field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;); - object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts, functionalInterfaceMethod=org/apache/spark/api/java/function/Function.call:(Ljava/lang/Object;)Ljava/lang/Object;, implementation=invokeSpecial org/broadinstitute/hellbender/tools/genome/SparkGenomeReadCounts.lambda$collectReads$24c02dc7$2:(Lhtsjdk/samtools/SAMSequenceDictionary;Lorg/broadinstitute/hellbender/utils/read/GATKRead;)Lorg/broadinstitute/hellbender/utils/SimpleInterval;, instantiatedMethodType=(Lorg/broadinstitute/hellbender/utils/read/GATKRead;)Lorg/broadinstitute/hellbender/utils/SimpleInterval;, numCaptured=2]); - writeReplace data (class: java.lang.invoke.SerializedLambda); - object (class org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts$$Lambda$1140/887035829, org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts$$Lambda$1140/887035829@1a912c1e); - field (class: org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1, name: fun$1, type: interface org.apache.spark.api.java.function.Function); - object (class org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1, <function1>); at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101); at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301); ... 23 more; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2003
https://github.com/broadinstitute/gatk/issues/2003:27,Testability,test,tests,27,really strange that public tests on spark dont catch this. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCountsIntegrationTest.testSparkGenomeReadCounts FAILED; org.apache.spark.SparkException: Task not serializable; at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:304); at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122); at org.apache.spark.SparkContext.clean(SparkContext.scala:2055); at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:324); at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:323); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); at org.apache.spark.rdd.RDD.map(RDD.scala:323); at org.apache.spark.api.java.JavaRDDLike$class.map(JavaRDDLike.scala:96); at org.apache.spark.api.java.AbstractJavaRDDLike.map(JavaRDDLike.scala:46); at org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts.collectReads(SparkGenomeReadCounts.java:127); at org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts.runTool(SparkGenomeReadCounts.java:207); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2003
https://github.com/broadinstitute/gatk/issues/2003:86,Testability,test,test,86,really strange that public tests on spark dont catch this. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCountsIntegrationTest.testSparkGenomeReadCounts FAILED; org.apache.spark.SparkException: Task not serializable; at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:304); at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122); at org.apache.spark.SparkContext.clean(SparkContext.scala:2055); at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:324); at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:323); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); at org.apache.spark.rdd.RDD.map(RDD.scala:323); at org.apache.spark.api.java.JavaRDDLike$class.map(JavaRDDLike.scala:96); at org.apache.spark.api.java.AbstractJavaRDDLike.map(JavaRDDLike.scala:46); at org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts.collectReads(SparkGenomeReadCounts.java:127); at org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts.runTool(SparkGenomeReadCounts.java:207); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2003
https://github.com/broadinstitute/gatk/issues/2003:173,Testability,test,testSparkGenomeReadCounts,173,really strange that public tests on spark dont catch this. ```; Gradle suite > Gradle test > org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCountsIntegrationTest.testSparkGenomeReadCounts FAILED; org.apache.spark.SparkException: Task not serializable; at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:304); at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122); at org.apache.spark.SparkContext.clean(SparkContext.scala:2055); at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:324); at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:323); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); at org.apache.spark.rdd.RDD.map(RDD.scala:323); at org.apache.spark.api.java.JavaRDDLike$class.map(JavaRDDLike.scala:96); at org.apache.spark.api.java.AbstractJavaRDDLike.map(JavaRDDLike.scala:46); at org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts.collectReads(SparkGenomeReadCounts.java:127); at org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts.runTool(SparkGenomeReadCounts.java:207); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2003
https://github.com/broadinstitute/gatk/issues/2003:2320,Testability,test,testSparkGenomeReadCounts,2320,"eadCounts.java:207); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:83); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:76); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:80); at org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCountsIntegrationTest.testSparkGenomeReadCounts(SparkGenomeReadCountsIntegrationTest.java:28). Caused by:; java.io.NotSerializableException: org.broadinstitute.hellbender.engine.TaggedInputFileArgument; Serialization stack:; - object not serializable (class: org.broadinstitute.hellbender.engine.TaggedInputFileArgument, value: /home/travis/build/broadinstitute/gatk-protected/src/test/resources/org/broadinstitute/hellbender/tools/genome/HCC1143_chr3_1K_11K.tiny.bam); - writeObject data (class: java.util.ArrayList); - object (class java.util.ArrayList, [/home/travis/build/broadinstitute/gatk-protected/src/test/resources/org/broadinstitute/hellbender/tools/genome/HCC1143_chr3_1K_11K.tiny.bam]); - field (class: org.broadinstitute.hellbender.cmdline.argumentcollections.OptionalReadInputArgumentCollection, name: readInputs, type: interface java.util.List); - object (class org.broadinstitute.hellbender.cmdline.argumentcollections.OptionalReadInputArgumentCollection, org.broadinstitute.hellbender.cmdline.argumentcollections.OptionalReadInputArgume",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2003
https://github.com/broadinstitute/gatk/issues/2003:2679,Testability,test,test,2679,"); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:83); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:76); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:80); at org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCountsIntegrationTest.testSparkGenomeReadCounts(SparkGenomeReadCountsIntegrationTest.java:28). Caused by:; java.io.NotSerializableException: org.broadinstitute.hellbender.engine.TaggedInputFileArgument; Serialization stack:; - object not serializable (class: org.broadinstitute.hellbender.engine.TaggedInputFileArgument, value: /home/travis/build/broadinstitute/gatk-protected/src/test/resources/org/broadinstitute/hellbender/tools/genome/HCC1143_chr3_1K_11K.tiny.bam); - writeObject data (class: java.util.ArrayList); - object (class java.util.ArrayList, [/home/travis/build/broadinstitute/gatk-protected/src/test/resources/org/broadinstitute/hellbender/tools/genome/HCC1143_chr3_1K_11K.tiny.bam]); - field (class: org.broadinstitute.hellbender.cmdline.argumentcollections.OptionalReadInputArgumentCollection, name: readInputs, type: interface java.util.List); - object (class org.broadinstitute.hellbender.cmdline.argumentcollections.OptionalReadInputArgumentCollection, org.broadinstitute.hellbender.cmdline.argumentcollections.OptionalReadInputArgumentCollection@21d212c8); - field (class: org.broadinstitute.hellbender.engine.spark.GATKSparkTool, name: readArguments, type: class org.broadinstitute.hellbender.cmdline.argumentcollections.ReadInputArgumentCollection); - object (class org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts, org.broadinstitute.hell",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2003
https://github.com/broadinstitute/gatk/issues/2003:2908,Testability,test,test,2908,"tute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:83); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:76); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:80); at org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCountsIntegrationTest.testSparkGenomeReadCounts(SparkGenomeReadCountsIntegrationTest.java:28). Caused by:; java.io.NotSerializableException: org.broadinstitute.hellbender.engine.TaggedInputFileArgument; Serialization stack:; - object not serializable (class: org.broadinstitute.hellbender.engine.TaggedInputFileArgument, value: /home/travis/build/broadinstitute/gatk-protected/src/test/resources/org/broadinstitute/hellbender/tools/genome/HCC1143_chr3_1K_11K.tiny.bam); - writeObject data (class: java.util.ArrayList); - object (class java.util.ArrayList, [/home/travis/build/broadinstitute/gatk-protected/src/test/resources/org/broadinstitute/hellbender/tools/genome/HCC1143_chr3_1K_11K.tiny.bam]); - field (class: org.broadinstitute.hellbender.cmdline.argumentcollections.OptionalReadInputArgumentCollection, name: readInputs, type: interface java.util.List); - object (class org.broadinstitute.hellbender.cmdline.argumentcollections.OptionalReadInputArgumentCollection, org.broadinstitute.hellbender.cmdline.argumentcollections.OptionalReadInputArgumentCollection@21d212c8); - field (class: org.broadinstitute.hellbender.engine.spark.GATKSparkTool, name: readArguments, type: class org.broadinstitute.hellbender.cmdline.argumentcollections.ReadInputArgumentCollection); - object (class org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts, org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts@5aef1838); - element of array (index: 0); - array (class [Ljava.lang.Object;, size 2); - field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;); - obj",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2003
https://github.com/broadinstitute/gatk/issues/2003:4434,Usability,Simpl,SimpleInterval,4434,"me: readArguments, type: class org.broadinstitute.hellbender.cmdline.argumentcollections.ReadInputArgumentCollection); - object (class org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts, org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts@5aef1838); - element of array (index: 0); - array (class [Ljava.lang.Object;, size 2); - field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;); - object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts, functionalInterfaceMethod=org/apache/spark/api/java/function/Function.call:(Ljava/lang/Object;)Ljava/lang/Object;, implementation=invokeSpecial org/broadinstitute/hellbender/tools/genome/SparkGenomeReadCounts.lambda$collectReads$24c02dc7$2:(Lhtsjdk/samtools/SAMSequenceDictionary;Lorg/broadinstitute/hellbender/utils/read/GATKRead;)Lorg/broadinstitute/hellbender/utils/SimpleInterval;, instantiatedMethodType=(Lorg/broadinstitute/hellbender/utils/read/GATKRead;)Lorg/broadinstitute/hellbender/utils/SimpleInterval;, numCaptured=2]); - writeReplace data (class: java.lang.invoke.SerializedLambda); - object (class org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts$$Lambda$1140/887035829, org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts$$Lambda$1140/887035829@1a912c1e); - field (class: org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1, name: fun$1, type: interface org.apache.spark.api.java.function.Function); - object (class org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1, <function1>); at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101); at org.apache.spark.util.ClosureCleaner$.ensur",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2003
https://github.com/broadinstitute/gatk/issues/2003:4564,Usability,Simpl,SimpleInterval,4564,"me: readArguments, type: class org.broadinstitute.hellbender.cmdline.argumentcollections.ReadInputArgumentCollection); - object (class org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts, org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts@5aef1838); - element of array (index: 0); - array (class [Ljava.lang.Object;, size 2); - field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;); - object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts, functionalInterfaceMethod=org/apache/spark/api/java/function/Function.call:(Ljava/lang/Object;)Ljava/lang/Object;, implementation=invokeSpecial org/broadinstitute/hellbender/tools/genome/SparkGenomeReadCounts.lambda$collectReads$24c02dc7$2:(Lhtsjdk/samtools/SAMSequenceDictionary;Lorg/broadinstitute/hellbender/utils/read/GATKRead;)Lorg/broadinstitute/hellbender/utils/SimpleInterval;, instantiatedMethodType=(Lorg/broadinstitute/hellbender/utils/read/GATKRead;)Lorg/broadinstitute/hellbender/utils/SimpleInterval;, numCaptured=2]); - writeReplace data (class: java.lang.invoke.SerializedLambda); - object (class org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts$$Lambda$1140/887035829, org.broadinstitute.hellbender.tools.genome.SparkGenomeReadCounts$$Lambda$1140/887035829@1a912c1e); - field (class: org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1, name: fun$1, type: interface org.apache.spark.api.java.function.Function); - object (class org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1, <function1>); at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101); at org.apache.spark.util.ClosureCleaner$.ensur",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2003
https://github.com/broadinstitute/gatk/pull/2005:51,Testability,test,test,51,Please ignore this. It will not be merged. Need to test Jenkins hook.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2005
https://github.com/broadinstitute/gatk/pull/2006:8,Testability,test,test,8,another test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2006
https://github.com/broadinstitute/gatk/pull/2011:100,Deployability,update,updates,100,"We got an email complaining that we were the only users in the world still using the beta API. This updates to the most recent version, they changed the api from `List<String> -> List<Object>`, but the documentation still says it has to be a `String`, so I've just added some explicit casts. . Fix for #2009",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2011
https://github.com/broadinstitute/gatk/issues/2012:22,Performance,perform,performance,22,"Many people are doing performance work on spark, but we're not collecting that data in a single place so people are reinventing the wheel. We should try to collect at least some spark best practice information in a single place.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2012
https://github.com/broadinstitute/gatk/pull/2013:6,Integrability,depend,dependency,6,Add a dependency to the official NIO jar and include tests for gcloud-java-nio. This is based on the earlier NIO example. Now that NIO support is official we can move this test to our main branch. I'm resubmitting this as a branch from broadinstitute/ directly so (hopefully) the Travis cloud tests will run. CC: @lbergelson @droazen,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2013
https://github.com/broadinstitute/gatk/pull/2013:53,Testability,test,tests,53,Add a dependency to the official NIO jar and include tests for gcloud-java-nio. This is based on the earlier NIO example. Now that NIO support is official we can move this test to our main branch. I'm resubmitting this as a branch from broadinstitute/ directly so (hopefully) the Travis cloud tests will run. CC: @lbergelson @droazen,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2013
https://github.com/broadinstitute/gatk/pull/2013:172,Testability,test,test,172,Add a dependency to the official NIO jar and include tests for gcloud-java-nio. This is based on the earlier NIO example. Now that NIO support is official we can move this test to our main branch. I'm resubmitting this as a branch from broadinstitute/ directly so (hopefully) the Travis cloud tests will run. CC: @lbergelson @droazen,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2013
https://github.com/broadinstitute/gatk/pull/2013:293,Testability,test,tests,293,Add a dependency to the official NIO jar and include tests for gcloud-java-nio. This is based on the earlier NIO example. Now that NIO support is official we can move this test to our main branch. I'm resubmitting this as a branch from broadinstitute/ directly so (hopefully) the Travis cloud tests will run. CC: @lbergelson @droazen,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2013
https://github.com/broadinstitute/gatk/issues/2014:97,Availability,avail,available,97,@cwhelan mentioned at @jamesemery's presentation last week that there are more efficient options available for getting data into HDFS. It's worth exploring these.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2014
https://github.com/broadinstitute/gatk/issues/2014:79,Energy Efficiency,efficient,efficient,79,@cwhelan mentioned at @jamesemery's presentation last week that there are more efficient options available for getting data into HDFS. It's worth exploring these.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2014
https://github.com/broadinstitute/gatk/issues/2015:149,Energy Efficiency,reduce,reduce,149,"With the new GCS NIO reader, it may well be preferable to access large side inputs directly in GCS buckets rather than broadcasting them. This would reduce our memory usage dramatically relative to broadcast, and if the performance is the same or better it seems like the way to go.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2015
https://github.com/broadinstitute/gatk/issues/2015:220,Performance,perform,performance,220,"With the new GCS NIO reader, it may well be preferable to access large side inputs directly in GCS buckets rather than broadcasting them. This would reduce our memory usage dramatically relative to broadcast, and if the performance is the same or better it seems like the way to go.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2015
https://github.com/broadinstitute/gatk/issues/2015:58,Security,access,access,58,"With the new GCS NIO reader, it may well be preferable to access large side inputs directly in GCS buckets rather than broadcasting them. This would reduce our memory usage dramatically relative to broadcast, and if the performance is the same or better it seems like the way to go.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2015
https://github.com/broadinstitute/gatk/issues/2019:71,Deployability,A/B,A/B,71,"`HaplotypeCaller` works with unphased genotypes. The unphased genotype A/B represent the phased genotpyes A/B _and_ B/A, for a combinatorial factor of 2, in contrast to A/A, which has a combinatorial factor of 1. It does not appear that the genotype likelihoods emitted by `GenotypeLikelihoodCalculator` account for this. That is, the genotype likelihoods for heterozygous diploid genotypes are too small by a factor of 2. The relevant code in `GenotypeLikelihoodCalculator` is. ``` java; private double[] genotypeLikelihoods( [arguments]) {; ...; for (int g = 0; g < genotypeCount; g++) {; result[g] = MathUtils.sum(readLikelihoodsByGenotypeIndex[g], 0, readCount) - denominator;; [ we need result += (GenotypeAlleleCounts of genotype g).log10CombinationCount() ]; }; ...; ```. @ldgauthier @vruano do you agree?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019
https://github.com/broadinstitute/gatk/issues/2019:106,Deployability,A/B,A/B,106,"`HaplotypeCaller` works with unphased genotypes. The unphased genotype A/B represent the phased genotpyes A/B _and_ B/A, for a combinatorial factor of 2, in contrast to A/A, which has a combinatorial factor of 1. It does not appear that the genotype likelihoods emitted by `GenotypeLikelihoodCalculator` account for this. That is, the genotype likelihoods for heterozygous diploid genotypes are too small by a factor of 2. The relevant code in `GenotypeLikelihoodCalculator` is. ``` java; private double[] genotypeLikelihoods( [arguments]) {; ...; for (int g = 0; g < genotypeCount; g++) {; result[g] = MathUtils.sum(readLikelihoodsByGenotypeIndex[g], 0, readCount) - denominator;; [ we need result += (GenotypeAlleleCounts of genotype g).log10CombinationCount() ]; }; ...; ```. @ldgauthier @vruano do you agree?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019
https://github.com/broadinstitute/gatk/issues/2020:2129,Deployability,deploy,deploy,2129,ellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:464); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:458); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.validateToolInputs(GATKSparkTool.java:402); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:312); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2020
https://github.com/broadinstitute/gatk/issues/2020:2166,Deployability,deploy,deploy,2166,ellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:464); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:458); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.validateToolInputs(GATKSparkTool.java:402); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:312); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2020
https://github.com/broadinstitute/gatk/issues/2020:2238,Deployability,deploy,deploy,2238,ellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:464); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:458); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.validateToolInputs(GATKSparkTool.java:402); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:312); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2020
https://github.com/broadinstitute/gatk/issues/2020:2314,Deployability,deploy,deploy,2314,ellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:464); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:458); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.validateToolInputs(GATKSparkTool.java:402); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:312); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2020
https://github.com/broadinstitute/gatk/issues/2020:2385,Deployability,deploy,deploy,2385,ellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:464); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:458); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.validateToolInputs(GATKSparkTool.java:402); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:312); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2020
https://github.com/broadinstitute/gatk/issues/2020:2454,Deployability,deploy,deploy,2454,ellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:464); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:458); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.validateToolInputs(GATKSparkTool.java:402); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:312); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2020
https://github.com/broadinstitute/gatk/issues/2020:893,Security,validat,validateDictionaries,893,Reported by Intel. Should be a `UserException` instead. ```; ./gatk-launch BwaSpark -I hdfs://sn1:8020/user/$USER/gatk/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -R hdfs://sn1:8020/user/$USER/gatk/human_g1k_v37.fasta -O hdfs://sn1:8020/user/$USER/gatk/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -- --sparkRunner SPARK --sparkMaster spark://sn1:7077 --driver-memory 8G --num-executors 4 --executor-cores 9 --executor-memory 27g; ```. ```; java.lang.NullPointerException; at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:464); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:458); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.validateToolInputs(GATKSparkTool.java:402); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:312); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAcce,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2020
https://github.com/broadinstitute/gatk/issues/2020:1012,Security,validat,validateDictionaries,1012,erException` instead. ```; ./gatk-launch BwaSpark -I hdfs://sn1:8020/user/$USER/gatk/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -R hdfs://sn1:8020/user/$USER/gatk/human_g1k_v37.fasta -O hdfs://sn1:8020/user/$USER/gatk/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -- --sparkRunner SPARK --sparkMaster spark://sn1:7077 --driver-memory 8G --num-executors 4 --executor-cores 9 --executor-memory 27g; ```. ```; java.lang.NullPointerException; at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:464); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:458); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.validateToolInputs(GATKSparkTool.java:402); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:312); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2020
https://github.com/broadinstitute/gatk/issues/2020:1127,Security,validat,validateToolInputs,1127,12892.readnamesort.bam -R hdfs://sn1:8020/user/$USER/gatk/human_g1k_v37.fasta -O hdfs://sn1:8020/user/$USER/gatk/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -- --sparkRunner SPARK --sparkMaster spark://sn1:7077 --driver-memory 8G --num-executors 4 --executor-cores 9 --executor-memory 27g; ```. ```; java.lang.NullPointerException; at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:464); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:458); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.validateToolInputs(GATKSparkTool.java:402); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:312); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSub,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2020
https://github.com/broadinstitute/gatk/issues/2022:150,Performance,perform,perform,150,"While working on #1394 I noticed that there are some issues with clipping op that need decisions to be made about how they work. Specifically, if you perform a Softclip operation on a read where you tell it to Softclip the entire read it will silently shorten the clip to include everything but the last base of the read in the following code from ClippingOp.java:. `if ((stop + 1 - start) == readCopied.getLength()) { ; // BAM representation issue -- we can't SOFTCLIP away all bases in a read, just leave it alone; myStop--; // just decrement stop; }`. This behavior is inconsistent with the behavior of a hardclip operation, where the tool will simply produce a read with an empty cigar string. Arbitrarily deciding not to clip the read to one particular base does not seem like the right way to deal with this. I added a test ReadClipperUnitTest.java that explores this behavior in #2021 pull request.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2022
https://github.com/broadinstitute/gatk/issues/2022:825,Testability,test,test,825,"While working on #1394 I noticed that there are some issues with clipping op that need decisions to be made about how they work. Specifically, if you perform a Softclip operation on a read where you tell it to Softclip the entire read it will silently shorten the clip to include everything but the last base of the read in the following code from ClippingOp.java:. `if ((stop + 1 - start) == readCopied.getLength()) { ; // BAM representation issue -- we can't SOFTCLIP away all bases in a read, just leave it alone; myStop--; // just decrement stop; }`. This behavior is inconsistent with the behavior of a hardclip operation, where the tool will simply produce a read with an empty cigar string. Arbitrarily deciding not to clip the read to one particular base does not seem like the right way to deal with this. I added a test ReadClipperUnitTest.java that explores this behavior in #2021 pull request.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2022
https://github.com/broadinstitute/gatk/issues/2022:648,Usability,simpl,simply,648,"While working on #1394 I noticed that there are some issues with clipping op that need decisions to be made about how they work. Specifically, if you perform a Softclip operation on a read where you tell it to Softclip the entire read it will silently shorten the clip to include everything but the last base of the read in the following code from ClippingOp.java:. `if ((stop + 1 - start) == readCopied.getLength()) { ; // BAM representation issue -- we can't SOFTCLIP away all bases in a read, just leave it alone; myStop--; // just decrement stop; }`. This behavior is inconsistent with the behavior of a hardclip operation, where the tool will simply produce a read with an empty cigar string. Arbitrarily deciding not to clip the read to one particular base does not seem like the right way to deal with this. I added a test ReadClipperUnitTest.java that explores this behavior in #2021 pull request.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2022
https://github.com/broadinstitute/gatk/issues/2026:3296,Availability,down,down,3296,"VICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 15:31:00.557 INFO SplitNCigarReads - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 15:31:00.557 INFO SplitNCigarReads - Defaults.REFERENCE_FASTA : null; 15:31:00.557 INFO SplitNCigarReads - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:31:00.558 INFO SplitNCigarReads - Deflater IntelDeflater; 15:31:00.558 INFO SplitNCigarReads - Initializing engine; 15:31:00.659 INFO SplitNCigarReads - Done initializing engine; 15:31:00.679 INFO ProgressMeter - Starting traversal; 15:31:00.679 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 15:31:05.088 INFO SplitNCigarReads - Shutting down engine; [July 20, 2016 3:31:05 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1011875840; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:250,Deployability,patch,patches,250,"SplitNCigarReads is failing due to incompatibilities between htsjdk's version of snappy and Spark's version. Temporary solution is to add system property 'disable.snappy' to force htsjdk to fallback to pure java. Longer term solution likely involves patches to htsjdk and possibly snappy itself. ```; ./gatk-launch SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta. Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta; 15:31:00.516 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; 15:31:00.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [July 20, 2016 3:31:00 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads --output out.bam --input src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --reference src/test/resources/large/human_g1k_v37.20.21.fasta --refactor_NDN_cigar_string false --maxReadsInMemory 150000 --maxMismatchesInOverhang 1 --maxBasesInOverhang 40 --doNotFixOverhangs false --disable_all_read_filters false --interval_set_rule UNION --interval_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [July 20, 2016 3:31:00 PM EDT] Executing as louisb@wm1b0-8ab on Mac OS X 10.10.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-217-g3ff51ed-SNAPSHOT; 15:31:00.557 INFO SplitNCigarReads - Defaults.BUFFER_SIZE : 131072; 15:31:00.557 INFO SplitNCigarReads - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:511,Deployability,install,install,511,"SplitNCigarReads is failing due to incompatibilities between htsjdk's version of snappy and Spark's version. Temporary solution is to add system property 'disable.snappy' to force htsjdk to fallback to pure java. Longer term solution likely involves patches to htsjdk and possibly snappy itself. ```; ./gatk-launch SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta. Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta; 15:31:00.516 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; 15:31:00.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [July 20, 2016 3:31:00 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads --output out.bam --input src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --reference src/test/resources/large/human_g1k_v37.20.21.fasta --refactor_NDN_cigar_string false --maxReadsInMemory 150000 --maxMismatchesInOverhang 1 --maxBasesInOverhang 40 --doNotFixOverhangs false --disable_all_read_filters false --interval_set_rule UNION --interval_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [July 20, 2016 3:31:00 PM EDT] Executing as louisb@wm1b0-8ab on Mac OS X 10.10.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-217-g3ff51ed-SNAPSHOT; 15:31:00.557 INFO SplitNCigarReads - Defaults.BUFFER_SIZE : 131072; 15:31:00.557 INFO SplitNCigarReads - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:802,Deployability,install,install,802,"SplitNCigarReads is failing due to incompatibilities between htsjdk's version of snappy and Spark's version. Temporary solution is to add system property 'disable.snappy' to force htsjdk to fallback to pure java. Longer term solution likely involves patches to htsjdk and possibly snappy itself. ```; ./gatk-launch SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta. Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta; 15:31:00.516 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; 15:31:00.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [July 20, 2016 3:31:00 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads --output out.bam --input src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --reference src/test/resources/large/human_g1k_v37.20.21.fasta --refactor_NDN_cigar_string false --maxReadsInMemory 150000 --maxMismatchesInOverhang 1 --maxBasesInOverhang 40 --doNotFixOverhangs false --disable_all_read_filters false --interval_set_rule UNION --interval_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [July 20, 2016 3:31:00 PM EDT] Executing as louisb@wm1b0-8ab on Mac OS X 10.10.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-217-g3ff51ed-SNAPSHOT; 15:31:00.557 INFO SplitNCigarReads - Defaults.BUFFER_SIZE : 131072; 15:31:00.557 INFO SplitNCigarReads - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:3828,Integrability,wrap,wrapTempOutputStream,3828,":00.557 INFO SplitNCigarReads - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:31:00.558 INFO SplitNCigarReads - Deflater IntelDeflater; 15:31:00.558 INFO SplitNCigarReads - Initializing engine; 15:31:00.659 INFO SplitNCigarReads - Done initializing engine; 15:31:00.679 INFO ProgressMeter - Starting traversal; 15:31:00.679 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 15:31:05.088 INFO SplitNCigarReads - Shutting down engine; [July 20, 2016 3:31:05 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1011875840; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:728,Performance,load,load,728,"SplitNCigarReads is failing due to incompatibilities between htsjdk's version of snappy and Spark's version. Temporary solution is to add system property 'disable.snappy' to force htsjdk to fallback to pure java. Longer term solution likely involves patches to htsjdk and possibly snappy itself. ```; ./gatk-launch SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta. Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta; 15:31:00.516 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; 15:31:00.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [July 20, 2016 3:31:00 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads --output out.bam --input src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --reference src/test/resources/large/human_g1k_v37.20.21.fasta --refactor_NDN_cigar_string false --maxReadsInMemory 150000 --maxMismatchesInOverhang 1 --maxBasesInOverhang 40 --doNotFixOverhangs false --disable_all_read_filters false --interval_set_rule UNION --interval_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [July 20, 2016 3:31:00 PM EDT] Executing as louisb@wm1b0-8ab on Mac OS X 10.10.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-217-g3ff51ed-SNAPSHOT; 15:31:00.557 INFO SplitNCigarReads - Defaults.BUFFER_SIZE : 131072; 15:31:00.557 INFO SplitNCigarReads - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:926,Performance,load,loaded,926,"SplitNCigarReads is failing due to incompatibilities between htsjdk's version of snappy and Spark's version. Temporary solution is to add system property 'disable.snappy' to force htsjdk to fallback to pure java. Longer term solution likely involves patches to htsjdk and possibly snappy itself. ```; ./gatk-launch SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta. Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta; 15:31:00.516 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; 15:31:00.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [July 20, 2016 3:31:00 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads --output out.bam --input src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --reference src/test/resources/large/human_g1k_v37.20.21.fasta --refactor_NDN_cigar_string false --maxReadsInMemory 150000 --maxMismatchesInOverhang 1 --maxBasesInOverhang 40 --doNotFixOverhangs false --disable_all_read_filters false --interval_set_rule UNION --interval_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [July 20, 2016 3:31:00 PM EDT] Executing as louisb@wm1b0-8ab on Mac OS X 10.10.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-217-g3ff51ed-SNAPSHOT; 15:31:00.557 INFO SplitNCigarReads - Defaults.BUFFER_SIZE : 131072; 15:31:00.557 INFO SplitNCigarReads - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:3554,Performance,Load,LoadSnappy,3554,"ts.SAM_FLAG_FIELD_FORMAT : DECIMAL; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:31:00.558 INFO SplitNCigarReads - Deflater IntelDeflater; 15:31:00.558 INFO SplitNCigarReads - Initializing engine; 15:31:00.659 INFO SplitNCigarReads - Done initializing engine; 15:31:00.679 INFO ProgressMeter - Starting traversal; 15:31:00.679 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 15:31:05.088 INFO SplitNCigarReads - Shutting down engine; [July 20, 2016 3:31:05 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1011875840; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:4485,Performance,Load,LoadSnappy,4485,":00.557 INFO SplitNCigarReads - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:31:00.558 INFO SplitNCigarReads - Deflater IntelDeflater; 15:31:00.558 INFO SplitNCigarReads - Initializing engine; 15:31:00.659 INFO SplitNCigarReads - Done initializing engine; 15:31:00.679 INFO ProgressMeter - Starting traversal; 15:31:00.679 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 15:31:05.088 INFO SplitNCigarReads - Shutting down engine; [July 20, 2016 3:31:05 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1011875840; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:4585,Performance,load,loadClass,4585,":00.557 INFO SplitNCigarReads - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:31:00.558 INFO SplitNCigarReads - Deflater IntelDeflater; 15:31:00.558 INFO SplitNCigarReads - Initializing engine; 15:31:00.659 INFO SplitNCigarReads - Done initializing engine; 15:31:00.679 INFO ProgressMeter - Starting traversal; 15:31:00.679 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 15:31:05.088 INFO SplitNCigarReads - Shutting down engine; [July 20, 2016 3:31:05 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1011875840; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:4654,Performance,load,loadClass,4654,":00.557 INFO SplitNCigarReads - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:31:00.558 INFO SplitNCigarReads - Deflater IntelDeflater; 15:31:00.558 INFO SplitNCigarReads - Initializing engine; 15:31:00.659 INFO SplitNCigarReads - Done initializing engine; 15:31:00.679 INFO ProgressMeter - Starting traversal; 15:31:00.679 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 15:31:05.088 INFO SplitNCigarReads - Shutting down engine; [July 20, 2016 3:31:05 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1011875840; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:4709,Performance,load,loadClass,4709,":00.557 INFO SplitNCigarReads - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:31:00.557 INFO SplitNCigarReads - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:31:00.558 INFO SplitNCigarReads - Deflater IntelDeflater; 15:31:00.558 INFO SplitNCigarReads - Initializing engine; 15:31:00.659 INFO SplitNCigarReads - Done initializing engine; 15:31:00.679 INFO ProgressMeter - Starting traversal; 15:31:00.679 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 15:31:05.088 INFO SplitNCigarReads - Shutting down engine; [July 20, 2016 3:31:05 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads done. Elapsed time: 0.08 minutes.; Runtime.totalMemory()=1011875840; Exception in thread ""main"" java.lang.NoClassDefFoundError: org/xerial/snappy/LoadSnappy; at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:86); at htsjdk.samtools.util.SnappyLoader.<init>(SnappyLoader.java:52); at htsjdk.samtools.util.TempStreamFactory.getSnappyLoader(TempStreamFactory.java:42); at htsjdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:339,Testability,test,test,339,"SplitNCigarReads is failing due to incompatibilities between htsjdk's version of snappy and Spark's version. Temporary solution is to add system property 'disable.snappy' to force htsjdk to fallback to pure java. Longer term solution likely involves patches to htsjdk and possibly snappy itself. ```; ./gatk-launch SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta. Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta; 15:31:00.516 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; 15:31:00.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [July 20, 2016 3:31:00 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads --output out.bam --input src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --reference src/test/resources/large/human_g1k_v37.20.21.fasta --refactor_NDN_cigar_string false --maxReadsInMemory 150000 --maxMismatchesInOverhang 1 --maxBasesInOverhang 40 --doNotFixOverhangs false --disable_all_read_filters false --interval_set_rule UNION --interval_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [July 20, 2016 3:31:00 PM EDT] Executing as louisb@wm1b0-8ab on Mac OS X 10.10.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-217-g3ff51ed-SNAPSHOT; 15:31:00.557 INFO SplitNCigarReads - Defaults.BUFFER_SIZE : 131072; 15:31:00.557 INFO SplitNCigarReads - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:418,Testability,test,test,418,"SplitNCigarReads is failing due to incompatibilities between htsjdk's version of snappy and Spark's version. Temporary solution is to add system property 'disable.snappy' to force htsjdk to fallback to pure java. Longer term solution likely involves patches to htsjdk and possibly snappy itself. ```; ./gatk-launch SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta. Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta; 15:31:00.516 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; 15:31:00.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [July 20, 2016 3:31:00 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads --output out.bam --input src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --reference src/test/resources/large/human_g1k_v37.20.21.fasta --refactor_NDN_cigar_string false --maxReadsInMemory 150000 --maxMismatchesInOverhang 1 --maxBasesInOverhang 40 --doNotFixOverhangs false --disable_all_read_filters false --interval_set_rule UNION --interval_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [July 20, 2016 3:31:00 PM EDT] Executing as louisb@wm1b0-8ab on Mac OS X 10.10.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-217-g3ff51ed-SNAPSHOT; 15:31:00.557 INFO SplitNCigarReads - Defaults.BUFFER_SIZE : 131072; 15:31:00.557 INFO SplitNCigarReads - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:557,Testability,test,test,557,"SplitNCigarReads is failing due to incompatibilities between htsjdk's version of snappy and Spark's version. Temporary solution is to add system property 'disable.snappy' to force htsjdk to fallback to pure java. Longer term solution likely involves patches to htsjdk and possibly snappy itself. ```; ./gatk-launch SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta. Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta; 15:31:00.516 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; 15:31:00.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [July 20, 2016 3:31:00 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads --output out.bam --input src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --reference src/test/resources/large/human_g1k_v37.20.21.fasta --refactor_NDN_cigar_string false --maxReadsInMemory 150000 --maxMismatchesInOverhang 1 --maxBasesInOverhang 40 --doNotFixOverhangs false --disable_all_read_filters false --interval_set_rule UNION --interval_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [July 20, 2016 3:31:00 PM EDT] Executing as louisb@wm1b0-8ab on Mac OS X 10.10.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-217-g3ff51ed-SNAPSHOT; 15:31:00.557 INFO SplitNCigarReads - Defaults.BUFFER_SIZE : 131072; 15:31:00.557 INFO SplitNCigarReads - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:636,Testability,test,test,636,"SplitNCigarReads is failing due to incompatibilities between htsjdk's version of snappy and Spark's version. Temporary solution is to add system property 'disable.snappy' to force htsjdk to fallback to pure java. Longer term solution likely involves patches to htsjdk and possibly snappy itself. ```; ./gatk-launch SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta. Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta; 15:31:00.516 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; 15:31:00.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [July 20, 2016 3:31:00 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads --output out.bam --input src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --reference src/test/resources/large/human_g1k_v37.20.21.fasta --refactor_NDN_cigar_string false --maxReadsInMemory 150000 --maxMismatchesInOverhang 1 --maxBasesInOverhang 40 --doNotFixOverhangs false --disable_all_read_filters false --interval_set_rule UNION --interval_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [July 20, 2016 3:31:00 PM EDT] Executing as louisb@wm1b0-8ab on Mac OS X 10.10.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-217-g3ff51ed-SNAPSHOT; 15:31:00.557 INFO SplitNCigarReads - Defaults.BUFFER_SIZE : 131072; 15:31:00.557 INFO SplitNCigarReads - ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:1078,Testability,test,test,1078,"ppy and Spark's version. Temporary solution is to add system property 'disable.snappy' to force htsjdk to fallback to pure java. Longer term solution likely involves patches to htsjdk and possibly snappy itself. ```; ./gatk-launch SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta. Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta; 15:31:00.516 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; 15:31:00.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [July 20, 2016 3:31:00 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads --output out.bam --input src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --reference src/test/resources/large/human_g1k_v37.20.21.fasta --refactor_NDN_cigar_string false --maxReadsInMemory 150000 --maxMismatchesInOverhang 1 --maxBasesInOverhang 40 --doNotFixOverhangs false --disable_all_read_filters false --interval_set_rule UNION --interval_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [July 20, 2016 3:31:00 PM EDT] Executing as louisb@wm1b0-8ab on Mac OS X 10.10.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-217-g3ff51ed-SNAPSHOT; 15:31:00.557 INFO SplitNCigarReads - Defaults.BUFFER_SIZE : 131072; 15:31:00.557 INFO SplitNCigarReads - Defaults.COMPRESSION_LEVEL : 1; 15:31:00.557 INFO SplitNCigarReads - Defaults.CREATE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/issues/2026:1155,Testability,test,test,1155,".snappy' to force htsjdk to fallback to pure java. Longer term solution likely involves patches to htsjdk and possibly snappy itself. ```; ./gatk-launch SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta. Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk SplitNCigarReads -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O out.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta; 15:31:00.516 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; 15:31:00.552 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; [July 20, 2016 3:31:00 PM EDT] org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads --output out.bam --input src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam --reference src/test/resources/large/human_g1k_v37.20.21.fasta --refactor_NDN_cigar_string false --maxReadsInMemory 150000 --maxMismatchesInOverhang 1 --maxBasesInOverhang 40 --doNotFixOverhangs false --disable_all_read_filters false --interval_set_rule UNION --interval_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --addOutputSAMProgramRecord true --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false; [July 20, 2016 3:31:00 PM EDT] Executing as louisb@wm1b0-8ab on Mac OS X 10.10.5 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-217-g3ff51ed-SNAPSHOT; 15:31:00.557 INFO SplitNCigarReads - Defaults.BUFFER_SIZE : 131072; 15:31:00.557 INFO SplitNCigarReads - Defaults.COMPRESSION_LEVEL : 1; 15:31:00.557 INFO SplitNCigarReads - Defaults.CREATE_INDEX : false; 15:31:00.557 INFO SplitNCigarReads - Defaults.CREATE_MD5 : fal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2026
https://github.com/broadinstitute/gatk/pull/2028:116,Deployability,patch,patch,116,adding snappy.disable=true to prevent htsjdk from using snappy; this is a temporary solution for #2026 until we can patch htsjdk to fix incompatibilities with more modern snappy. fixes #2026,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2028
https://github.com/broadinstitute/gatk/pull/2032:36,Testability,benchmark,benchmarked,36,"this PR gives 5-7% speedup on LIBS (benchmarked using ExampleLocusWalker). it's done by not repeating calls to isPaired and isMapped . Note: there's still good speedup potential in `PileupElement.<init>` because almost all the time spent there is in the parameter checks (which are always passing when the ctor is called from the AlignmentStateMachine. @droazen please review - if the general idea seems fine by you, I'll complete the documentation for new default methods on GATKRead and cleanup a bit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2032
https://github.com/broadinstitute/gatk/issues/2033:474,Availability,error,error,474,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033
https://github.com/broadinstitute/gatk/issues/2033:103,Deployability,integrat,integration,103,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033
https://github.com/broadinstitute/gatk/issues/2033:128,Deployability,Integrat,IntegrationTestSpec,128,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033
https://github.com/broadinstitute/gatk/issues/2033:1221,Deployability,integrat,integrations,1221,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033
https://github.com/broadinstitute/gatk/issues/2033:103,Integrability,integrat,integration,103,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033
https://github.com/broadinstitute/gatk/issues/2033:128,Integrability,Integrat,IntegrationTestSpec,128,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033
https://github.com/broadinstitute/gatk/issues/2033:1221,Integrability,integrat,integrations,1221,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033
https://github.com/broadinstitute/gatk/issues/2033:257,Modifiability,extend,extending,257,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033
https://github.com/broadinstitute/gatk/issues/2033:769,Modifiability,extend,extending,769,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033
https://github.com/broadinstitute/gatk/issues/2033:1097,Modifiability,extend,extending,1097,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033
https://github.com/broadinstitute/gatk/issues/2033:95,Performance,perform,perform,95,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033
https://github.com/broadinstitute/gatk/issues/2033:395,Performance,load,loading,395,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033
https://github.com/broadinstitute/gatk/issues/2033:115,Testability,test,tests,115,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033
https://github.com/broadinstitute/gatk/issues/2033:349,Testability,test,test,349,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033
https://github.com/broadinstitute/gatk/issues/2033:1199,Testability,test,test,1199,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033
https://github.com/broadinstitute/gatk/issues/2033:1234,Testability,test,tests,1234,"I'm using GATK4 as a framework to implement my own tools, and it will be nice to have a way of perform integration tests using `IntegrationTestSpec`. Nevertheless, it requires the extension of the `CommandLineProgramTest` to run the command, and thus it is extending `BaseTest`. The issues that this infrastructure generates when trying to use this test classes are the following:; - `BaseTest` loading of `GenomeLocParser` is annotated with `@BeforeClass`, which throws an error because the reference genome (hg19MiniReference) is not present in the repository.; - `CommandLineProgramTest` is using `org.broadinstitute.hellbender.Main` for running the commands, but for custom tools the instanceMain with a different list of packages. Although this could be solved by extending the class by another abstract class. I propose (and I can implemented if you agree) the following:; - `CommandLineProgramTest` not implementing `BaseTest`.; - `CommandLineProgramTest` as a real abstract class without implementations of `getTestDataDir()` or `runCommandLine()`; - Abstract `GATKCommandLineProgramTest` extending both `CommandLineProgramTest` and `BaseTest`, sited in `org.broadinstitute.hellbender.utils.test` and used in all integrations tests in this repository and the protected repository.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2033
https://github.com/broadinstitute/gatk/pull/2035:79,Deployability,update,update,79,no reason to depend on spark when we depend on spark ML lib anyway. Simpler to update 1 dependency than two. @lbergelson wdyt?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2035
https://github.com/broadinstitute/gatk/pull/2035:13,Integrability,depend,depend,13,no reason to depend on spark when we depend on spark ML lib anyway. Simpler to update 1 dependency than two. @lbergelson wdyt?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2035
https://github.com/broadinstitute/gatk/pull/2035:37,Integrability,depend,depend,37,no reason to depend on spark when we depend on spark ML lib anyway. Simpler to update 1 dependency than two. @lbergelson wdyt?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2035
https://github.com/broadinstitute/gatk/pull/2035:88,Integrability,depend,dependency,88,no reason to depend on spark when we depend on spark ML lib anyway. Simpler to update 1 dependency than two. @lbergelson wdyt?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2035
https://github.com/broadinstitute/gatk/pull/2035:68,Usability,Simpl,Simpler,68,no reason to depend on spark when we depend on spark ML lib anyway. Simpler to update 1 dependency than two. @lbergelson wdyt?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2035
https://github.com/broadinstitute/gatk/issues/2038:71,Testability,test,test,71,checkout the ak_bwa_nothing branch; run `./gatk-launch BwaSpark -I src/test/resources/org/broadinstitute/hellbender/tools/spark/bwa/BwaSpark/testBWA.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta -O foo.bam`. see foo.bam - it only has a header. I expected 10 reads,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2038
https://github.com/broadinstitute/gatk/issues/2038:141,Testability,test,testBWA,141,checkout the ak_bwa_nothing branch; run `./gatk-launch BwaSpark -I src/test/resources/org/broadinstitute/hellbender/tools/spark/bwa/BwaSpark/testBWA.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta -O foo.bam`. see foo.bam - it only has a header. I expected 10 reads,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2038
https://github.com/broadinstitute/gatk/issues/2038:160,Testability,test,test,160,checkout the ak_bwa_nothing branch; run `./gatk-launch BwaSpark -I src/test/resources/org/broadinstitute/hellbender/tools/spark/bwa/BwaSpark/testBWA.bam -R src/test/resources/large/human_g1k_v37.20.21.fasta -O foo.bam`. see foo.bam - it only has a header. I expected 10 reads,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2038
https://github.com/broadinstitute/gatk/issues/2039:63,Availability,failure,failure,63,"```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): ; java.lang.IllegalArgumentException: String tag does not have length() == 2:. at htsjdk.samtools.SAMTagUtil.makeBinaryTag(SAMTagUtil.java:100); at htsjdk.samtools.SAMRecord.setAttribute(SAMRecord.java:1364); at htsjdk.samtools.SAMLineParser.parseTag(SAMLineParser.java:436); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:346); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:213); at org.broadinstitute.hellbender.tools.spark.bwa.BwaSparkEngine.lambda$alignWithBWA$464b6154$1(BwaSparkEngine.java:75); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:42); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:261); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:259); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2039
https://github.com/broadinstitute/gatk/issues/2039:120,Availability,failure,failure,120,"```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): ; java.lang.IllegalArgumentException: String tag does not have length() == 2:. at htsjdk.samtools.SAMTagUtil.makeBinaryTag(SAMTagUtil.java:100); at htsjdk.samtools.SAMRecord.setAttribute(SAMRecord.java:1364); at htsjdk.samtools.SAMLineParser.parseTag(SAMLineParser.java:436); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:346); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:213); at org.broadinstitute.hellbender.tools.spark.bwa.BwaSparkEngine.lambda$alignWithBWA$464b6154$1(BwaSparkEngine.java:75); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:42); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:261); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:259); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2039
https://github.com/broadinstitute/gatk/issues/2039:1737,Energy Efficiency,schedul,scheduler,1737,st): ; java.lang.IllegalArgumentException: String tag does not have length() == 2:. at htsjdk.samtools.SAMTagUtil.makeBinaryTag(SAMTagUtil.java:100); at htsjdk.samtools.SAMRecord.setAttribute(SAMRecord.java:1364); at htsjdk.samtools.SAMLineParser.parseTag(SAMLineParser.java:436); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:346); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:213); at org.broadinstitute.hellbender.tools.spark.bwa.BwaSparkEngine.lambda$alignWithBWA$464b6154$1(BwaSparkEngine.java:75); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:42); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:261); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:259); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```. I'm trying to isolate an offending read,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2039
https://github.com/broadinstitute/gatk/issues/2039:1808,Energy Efficiency,schedul,scheduler,1808,st): ; java.lang.IllegalArgumentException: String tag does not have length() == 2:. at htsjdk.samtools.SAMTagUtil.makeBinaryTag(SAMTagUtil.java:100); at htsjdk.samtools.SAMRecord.setAttribute(SAMRecord.java:1364); at htsjdk.samtools.SAMLineParser.parseTag(SAMLineParser.java:436); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:346); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:213); at org.broadinstitute.hellbender.tools.spark.bwa.BwaSparkEngine.lambda$alignWithBWA$464b6154$1(BwaSparkEngine.java:75); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:42); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:261); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:259); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```. I'm trying to isolate an offending read,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2039
https://github.com/broadinstitute/gatk/issues/2039:1930,Performance,concurren,concurrent,1930,st): ; java.lang.IllegalArgumentException: String tag does not have length() == 2:. at htsjdk.samtools.SAMTagUtil.makeBinaryTag(SAMTagUtil.java:100); at htsjdk.samtools.SAMRecord.setAttribute(SAMRecord.java:1364); at htsjdk.samtools.SAMLineParser.parseTag(SAMLineParser.java:436); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:346); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:213); at org.broadinstitute.hellbender.tools.spark.bwa.BwaSparkEngine.lambda$alignWithBWA$464b6154$1(BwaSparkEngine.java:75); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:42); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:261); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:259); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```. I'm trying to isolate an offending read,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2039
https://github.com/broadinstitute/gatk/issues/2039:2014,Performance,concurren,concurrent,2014,st): ; java.lang.IllegalArgumentException: String tag does not have length() == 2:. at htsjdk.samtools.SAMTagUtil.makeBinaryTag(SAMTagUtil.java:100); at htsjdk.samtools.SAMRecord.setAttribute(SAMRecord.java:1364); at htsjdk.samtools.SAMLineParser.parseTag(SAMLineParser.java:436); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:346); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:213); at org.broadinstitute.hellbender.tools.spark.bwa.BwaSparkEngine.lambda$alignWithBWA$464b6154$1(BwaSparkEngine.java:75); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:42); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:261); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:259); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```. I'm trying to isolate an offending read,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2039
https://github.com/broadinstitute/gatk/issues/2039:42,Safety,abort,aborted,42,"```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): ; java.lang.IllegalArgumentException: String tag does not have length() == 2:. at htsjdk.samtools.SAMTagUtil.makeBinaryTag(SAMTagUtil.java:100); at htsjdk.samtools.SAMRecord.setAttribute(SAMRecord.java:1364); at htsjdk.samtools.SAMLineParser.parseTag(SAMLineParser.java:436); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:346); at htsjdk.samtools.SAMLineParser.parseLine(SAMLineParser.java:213); at org.broadinstitute.hellbender.tools.spark.bwa.BwaSparkEngine.lambda$alignWithBWA$464b6154$1(BwaSparkEngine.java:75); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1015); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:42); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:261); at org.apache.spark.RangePartitioner$$anonfun$9.apply(Partitioner.scala:259); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$22.apply(RDD.scala:745); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2039
https://github.com/broadinstitute/gatk/pull/2041:553,Availability,down,downsample,553,"As described in #2034, samtools mpileup and the internal pileup generated by `LocusIteratorByState` (LIBS) is not providing the same result because overlapping read-pairs are not taken into account. In this PR I addressed this issue using the same approach as samtools to combine qualities, including new functionality to LIBS and `LocusWalker`:; - Refactoring constructors for LIBS, solving #1879.; - Including an option for ignore overlapping read-pairs in LIBS; - Including command line option in `LocusWalker`to ignore overlapping read-pairs and to downsample with a maximum coverage by sample.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2041
https://github.com/broadinstitute/gatk/pull/2041:349,Modifiability,Refactor,Refactoring,349,"As described in #2034, samtools mpileup and the internal pileup generated by `LocusIteratorByState` (LIBS) is not providing the same result because overlapping read-pairs are not taken into account. In this PR I addressed this issue using the same approach as samtools to combine qualities, including new functionality to LIBS and `LocusWalker`:; - Refactoring constructors for LIBS, solving #1879.; - Including an option for ignore overlapping read-pairs in LIBS; - Including command line option in `LocusWalker`to ignore overlapping read-pairs and to downsample with a maximum coverage by sample.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2041
https://github.com/broadinstitute/gatk/issues/2044:1452,Availability,down,download,1452,"### Background: jar dependency hell. Getting GATK to compile with the gcloud-java-nio dependency is already a little bit of a struggle because of conflicting versions of the com.google.protobuf class (see #2013). Running it on Dataproc results in a version conflict somewhere else, probably linked to the jars that are automatically included on Dataproc. Getting gcloud-java-nio to work on vanilla Spark runs into the exact same category of problems (dependency conflicts), see eg. [this question on StackOverflow](http://stackoverflow.com/questions/38536004/spark-java-noclassdeffounderror-when-adding-dependency/38575957#38575957). The root of the problem is that both Hadoop and GCloud-Java rely on the same core Google libraries (Guava, Protobuf), but they use different versions that are incompatible. Java is not currently able to have two different versions of the same library at once, so instead you end up forcing one version of the other. Neither version appears to work for all of our dependencies, so we have a problem.; ### Workaround. Barring improvements in Java, our only way forward may be to use a shaded ""fat jar"" version of gcloud-java-nio that (i) includes all of its dependencies in a single jar and (ii) renames the shared dependencies so they won't conflict with what eg. Hadoop brings in. Luckily, gcloud-java-nio already builds such a jar by default. However that jar is not published on Maven Central. So the solution is to download the gcloud-java-nio source code, compile it ourselves (without any modification), and use the resulting jar (target/gcloud-java-nio-0.2.7-SNAPSHOT-shaded.jar). . This solution works for both vanilla Spark and GATK. I suspect it would be acceptable for us to directly include that jar in our source tree so that our own users wouldn't have to take any special steps to use GATK: just compile and run, as usual. Let's discuss. CC: @droazen @lbergelson",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2044
https://github.com/broadinstitute/gatk/issues/2044:20,Integrability,depend,dependency,20,"### Background: jar dependency hell. Getting GATK to compile with the gcloud-java-nio dependency is already a little bit of a struggle because of conflicting versions of the com.google.protobuf class (see #2013). Running it on Dataproc results in a version conflict somewhere else, probably linked to the jars that are automatically included on Dataproc. Getting gcloud-java-nio to work on vanilla Spark runs into the exact same category of problems (dependency conflicts), see eg. [this question on StackOverflow](http://stackoverflow.com/questions/38536004/spark-java-noclassdeffounderror-when-adding-dependency/38575957#38575957). The root of the problem is that both Hadoop and GCloud-Java rely on the same core Google libraries (Guava, Protobuf), but they use different versions that are incompatible. Java is not currently able to have two different versions of the same library at once, so instead you end up forcing one version of the other. Neither version appears to work for all of our dependencies, so we have a problem.; ### Workaround. Barring improvements in Java, our only way forward may be to use a shaded ""fat jar"" version of gcloud-java-nio that (i) includes all of its dependencies in a single jar and (ii) renames the shared dependencies so they won't conflict with what eg. Hadoop brings in. Luckily, gcloud-java-nio already builds such a jar by default. However that jar is not published on Maven Central. So the solution is to download the gcloud-java-nio source code, compile it ourselves (without any modification), and use the resulting jar (target/gcloud-java-nio-0.2.7-SNAPSHOT-shaded.jar). . This solution works for both vanilla Spark and GATK. I suspect it would be acceptable for us to directly include that jar in our source tree so that our own users wouldn't have to take any special steps to use GATK: just compile and run, as usual. Let's discuss. CC: @droazen @lbergelson",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2044
https://github.com/broadinstitute/gatk/issues/2044:86,Integrability,depend,dependency,86,"### Background: jar dependency hell. Getting GATK to compile with the gcloud-java-nio dependency is already a little bit of a struggle because of conflicting versions of the com.google.protobuf class (see #2013). Running it on Dataproc results in a version conflict somewhere else, probably linked to the jars that are automatically included on Dataproc. Getting gcloud-java-nio to work on vanilla Spark runs into the exact same category of problems (dependency conflicts), see eg. [this question on StackOverflow](http://stackoverflow.com/questions/38536004/spark-java-noclassdeffounderror-when-adding-dependency/38575957#38575957). The root of the problem is that both Hadoop and GCloud-Java rely on the same core Google libraries (Guava, Protobuf), but they use different versions that are incompatible. Java is not currently able to have two different versions of the same library at once, so instead you end up forcing one version of the other. Neither version appears to work for all of our dependencies, so we have a problem.; ### Workaround. Barring improvements in Java, our only way forward may be to use a shaded ""fat jar"" version of gcloud-java-nio that (i) includes all of its dependencies in a single jar and (ii) renames the shared dependencies so they won't conflict with what eg. Hadoop brings in. Luckily, gcloud-java-nio already builds such a jar by default. However that jar is not published on Maven Central. So the solution is to download the gcloud-java-nio source code, compile it ourselves (without any modification), and use the resulting jar (target/gcloud-java-nio-0.2.7-SNAPSHOT-shaded.jar). . This solution works for both vanilla Spark and GATK. I suspect it would be acceptable for us to directly include that jar in our source tree so that our own users wouldn't have to take any special steps to use GATK: just compile and run, as usual. Let's discuss. CC: @droazen @lbergelson",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2044
https://github.com/broadinstitute/gatk/issues/2044:451,Integrability,depend,dependency,451,"### Background: jar dependency hell. Getting GATK to compile with the gcloud-java-nio dependency is already a little bit of a struggle because of conflicting versions of the com.google.protobuf class (see #2013). Running it on Dataproc results in a version conflict somewhere else, probably linked to the jars that are automatically included on Dataproc. Getting gcloud-java-nio to work on vanilla Spark runs into the exact same category of problems (dependency conflicts), see eg. [this question on StackOverflow](http://stackoverflow.com/questions/38536004/spark-java-noclassdeffounderror-when-adding-dependency/38575957#38575957). The root of the problem is that both Hadoop and GCloud-Java rely on the same core Google libraries (Guava, Protobuf), but they use different versions that are incompatible. Java is not currently able to have two different versions of the same library at once, so instead you end up forcing one version of the other. Neither version appears to work for all of our dependencies, so we have a problem.; ### Workaround. Barring improvements in Java, our only way forward may be to use a shaded ""fat jar"" version of gcloud-java-nio that (i) includes all of its dependencies in a single jar and (ii) renames the shared dependencies so they won't conflict with what eg. Hadoop brings in. Luckily, gcloud-java-nio already builds such a jar by default. However that jar is not published on Maven Central. So the solution is to download the gcloud-java-nio source code, compile it ourselves (without any modification), and use the resulting jar (target/gcloud-java-nio-0.2.7-SNAPSHOT-shaded.jar). . This solution works for both vanilla Spark and GATK. I suspect it would be acceptable for us to directly include that jar in our source tree so that our own users wouldn't have to take any special steps to use GATK: just compile and run, as usual. Let's discuss. CC: @droazen @lbergelson",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2044
https://github.com/broadinstitute/gatk/issues/2044:603,Integrability,depend,dependency,603,"### Background: jar dependency hell. Getting GATK to compile with the gcloud-java-nio dependency is already a little bit of a struggle because of conflicting versions of the com.google.protobuf class (see #2013). Running it on Dataproc results in a version conflict somewhere else, probably linked to the jars that are automatically included on Dataproc. Getting gcloud-java-nio to work on vanilla Spark runs into the exact same category of problems (dependency conflicts), see eg. [this question on StackOverflow](http://stackoverflow.com/questions/38536004/spark-java-noclassdeffounderror-when-adding-dependency/38575957#38575957). The root of the problem is that both Hadoop and GCloud-Java rely on the same core Google libraries (Guava, Protobuf), but they use different versions that are incompatible. Java is not currently able to have two different versions of the same library at once, so instead you end up forcing one version of the other. Neither version appears to work for all of our dependencies, so we have a problem.; ### Workaround. Barring improvements in Java, our only way forward may be to use a shaded ""fat jar"" version of gcloud-java-nio that (i) includes all of its dependencies in a single jar and (ii) renames the shared dependencies so they won't conflict with what eg. Hadoop brings in. Luckily, gcloud-java-nio already builds such a jar by default. However that jar is not published on Maven Central. So the solution is to download the gcloud-java-nio source code, compile it ourselves (without any modification), and use the resulting jar (target/gcloud-java-nio-0.2.7-SNAPSHOT-shaded.jar). . This solution works for both vanilla Spark and GATK. I suspect it would be acceptable for us to directly include that jar in our source tree so that our own users wouldn't have to take any special steps to use GATK: just compile and run, as usual. Let's discuss. CC: @droazen @lbergelson",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2044
https://github.com/broadinstitute/gatk/issues/2044:997,Integrability,depend,dependencies,997,"### Background: jar dependency hell. Getting GATK to compile with the gcloud-java-nio dependency is already a little bit of a struggle because of conflicting versions of the com.google.protobuf class (see #2013). Running it on Dataproc results in a version conflict somewhere else, probably linked to the jars that are automatically included on Dataproc. Getting gcloud-java-nio to work on vanilla Spark runs into the exact same category of problems (dependency conflicts), see eg. [this question on StackOverflow](http://stackoverflow.com/questions/38536004/spark-java-noclassdeffounderror-when-adding-dependency/38575957#38575957). The root of the problem is that both Hadoop and GCloud-Java rely on the same core Google libraries (Guava, Protobuf), but they use different versions that are incompatible. Java is not currently able to have two different versions of the same library at once, so instead you end up forcing one version of the other. Neither version appears to work for all of our dependencies, so we have a problem.; ### Workaround. Barring improvements in Java, our only way forward may be to use a shaded ""fat jar"" version of gcloud-java-nio that (i) includes all of its dependencies in a single jar and (ii) renames the shared dependencies so they won't conflict with what eg. Hadoop brings in. Luckily, gcloud-java-nio already builds such a jar by default. However that jar is not published on Maven Central. So the solution is to download the gcloud-java-nio source code, compile it ourselves (without any modification), and use the resulting jar (target/gcloud-java-nio-0.2.7-SNAPSHOT-shaded.jar). . This solution works for both vanilla Spark and GATK. I suspect it would be acceptable for us to directly include that jar in our source tree so that our own users wouldn't have to take any special steps to use GATK: just compile and run, as usual. Let's discuss. CC: @droazen @lbergelson",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2044
https://github.com/broadinstitute/gatk/issues/2044:1190,Integrability,depend,dependencies,1190,"### Background: jar dependency hell. Getting GATK to compile with the gcloud-java-nio dependency is already a little bit of a struggle because of conflicting versions of the com.google.protobuf class (see #2013). Running it on Dataproc results in a version conflict somewhere else, probably linked to the jars that are automatically included on Dataproc. Getting gcloud-java-nio to work on vanilla Spark runs into the exact same category of problems (dependency conflicts), see eg. [this question on StackOverflow](http://stackoverflow.com/questions/38536004/spark-java-noclassdeffounderror-when-adding-dependency/38575957#38575957). The root of the problem is that both Hadoop and GCloud-Java rely on the same core Google libraries (Guava, Protobuf), but they use different versions that are incompatible. Java is not currently able to have two different versions of the same library at once, so instead you end up forcing one version of the other. Neither version appears to work for all of our dependencies, so we have a problem.; ### Workaround. Barring improvements in Java, our only way forward may be to use a shaded ""fat jar"" version of gcloud-java-nio that (i) includes all of its dependencies in a single jar and (ii) renames the shared dependencies so they won't conflict with what eg. Hadoop brings in. Luckily, gcloud-java-nio already builds such a jar by default. However that jar is not published on Maven Central. So the solution is to download the gcloud-java-nio source code, compile it ourselves (without any modification), and use the resulting jar (target/gcloud-java-nio-0.2.7-SNAPSHOT-shaded.jar). . This solution works for both vanilla Spark and GATK. I suspect it would be acceptable for us to directly include that jar in our source tree so that our own users wouldn't have to take any special steps to use GATK: just compile and run, as usual. Let's discuss. CC: @droazen @lbergelson",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2044
https://github.com/broadinstitute/gatk/issues/2044:1247,Integrability,depend,dependencies,1247,"### Background: jar dependency hell. Getting GATK to compile with the gcloud-java-nio dependency is already a little bit of a struggle because of conflicting versions of the com.google.protobuf class (see #2013). Running it on Dataproc results in a version conflict somewhere else, probably linked to the jars that are automatically included on Dataproc. Getting gcloud-java-nio to work on vanilla Spark runs into the exact same category of problems (dependency conflicts), see eg. [this question on StackOverflow](http://stackoverflow.com/questions/38536004/spark-java-noclassdeffounderror-when-adding-dependency/38575957#38575957). The root of the problem is that both Hadoop and GCloud-Java rely on the same core Google libraries (Guava, Protobuf), but they use different versions that are incompatible. Java is not currently able to have two different versions of the same library at once, so instead you end up forcing one version of the other. Neither version appears to work for all of our dependencies, so we have a problem.; ### Workaround. Barring improvements in Java, our only way forward may be to use a shaded ""fat jar"" version of gcloud-java-nio that (i) includes all of its dependencies in a single jar and (ii) renames the shared dependencies so they won't conflict with what eg. Hadoop brings in. Luckily, gcloud-java-nio already builds such a jar by default. However that jar is not published on Maven Central. So the solution is to download the gcloud-java-nio source code, compile it ourselves (without any modification), and use the resulting jar (target/gcloud-java-nio-0.2.7-SNAPSHOT-shaded.jar). . This solution works for both vanilla Spark and GATK. I suspect it would be acceptable for us to directly include that jar in our source tree so that our own users wouldn't have to take any special steps to use GATK: just compile and run, as usual. Let's discuss. CC: @droazen @lbergelson",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2044
https://github.com/broadinstitute/gatk/pull/2049:213,Testability,test,tests,213,The first commit is included in another PR and should be reviewed/committed independently there. Only the second commit in this PR contains the example collectors. There are two examples included here with simple tests; a single level collector and a multi level collector.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2049
https://github.com/broadinstitute/gatk/pull/2049:206,Usability,simpl,simple,206,The first commit is included in another PR and should be reviewed/committed independently there. Only the second commit in this PR contains the example collectors. There are two examples included here with simple tests; a single level collector and a multi level collector.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2049
https://github.com/broadinstitute/gatk/issues/2050:1154,Performance,race condition,race condition,1154,"in `BwaSparkEngine` the method `alignWithBwa` is like this. ```; public JavaRDD<GATKRead> alignWithBWA(final JavaSparkContext ctx, final JavaRDD<GATKRead> unalignedReads, final SAMFileHeader readsHeader) {; //Note: SparkContext is not serializable so we don't store it in the engine and set this property here. Setting it multiple times is fine.; // ensure reads in a pair fall in the same partition (input split), so they are processed together; ctx.hadoopConfiguration().setBoolean(BAMInputFormat.KEEP_PAIRED_READS_TOGETHER_PROPERTY, true);. final JavaRDD<Tuple2<ShortRead, ShortRead>> shortReadPairs = convertToUnalignedReadPairs(unalignedReads);; final JavaRDD<String> samLines = align(shortReadPairs);; final SAMLineParser samLineParser = new SAMLineParser(new DefaultSAMRecordFactory(), ValidationStringency.SILENT, readsHeader, null, null);; final Broadcast<SAMLineParser> samLineParserBroadcast = ctx.broadcast(samLineParser);; return samLines.map(r -> new SAMRecordToGATKReadAdapter(samLineParserBroadcast.getValue().parseLine(r)));; }; ```. note that the parser is distributed by broadcast and thus shared by all tasks in an executor. That's a race condition because the parser is mutable (eg the `fields` field in the coded that gets mutated for each decode call). https://github.com/broadinstitute/gatk/issues/2039 may be caused by this bug",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2050
https://github.com/broadinstitute/gatk/issues/2050:793,Security,Validat,ValidationStringency,793,"in `BwaSparkEngine` the method `alignWithBwa` is like this. ```; public JavaRDD<GATKRead> alignWithBWA(final JavaSparkContext ctx, final JavaRDD<GATKRead> unalignedReads, final SAMFileHeader readsHeader) {; //Note: SparkContext is not serializable so we don't store it in the engine and set this property here. Setting it multiple times is fine.; // ensure reads in a pair fall in the same partition (input split), so they are processed together; ctx.hadoopConfiguration().setBoolean(BAMInputFormat.KEEP_PAIRED_READS_TOGETHER_PROPERTY, true);. final JavaRDD<Tuple2<ShortRead, ShortRead>> shortReadPairs = convertToUnalignedReadPairs(unalignedReads);; final JavaRDD<String> samLines = align(shortReadPairs);; final SAMLineParser samLineParser = new SAMLineParser(new DefaultSAMRecordFactory(), ValidationStringency.SILENT, readsHeader, null, null);; final Broadcast<SAMLineParser> samLineParserBroadcast = ctx.broadcast(samLineParser);; return samLines.map(r -> new SAMRecordToGATKReadAdapter(samLineParserBroadcast.getValue().parseLine(r)));; }; ```. note that the parser is distributed by broadcast and thus shared by all tasks in an executor. That's a race condition because the parser is mutable (eg the `fields` field in the coded that gets mutated for each decode call). https://github.com/broadinstitute/gatk/issues/2039 may be caused by this bug",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2050
https://github.com/broadinstitute/gatk/issues/2052:15,Performance,load,loading,15,A Java API for loading both vcfs stored as files (single and multi-sample) and streams of `VariantContext` objects into TileDB.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2052
https://github.com/broadinstitute/gatk/issues/2054:1276,Energy Efficiency,reduce,reduced,1276,"when you run BwaSpark, a lot of logging info is produced, like this:. ```; [M::mem_pestat] analyzing insert size distribution for orientation RR...; [M::mem_pestat] (25, 50, 75) percentile: (37, 57, 86); [M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 184); [M::mem_pestat] mean and std.dev: (63.93, 37.92); [M::mem_pestat] low and high boundaries for proper pairs: (1, 233); [M::mem_process_seqs] Processed 1316 reads in 1.312 CPU sec, 0.182 real sec; [M::mem_process_seqs] Processed 1316 reads in 1.365 CPU sec, 0.188 real sec; [M::mem_process_seqs] Processed 1316 reads in 1.479 CPU sec, 0.204 real sec; [M::mem_process_seqs] Processed 1316 reads in 1.721 CPU sec, 0.237 real sec; [M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (272, 0, 0, 329); [M::mem_pestat] analyzing insert size distribution for orientation FF...; [M::mem_pestat] (25, 50, 75) percentile: (34, 57, 81); [M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 175); [M::mem_pestat] mean and std.dev: (59.82, 33.44); [M::mem_pestat] low and high boundaries for proper pairs: (1, 222); [M::mem_pestat] skip orientation FR as there are not enough pairs; [M::mem_pestat] skip orientation RF as there are not enough pairs; ```. this should be reduced or removed or controllable somehow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2054
https://github.com/broadinstitute/gatk/issues/2054:32,Testability,log,logging,32,"when you run BwaSpark, a lot of logging info is produced, like this:. ```; [M::mem_pestat] analyzing insert size distribution for orientation RR...; [M::mem_pestat] (25, 50, 75) percentile: (37, 57, 86); [M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 184); [M::mem_pestat] mean and std.dev: (63.93, 37.92); [M::mem_pestat] low and high boundaries for proper pairs: (1, 233); [M::mem_process_seqs] Processed 1316 reads in 1.312 CPU sec, 0.182 real sec; [M::mem_process_seqs] Processed 1316 reads in 1.365 CPU sec, 0.188 real sec; [M::mem_process_seqs] Processed 1316 reads in 1.479 CPU sec, 0.204 real sec; [M::mem_process_seqs] Processed 1316 reads in 1.721 CPU sec, 0.237 real sec; [M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (272, 0, 0, 329); [M::mem_pestat] analyzing insert size distribution for orientation FF...; [M::mem_pestat] (25, 50, 75) percentile: (34, 57, 81); [M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 175); [M::mem_pestat] mean and std.dev: (59.82, 33.44); [M::mem_pestat] low and high boundaries for proper pairs: (1, 222); [M::mem_pestat] skip orientation FR as there are not enough pairs; [M::mem_pestat] skip orientation RF as there are not enough pairs; ```. this should be reduced or removed or controllable somehow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2054
https://github.com/broadinstitute/gatk/issues/2056:78,Safety,unsafe,unsafe,78,"`BCF2Codec` is out-of-date, and the way it's being used in TileDB is possibly unsafe as a result.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2056
https://github.com/broadinstitute/gatk/issues/2058:113,Performance,perform,performance,113,"some queries like `isPaired` or `isUnmapped` are called over and over on GATKReads and definitely have impact on performance, eg see #2032. It may make sense to cache those values - the risk is that reads are mutable and so caches need to be invalidated and that cached valued (even booleans) add overhead to memory footprint and shuffle footprint. . This ticket is to implement caching of those frequently called queries and measure performance impact.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2058
https://github.com/broadinstitute/gatk/issues/2058:161,Performance,cache,cache,161,"some queries like `isPaired` or `isUnmapped` are called over and over on GATKReads and definitely have impact on performance, eg see #2032. It may make sense to cache those values - the risk is that reads are mutable and so caches need to be invalidated and that cached valued (even booleans) add overhead to memory footprint and shuffle footprint. . This ticket is to implement caching of those frequently called queries and measure performance impact.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2058
https://github.com/broadinstitute/gatk/issues/2058:224,Performance,cache,caches,224,"some queries like `isPaired` or `isUnmapped` are called over and over on GATKReads and definitely have impact on performance, eg see #2032. It may make sense to cache those values - the risk is that reads are mutable and so caches need to be invalidated and that cached valued (even booleans) add overhead to memory footprint and shuffle footprint. . This ticket is to implement caching of those frequently called queries and measure performance impact.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2058
https://github.com/broadinstitute/gatk/issues/2058:263,Performance,cache,cached,263,"some queries like `isPaired` or `isUnmapped` are called over and over on GATKReads and definitely have impact on performance, eg see #2032. It may make sense to cache those values - the risk is that reads are mutable and so caches need to be invalidated and that cached valued (even booleans) add overhead to memory footprint and shuffle footprint. . This ticket is to implement caching of those frequently called queries and measure performance impact.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2058
https://github.com/broadinstitute/gatk/issues/2058:434,Performance,perform,performance,434,"some queries like `isPaired` or `isUnmapped` are called over and over on GATKReads and definitely have impact on performance, eg see #2032. It may make sense to cache those values - the risk is that reads are mutable and so caches need to be invalidated and that cached valued (even booleans) add overhead to memory footprint and shuffle footprint. . This ticket is to implement caching of those frequently called queries and measure performance impact.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2058
https://github.com/broadinstitute/gatk/issues/2058:186,Safety,risk,risk,186,"some queries like `isPaired` or `isUnmapped` are called over and over on GATKReads and definitely have impact on performance, eg see #2032. It may make sense to cache those values - the risk is that reads are mutable and so caches need to be invalidated and that cached valued (even booleans) add overhead to memory footprint and shuffle footprint. . This ticket is to implement caching of those frequently called queries and measure performance impact.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2058
https://github.com/broadinstitute/gatk/issues/2060:131,Performance,perform,performance,131,"A recent palantir investigation showed that disabling BAQ has only a very miniscule effect on variant calls, while providing a 77% performance boost. We should disable it by default in GATK4 (unless @vdauwera objects).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2060
https://github.com/broadinstitute/gatk/issues/2062:42,Security,validat,validating,42,This epic is to track work on porting and validating VQSR for alpha-3. Feel free to add related tickets to the epic.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2062
https://github.com/broadinstitute/gatk/pull/2067:31,Testability,log,logical,31,Needs to be rebased after read logical names are re-introduced.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2067
https://github.com/broadinstitute/gatk/issues/2072:88,Testability,test,testing,88,"Limited manual eyeballing seem to suggest it's worth a try, and go large scale for more testing.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2072
https://github.com/broadinstitute/gatk/issues/2073:166,Availability,error,error,166,"Last week, spark 2.0.0 is formally released. However, when I tested gatk4 on spark2.0.0, I found they were incompatible. It seems that the interface isn't match. The error log looks like below. Exception in thread ""main"" java.lang.NoSuchMethodError: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:35,Deployability,release,released,35,"Last week, spark 2.0.0 is formally released. However, when I tested gatk4 on spark2.0.0, I found they were incompatible. It seems that the interface isn't match. The error log looks like below. Exception in thread ""main"" java.lang.NoSuchMethodError: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:2874,Deployability,deploy,deploy,2874,.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/08/02 20:24:09 INFO util.ShutdownHookManager: Shutdown hook called; 16/08/02 20:24:09 INFO util.ShutdownHookManager: Deleting directory /ssd_hdfs2/spark_tmp/spark-42d0223f-b492-43e3-a6fa-d4edd98b2324,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:2911,Deployability,deploy,deploy,2911,.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/08/02 20:24:09 INFO util.ShutdownHookManager: Shutdown hook called; 16/08/02 20:24:09 INFO util.ShutdownHookManager: Deleting directory /ssd_hdfs2/spark_tmp/spark-42d0223f-b492-43e3-a6fa-d4edd98b2324,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:2983,Deployability,deploy,deploy,2983,.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/08/02 20:24:09 INFO util.ShutdownHookManager: Shutdown hook called; 16/08/02 20:24:09 INFO util.ShutdownHookManager: Deleting directory /ssd_hdfs2/spark_tmp/spark-42d0223f-b492-43e3-a6fa-d4edd98b2324,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:3059,Deployability,deploy,deploy,3059,.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/08/02 20:24:09 INFO util.ShutdownHookManager: Shutdown hook called; 16/08/02 20:24:09 INFO util.ShutdownHookManager: Deleting directory /ssd_hdfs2/spark_tmp/spark-42d0223f-b492-43e3-a6fa-d4edd98b2324,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:3130,Deployability,deploy,deploy,3130,.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/08/02 20:24:09 INFO util.ShutdownHookManager: Shutdown hook called; 16/08/02 20:24:09 INFO util.ShutdownHookManager: Deleting directory /ssd_hdfs2/spark_tmp/spark-42d0223f-b492-43e3-a6fa-d4edd98b2324,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:3199,Deployability,deploy,deploy,3199,.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/08/02 20:24:09 INFO util.ShutdownHookManager: Shutdown hook called; 16/08/02 20:24:09 INFO util.ShutdownHookManager: Deleting directory /ssd_hdfs2/spark_tmp/spark-42d0223f-b492-43e3-a6fa-d4edd98b2324,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:139,Integrability,interface,interface,139,"Last week, spark 2.0.0 is formally released. However, when I tested gatk4 on spark2.0.0, I found they were incompatible. It seems that the interface isn't match. The error log looks like below. Exception in thread ""main"" java.lang.NoSuchMethodError: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:1064,Security,Hash,HashMap,1064,"4 on spark2.0.0, I found they were incompatible. It seems that the interface isn't match. The error log looks like below. Exception in thread ""main"" java.lang.NoSuchMethodError: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:1089,Security,Hash,HashMap,1089,"I found they were incompatible. It seems that the interface isn't match. The error log looks like below. Exception in thread ""main"" java.lang.NoSuchMethodError: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:1139,Security,Hash,HashMap,1139,"isn't match. The error log looks like below. Exception in thread ""main"" java.lang.NoSuchMethodError: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:1147,Security,Hash,HashTrieMap,1147,"isn't match. The error log looks like below. Exception in thread ""main"" java.lang.NoSuchMethodError: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:1167,Security,Hash,HashMap,1167,"rror log looks like below. Exception in thread ""main"" java.lang.NoSuchMethodError: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.Co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:1217,Security,Hash,HashMap,1217,ang.NoSuchMethodError: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:1225,Security,Hash,HashTrieMap,1225,ang.NoSuchMethodError: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:1245,Security,Hash,HashMap,1245,ror: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:311); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:61,Testability,test,tested,61,"Last week, spark 2.0.0 is formally released. However, when I tested gatk4 on spark2.0.0, I found they were incompatible. It seems that the interface isn't match. The error log looks like below. Exception in thread ""main"" java.lang.NoSuchMethodError: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2073:172,Testability,log,log,172,"Last week, spark 2.0.0 is formally released. However, when I tested gatk4 on spark2.0.0, I found they were incompatible. It seems that the interface isn't match. The error log looks like below. Exception in thread ""main"" java.lang.NoSuchMethodError: scala.collection.Seq.aggregate(Ljava/lang/Object;Lscala/Function2;Lscala/Function2;)Ljava/lang/Object;; at org.bdgenomics.adam.models.NonoverlappingRegions.mergeRegions(NonoverlappingRegions.scala:75); at org.bdgenomics.adam.models.NonoverlappingRegions.<init>(NonoverlappingRegions.scala:55); at org.bdgenomics.adam.models.NonoverlappingRegions$.apply(NonoverlappingRegions.scala:169); at org.bdgenomics.adam.util.TwoBitRecord$.apply(TwoBitFile.scala:193); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at org.bdgenomics.adam.util.TwoBitFile$$anonfun$6.apply(TwoBitFile.scala:70); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at org.bdgenomics.adam.util.TwoBitFile.<init>(TwoBitFile.scala:70); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource.<init>(ReferenceTwoBitSource.java:43); at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:41); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:320); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2073
https://github.com/broadinstitute/gatk/issues/2075:213,Modifiability,refactor,refactored,213,As I discovered when making the fix in PR #2021 bam files will fail validation if overhang clipping is used when running SplitNCigarRead because the mate reference start position might be changed. The tool can be refactored to perform a second walker pass over the reads in order to identify locations where this will be a problem by checking for sites where the primary read gets clipped by OverhangClippingManager.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2075
https://github.com/broadinstitute/gatk/issues/2075:227,Performance,perform,perform,227,As I discovered when making the fix in PR #2021 bam files will fail validation if overhang clipping is used when running SplitNCigarRead because the mate reference start position might be changed. The tool can be refactored to perform a second walker pass over the reads in order to identify locations where this will be a problem by checking for sites where the primary read gets clipped by OverhangClippingManager.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2075
https://github.com/broadinstitute/gatk/issues/2075:68,Security,validat,validation,68,As I discovered when making the fix in PR #2021 bam files will fail validation if overhang clipping is used when running SplitNCigarRead because the mate reference start position might be changed. The tool can be refactored to perform a second walker pass over the reads in order to identify locations where this will be a problem by checking for sites where the primary read gets clipped by OverhangClippingManager.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2075
https://github.com/broadinstitute/gatk/issues/2076:127,Testability,test,test,127,We should be able to specify the version of htjskd to build against by setting a system property when building. i.e. ./gradlew test -Dhtsjdk.version=2.5.1.whatever.SNAPSHOT,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2076
https://github.com/broadinstitute/gatk/pull/2077:141,Testability,test,test,141,adding htsjdk.version and hadoopBam.version system properties to specify the versions on the command line when building. ex:. ```; ./gradlew test -Dhtsjdk.version=2.6.0 -DhadoopBam.version=7.5.0; ```. this will make testing against different htsjdk versions easier. fixes #2076,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2077
https://github.com/broadinstitute/gatk/pull/2077:216,Testability,test,testing,216,adding htsjdk.version and hadoopBam.version system properties to specify the versions on the command line when building. ex:. ```; ./gradlew test -Dhtsjdk.version=2.6.0 -DhadoopBam.version=7.5.0; ```. this will make testing against different htsjdk versions easier. fixes #2076,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2077
https://github.com/broadinstitute/gatk/pull/2078:69,Performance,load,loader,69,moving from jbwa 1.0.0 -> 1.0.0_ppc64; updating the BWANativeLibrary loader to be ppc aware,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2078
https://github.com/broadinstitute/gatk/issues/2081:407,Availability,error,error,407,"If a `VariantWalker` driving variant is indexed with tribble but does not have an sequence dictionary in the header, the dictionary is loaded from the index. Nevertheless, this is a truncated dictionary because the end coordinate for each chromosome is the last variant in that contig. Thus, even if a proper interval for the genome is provided (regarding the reference sequence), the program throw an user error exception. This could be reproduced with the following test in `ExampleVariantWalkerIntegrationTest`:. ``` java; @Test; public void testExampleVariantWalkerInvalidDictionary() throws IOException {; final IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -L 1:200-1125"" +; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -V "" + TEST_DATA_DIRECTORY + ""example_variants.vcf"" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"", Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleVariantWalkerIntegrationTest_output.txt""));; testSpec.executeTest(""testExampleVariantWalker_UndefinedContigLengthsInDictionary"", this);; }; ```. The thrown exceptions is the following:. ``` java; java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: A USER ERROR has occurred: Badly formed genome loc: Failed to parse Genome Location string: 1:200-1125; ```. This comes from the overrided method `VariantWalker.getBestAvailableSequenceDictionary()`, which prefers the one from the driving variant (in this case, the one which comes from the index), not using the one from the reference/reads if available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2081
https://github.com/broadinstitute/gatk/issues/2081:1297,Availability,ERROR,ERROR,1297,"If a `VariantWalker` driving variant is indexed with tribble but does not have an sequence dictionary in the header, the dictionary is loaded from the index. Nevertheless, this is a truncated dictionary because the end coordinate for each chromosome is the last variant in that contig. Thus, even if a proper interval for the genome is provided (regarding the reference sequence), the program throw an user error exception. This could be reproduced with the following test in `ExampleVariantWalkerIntegrationTest`:. ``` java; @Test; public void testExampleVariantWalkerInvalidDictionary() throws IOException {; final IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -L 1:200-1125"" +; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -V "" + TEST_DATA_DIRECTORY + ""example_variants.vcf"" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"", Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleVariantWalkerIntegrationTest_output.txt""));; testSpec.executeTest(""testExampleVariantWalker_UndefinedContigLengthsInDictionary"", this);; }; ```. The thrown exceptions is the following:. ``` java; java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: A USER ERROR has occurred: Badly formed genome loc: Failed to parse Genome Location string: 1:200-1125; ```. This comes from the overrided method `VariantWalker.getBestAvailableSequenceDictionary()`, which prefers the one from the driving variant (in this case, the one which comes from the index), not using the one from the reference/reads if available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2081
https://github.com/broadinstitute/gatk/issues/2081:1635,Availability,avail,available,1635,"If a `VariantWalker` driving variant is indexed with tribble but does not have an sequence dictionary in the header, the dictionary is loaded from the index. Nevertheless, this is a truncated dictionary because the end coordinate for each chromosome is the last variant in that contig. Thus, even if a proper interval for the genome is provided (regarding the reference sequence), the program throw an user error exception. This could be reproduced with the following test in `ExampleVariantWalkerIntegrationTest`:. ``` java; @Test; public void testExampleVariantWalkerInvalidDictionary() throws IOException {; final IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -L 1:200-1125"" +; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -V "" + TEST_DATA_DIRECTORY + ""example_variants.vcf"" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"", Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleVariantWalkerIntegrationTest_output.txt""));; testSpec.executeTest(""testExampleVariantWalker_UndefinedContigLengthsInDictionary"", this);; }; ```. The thrown exceptions is the following:. ``` java; java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: A USER ERROR has occurred: Badly formed genome loc: Failed to parse Genome Location string: 1:200-1125; ```. This comes from the overrided method `VariantWalker.getBestAvailableSequenceDictionary()`, which prefers the one from the driving variant (in this case, the one which comes from the index), not using the one from the reference/reads if available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2081
https://github.com/broadinstitute/gatk/issues/2081:617,Deployability,Integrat,IntegrationTestSpec,617,"If a `VariantWalker` driving variant is indexed with tribble but does not have an sequence dictionary in the header, the dictionary is loaded from the index. Nevertheless, this is a truncated dictionary because the end coordinate for each chromosome is the last variant in that contig. Thus, even if a proper interval for the genome is provided (regarding the reference sequence), the program throw an user error exception. This could be reproduced with the following test in `ExampleVariantWalkerIntegrationTest`:. ``` java; @Test; public void testExampleVariantWalkerInvalidDictionary() throws IOException {; final IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -L 1:200-1125"" +; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -V "" + TEST_DATA_DIRECTORY + ""example_variants.vcf"" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"", Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleVariantWalkerIntegrationTest_output.txt""));; testSpec.executeTest(""testExampleVariantWalker_UndefinedContigLengthsInDictionary"", this);; }; ```. The thrown exceptions is the following:. ``` java; java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: A USER ERROR has occurred: Badly formed genome loc: Failed to parse Genome Location string: 1:200-1125; ```. This comes from the overrided method `VariantWalker.getBestAvailableSequenceDictionary()`, which prefers the one from the driving variant (in this case, the one which comes from the index), not using the one from the reference/reads if available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2081
https://github.com/broadinstitute/gatk/issues/2081:652,Deployability,Integrat,IntegrationTestSpec,652,"If a `VariantWalker` driving variant is indexed with tribble but does not have an sequence dictionary in the header, the dictionary is loaded from the index. Nevertheless, this is a truncated dictionary because the end coordinate for each chromosome is the last variant in that contig. Thus, even if a proper interval for the genome is provided (regarding the reference sequence), the program throw an user error exception. This could be reproduced with the following test in `ExampleVariantWalkerIntegrationTest`:. ``` java; @Test; public void testExampleVariantWalkerInvalidDictionary() throws IOException {; final IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -L 1:200-1125"" +; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -V "" + TEST_DATA_DIRECTORY + ""example_variants.vcf"" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"", Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleVariantWalkerIntegrationTest_output.txt""));; testSpec.executeTest(""testExampleVariantWalker_UndefinedContigLengthsInDictionary"", this);; }; ```. The thrown exceptions is the following:. ``` java; java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: A USER ERROR has occurred: Badly formed genome loc: Failed to parse Genome Location string: 1:200-1125; ```. This comes from the overrided method `VariantWalker.getBestAvailableSequenceDictionary()`, which prefers the one from the driving variant (in this case, the one which comes from the index), not using the one from the reference/reads if available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2081
https://github.com/broadinstitute/gatk/issues/2081:617,Integrability,Integrat,IntegrationTestSpec,617,"If a `VariantWalker` driving variant is indexed with tribble but does not have an sequence dictionary in the header, the dictionary is loaded from the index. Nevertheless, this is a truncated dictionary because the end coordinate for each chromosome is the last variant in that contig. Thus, even if a proper interval for the genome is provided (regarding the reference sequence), the program throw an user error exception. This could be reproduced with the following test in `ExampleVariantWalkerIntegrationTest`:. ``` java; @Test; public void testExampleVariantWalkerInvalidDictionary() throws IOException {; final IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -L 1:200-1125"" +; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -V "" + TEST_DATA_DIRECTORY + ""example_variants.vcf"" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"", Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleVariantWalkerIntegrationTest_output.txt""));; testSpec.executeTest(""testExampleVariantWalker_UndefinedContigLengthsInDictionary"", this);; }; ```. The thrown exceptions is the following:. ``` java; java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: A USER ERROR has occurred: Badly formed genome loc: Failed to parse Genome Location string: 1:200-1125; ```. This comes from the overrided method `VariantWalker.getBestAvailableSequenceDictionary()`, which prefers the one from the driving variant (in this case, the one which comes from the index), not using the one from the reference/reads if available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2081
https://github.com/broadinstitute/gatk/issues/2081:652,Integrability,Integrat,IntegrationTestSpec,652,"If a `VariantWalker` driving variant is indexed with tribble but does not have an sequence dictionary in the header, the dictionary is loaded from the index. Nevertheless, this is a truncated dictionary because the end coordinate for each chromosome is the last variant in that contig. Thus, even if a proper interval for the genome is provided (regarding the reference sequence), the program throw an user error exception. This could be reproduced with the following test in `ExampleVariantWalkerIntegrationTest`:. ``` java; @Test; public void testExampleVariantWalkerInvalidDictionary() throws IOException {; final IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -L 1:200-1125"" +; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -V "" + TEST_DATA_DIRECTORY + ""example_variants.vcf"" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"", Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleVariantWalkerIntegrationTest_output.txt""));; testSpec.executeTest(""testExampleVariantWalker_UndefinedContigLengthsInDictionary"", this);; }; ```. The thrown exceptions is the following:. ``` java; java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: A USER ERROR has occurred: Badly formed genome loc: Failed to parse Genome Location string: 1:200-1125; ```. This comes from the overrided method `VariantWalker.getBestAvailableSequenceDictionary()`, which prefers the one from the driving variant (in this case, the one which comes from the index), not using the one from the reference/reads if available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2081
https://github.com/broadinstitute/gatk/issues/2081:135,Performance,load,loaded,135,"If a `VariantWalker` driving variant is indexed with tribble but does not have an sequence dictionary in the header, the dictionary is loaded from the index. Nevertheless, this is a truncated dictionary because the end coordinate for each chromosome is the last variant in that contig. Thus, even if a proper interval for the genome is provided (regarding the reference sequence), the program throw an user error exception. This could be reproduced with the following test in `ExampleVariantWalkerIntegrationTest`:. ``` java; @Test; public void testExampleVariantWalkerInvalidDictionary() throws IOException {; final IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -L 1:200-1125"" +; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -V "" + TEST_DATA_DIRECTORY + ""example_variants.vcf"" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"", Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleVariantWalkerIntegrationTest_output.txt""));; testSpec.executeTest(""testExampleVariantWalker_UndefinedContigLengthsInDictionary"", this);; }; ```. The thrown exceptions is the following:. ``` java; java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: A USER ERROR has occurred: Badly formed genome loc: Failed to parse Genome Location string: 1:200-1125; ```. This comes from the overrided method `VariantWalker.getBestAvailableSequenceDictionary()`, which prefers the one from the driving variant (in this case, the one which comes from the index), not using the one from the reference/reads if available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2081
https://github.com/broadinstitute/gatk/issues/2081:468,Testability,test,test,468,"If a `VariantWalker` driving variant is indexed with tribble but does not have an sequence dictionary in the header, the dictionary is loaded from the index. Nevertheless, this is a truncated dictionary because the end coordinate for each chromosome is the last variant in that contig. Thus, even if a proper interval for the genome is provided (regarding the reference sequence), the program throw an user error exception. This could be reproduced with the following test in `ExampleVariantWalkerIntegrationTest`:. ``` java; @Test; public void testExampleVariantWalkerInvalidDictionary() throws IOException {; final IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -L 1:200-1125"" +; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -V "" + TEST_DATA_DIRECTORY + ""example_variants.vcf"" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"", Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleVariantWalkerIntegrationTest_output.txt""));; testSpec.executeTest(""testExampleVariantWalker_UndefinedContigLengthsInDictionary"", this);; }; ```. The thrown exceptions is the following:. ``` java; java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: A USER ERROR has occurred: Badly formed genome loc: Failed to parse Genome Location string: 1:200-1125; ```. This comes from the overrided method `VariantWalker.getBestAvailableSequenceDictionary()`, which prefers the one from the driving variant (in this case, the one which comes from the index), not using the one from the reference/reads if available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2081
https://github.com/broadinstitute/gatk/issues/2081:527,Testability,Test,Test,527,"If a `VariantWalker` driving variant is indexed with tribble but does not have an sequence dictionary in the header, the dictionary is loaded from the index. Nevertheless, this is a truncated dictionary because the end coordinate for each chromosome is the last variant in that contig. Thus, even if a proper interval for the genome is provided (regarding the reference sequence), the program throw an user error exception. This could be reproduced with the following test in `ExampleVariantWalkerIntegrationTest`:. ``` java; @Test; public void testExampleVariantWalkerInvalidDictionary() throws IOException {; final IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -L 1:200-1125"" +; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -V "" + TEST_DATA_DIRECTORY + ""example_variants.vcf"" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"", Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleVariantWalkerIntegrationTest_output.txt""));; testSpec.executeTest(""testExampleVariantWalker_UndefinedContigLengthsInDictionary"", this);; }; ```. The thrown exceptions is the following:. ``` java; java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: A USER ERROR has occurred: Badly formed genome loc: Failed to parse Genome Location string: 1:200-1125; ```. This comes from the overrided method `VariantWalker.getBestAvailableSequenceDictionary()`, which prefers the one from the driving variant (in this case, the one which comes from the index), not using the one from the reference/reads if available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2081
https://github.com/broadinstitute/gatk/issues/2081:545,Testability,test,testExampleVariantWalkerInvalidDictionary,545,"If a `VariantWalker` driving variant is indexed with tribble but does not have an sequence dictionary in the header, the dictionary is loaded from the index. Nevertheless, this is a truncated dictionary because the end coordinate for each chromosome is the last variant in that contig. Thus, even if a proper interval for the genome is provided (regarding the reference sequence), the program throw an user error exception. This could be reproduced with the following test in `ExampleVariantWalkerIntegrationTest`:. ``` java; @Test; public void testExampleVariantWalkerInvalidDictionary() throws IOException {; final IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -L 1:200-1125"" +; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -V "" + TEST_DATA_DIRECTORY + ""example_variants.vcf"" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"", Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleVariantWalkerIntegrationTest_output.txt""));; testSpec.executeTest(""testExampleVariantWalker_UndefinedContigLengthsInDictionary"", this);; }; ```. The thrown exceptions is the following:. ``` java; java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: A USER ERROR has occurred: Badly formed genome loc: Failed to parse Genome Location string: 1:200-1125; ```. This comes from the overrided method `VariantWalker.getBestAvailableSequenceDictionary()`, which prefers the one from the driving variant (in this case, the one which comes from the index), not using the one from the reference/reads if available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2081
https://github.com/broadinstitute/gatk/issues/2081:637,Testability,test,testSpec,637,"If a `VariantWalker` driving variant is indexed with tribble but does not have an sequence dictionary in the header, the dictionary is loaded from the index. Nevertheless, this is a truncated dictionary because the end coordinate for each chromosome is the last variant in that contig. Thus, even if a proper interval for the genome is provided (regarding the reference sequence), the program throw an user error exception. This could be reproduced with the following test in `ExampleVariantWalkerIntegrationTest`:. ``` java; @Test; public void testExampleVariantWalkerInvalidDictionary() throws IOException {; final IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -L 1:200-1125"" +; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -V "" + TEST_DATA_DIRECTORY + ""example_variants.vcf"" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"", Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleVariantWalkerIntegrationTest_output.txt""));; testSpec.executeTest(""testExampleVariantWalker_UndefinedContigLengthsInDictionary"", this);; }; ```. The thrown exceptions is the following:. ``` java; java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: A USER ERROR has occurred: Badly formed genome loc: Failed to parse Genome Location string: 1:200-1125; ```. This comes from the overrided method `VariantWalker.getBestAvailableSequenceDictionary()`, which prefers the one from the driving variant (in this case, the one which comes from the index), not using the one from the reference/reads if available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2081
https://github.com/broadinstitute/gatk/issues/2081:1036,Testability,test,testSpec,1036,"If a `VariantWalker` driving variant is indexed with tribble but does not have an sequence dictionary in the header, the dictionary is loaded from the index. Nevertheless, this is a truncated dictionary because the end coordinate for each chromosome is the last variant in that contig. Thus, even if a proper interval for the genome is provided (regarding the reference sequence), the program throw an user error exception. This could be reproduced with the following test in `ExampleVariantWalkerIntegrationTest`:. ``` java; @Test; public void testExampleVariantWalkerInvalidDictionary() throws IOException {; final IntegrationTestSpec testSpec = new IntegrationTestSpec(; "" -L 1:200-1125"" +; "" -R "" + hg19MiniReference +; "" -I "" + TEST_DATA_DIRECTORY + ""reads_data_source_test1.bam"" +; "" -V "" + TEST_DATA_DIRECTORY + ""example_variants.vcf"" +; "" -auxiliaryVariants "" + TEST_DATA_DIRECTORY + ""feature_data_source_test.vcf"" +; "" -O %s"", Arrays.asList(TEST_OUTPUT_DIRECTORY + ""expected_ExampleVariantWalkerIntegrationTest_output.txt""));; testSpec.executeTest(""testExampleVariantWalker_UndefinedContigLengthsInDictionary"", this);; }; ```. The thrown exceptions is the following:. ``` java; java.lang.RuntimeException: org.broadinstitute.hellbender.exceptions.UserException$MalformedGenomeLoc: A USER ERROR has occurred: Badly formed genome loc: Failed to parse Genome Location string: 1:200-1125; ```. This comes from the overrided method `VariantWalker.getBestAvailableSequenceDictionary()`, which prefers the one from the driving variant (in this case, the one which comes from the index), not using the one from the reference/reads if available.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2081
https://github.com/broadinstitute/gatk/issues/2083:286,Availability,down,downloaded,286,`JavaSparkContext.addFile` seems to be a runtime version of --files. According to this [documentation](https://spark.apache.org/docs/1.6.0/api/java/org/apache/spark/SparkContext.html#addFile%28java.lang.String%29) it is used in concert with `SparkFIles.get()` on the node to lookup the downloaded file location. It may be possible to persist these files (and those passed with `--files`) by setting 'spark.yarn.preserve.staging.files true`. We should investigate incorporating calls to these methods as a way of sharing small-medium sized inputs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2083
https://github.com/broadinstitute/gatk/issues/2087:27,Integrability,interface,interface,27,"An initial very basic java interface to allow loading vcfs into GenomicsDB from GATK.; This is expected as a first step towards greater write support for genomics db. Could possibly require json files, but ideally could create them itself. one possible example interface:. ```; GenomicsDBLoader.loadSingleSampleGVCFs(URI workspace, String arrayname, File vidJson, List<File> vcfs); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2087
https://github.com/broadinstitute/gatk/issues/2087:261,Integrability,interface,interface,261,"An initial very basic java interface to allow loading vcfs into GenomicsDB from GATK.; This is expected as a first step towards greater write support for genomics db. Could possibly require json files, but ideally could create them itself. one possible example interface:. ```; GenomicsDBLoader.loadSingleSampleGVCFs(URI workspace, String arrayname, File vidJson, List<File> vcfs); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2087
https://github.com/broadinstitute/gatk/issues/2087:46,Performance,load,loading,46,"An initial very basic java interface to allow loading vcfs into GenomicsDB from GATK.; This is expected as a first step towards greater write support for genomics db. Could possibly require json files, but ideally could create them itself. one possible example interface:. ```; GenomicsDBLoader.loadSingleSampleGVCFs(URI workspace, String arrayname, File vidJson, List<File> vcfs); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2087
https://github.com/broadinstitute/gatk/issues/2087:295,Performance,load,loadSingleSampleGVCFs,295,"An initial very basic java interface to allow loading vcfs into GenomicsDB from GATK.; This is expected as a first step towards greater write support for genomics db. Could possibly require json files, but ideally could create them itself. one possible example interface:. ```; GenomicsDBLoader.loadSingleSampleGVCFs(URI workspace, String arrayname, File vidJson, List<File> vcfs); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2087
https://github.com/broadinstitute/gatk/issues/2089:218,Deployability,release,released,218,"Follow up to https://github.com/broadinstitute/gatk/pull/2010.; Remove the following code from `IntervalUtils. intervalFileToList()` when a new exome, correctly converted interval list (with no -1 length intervals) is released :. ```; if (interval.getStart() - interval.getEnd() == 1 ) { ; logger.warn(""Possible incorrectly converted length 1 interval : "" + interval);; }; ```. Remove corresponding test `IntervalUtilsUnitTest.testIntervalFileToListNegativeOneLength`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2089
https://github.com/broadinstitute/gatk/issues/2089:290,Testability,log,logger,290,"Follow up to https://github.com/broadinstitute/gatk/pull/2010.; Remove the following code from `IntervalUtils. intervalFileToList()` when a new exome, correctly converted interval list (with no -1 length intervals) is released :. ```; if (interval.getStart() - interval.getEnd() == 1 ) { ; logger.warn(""Possible incorrectly converted length 1 interval : "" + interval);; }; ```. Remove corresponding test `IntervalUtilsUnitTest.testIntervalFileToListNegativeOneLength`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2089
https://github.com/broadinstitute/gatk/issues/2089:399,Testability,test,test,399,"Follow up to https://github.com/broadinstitute/gatk/pull/2010.; Remove the following code from `IntervalUtils. intervalFileToList()` when a new exome, correctly converted interval list (with no -1 length intervals) is released :. ```; if (interval.getStart() - interval.getEnd() == 1 ) { ; logger.warn(""Possible incorrectly converted length 1 interval : "" + interval);; }; ```. Remove corresponding test `IntervalUtilsUnitTest.testIntervalFileToListNegativeOneLength`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2089
https://github.com/broadinstitute/gatk/issues/2089:427,Testability,test,testIntervalFileToListNegativeOneLength,427,"Follow up to https://github.com/broadinstitute/gatk/pull/2010.; Remove the following code from `IntervalUtils. intervalFileToList()` when a new exome, correctly converted interval list (with no -1 length intervals) is released :. ```; if (interval.getStart() - interval.getEnd() == 1 ) { ; logger.warn(""Possible incorrectly converted length 1 interval : "" + interval);; }; ```. Remove corresponding test `IntervalUtilsUnitTest.testIntervalFileToListNegativeOneLength`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2089
https://github.com/broadinstitute/gatk/pull/2090:253,Integrability,Wrap,Wrapper,253,"-Will now look for both shadow and spark jars in the same directory; as the gatk-launch script, and use them if found. Also checks; BIN_PATH for jars. -Environment variable overrides GATK_SHADOW_JAR and GATK_SPARK_JAR; take precedence over everything. -Wrapper script is used if found and GATK_SHADOW_JAR is not set. Resolves #1693",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2090
https://github.com/broadinstitute/gatk/pull/2090:164,Modifiability,variab,variable,164,"-Will now look for both shadow and spark jars in the same directory; as the gatk-launch script, and use them if found. Also checks; BIN_PATH for jars. -Environment variable overrides GATK_SHADOW_JAR and GATK_SPARK_JAR; take precedence over everything. -Wrapper script is used if found and GATK_SHADOW_JAR is not set. Resolves #1693",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2090
https://github.com/broadinstitute/gatk/pull/2091:99,Availability,avail,available,99,… over the VCF index if its better. Handle sequence interval validation when no sequence length is available. Fixes https://github.com/broadinstitute/gatk/issues/1999 and the downstream genomeLoc parser validation fallout.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2091
https://github.com/broadinstitute/gatk/pull/2091:175,Availability,down,downstream,175,… over the VCF index if its better. Handle sequence interval validation when no sequence length is available. Fixes https://github.com/broadinstitute/gatk/issues/1999 and the downstream genomeLoc parser validation fallout.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2091
https://github.com/broadinstitute/gatk/pull/2091:61,Security,validat,validation,61,… over the VCF index if its better. Handle sequence interval validation when no sequence length is available. Fixes https://github.com/broadinstitute/gatk/issues/1999 and the downstream genomeLoc parser validation fallout.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2091
https://github.com/broadinstitute/gatk/pull/2091:203,Security,validat,validation,203,… over the VCF index if its better. Handle sequence interval validation when no sequence length is available. Fixes https://github.com/broadinstitute/gatk/issues/1999 and the downstream genomeLoc parser validation fallout.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2091
https://github.com/broadinstitute/gatk/issues/2092:410,Deployability,install,installed,410,"htsjdk does not support the latest VCF/BCF specs, and it's starting to hurt us (see, eg., https://github.com/broadinstitute/gatk/issues/2056). Let's fix this. The changes to the spec from 4.2 -> 4.3 can be seen by cloning https://github.com/samtools/hts-specs and running:. ```; latexdiff VCFv4.2.tex VCFv4.3.tex > diff.tex; pdflatex diff.tex; ```. and then examining `diff.pdf` (note that you must have latex installed for this to work). . To build full pdfs of all the specs documents, run `make`. The major htsjdk classes involved are:. `VCFCodec` (handles VCF reading -- must be updated while retaining backwards compatibility with previous VCF 4.x versions). `VCFWriter` (handles VCF writing -- only needs to support writing the latest version of the spec). Note that `VCFCodec` shares a lot of code with `VCF3Codec` via the `AbstractVCFCodec` -- we may need to refactor this to better isolate the legacy v3 codec from the v4 codec. @cmnbroad has already started working on updating BCF support in the branch https://github.com/cmnbroad/htsjdk/tree/cn_bcf2. Before starting implementation, we should come up with an itemized summary of how we intend to deal with each change in the spec, and make sure we agree on the approach. This code is extremely performance-sensitive, so we need to trade off on performance vs. strict fidelity to the spec. For each spec change that requires a code change in htsjdk, we should be sure to add a good unit test. We should also add tests proving that support for older versions of the spec is not broken.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2092
https://github.com/broadinstitute/gatk/issues/2092:583,Deployability,update,updated,583,"htsjdk does not support the latest VCF/BCF specs, and it's starting to hurt us (see, eg., https://github.com/broadinstitute/gatk/issues/2056). Let's fix this. The changes to the spec from 4.2 -> 4.3 can be seen by cloning https://github.com/samtools/hts-specs and running:. ```; latexdiff VCFv4.2.tex VCFv4.3.tex > diff.tex; pdflatex diff.tex; ```. and then examining `diff.pdf` (note that you must have latex installed for this to work). . To build full pdfs of all the specs documents, run `make`. The major htsjdk classes involved are:. `VCFCodec` (handles VCF reading -- must be updated while retaining backwards compatibility with previous VCF 4.x versions). `VCFWriter` (handles VCF writing -- only needs to support writing the latest version of the spec). Note that `VCFCodec` shares a lot of code with `VCF3Codec` via the `AbstractVCFCodec` -- we may need to refactor this to better isolate the legacy v3 codec from the v4 codec. @cmnbroad has already started working on updating BCF support in the branch https://github.com/cmnbroad/htsjdk/tree/cn_bcf2. Before starting implementation, we should come up with an itemized summary of how we intend to deal with each change in the spec, and make sure we agree on the approach. This code is extremely performance-sensitive, so we need to trade off on performance vs. strict fidelity to the spec. For each spec change that requires a code change in htsjdk, we should be sure to add a good unit test. We should also add tests proving that support for older versions of the spec is not broken.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2092
https://github.com/broadinstitute/gatk/issues/2092:867,Modifiability,refactor,refactor,867,"htsjdk does not support the latest VCF/BCF specs, and it's starting to hurt us (see, eg., https://github.com/broadinstitute/gatk/issues/2056). Let's fix this. The changes to the spec from 4.2 -> 4.3 can be seen by cloning https://github.com/samtools/hts-specs and running:. ```; latexdiff VCFv4.2.tex VCFv4.3.tex > diff.tex; pdflatex diff.tex; ```. and then examining `diff.pdf` (note that you must have latex installed for this to work). . To build full pdfs of all the specs documents, run `make`. The major htsjdk classes involved are:. `VCFCodec` (handles VCF reading -- must be updated while retaining backwards compatibility with previous VCF 4.x versions). `VCFWriter` (handles VCF writing -- only needs to support writing the latest version of the spec). Note that `VCFCodec` shares a lot of code with `VCF3Codec` via the `AbstractVCFCodec` -- we may need to refactor this to better isolate the legacy v3 codec from the v4 codec. @cmnbroad has already started working on updating BCF support in the branch https://github.com/cmnbroad/htsjdk/tree/cn_bcf2. Before starting implementation, we should come up with an itemized summary of how we intend to deal with each change in the spec, and make sure we agree on the approach. This code is extremely performance-sensitive, so we need to trade off on performance vs. strict fidelity to the spec. For each spec change that requires a code change in htsjdk, we should be sure to add a good unit test. We should also add tests proving that support for older versions of the spec is not broken.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2092
